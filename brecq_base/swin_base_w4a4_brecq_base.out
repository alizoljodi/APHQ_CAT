Starting Swin-Base W4A4 BRECQ experiment at Mon Sep  8 03:57:32 PM CEST 2025
2025-09-08 15:57:35,544 - INFO - Starting multi-seed experiment
2025-09-08 15:57:35,544 - INFO - Architecture: swin_base
2025-09-08 15:57:35,545 - INFO - Weight bits: 4
2025-09-08 15:57:35,545 - INFO - Activation bits: 4
2025-09-08 15:57:35,545 - INFO - Seeds: [1001, 1002, 1003]
2025-09-08 15:57:35,545 - INFO - Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
2025-09-08 15:57:35,545 - INFO - Cluster numbers: [8, 16, 32, 64, 128, 256]
2025-09-08 15:57:35,545 - INFO - PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]
2025-09-08 15:57:35,545 - INFO - Output directory: ./experiment_results/swin_base_w4_a4_20250908_155735
2025-09-08 15:57:35,545 - INFO - Checking basic requirements...
2025-09-08 15:57:35,545 - INFO - Basic checks passed
2025-09-08 15:57:35,545 - INFO - 
Starting experiments for 3 seeds...
2025-09-08 15:57:35,545 - INFO - Total parameter combinations: 600
2025-09-08 15:57:35,545 - INFO - Total experiments: 1800
2025-09-08 15:57:35,545 - INFO - 
============================================================
2025-09-08 15:57:35,545 - INFO - Running experiment 1/3 for seed 1001
2025-09-08 15:57:35,546 - INFO - ============================================================
2025-09-08 15:57:35,546 - INFO - Running experiment for seed 1001
2025-09-08 15:57:35,546 - INFO - Command: /home/alz07xz/project/PD-Quant/pd_quant/bin/python ../test_quant.py --model swin_base --w_bit 4 --a_bit 4 --seed 1001 --config ../configs/4bit/brecq_baseline.py --dataset /home/alz07xz/imagenet --calib-size 1000 --calib-batch-size 32 --val-batch-size 500 --num-workers 8 --device cuda --alpha-list 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 --num-clusters-list 8 16 32 64 128 256 --pca-dim-list 1 25 50 75 100 125 150 175 200 220 --calibrate --optimize
2025-09-08 15:57:35,546 - INFO - Working directory: /home/alz07xz/project/APHQ_CAT/brecq_base
2025-09-08 16:06:09 - start the process.
Namespace(model='swin_base', config='../configs/4bit/brecq_baseline.py', dataset='/home/alz07xz/imagenet', val_batch_size=500, num_workers=8, device='cuda', reconstruct_mlp=False, load_reconstruct_checkpoint=False, test_reconstruct_checkpoint=False, calibrate=True, load_calibrate_checkpoint=None, test_calibrate_checkpoint=False, optimize=True, load_optimize_checkpoint='', test_optimize_checkpoint=False, print_freq=10, seed=1001, alpha_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], num_clusters_list=[8, 16, 32, 64, 128, 256], pca_dim_list=[1, 25, 50, 75, 100, 125, 150, 175, 200, 220], w_bit=4, a_bit=4, calib_size=1000, calib_batch_size=32)
deefe /home/alz07xz/project/APHQ_CAT/configs/4bit
Successfully imported Config class!
optim_size: 1024
calib_size: 1000
optim_batch_size: 32
calib_batch_size: 32
w_bit: 4
a_bit: 4
qconv_a_bit: 8
qhead_a_bit: 4
calib_metric: mse
matmul_head_channel_wise: True
token_channel_wise: False
eq_n: 128
search_round: 3
keep_gpu: True
optim_metric: mse
use_mean_hessian: False
temp: 20
recon_metric: hessian_perturb
pct: 0.99
optim_mode: rinp
drop_prob: 1.0
reconstruct_mlp: False
Building model ...
/home/alz07xz/project/PD-Quant/pd_quant/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
No checkpoint found at './checkpoint/vit_raw/swin_base_patch4_window7_224.bin'
Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)
[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Building validation dataloader ...
Validating on test set on fp ...
Test: [0/100]	Time 11.819 (11.819)	Loss 0.4076 (0.4076)	Prec@1 91.800 (91.800)	Prec@5 98.400 (98.400)
Test: [10/100]	Time 1.058 (2.193)	Loss 0.4707 (0.5107)	Prec@1 91.600 (88.745)	Prec@5 98.800 (98.491)
Test: [20/100]	Time 1.052 (1.648)	Loss 0.5991 (0.5373)	Prec@1 86.000 (88.381)	Prec@5 98.000 (98.171)
Test: [30/100]	Time 1.053 (1.456)	Loss 0.4928 (0.5636)	Prec@1 88.200 (87.555)	Prec@5 99.400 (98.129)
Test: [40/100]	Time 1.053 (1.358)	Loss 0.7451 (0.5610)	Prec@1 82.400 (87.663)	Prec@5 97.000 (98.185)
Test: [50/100]	Time 1.061 (1.300)	Loss 0.9181 (0.6040)	Prec@1 77.800 (86.451)	Prec@5 94.800 (97.808)
Test: [60/100]	Time 1.061 (1.261)	Loss 0.5948 (0.6094)	Prec@1 87.200 (86.338)	Prec@5 96.600 (97.764)
Test: [70/100]	Time 1.056 (1.244)	Loss 0.6936 (0.6248)	Prec@1 84.200 (85.859)	Prec@5 97.800 (97.668)
Test: [80/100]	Time 1.061 (1.231)	Loss 0.4770 (0.6272)	Prec@1 88.400 (85.780)	Prec@5 99.200 (97.602)
Test: [90/100]	Time 1.066 (1.213)	Loss 0.9203 (0.6428)	Prec@1 77.000 (85.305)	Prec@5 95.400 (97.525)
 * Prec@1 85.274 Prec@5 97.568 Loss 0.641 Time 120.081
Wraping quantiztion modules (reparam: False, recon: False) ...
2025-09-08 16:09:00 - start mse guided calibration
  0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   1%|          | 1/149 [00:14<36:06, 14.64s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|          | 1/149 [00:14<36:06, 14.64s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|▏         | 2/149 [01:33<2:08:40, 52.52s/it]calibrating layers.0.blocks.0.attn.proj:   1%|▏         | 2/149 [01:33<2:08:40, 52.52s/it]calibrating layers.0.blocks.0.attn.proj:   2%|▏         | 3/149 [02:13<1:53:43, 46.74s/it]calibrating layers.0.blocks.0.attn.matmul1:   2%|▏         | 3/149 [02:13<1:53:43, 46.74s/it]calibrating layers.0.blocks.0.attn.matmul1:   3%|▎         | 4/149 [07:38<6:17:56, 156.39s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 4/149 [07:38<6:17:56, 156.39s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 5/149 [13:05<8:43:26, 218.10s/it]calibrating layers.0.blocks.0.mlp.fc1:   3%|▎         | 5/149 [13:05<8:43:26, 218.10s/it]     calibrating layers.0.blocks.0.mlp.fc1:   4%|▍         | 6/149 [16:07<8:10:45, 205.91s/it]calibrating layers.0.blocks.0.mlp.fc2:   4%|▍         | 6/149 [16:07<8:10:45, 205.91s/it]calibrating layers.0.blocks.0.mlp.fc2:   5%|▍         | 7/149 [19:17<7:54:30, 200.50s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▍         | 7/149 [19:17<7:54:30, 200.50s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▌         | 8/149 [20:35<6:20:09, 161.77s/it]calibrating layers.0.blocks.1.attn.proj:   5%|▌         | 8/149 [20:35<6:20:09, 161.77s/it]calibrating layers.0.blocks.1.attn.proj:   6%|▌         | 9/149 [21:15<4:48:35, 123.68s/it]calibrating layers.0.blocks.1.attn.matmul1:   6%|▌         | 9/149 [21:15<4:48:35, 123.68s/it]calibrating layers.0.blocks.1.attn.matmul1:   7%|▋         | 10/149 [26:42<7:11:20, 186.19s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 10/149 [26:42<7:11:20, 186.19s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 11/149 [32:10<8:48:39, 229.85s/it]calibrating layers.0.blocks.1.mlp.fc1:   7%|▋         | 11/149 [32:10<8:48:39, 229.85s/it]     calibrating layers.0.blocks.1.mlp.fc1:   8%|▊         | 12/149 [35:13<8:11:55, 215.44s/it]calibrating layers.0.blocks.1.mlp.fc2:   8%|▊         | 12/149 [35:13<8:11:55, 215.44s/it]calibrating layers.0.blocks.1.mlp.fc2:   9%|▊         | 13/149 [38:22<7:50:22, 207.52s/it]calibrating layers.1.downsample.reduction:   9%|▊         | 13/149 [38:22<7:50:22, 207.52s/it]calibrating layers.1.downsample.reduction:   9%|▉         | 14/149 [39:04<5:54:16, 157.46s/it]calibrating layers.1.blocks.0.attn.qkv:   9%|▉         | 14/149 [39:04<5:54:16, 157.46s/it]   calibrating layers.1.blocks.0.attn.qkv:  10%|█         | 15/149 [39:50<4:36:29, 123.80s/it]calibrating layers.1.blocks.0.attn.proj:  10%|█         | 15/149 [39:50<4:36:29, 123.80s/it]calibrating layers.1.blocks.0.attn.proj:  11%|█         | 16/149 [40:13<3:27:03, 93.41s/it] calibrating layers.1.blocks.0.attn.matmul1:  11%|█         | 16/149 [40:13<3:27:03, 93.41s/it]calibrating layers.1.blocks.0.attn.matmul1:  11%|█▏        | 17/149 [40:51<2:48:48, 76.73s/it]calibrating layers.1.blocks.0.attn.matmul2:  11%|█▏        | 17/149 [40:51<2:48:48, 76.73s/it]calibrating layers.1.blocks.0.attn.matmul2:  12%|█▏        | 18/149 [41:45<2:32:37, 69.90s/it]calibrating layers.1.blocks.0.mlp.fc1:  12%|█▏        | 18/149 [41:45<2:32:37, 69.90s/it]     calibrating layers.1.blocks.0.mlp.fc1:  13%|█▎        | 19/149 [43:15<2:44:37, 75.98s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 19/149 [43:15<2:44:37, 75.98s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 20/149 [44:48<2:54:28, 81.15s/it]calibrating layers.1.blocks.1.attn.qkv:  13%|█▎        | 20/149 [44:48<2:54:28, 81.15s/it]calibrating layers.1.blocks.1.attn.qkv:  14%|█▍        | 21/149 [45:35<2:31:33, 71.04s/it]calibrating layers.1.blocks.1.attn.proj:  14%|█▍        | 21/149 [45:35<2:31:33, 71.04s/it]calibrating layers.1.blocks.1.attn.proj:  15%|█▍        | 22/149 [46:00<2:00:35, 56.97s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▍        | 22/149 [46:00<2:00:35, 56.97s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▌        | 23/149 [46:38<1:48:13, 51.53s/it]calibrating layers.1.blocks.1.attn.matmul2:  15%|█▌        | 23/149 [46:38<1:48:13, 51.53s/it]calibrating layers.1.blocks.1.attn.matmul2:  16%|█▌        | 24/149 [47:32<1:48:38, 52.15s/it]calibrating layers.1.blocks.1.mlp.fc1:  16%|█▌        | 24/149 [47:32<1:48:38, 52.15s/it]     calibrating layers.1.blocks.1.mlp.fc1:  17%|█▋        | 25/149 [49:03<2:11:39, 63.71s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 25/149 [49:03<2:11:39, 63.71s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 26/149 [50:37<2:29:09, 72.76s/it]calibrating layers.2.downsample.reduction:  17%|█▋        | 26/149 [50:37<2:29:09, 72.76s/it]calibrating layers.2.downsample.reduction:  18%|█▊        | 27/149 [51:02<1:59:12, 58.63s/it]calibrating layers.2.blocks.0.attn.qkv:  18%|█▊        | 27/149 [51:02<1:59:12, 58.63s/it]   calibrating layers.2.blocks.0.attn.qkv:  19%|█▉        | 28/149 [51:35<1:42:43, 50.94s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 28/149 [51:35<1:42:43, 50.94s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 29/149 [51:50<1:20:16, 40.14s/it]calibrating layers.2.blocks.0.attn.matmul1:  19%|█▉        | 29/149 [51:50<1:20:16, 40.14s/it]calibrating layers.2.blocks.0.attn.matmul1:  20%|██        | 30/149 [52:08<1:06:08, 33.35s/it]calibrating layers.2.blocks.0.attn.matmul2:  20%|██        | 30/149 [52:08<1:06:08, 33.35s/it]calibrating layers.2.blocks.0.attn.matmul2:  21%|██        | 31/149 [52:26<56:31, 28.74s/it]  calibrating layers.2.blocks.0.mlp.fc1:  21%|██        | 31/149 [52:26<56:31, 28.74s/it]     calibrating layers.2.blocks.0.mlp.fc1:  21%|██▏       | 32/149 [53:14<1:07:44, 34.74s/it]calibrating layers.2.blocks.0.mlp.fc2:  21%|██▏       | 32/149 [53:14<1:07:44, 34.74s/it]calibrating layers.2.blocks.0.mlp.fc2:  22%|██▏       | 33/149 [54:04<1:15:50, 39.23s/it]calibrating layers.2.blocks.1.attn.qkv:  22%|██▏       | 33/149 [54:04<1:15:50, 39.23s/it]calibrating layers.2.blocks.1.attn.qkv:  23%|██▎       | 34/149 [54:36<1:11:03, 37.07s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 34/149 [54:36<1:11:03, 37.07s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 35/149 [54:51<57:33, 30.30s/it]  calibrating layers.2.blocks.1.attn.matmul1:  23%|██▎       | 35/149 [54:51<57:33, 30.30s/it]calibrating layers.2.blocks.1.attn.matmul1:  24%|██▍       | 36/149 [55:08<49:51, 26.47s/it]calibrating layers.2.blocks.1.attn.matmul2:  24%|██▍       | 36/149 [55:08<49:51, 26.47s/it]calibrating layers.2.blocks.1.attn.matmul2:  25%|██▍       | 37/149 [55:26<44:43, 23.96s/it]calibrating layers.2.blocks.1.mlp.fc1:  25%|██▍       | 37/149 [55:26<44:43, 23.96s/it]     calibrating layers.2.blocks.1.mlp.fc1:  26%|██▌       | 38/149 [56:15<58:14, 31.48s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 38/149 [56:15<58:14, 31.48s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 39/149 [57:05<1:07:49, 37.00s/it]calibrating layers.2.blocks.2.attn.qkv:  26%|██▌       | 39/149 [57:05<1:07:49, 37.00s/it]calibrating layers.2.blocks.2.attn.qkv:  27%|██▋       | 40/149 [57:37<1:04:37, 35.58s/it]calibrating layers.2.blocks.2.attn.proj:  27%|██▋       | 40/149 [57:37<1:04:37, 35.58s/it]calibrating layers.2.blocks.2.attn.proj:  28%|██▊       | 41/149 [57:52<52:45, 29.31s/it]  calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 41/149 [57:52<52:45, 29.31s/it]calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 42/149 [58:10<45:57, 25.78s/it]calibrating layers.2.blocks.2.attn.matmul2:  28%|██▊       | 42/149 [58:10<45:57, 25.78s/it]calibrating layers.2.blocks.2.attn.matmul2:  29%|██▉       | 43/149 [58:27<41:19, 23.39s/it]calibrating layers.2.blocks.2.mlp.fc1:  29%|██▉       | 43/149 [58:27<41:19, 23.39s/it]     calibrating layers.2.blocks.2.mlp.fc1:  30%|██▉       | 44/149 [59:16<54:15, 31.01s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|██▉       | 44/149 [59:16<54:15, 31.01s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|███       | 45/149 [1:00:06<1:03:37, 36.71s/it]calibrating layers.2.blocks.3.attn.qkv:  30%|███       | 45/149 [1:00:06<1:03:37, 36.71s/it]calibrating layers.2.blocks.3.attn.qkv:  31%|███       | 46/149 [1:00:39<1:00:50, 35.44s/it]calibrating layers.2.blocks.3.attn.proj:  31%|███       | 46/149 [1:00:39<1:00:50, 35.44s/it]calibrating layers.2.blocks.3.attn.proj:  32%|███▏      | 47/149 [1:00:54<49:43, 29.25s/it]  calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 47/149 [1:00:54<49:43, 29.25s/it]calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 48/149 [1:01:11<43:26, 25.80s/it]calibrating layers.2.blocks.3.attn.matmul2:  32%|███▏      | 48/149 [1:01:11<43:26, 25.80s/it]calibrating layers.2.blocks.3.attn.matmul2:  33%|███▎      | 49/149 [1:01:29<39:06, 23.47s/it]calibrating layers.2.blocks.3.mlp.fc1:  33%|███▎      | 49/149 [1:01:29<39:06, 23.47s/it]     calibrating layers.2.blocks.3.mlp.fc1:  34%|███▎      | 50/149 [1:02:18<51:21, 31.13s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▎      | 50/149 [1:02:18<51:21, 31.13s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▍      | 51/149 [1:03:09<1:00:12, 36.87s/it]calibrating layers.2.blocks.4.attn.qkv:  34%|███▍      | 51/149 [1:03:09<1:00:12, 36.87s/it]calibrating layers.2.blocks.4.attn.qkv:  35%|███▍      | 52/149 [1:03:41<57:32, 35.59s/it]  calibrating layers.2.blocks.4.attn.proj:  35%|███▍      | 52/149 [1:03:41<57:32, 35.59s/it]calibrating layers.2.blocks.4.attn.proj:  36%|███▌      | 53/149 [1:03:56<47:00, 29.38s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 53/149 [1:03:56<47:00, 29.38s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 54/149 [1:04:14<40:57, 25.87s/it]calibrating layers.2.blocks.4.attn.matmul2:  36%|███▌      | 54/149 [1:04:14<40:57, 25.87s/it]calibrating layers.2.blocks.4.attn.matmul2:  37%|███▋      | 55/149 [1:04:32<36:51, 23.52s/it]calibrating layers.2.blocks.4.mlp.fc1:  37%|███▋      | 55/149 [1:04:32<36:51, 23.52s/it]     calibrating layers.2.blocks.4.mlp.fc1:  38%|███▊      | 56/149 [1:05:21<48:25, 31.25s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 56/149 [1:05:21<48:25, 31.25s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 57/149 [1:06:11<56:42, 36.99s/it]calibrating layers.2.blocks.5.attn.qkv:  38%|███▊      | 57/149 [1:06:11<56:42, 36.99s/it]calibrating layers.2.blocks.5.attn.qkv:  39%|███▉      | 58/149 [1:06:44<54:15, 35.78s/it]calibrating layers.2.blocks.5.attn.proj:  39%|███▉      | 58/149 [1:06:44<54:15, 35.78s/it]calibrating layers.2.blocks.5.attn.proj:  40%|███▉      | 59/149 [1:06:59<44:17, 29.53s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|███▉      | 59/149 [1:06:59<44:17, 29.53s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|████      | 60/149 [1:07:17<38:30, 25.96s/it]calibrating layers.2.blocks.5.attn.matmul2:  40%|████      | 60/149 [1:07:17<38:30, 25.96s/it]calibrating layers.2.blocks.5.attn.matmul2:  41%|████      | 61/149 [1:07:35<34:36, 23.60s/it]calibrating layers.2.blocks.5.mlp.fc1:  41%|████      | 61/149 [1:07:35<34:36, 23.60s/it]     calibrating layers.2.blocks.5.mlp.fc1:  42%|████▏     | 62/149 [1:08:24<45:21, 31.28s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 62/149 [1:08:24<45:21, 31.28s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 63/149 [1:09:14<52:55, 36.93s/it]calibrating layers.2.blocks.6.attn.qkv:  42%|████▏     | 63/149 [1:09:14<52:55, 36.93s/it]calibrating layers.2.blocks.6.attn.qkv:  43%|████▎     | 64/149 [1:09:47<50:25, 35.60s/it]calibrating layers.2.blocks.6.attn.proj:  43%|████▎     | 64/149 [1:09:47<50:25, 35.60s/it]calibrating layers.2.blocks.6.attn.proj:  44%|████▎     | 65/149 [1:10:02<41:07, 29.38s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▎     | 65/149 [1:10:02<41:07, 29.38s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▍     | 66/149 [1:10:19<35:46, 25.87s/it]calibrating layers.2.blocks.6.attn.matmul2:  44%|████▍     | 66/149 [1:10:19<35:46, 25.87s/it]calibrating layers.2.blocks.6.attn.matmul2:  45%|████▍     | 67/149 [1:10:37<32:06, 23.49s/it]calibrating layers.2.blocks.6.mlp.fc1:  45%|████▍     | 67/149 [1:10:37<32:06, 23.49s/it]     calibrating layers.2.blocks.6.mlp.fc1:  46%|████▌     | 68/149 [1:11:27<42:06, 31.19s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▌     | 68/149 [1:11:27<42:06, 31.19s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▋     | 69/149 [1:12:17<49:16, 36.96s/it]calibrating layers.2.blocks.7.attn.qkv:  46%|████▋     | 69/149 [1:12:17<49:16, 36.96s/it]calibrating layers.2.blocks.7.attn.qkv:  47%|████▋     | 70/149 [1:12:50<47:01, 35.72s/it]calibrating layers.2.blocks.7.attn.proj:  47%|████▋     | 70/149 [1:12:50<47:01, 35.72s/it]calibrating layers.2.blocks.7.attn.proj:  48%|████▊     | 71/149 [1:13:05<38:15, 29.43s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 71/149 [1:13:05<38:15, 29.43s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 72/149 [1:13:22<33:16, 25.93s/it]calibrating layers.2.blocks.7.attn.matmul2:  48%|████▊     | 72/149 [1:13:22<33:16, 25.93s/it]calibrating layers.2.blocks.7.attn.matmul2:  49%|████▉     | 73/149 [1:13:40<29:50, 23.55s/it]calibrating layers.2.blocks.7.mlp.fc1:  49%|████▉     | 73/149 [1:13:40<29:50, 23.55s/it]     calibrating layers.2.blocks.7.mlp.fc1:  50%|████▉     | 74/149 [1:14:29<39:01, 31.22s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|████▉     | 74/149 [1:14:29<39:01, 31.22s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|█████     | 75/149 [1:15:19<45:25, 36.83s/it]calibrating layers.2.blocks.8.attn.qkv:  50%|█████     | 75/149 [1:15:19<45:25, 36.83s/it]calibrating layers.2.blocks.8.attn.qkv:  51%|█████     | 76/149 [1:15:52<43:14, 35.54s/it]calibrating layers.2.blocks.8.attn.proj:  51%|█████     | 76/149 [1:15:52<43:14, 35.54s/it]calibrating layers.2.blocks.8.attn.proj:  52%|█████▏    | 77/149 [1:16:07<35:11, 29.32s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 77/149 [1:16:07<35:11, 29.32s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 78/149 [1:16:24<30:34, 25.84s/it]calibrating layers.2.blocks.8.attn.matmul2:  52%|█████▏    | 78/149 [1:16:24<30:34, 25.84s/it]calibrating layers.2.blocks.8.attn.matmul2:  53%|█████▎    | 79/149 [1:16:42<27:24, 23.49s/it]calibrating layers.2.blocks.8.mlp.fc1:  53%|█████▎    | 79/149 [1:16:42<27:24, 23.49s/it]     calibrating layers.2.blocks.8.mlp.fc1:  54%|█████▎    | 80/149 [1:17:31<35:50, 31.17s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▎    | 80/149 [1:17:31<35:50, 31.17s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▍    | 81/149 [1:18:22<41:50, 36.93s/it]calibrating layers.2.blocks.9.attn.qkv:  54%|█████▍    | 81/149 [1:18:22<41:50, 36.93s/it]calibrating layers.2.blocks.9.attn.qkv:  55%|█████▌    | 82/149 [1:18:54<39:48, 35.64s/it]calibrating layers.2.blocks.9.attn.proj:  55%|█████▌    | 82/149 [1:18:54<39:48, 35.64s/it]calibrating layers.2.blocks.9.attn.proj:  56%|█████▌    | 83/149 [1:19:09<32:20, 29.40s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▌    | 83/149 [1:19:09<32:20, 29.40s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▋    | 84/149 [1:19:27<28:06, 25.95s/it]calibrating layers.2.blocks.9.attn.matmul2:  56%|█████▋    | 84/149 [1:19:27<28:06, 25.95s/it]calibrating layers.2.blocks.9.attn.matmul2:  57%|█████▋    | 85/149 [1:19:45<25:13, 23.64s/it]calibrating layers.2.blocks.9.mlp.fc1:  57%|█████▋    | 85/149 [1:19:45<25:13, 23.64s/it]     calibrating layers.2.blocks.9.mlp.fc1:  58%|█████▊    | 86/149 [1:20:34<32:49, 31.26s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 86/149 [1:20:34<32:49, 31.26s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 87/149 [1:21:24<38:04, 36.84s/it]calibrating layers.2.blocks.10.attn.qkv:  58%|█████▊    | 87/149 [1:21:24<38:04, 36.84s/it]calibrating layers.2.blocks.10.attn.qkv:  59%|█████▉    | 88/149 [1:21:57<36:01, 35.44s/it]calibrating layers.2.blocks.10.attn.proj:  59%|█████▉    | 88/149 [1:21:57<36:01, 35.44s/it]calibrating layers.2.blocks.10.attn.proj:  60%|█████▉    | 89/149 [1:22:11<29:14, 29.24s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|█████▉    | 89/149 [1:22:11<29:14, 29.24s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|██████    | 90/149 [1:22:29<25:17, 25.72s/it]calibrating layers.2.blocks.10.attn.matmul2:  60%|██████    | 90/149 [1:22:29<25:17, 25.72s/it]calibrating layers.2.blocks.10.attn.matmul2:  61%|██████    | 91/149 [1:22:47<22:37, 23.40s/it]calibrating layers.2.blocks.10.mlp.fc1:  61%|██████    | 91/149 [1:22:47<22:37, 23.40s/it]     calibrating layers.2.blocks.10.mlp.fc1:  62%|██████▏   | 92/149 [1:23:36<29:37, 31.18s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 92/149 [1:23:36<29:37, 31.18s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 93/149 [1:24:27<34:29, 36.95s/it]calibrating layers.2.blocks.11.attn.qkv:  62%|██████▏   | 93/149 [1:24:27<34:29, 36.95s/it]calibrating layers.2.blocks.11.attn.qkv:  63%|██████▎   | 94/149 [1:24:59<32:43, 35.70s/it]calibrating layers.2.blocks.11.attn.proj:  63%|██████▎   | 94/149 [1:24:59<32:43, 35.70s/it]calibrating layers.2.blocks.11.attn.proj:  64%|██████▍   | 95/149 [1:25:14<26:30, 29.46s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 95/149 [1:25:14<26:30, 29.46s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 96/149 [1:25:32<22:58, 26.01s/it]calibrating layers.2.blocks.11.attn.matmul2:  64%|██████▍   | 96/149 [1:25:32<22:58, 26.01s/it]calibrating layers.2.blocks.11.attn.matmul2:  65%|██████▌   | 97/149 [1:25:50<20:30, 23.67s/it]calibrating layers.2.blocks.11.mlp.fc1:  65%|██████▌   | 97/149 [1:25:50<20:30, 23.67s/it]     calibrating layers.2.blocks.11.mlp.fc1:  66%|██████▌   | 98/149 [1:26:40<26:39, 31.37s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▌   | 98/149 [1:26:40<26:39, 31.37s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▋   | 99/149 [1:27:30<30:52, 37.05s/it]calibrating layers.2.blocks.12.attn.qkv:  66%|██████▋   | 99/149 [1:27:30<30:52, 37.05s/it]calibrating layers.2.blocks.12.attn.qkv:  67%|██████▋   | 100/149 [1:28:03<29:10, 35.72s/it]calibrating layers.2.blocks.12.attn.proj:  67%|██████▋   | 100/149 [1:28:03<29:10, 35.72s/it]calibrating layers.2.blocks.12.attn.proj:  68%|██████▊   | 101/149 [1:28:17<23:33, 29.44s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 101/149 [1:28:17<23:33, 29.44s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 102/149 [1:28:35<20:18, 25.92s/it]calibrating layers.2.blocks.12.attn.matmul2:  68%|██████▊   | 102/149 [1:28:35<20:18, 25.92s/it]calibrating layers.2.blocks.12.attn.matmul2:  69%|██████▉   | 103/149 [1:28:53<18:02, 23.52s/it]calibrating layers.2.blocks.12.mlp.fc1:  69%|██████▉   | 103/149 [1:28:53<18:02, 23.52s/it]     calibrating layers.2.blocks.12.mlp.fc1:  70%|██████▉   | 104/149 [1:29:42<23:26, 31.26s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|██████▉   | 104/149 [1:29:42<23:26, 31.26s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|███████   | 105/149 [1:30:33<27:07, 36.99s/it]calibrating layers.2.blocks.13.attn.qkv:  70%|███████   | 105/149 [1:30:33<27:07, 36.99s/it]calibrating layers.2.blocks.13.attn.qkv:  71%|███████   | 106/149 [1:31:06<25:37, 35.75s/it]calibrating layers.2.blocks.13.attn.proj:  71%|███████   | 106/149 [1:31:06<25:37, 35.75s/it]calibrating layers.2.blocks.13.attn.proj:  72%|███████▏  | 107/149 [1:31:20<20:38, 29.48s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 107/149 [1:31:20<20:38, 29.48s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 108/149 [1:31:38<17:43, 25.95s/it]calibrating layers.2.blocks.13.attn.matmul2:  72%|███████▏  | 108/149 [1:31:38<17:43, 25.95s/it]calibrating layers.2.blocks.13.attn.matmul2:  73%|███████▎  | 109/149 [1:31:56<15:44, 23.61s/it]calibrating layers.2.blocks.13.mlp.fc1:  73%|███████▎  | 109/149 [1:31:56<15:44, 23.61s/it]     calibrating layers.2.blocks.13.mlp.fc1:  74%|███████▍  | 110/149 [1:32:45<20:19, 31.26s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 110/149 [1:32:45<20:19, 31.26s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 111/149 [1:33:36<23:26, 37.02s/it]calibrating layers.2.blocks.14.attn.qkv:  74%|███████▍  | 111/149 [1:33:36<23:26, 37.02s/it]calibrating layers.2.blocks.14.attn.qkv:  75%|███████▌  | 112/149 [1:34:09<22:01, 35.72s/it]calibrating layers.2.blocks.14.attn.proj:  75%|███████▌  | 112/149 [1:34:09<22:01, 35.72s/it]calibrating layers.2.blocks.14.attn.proj:  76%|███████▌  | 113/149 [1:34:23<17:40, 29.45s/it]calibrating layers.2.blocks.14.attn.matmul1:  76%|███████▌  | 113/149 [1:34:23<17:40, 29.45s/it]calibrating layers.2.blocks.14.attn.matmul1:  77%|███████▋  | 114/149 [1:34:41<15:06, 25.91s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 114/149 [1:34:41<15:06, 25.91s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 115/149 [1:34:59<13:14, 23.38s/it]calibrating layers.2.blocks.14.mlp.fc1:  77%|███████▋  | 115/149 [1:34:59<13:14, 23.38s/it]     calibrating layers.2.blocks.14.mlp.fc1:  78%|███████▊  | 116/149 [1:35:48<17:06, 31.09s/it]calibrating layers.2.blocks.14.mlp.fc2:  78%|███████▊  | 116/149 [1:35:48<17:06, 31.09s/it]calibrating layers.2.blocks.14.mlp.fc2:  79%|███████▊  | 117/149 [1:36:38<19:39, 36.87s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▊  | 117/149 [1:36:38<19:39, 36.87s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▉  | 118/149 [1:37:11<18:23, 35.59s/it]calibrating layers.2.blocks.15.attn.proj:  79%|███████▉  | 118/149 [1:37:11<18:23, 35.59s/it]calibrating layers.2.blocks.15.attn.proj:  80%|███████▉  | 119/149 [1:37:25<14:39, 29.31s/it]calibrating layers.2.blocks.15.attn.matmul1:  80%|███████▉  | 119/149 [1:37:25<14:39, 29.31s/it]calibrating layers.2.blocks.15.attn.matmul1:  81%|████████  | 120/149 [1:37:43<12:26, 25.74s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 120/149 [1:37:43<12:26, 25.74s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 121/149 [1:38:01<10:57, 23.47s/it]calibrating layers.2.blocks.15.mlp.fc1:  81%|████████  | 121/149 [1:38:01<10:57, 23.47s/it]     calibrating layers.2.blocks.15.mlp.fc1:  82%|████████▏ | 122/149 [1:38:50<13:59, 31.11s/it]calibrating layers.2.blocks.15.mlp.fc2:  82%|████████▏ | 122/149 [1:38:50<13:59, 31.11s/it]calibrating layers.2.blocks.15.mlp.fc2:  83%|████████▎ | 123/149 [1:39:40<15:54, 36.73s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 123/149 [1:39:40<15:54, 36.73s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 124/149 [1:40:12<14:44, 35.37s/it]calibrating layers.2.blocks.16.attn.proj:  83%|████████▎ | 124/149 [1:40:12<14:44, 35.37s/it]calibrating layers.2.blocks.16.attn.proj:  84%|████████▍ | 125/149 [1:40:26<11:39, 29.14s/it]calibrating layers.2.blocks.16.attn.matmul1:  84%|████████▍ | 125/149 [1:40:26<11:39, 29.14s/it]calibrating layers.2.blocks.16.attn.matmul1:  85%|████████▍ | 126/149 [1:40:44<09:49, 25.63s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▍ | 126/149 [1:40:44<09:49, 25.63s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▌ | 127/149 [1:41:02<08:32, 23.29s/it]calibrating layers.2.blocks.16.mlp.fc1:  85%|████████▌ | 127/149 [1:41:02<08:32, 23.29s/it]     calibrating layers.2.blocks.16.mlp.fc1:  86%|████████▌ | 128/149 [1:41:51<10:50, 30.97s/it]calibrating layers.2.blocks.16.mlp.fc2:  86%|████████▌ | 128/149 [1:41:51<10:50, 30.97s/it]calibrating layers.2.blocks.16.mlp.fc2:  87%|████████▋ | 129/149 [1:42:40<12:12, 36.61s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 129/149 [1:42:40<12:12, 36.61s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 130/149 [1:43:12<11:09, 35.24s/it]calibrating layers.2.blocks.17.attn.proj:  87%|████████▋ | 130/149 [1:43:12<11:09, 35.24s/it]calibrating layers.2.blocks.17.attn.proj:  88%|████████▊ | 131/149 [1:43:27<08:41, 28.98s/it]calibrating layers.2.blocks.17.attn.matmul1:  88%|████████▊ | 131/149 [1:43:27<08:41, 28.98s/it]calibrating layers.2.blocks.17.attn.matmul1:  89%|████████▊ | 132/149 [1:43:44<07:13, 25.50s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▊ | 132/149 [1:43:44<07:13, 25.50s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▉ | 133/149 [1:44:02<06:13, 23.32s/it]calibrating layers.2.blocks.17.mlp.fc1:  89%|████████▉ | 133/149 [1:44:02<06:13, 23.32s/it]     calibrating layers.2.blocks.17.mlp.fc1:  90%|████████▉ | 134/149 [1:44:52<07:47, 31.15s/it]calibrating layers.2.blocks.17.mlp.fc2:  90%|████████▉ | 134/149 [1:44:52<07:47, 31.15s/it]calibrating layers.2.blocks.17.mlp.fc2:  91%|█████████ | 135/149 [1:45:42<08:34, 36.79s/it]calibrating layers.3.downsample.reduction:  91%|█████████ | 135/149 [1:45:42<08:34, 36.79s/it]calibrating layers.3.downsample.reduction:  91%|█████████▏| 136/149 [1:45:59<06:40, 30.82s/it]calibrating layers.3.blocks.0.attn.qkv:  91%|█████████▏| 136/149 [1:45:59<06:40, 30.82s/it]   calibrating layers.3.blocks.0.attn.qkv:  92%|█████████▏| 137/149 [1:46:24<05:50, 29.19s/it]calibrating layers.3.blocks.0.attn.proj:  92%|█████████▏| 137/149 [1:46:24<05:50, 29.19s/it]calibrating layers.3.blocks.0.attn.proj:  93%|█████████▎| 138/149 [1:46:35<04:20, 23.72s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 138/149 [1:46:35<04:20, 23.72s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 139/149 [1:46:52<03:36, 21.64s/it]calibrating layers.3.blocks.0.attn.matmul2:  93%|█████████▎| 139/149 [1:46:52<03:36, 21.64s/it]calibrating layers.3.blocks.0.attn.matmul2:  94%|█████████▍| 140/149 [1:47:08<02:59, 19.97s/it]calibrating layers.3.blocks.0.mlp.fc1:  94%|█████████▍| 140/149 [1:47:08<02:59, 19.97s/it]     calibrating layers.3.blocks.0.mlp.fc1:  95%|█████████▍| 141/149 [1:47:40<03:09, 23.70s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▍| 141/149 [1:47:40<03:09, 23.70s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▌| 142/149 [1:48:13<03:05, 26.43s/it]calibrating layers.3.blocks.1.attn.qkv:  95%|█████████▌| 142/149 [1:48:13<03:05, 26.43s/it]calibrating layers.3.blocks.1.attn.qkv:  96%|█████████▌| 143/149 [1:48:38<02:36, 26.11s/it]calibrating layers.3.blocks.1.attn.proj:  96%|█████████▌| 143/149 [1:48:38<02:36, 26.11s/it]calibrating layers.3.blocks.1.attn.proj:  97%|█████████▋| 144/149 [1:48:50<01:48, 21.66s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 144/149 [1:48:50<01:48, 21.66s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 145/149 [1:49:07<01:20, 20.23s/it]calibrating layers.3.blocks.1.attn.matmul2:  97%|█████████▋| 145/149 [1:49:07<01:20, 20.23s/it]calibrating layers.3.blocks.1.attn.matmul2:  98%|█████████▊| 146/149 [1:49:23<00:56, 18.98s/it]calibrating layers.3.blocks.1.mlp.fc1:  98%|█████████▊| 146/149 [1:49:23<00:56, 18.98s/it]     calibrating layers.3.blocks.1.mlp.fc1:  99%|█████████▊| 147/149 [1:49:55<00:45, 23.00s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▊| 147/149 [1:49:55<00:45, 23.00s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▉| 148/149 [1:50:28<00:25, 25.99s/it]calibrating head.fc:  99%|█████████▉| 148/149 [1:50:28<00:25, 25.99s/it]                  calibrating head.fc: 100%|██████████| 149/149 [1:50:32<00:00, 19.43s/it]calibrating head.fc: 100%|██████████| 149/149 [1:50:32<00:00, 44.51s/it]
2025-09-08 17:59:39 - mse guided calibration finished.
Saving checkpoint to ./checkpoint/quant_result/20250908_1606/swin_base_w4_a4_calibsize_1000_mse.pth
Validating after calibration ...
Test: [0/100]	Time 5.421 (5.421)	Loss 1.0504 (1.0504)	Prec@1 88.800 (88.800)	Prec@5 97.200 (97.200)
Test: [10/100]	Time 2.385 (2.659)	Loss 1.0418 (1.1585)	Prec@1 85.800 (84.891)	Prec@5 96.600 (97.018)
Test: [20/100]	Time 2.375 (2.525)	Loss 1.4887 (1.2373)	Prec@1 79.000 (83.143)	Prec@5 98.200 (96.571)
Test: [30/100]	Time 2.372 (2.477)	Loss 1.1162 (1.2758)	Prec@1 84.200 (82.239)	Prec@5 98.200 (96.542)
Test: [40/100]	Time 2.375 (2.452)	Loss 1.2923 (1.2464)	Prec@1 77.000 (82.654)	Prec@5 95.600 (96.576)
Test: [50/100]	Time 2.374 (2.437)	Loss 1.6322 (1.2784)	Prec@1 72.400 (81.369)	Prec@5 90.600 (96.118)
Test: [60/100]	Time 2.383 (2.428)	Loss 1.2226 (1.2806)	Prec@1 82.200 (81.180)	Prec@5 94.600 (95.954)
Test: [70/100]	Time 2.371 (2.420)	Loss 1.5251 (1.3020)	Prec@1 74.000 (80.363)	Prec@5 93.800 (95.732)
Test: [80/100]	Time 2.371 (2.414)	Loss 1.1245 (1.3093)	Prec@1 83.400 (80.165)	Prec@5 96.600 (95.499)
Test: [90/100]	Time 2.379 (2.410)	Loss 1.4809 (1.3282)	Prec@1 73.000 (79.549)	Prec@5 93.400 (95.325)
 * Prec@1 79.682 Prec@5 95.412 Loss 1.314 Time 240.923
Building calibrator ...
2025-09-08 18:03:44 - start mse guided block reconstruction
reconstructing patch_embed ...
initializing raw input and raw output ...
adaround training for patch_embed ...
wraping quantizers in patch_embed ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.139 (rec:0.139, round:0.000)	b=0.00	count=500
Total loss:	0.089 (rec:0.089, round:0.000)	b=0.00	count=1000
Total loss:	0.056 (rec:0.056, round:0.000)	b=0.00	count=1500
Total loss:	0.047 (rec:0.047, round:0.000)	b=0.00	count=2000
Total loss:	0.033 (rec:0.033, round:0.000)	b=0.00	count=2500
Total loss:	0.027 (rec:0.027, round:0.000)	b=0.00	count=3000
Total loss:	0.022 (rec:0.022, round:0.000)	b=0.00	count=3500
Total loss:	57.764 (rec:0.015, round:57.749)	b=20.00	count=4000
Total loss:	37.554 (rec:0.031, round:37.522)	b=19.44	count=4500
Total loss:	34.837 (rec:0.021, round:34.817)	b=18.88	count=5000
Total loss:	33.326 (rec:0.019, round:33.306)	b=18.31	count=5500
Total loss:	31.919 (rec:0.026, round:31.893)	b=17.75	count=6000
Total loss:	30.559 (rec:0.021, round:30.537)	b=17.19	count=6500
Total loss:	29.094 (rec:0.021, round:29.073)	b=16.62	count=7000
Total loss:	27.697 (rec:0.025, round:27.671)	b=16.06	count=7500
Total loss:	26.431 (rec:0.017, round:26.414)	b=15.50	count=8000
Total loss:	24.764 (rec:0.019, round:24.744)	b=14.94	count=8500
Total loss:	23.199 (rec:0.032, round:23.168)	b=14.38	count=9000
Total loss:	21.516 (rec:0.039, round:21.476)	b=13.81	count=9500
Total loss:	20.023 (rec:0.034, round:19.989)	b=13.25	count=10000
Total loss:	18.568 (rec:0.037, round:18.531)	b=12.69	count=10500
Total loss:	17.073 (rec:0.033, round:17.040)	b=12.12	count=11000
Total loss:	15.364 (rec:0.051, round:15.313)	b=11.56	count=11500
Total loss:	13.706 (rec:0.039, round:13.667)	b=11.00	count=12000
Total loss:	12.274 (rec:0.044, round:12.230)	b=10.44	count=12500
Total loss:	10.715 (rec:0.057, round:10.657)	b=9.88	count=13000
Total loss:	9.349 (rec:0.064, round:9.285)	b=9.31	count=13500
Total loss:	7.835 (rec:0.087, round:7.748)	b=8.75	count=14000
Total loss:	6.443 (rec:0.089, round:6.353)	b=8.19	count=14500
Total loss:	5.388 (rec:0.083, round:5.305)	b=7.62	count=15000
Total loss:	4.320 (rec:0.137, round:4.183)	b=7.06	count=15500
Total loss:	3.444 (rec:0.133, round:3.312)	b=6.50	count=16000
Total loss:	2.650 (rec:0.170, round:2.479)	b=5.94	count=16500
Total loss:	2.099 (rec:0.211, round:1.887)	b=5.38	count=17000
Total loss:	1.675 (rec:0.245, round:1.430)	b=4.81	count=17500
Total loss:	1.359 (rec:0.298, round:1.061)	b=4.25	count=18000
Total loss:	1.023 (rec:0.350, round:0.673)	b=3.69	count=18500
Total loss:	0.778 (rec:0.338, round:0.440)	b=3.12	count=19000
Total loss:	0.722 (rec:0.422, round:0.300)	b=2.56	count=19500
Total loss:	0.676 (rec:0.461, round:0.215)	b=2.00	count=20000
finished reconstructing patch_embed.
reconstructing layers.0.blocks.0 ...
initializing raw input and raw output ...
adaround training for layers.0.blocks.0 ...
wraping quantizers in layers.0.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.466 (rec:1.466, round:0.000)	b=0.00	count=500
Total loss:	1.011 (rec:1.011, round:0.000)	b=0.00	count=1000
Total loss:	0.908 (rec:0.908, round:0.000)	b=0.00	count=1500
Total loss:	0.881 (rec:0.881, round:0.000)	b=0.00	count=2000
Total loss:	0.731 (rec:0.731, round:0.000)	b=0.00	count=2500
Total loss:	0.710 (rec:0.710, round:0.000)	b=0.00	count=3000
Total loss:	0.708 (rec:0.708, round:0.000)	b=0.00	count=3500
Total loss:	1541.362 (rec:0.662, round:1540.700)	b=20.00	count=4000
Total loss:	670.012 (rec:0.652, round:669.359)	b=19.44	count=4500
Total loss:	586.292 (rec:0.590, round:585.702)	b=18.88	count=5000
Total loss:	521.306 (rec:0.590, round:520.716)	b=18.31	count=5500
Total loss:	465.171 (rec:0.583, round:464.588)	b=17.75	count=6000
Total loss:	414.061 (rec:0.600, round:413.460)	b=17.19	count=6500
Total loss:	368.120 (rec:0.618, round:367.502)	b=16.62	count=7000
Total loss:	328.773 (rec:0.573, round:328.200)	b=16.06	count=7500
Total loss:	295.159 (rec:0.593, round:294.565)	b=15.50	count=8000
Total loss:	265.670 (rec:0.540, round:265.130)	b=14.94	count=8500
Total loss:	239.071 (rec:0.579, round:238.492)	b=14.38	count=9000
Total loss:	214.883 (rec:0.556, round:214.327)	b=13.81	count=9500
Total loss:	193.276 (rec:0.601, round:192.675)	b=13.25	count=10000
Total loss:	173.358 (rec:0.569, round:172.789)	b=12.69	count=10500
Total loss:	154.994 (rec:0.548, round:154.447)	b=12.12	count=11000
Total loss:	137.838 (rec:0.572, round:137.266)	b=11.56	count=11500
Total loss:	121.612 (rec:0.561, round:121.051)	b=11.00	count=12000
Total loss:	105.844 (rec:0.547, round:105.297)	b=10.44	count=12500
Total loss:	90.857 (rec:0.577, round:90.280)	b=9.88	count=13000
Total loss:	77.140 (rec:0.603, round:76.537)	b=9.31	count=13500
Total loss:	64.368 (rec:0.544, round:63.824)	b=8.75	count=14000
Total loss:	52.686 (rec:0.548, round:52.138)	b=8.19	count=14500
Total loss:	41.663 (rec:0.553, round:41.110)	b=7.62	count=15000
Total loss:	31.904 (rec:0.560, round:31.344)	b=7.06	count=15500
Total loss:	23.259 (rec:0.577, round:22.682)	b=6.50	count=16000
Total loss:	16.076 (rec:0.552, round:15.524)	b=5.94	count=16500
Total loss:	9.895 (rec:0.573, round:9.323)	b=5.38	count=17000
Total loss:	5.421 (rec:0.545, round:4.875)	b=4.81	count=17500
Total loss:	2.702 (rec:0.542, round:2.160)	b=4.25	count=18000
Total loss:	1.381 (rec:0.579, round:0.803)	b=3.69	count=18500
Total loss:	0.732 (rec:0.556, round:0.177)	b=3.12	count=19000
Total loss:	0.587 (rec:0.557, round:0.030)	b=2.56	count=19500
Total loss:	0.554 (rec:0.551, round:0.003)	b=2.00	count=20000
finished reconstructing layers.0.blocks.0.
reconstructing layers.0.blocks.1 ...
initializing raw input and raw output ...
adaround training for layers.0.blocks.1 ...
wraping quantizers in layers.0.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.482 (rec:1.482, round:0.000)	b=0.00	count=500
Total loss:	1.312 (rec:1.312, round:0.000)	b=0.00	count=1000
Total loss:	1.297 (rec:1.297, round:0.000)	b=0.00	count=1500
Total loss:	1.309 (rec:1.309, round:0.000)	b=0.00	count=2000
Total loss:	1.326 (rec:1.326, round:0.000)	b=0.00	count=2500
Total loss:	1.239 (rec:1.239, round:0.000)	b=0.00	count=3000
Total loss:	1.206 (rec:1.206, round:0.000)	b=0.00	count=3500
Total loss:	1623.565 (rec:1.117, round:1622.448)	b=20.00	count=4000
Total loss:	876.752 (rec:1.155, round:875.597)	b=19.44	count=4500
Total loss:	795.980 (rec:1.176, round:794.805)	b=18.88	count=5000
Total loss:	737.040 (rec:1.168, round:735.872)	b=18.31	count=5500
Total loss:	685.270 (rec:1.091, round:684.179)	b=17.75	count=6000
Total loss:	639.677 (rec:1.163, round:638.514)	b=17.19	count=6500
Total loss:	597.388 (rec:1.160, round:596.229)	b=16.62	count=7000
Total loss:	557.246 (rec:1.142, round:556.104)	b=16.06	count=7500
Total loss:	519.865 (rec:1.180, round:518.684)	b=15.50	count=8000
Total loss:	485.673 (rec:1.150, round:484.523)	b=14.94	count=8500
Total loss:	452.745 (rec:1.106, round:451.640)	b=14.38	count=9000
Total loss:	421.942 (rec:1.142, round:420.800)	b=13.81	count=9500
Total loss:	392.927 (rec:1.133, round:391.794)	b=13.25	count=10000
Total loss:	364.468 (rec:1.177, round:363.291)	b=12.69	count=10500
Total loss:	336.716 (rec:1.191, round:335.525)	b=12.12	count=11000
Total loss:	309.101 (rec:1.116, round:307.985)	b=11.56	count=11500
Total loss:	282.222 (rec:1.196, round:281.027)	b=11.00	count=12000
Total loss:	254.313 (rec:1.173, round:253.140)	b=10.44	count=12500
Total loss:	227.229 (rec:1.039, round:226.190)	b=9.88	count=13000
Total loss:	200.428 (rec:1.284, round:199.144)	b=9.31	count=13500
Total loss:	173.586 (rec:1.201, round:172.385)	b=8.75	count=14000
Total loss:	146.764 (rec:1.174, round:145.590)	b=8.19	count=14500
Total loss:	120.179 (rec:1.199, round:118.980)	b=7.62	count=15000
Total loss:	95.033 (rec:1.126, round:93.907)	b=7.06	count=15500
Total loss:	70.137 (rec:1.309, round:68.828)	b=6.50	count=16000
Total loss:	47.762 (rec:1.240, round:46.522)	b=5.94	count=16500
Total loss:	28.895 (rec:1.150, round:27.744)	b=5.38	count=17000
Total loss:	15.784 (rec:1.234, round:14.550)	b=4.81	count=17500
Total loss:	7.478 (rec:1.229, round:6.248)	b=4.25	count=18000
Total loss:	3.060 (rec:1.136, round:1.925)	b=3.69	count=18500
Total loss:	1.613 (rec:1.214, round:0.399)	b=3.12	count=19000
Total loss:	1.160 (rec:1.106, round:0.054)	b=2.56	count=19500
Total loss:	1.138 (rec:1.136, round:0.003)	b=2.00	count=20000
finished reconstructing layers.0.blocks.1.
reconstructing layers.1.downsample ...
initializing raw input and raw output ...
adaround training for layers.1.downsample ...
wraping quantizers in layers.1.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.742 (rec:1.742, round:0.000)	b=0.00	count=500
Total loss:	1.812 (rec:1.812, round:0.000)	b=0.00	count=1000
Total loss:	1.647 (rec:1.647, round:0.000)	b=0.00	count=1500
Total loss:	1.841 (rec:1.841, round:0.000)	b=0.00	count=2000
Total loss:	1.591 (rec:1.591, round:0.000)	b=0.00	count=2500
Total loss:	1.659 (rec:1.659, round:0.000)	b=0.00	count=3000
Total loss:	1.475 (rec:1.475, round:0.000)	b=0.00	count=3500
Total loss:	1007.616 (rec:1.512, round:1006.104)	b=20.00	count=4000
Total loss:	526.666 (rec:1.554, round:525.112)	b=19.44	count=4500
Total loss:	476.881 (rec:1.602, round:475.279)	b=18.88	count=5000
Total loss:	442.450 (rec:1.533, round:440.917)	b=18.31	count=5500
Total loss:	415.197 (rec:1.554, round:413.643)	b=17.75	count=6000
Total loss:	391.185 (rec:1.448, round:389.737)	b=17.19	count=6500
Total loss:	369.925 (rec:1.649, round:368.276)	b=16.62	count=7000
Total loss:	350.411 (rec:1.538, round:348.873)	b=16.06	count=7500
Total loss:	331.950 (rec:1.537, round:330.413)	b=15.50	count=8000
Total loss:	314.319 (rec:1.707, round:312.611)	b=14.94	count=8500
Total loss:	296.575 (rec:1.455, round:295.120)	b=14.38	count=9000
Total loss:	279.994 (rec:1.669, round:278.325)	b=13.81	count=9500
Total loss:	263.503 (rec:1.482, round:262.021)	b=13.25	count=10000
Total loss:	246.647 (rec:1.652, round:244.995)	b=12.69	count=10500
Total loss:	229.132 (rec:1.564, round:227.568)	b=12.12	count=11000
Total loss:	211.570 (rec:1.548, round:210.022)	b=11.56	count=11500
Total loss:	193.916 (rec:1.633, round:192.284)	b=11.00	count=12000
Total loss:	175.738 (rec:1.574, round:174.164)	b=10.44	count=12500
Total loss:	156.994 (rec:1.542, round:155.452)	b=9.88	count=13000
Total loss:	137.366 (rec:1.590, round:135.776)	b=9.31	count=13500
Total loss:	118.299 (rec:1.567, round:116.732)	b=8.75	count=14000
Total loss:	99.060 (rec:1.714, round:97.346)	b=8.19	count=14500
Total loss:	79.385 (rec:1.491, round:77.894)	b=7.62	count=15000
Total loss:	61.065 (rec:1.595, round:59.470)	b=7.06	count=15500
Total loss:	44.030 (rec:1.546, round:42.483)	b=6.50	count=16000
Total loss:	29.139 (rec:1.502, round:27.637)	b=5.94	count=16500
Total loss:	18.344 (rec:1.667, round:16.676)	b=5.38	count=17000
Total loss:	10.821 (rec:1.643, round:9.178)	b=4.81	count=17500
Total loss:	5.854 (rec:1.620, round:4.234)	b=4.25	count=18000
Total loss:	3.269 (rec:1.598, round:1.671)	b=3.69	count=18500
Total loss:	2.272 (rec:1.754, round:0.518)	b=3.12	count=19000
Total loss:	1.639 (rec:1.551, round:0.089)	b=2.56	count=19500
Total loss:	1.659 (rec:1.655, round:0.004)	b=2.00	count=20000
finished reconstructing layers.1.downsample.
reconstructing layers.1.blocks.0 ...
initializing raw input and raw output ...
adaround training for layers.1.blocks.0 ...
wraping quantizers in layers.1.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.552 (rec:1.552, round:0.000)	b=0.00	count=500
Total loss:	1.431 (rec:1.431, round:0.000)	b=0.00	count=1000
Total loss:	1.397 (rec:1.397, round:0.000)	b=0.00	count=1500
Total loss:	1.360 (rec:1.360, round:0.000)	b=0.00	count=2000
Total loss:	1.384 (rec:1.384, round:0.000)	b=0.00	count=2500
Total loss:	1.328 (rec:1.328, round:0.000)	b=0.00	count=3000
Total loss:	1.327 (rec:1.327, round:0.000)	b=0.00	count=3500
Total loss:	6757.008 (rec:1.347, round:6755.661)	b=20.00	count=4000
Total loss:	3522.578 (rec:1.302, round:3521.276)	b=19.44	count=4500
Total loss:	3218.904 (rec:1.323, round:3217.582)	b=18.88	count=5000
Total loss:	3006.638 (rec:1.315, round:3005.323)	b=18.31	count=5500
Total loss:	2826.832 (rec:1.283, round:2825.549)	b=17.75	count=6000
Total loss:	2659.594 (rec:1.355, round:2658.239)	b=17.19	count=6500
Total loss:	2504.019 (rec:1.323, round:2502.697)	b=16.62	count=7000
Total loss:	2357.818 (rec:1.357, round:2356.461)	b=16.06	count=7500
Total loss:	2219.876 (rec:1.356, round:2218.520)	b=15.50	count=8000
Total loss:	2085.694 (rec:1.340, round:2084.354)	b=14.94	count=8500
Total loss:	1955.689 (rec:1.356, round:1954.333)	b=14.38	count=9000
Total loss:	1831.677 (rec:1.337, round:1830.339)	b=13.81	count=9500
Total loss:	1709.319 (rec:1.348, round:1707.971)	b=13.25	count=10000
Total loss:	1585.812 (rec:1.337, round:1584.475)	b=12.69	count=10500
Total loss:	1463.963 (rec:1.334, round:1462.629)	b=12.12	count=11000
Total loss:	1341.741 (rec:1.362, round:1340.379)	b=11.56	count=11500
Total loss:	1217.706 (rec:1.361, round:1216.345)	b=11.00	count=12000
Total loss:	1096.039 (rec:1.344, round:1094.694)	b=10.44	count=12500
Total loss:	972.393 (rec:1.416, round:970.977)	b=9.88	count=13000
Total loss:	848.184 (rec:1.374, round:846.810)	b=9.31	count=13500
Total loss:	721.277 (rec:1.384, round:719.894)	b=8.75	count=14000
Total loss:	596.167 (rec:1.457, round:594.710)	b=8.19	count=14500
Total loss:	474.975 (rec:1.446, round:473.528)	b=7.62	count=15000
Total loss:	357.179 (rec:1.404, round:355.775)	b=7.06	count=15500
Total loss:	248.057 (rec:1.425, round:246.631)	b=6.50	count=16000
Total loss:	155.515 (rec:1.448, round:154.066)	b=5.94	count=16500
Total loss:	85.853 (rec:1.496, round:84.357)	b=5.38	count=17000
Total loss:	38.926 (rec:1.437, round:37.489)	b=4.81	count=17500
Total loss:	14.232 (rec:1.479, round:12.753)	b=4.25	count=18000
Total loss:	4.636 (rec:1.446, round:3.190)	b=3.69	count=18500
Total loss:	2.031 (rec:1.500, round:0.531)	b=3.12	count=19000
Total loss:	1.517 (rec:1.488, round:0.029)	b=2.56	count=19500
Total loss:	1.407 (rec:1.407, round:0.000)	b=2.00	count=20000
finished reconstructing layers.1.blocks.0.
reconstructing layers.1.blocks.1 ...
initializing raw input and raw output ...
adaround training for layers.1.blocks.1 ...
wraping quantizers in layers.1.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.587 (rec:1.587, round:0.000)	b=0.00	count=500
Total loss:	1.487 (rec:1.487, round:0.000)	b=0.00	count=1000
Total loss:	1.417 (rec:1.417, round:0.000)	b=0.00	count=1500
Total loss:	1.407 (rec:1.407, round:0.000)	b=0.00	count=2000
Total loss:	1.474 (rec:1.474, round:0.000)	b=0.00	count=2500
Total loss:	1.413 (rec:1.413, round:0.000)	b=0.00	count=3000
Total loss:	1.368 (rec:1.368, round:0.000)	b=0.00	count=3500
Total loss:	6727.645 (rec:1.353, round:6726.292)	b=20.00	count=4000
Total loss:	3539.319 (rec:1.377, round:3537.942)	b=19.44	count=4500
Total loss:	3241.101 (rec:1.313, round:3239.788)	b=18.88	count=5000
Total loss:	3032.598 (rec:1.290, round:3031.308)	b=18.31	count=5500
Total loss:	2853.374 (rec:1.323, round:2852.051)	b=17.75	count=6000
Total loss:	2688.308 (rec:1.333, round:2686.975)	b=17.19	count=6500
Total loss:	2534.324 (rec:1.359, round:2532.965)	b=16.62	count=7000
Total loss:	2386.592 (rec:1.398, round:2385.194)	b=16.06	count=7500
Total loss:	2246.443 (rec:1.355, round:2245.088)	b=15.50	count=8000
Total loss:	2114.384 (rec:1.288, round:2113.095)	b=14.94	count=8500
Total loss:	1983.095 (rec:1.308, round:1981.787)	b=14.38	count=9000
Total loss:	1859.677 (rec:1.340, round:1858.336)	b=13.81	count=9500
Total loss:	1736.644 (rec:1.370, round:1735.274)	b=13.25	count=10000
Total loss:	1616.956 (rec:1.375, round:1615.582)	b=12.69	count=10500
Total loss:	1497.994 (rec:1.349, round:1496.645)	b=12.12	count=11000
Total loss:	1378.980 (rec:1.328, round:1377.652)	b=11.56	count=11500
Total loss:	1261.248 (rec:1.359, round:1259.889)	b=11.00	count=12000
Total loss:	1143.062 (rec:1.388, round:1141.674)	b=10.44	count=12500
Total loss:	1023.849 (rec:1.359, round:1022.490)	b=9.88	count=13000
Total loss:	902.892 (rec:1.386, round:901.505)	b=9.31	count=13500
Total loss:	781.212 (rec:1.324, round:779.888)	b=8.75	count=14000
Total loss:	658.683 (rec:1.446, round:657.237)	b=8.19	count=14500
Total loss:	535.204 (rec:1.366, round:533.838)	b=7.62	count=15000
Total loss:	415.338 (rec:1.394, round:413.944)	b=7.06	count=15500
Total loss:	300.774 (rec:1.357, round:299.418)	b=6.50	count=16000
Total loss:	197.459 (rec:1.411, round:196.048)	b=5.94	count=16500
Total loss:	113.388 (rec:1.389, round:111.999)	b=5.38	count=17000
Total loss:	53.896 (rec:1.432, round:52.463)	b=4.81	count=17500
Total loss:	20.208 (rec:1.419, round:18.789)	b=4.25	count=18000
Total loss:	6.004 (rec:1.415, round:4.589)	b=3.69	count=18500
Total loss:	1.963 (rec:1.382, round:0.581)	b=3.12	count=19000
Total loss:	1.416 (rec:1.394, round:0.021)	b=2.56	count=19500
Total loss:	1.427 (rec:1.427, round:0.000)	b=2.00	count=20000
finished reconstructing layers.1.blocks.1.
reconstructing layers.2.downsample ...
initializing raw input and raw output ...
adaround training for layers.2.downsample ...
wraping quantizers in layers.2.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	2.003 (rec:2.003, round:0.000)	b=0.00	count=500
Total loss:	1.785 (rec:1.785, round:0.000)	b=0.00	count=1000
Total loss:	1.756 (rec:1.756, round:0.000)	b=0.00	count=1500
Total loss:	1.576 (rec:1.576, round:0.000)	b=0.00	count=2000
Total loss:	1.557 (rec:1.557, round:0.000)	b=0.00	count=2500
Total loss:	1.516 (rec:1.516, round:0.000)	b=0.00	count=3000
Total loss:	1.506 (rec:1.506, round:0.000)	b=0.00	count=3500
Total loss:	4222.705 (rec:1.552, round:4221.153)	b=20.00	count=4000
Total loss:	2039.036 (rec:1.437, round:2037.599)	b=19.44	count=4500
Total loss:	1841.298 (rec:1.531, round:1839.766)	b=18.88	count=5000
Total loss:	1699.034 (rec:1.455, round:1697.579)	b=18.31	count=5500
Total loss:	1578.732 (rec:1.541, round:1577.191)	b=17.75	count=6000
Total loss:	1471.658 (rec:1.485, round:1470.173)	b=17.19	count=6500
Total loss:	1374.809 (rec:1.566, round:1373.243)	b=16.62	count=7000
Total loss:	1285.890 (rec:1.579, round:1284.311)	b=16.06	count=7500
Total loss:	1202.830 (rec:1.449, round:1201.381)	b=15.50	count=8000
Total loss:	1124.818 (rec:1.554, round:1123.264)	b=14.94	count=8500
Total loss:	1051.009 (rec:1.498, round:1049.511)	b=14.38	count=9000
Total loss:	979.462 (rec:1.403, round:978.059)	b=13.81	count=9500
Total loss:	910.691 (rec:1.663, round:909.028)	b=13.25	count=10000
Total loss:	843.229 (rec:1.688, round:841.542)	b=12.69	count=10500
Total loss:	775.621 (rec:1.481, round:774.140)	b=12.12	count=11000
Total loss:	709.864 (rec:1.693, round:708.171)	b=11.56	count=11500
Total loss:	645.230 (rec:1.490, round:643.740)	b=11.00	count=12000
Total loss:	579.352 (rec:1.460, round:577.892)	b=10.44	count=12500
Total loss:	514.090 (rec:1.558, round:512.532)	b=9.88	count=13000
Total loss:	447.348 (rec:1.534, round:445.815)	b=9.31	count=13500
Total loss:	380.610 (rec:1.624, round:378.986)	b=8.75	count=14000
Total loss:	315.730 (rec:1.497, round:314.232)	b=8.19	count=14500
Total loss:	250.530 (rec:1.591, round:248.939)	b=7.62	count=15000
Total loss:	190.825 (rec:1.419, round:189.406)	b=7.06	count=15500
Total loss:	137.232 (rec:1.454, round:135.778)	b=6.50	count=16000
Total loss:	90.621 (rec:1.583, round:89.038)	b=5.94	count=16500
Total loss:	53.615 (rec:1.415, round:52.200)	b=5.38	count=17000
Total loss:	28.080 (rec:1.491, round:26.589)	b=4.81	count=17500
Total loss:	12.844 (rec:1.629, round:11.215)	b=4.25	count=18000
Total loss:	5.054 (rec:1.478, round:3.577)	b=3.69	count=18500
Total loss:	2.305 (rec:1.539, round:0.766)	b=3.12	count=19000
Total loss:	1.639 (rec:1.523, round:0.116)	b=2.56	count=19500
Total loss:	1.576 (rec:1.566, round:0.010)	b=2.00	count=20000
finished reconstructing layers.2.downsample.
reconstructing layers.2.blocks.0 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.0 ...
wraping quantizers in layers.2.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.098 (rec:1.098, round:0.000)	b=0.00	count=500
Total loss:	1.073 (rec:1.073, round:0.000)	b=0.00	count=1000
Total loss:	0.951 (rec:0.951, round:0.000)	b=0.00	count=1500
Total loss:	0.966 (rec:0.966, round:0.000)	b=0.00	count=2000
Total loss:	0.915 (rec:0.915, round:0.000)	b=0.00	count=2500
Total loss:	0.865 (rec:0.865, round:0.000)	b=0.00	count=3000
Total loss:	0.860 (rec:0.860, round:0.000)	b=0.00	count=3500
Total loss:	28480.746 (rec:0.830, round:28479.916)	b=20.00	count=4000
Total loss:	14104.018 (rec:0.802, round:14103.215)	b=19.44	count=4500
Total loss:	12990.518 (rec:0.820, round:12989.698)	b=18.88	count=5000
Total loss:	12221.297 (rec:0.811, round:12220.486)	b=18.31	count=5500
Total loss:	11544.816 (rec:0.809, round:11544.008)	b=17.75	count=6000
Total loss:	10913.101 (rec:0.814, round:10912.286)	b=17.19	count=6500
Total loss:	10305.074 (rec:0.790, round:10304.284)	b=16.62	count=7000
Total loss:	9712.998 (rec:0.793, round:9712.205)	b=16.06	count=7500
Total loss:	9134.544 (rec:0.768, round:9133.775)	b=15.50	count=8000
Total loss:	8566.316 (rec:0.765, round:8565.552)	b=14.94	count=8500
Total loss:	8009.940 (rec:0.783, round:8009.157)	b=14.38	count=9000
Total loss:	7463.543 (rec:0.757, round:7462.786)	b=13.81	count=9500
Total loss:	6929.266 (rec:0.772, round:6928.494)	b=13.25	count=10000
Total loss:	6399.872 (rec:0.797, round:6399.075)	b=12.69	count=10500
Total loss:	5882.013 (rec:0.789, round:5881.224)	b=12.12	count=11000
Total loss:	5374.470 (rec:0.782, round:5373.688)	b=11.56	count=11500
Total loss:	4873.012 (rec:0.783, round:4872.228)	b=11.00	count=12000
Total loss:	4383.441 (rec:0.795, round:4382.646)	b=10.44	count=12500
Total loss:	3902.648 (rec:0.797, round:3901.851)	b=9.88	count=13000
Total loss:	3428.854 (rec:0.812, round:3428.041)	b=9.31	count=13500
Total loss:	2966.241 (rec:0.802, round:2965.439)	b=8.75	count=14000
Total loss:	2511.000 (rec:0.769, round:2510.231)	b=8.19	count=14500
Total loss:	2067.148 (rec:0.789, round:2066.359)	b=7.62	count=15000
Total loss:	1641.646 (rec:0.808, round:1640.838)	b=7.06	count=15500
Total loss:	1236.714 (rec:0.801, round:1235.912)	b=6.50	count=16000
Total loss:	854.625 (rec:0.798, round:853.827)	b=5.94	count=16500
Total loss:	492.434 (rec:0.851, round:491.583)	b=5.38	count=17000
Total loss:	186.581 (rec:0.856, round:185.725)	b=4.81	count=17500
Total loss:	46.208 (rec:0.855, round:45.353)	b=4.25	count=18000
Total loss:	9.872 (rec:0.834, round:9.038)	b=3.69	count=18500
Total loss:	2.114 (rec:0.829, round:1.285)	b=3.12	count=19000
Total loss:	0.920 (rec:0.854, round:0.066)	b=2.56	count=19500
Total loss:	0.843 (rec:0.842, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.0.
reconstructing layers.2.blocks.1 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.1 ...
wraping quantizers in layers.2.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.450 (rec:1.450, round:0.000)	b=0.00	count=500
Total loss:	1.354 (rec:1.354, round:0.000)	b=0.00	count=1000
Total loss:	1.279 (rec:1.279, round:0.000)	b=0.00	count=1500
Total loss:	1.271 (rec:1.271, round:0.000)	b=0.00	count=2000
Total loss:	1.230 (rec:1.230, round:0.000)	b=0.00	count=2500
Total loss:	1.228 (rec:1.228, round:0.000)	b=0.00	count=3000
Total loss:	1.226 (rec:1.226, round:0.000)	b=0.00	count=3500
Total loss:	28423.578 (rec:1.202, round:28422.377)	b=20.00	count=4000
Total loss:	14483.227 (rec:1.259, round:14481.967)	b=19.44	count=4500
Total loss:	13366.122 (rec:1.207, round:13364.915)	b=18.88	count=5000
Total loss:	12606.966 (rec:1.247, round:12605.719)	b=18.31	count=5500
Total loss:	11948.876 (rec:1.195, round:11947.682)	b=17.75	count=6000
Total loss:	11337.094 (rec:1.200, round:11335.895)	b=17.19	count=6500
Total loss:	10748.815 (rec:1.207, round:10747.608)	b=16.62	count=7000
Total loss:	10178.725 (rec:1.153, round:10177.571)	b=16.06	count=7500
Total loss:	9626.192 (rec:1.197, round:9624.995)	b=15.50	count=8000
Total loss:	9080.483 (rec:1.228, round:9079.256)	b=14.94	count=8500
Total loss:	8540.284 (rec:1.225, round:8539.059)	b=14.38	count=9000
Total loss:	8006.406 (rec:1.214, round:8005.192)	b=13.81	count=9500
Total loss:	7477.590 (rec:1.227, round:7476.363)	b=13.25	count=10000
Total loss:	6950.205 (rec:1.247, round:6948.958)	b=12.69	count=10500
Total loss:	6427.192 (rec:1.210, round:6425.982)	b=12.12	count=11000
Total loss:	5905.750 (rec:1.219, round:5904.531)	b=11.56	count=11500
Total loss:	5388.054 (rec:1.194, round:5386.860)	b=11.00	count=12000
Total loss:	4868.726 (rec:1.202, round:4867.524)	b=10.44	count=12500
Total loss:	4353.439 (rec:1.208, round:4352.231)	b=9.88	count=13000
Total loss:	3842.040 (rec:1.253, round:3840.788)	b=9.31	count=13500
Total loss:	3330.051 (rec:1.220, round:3328.832)	b=8.75	count=14000
Total loss:	2825.955 (rec:1.252, round:2824.703)	b=8.19	count=14500
Total loss:	2329.230 (rec:1.236, round:2327.994)	b=7.62	count=15000
Total loss:	1846.500 (rec:1.255, round:1845.245)	b=7.06	count=15500
Total loss:	1382.482 (rec:1.283, round:1381.199)	b=6.50	count=16000
Total loss:	950.616 (rec:1.269, round:949.347)	b=5.94	count=16500
Total loss:	555.979 (rec:1.277, round:554.703)	b=5.38	count=17000
Total loss:	219.500 (rec:1.333, round:218.167)	b=4.81	count=17500
Total loss:	53.665 (rec:1.344, round:52.321)	b=4.25	count=18000
Total loss:	10.552 (rec:1.285, round:9.267)	b=3.69	count=18500
Total loss:	2.247 (rec:1.279, round:0.968)	b=3.12	count=19000
Total loss:	1.366 (rec:1.315, round:0.051)	b=2.56	count=19500
Total loss:	1.320 (rec:1.320, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.1.
reconstructing layers.2.blocks.2 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.2 ...
wraping quantizers in layers.2.blocks.2 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.607 (rec:1.607, round:0.000)	b=0.00	count=500
Total loss:	1.532 (rec:1.532, round:0.000)	b=0.00	count=1000
Total loss:	1.385 (rec:1.385, round:0.000)	b=0.00	count=1500
Total loss:	1.321 (rec:1.321, round:0.000)	b=0.00	count=2000
Total loss:	1.332 (rec:1.332, round:0.000)	b=0.00	count=2500
Total loss:	1.291 (rec:1.291, round:0.000)	b=0.00	count=3000
Total loss:	1.277 (rec:1.277, round:0.000)	b=0.00	count=3500
Total loss:	28189.309 (rec:1.254, round:28188.055)	b=20.00	count=4000
Total loss:	14350.768 (rec:1.271, round:14349.496)	b=19.44	count=4500
Total loss:	13228.492 (rec:1.261, round:13227.231)	b=18.88	count=5000
Total loss:	12457.954 (rec:1.269, round:12456.686)	b=18.31	count=5500
Total loss:	11790.017 (rec:1.229, round:11788.787)	b=17.75	count=6000
Total loss:	11169.723 (rec:1.225, round:11168.498)	b=17.19	count=6500
Total loss:	10579.469 (rec:1.192, round:10578.276)	b=16.62	count=7000
Total loss:	10003.221 (rec:1.235, round:10001.985)	b=16.06	count=7500
Total loss:	9443.577 (rec:1.226, round:9442.352)	b=15.50	count=8000
Total loss:	8894.744 (rec:1.224, round:8893.520)	b=14.94	count=8500
Total loss:	8351.145 (rec:1.236, round:8349.909)	b=14.38	count=9000
Total loss:	7819.159 (rec:1.234, round:7817.925)	b=13.81	count=9500
Total loss:	7289.959 (rec:1.248, round:7288.712)	b=13.25	count=10000
Total loss:	6768.324 (rec:1.216, round:6767.107)	b=12.69	count=10500
Total loss:	6251.137 (rec:1.210, round:6249.927)	b=12.12	count=11000
Total loss:	5741.248 (rec:1.263, round:5739.984)	b=11.56	count=11500
Total loss:	5232.062 (rec:1.209, round:5230.854)	b=11.00	count=12000
Total loss:	4720.193 (rec:1.236, round:4718.957)	b=10.44	count=12500
Total loss:	4217.410 (rec:1.236, round:4216.174)	b=9.88	count=13000
Total loss:	3713.867 (rec:1.247, round:3712.621)	b=9.31	count=13500
Total loss:	3217.017 (rec:1.246, round:3215.772)	b=8.75	count=14000
Total loss:	2727.691 (rec:1.283, round:2726.409)	b=8.19	count=14500
Total loss:	2249.313 (rec:1.278, round:2248.035)	b=7.62	count=15000
Total loss:	1780.668 (rec:1.288, round:1779.380)	b=7.06	count=15500
Total loss:	1330.689 (rec:1.271, round:1329.418)	b=6.50	count=16000
Total loss:	910.143 (rec:1.287, round:908.857)	b=5.94	count=16500
Total loss:	526.437 (rec:1.288, round:525.149)	b=5.38	count=17000
Total loss:	209.832 (rec:1.298, round:208.535)	b=4.81	count=17500
Total loss:	55.829 (rec:1.299, round:54.530)	b=4.25	count=18000
Total loss:	11.337 (rec:1.286, round:10.051)	b=3.69	count=18500
Total loss:	2.284 (rec:1.323, round:0.961)	b=3.12	count=19000
Total loss:	1.335 (rec:1.302, round:0.033)	b=2.56	count=19500
Total loss:	1.329 (rec:1.329, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.2.
reconstructing layers.2.blocks.3 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.3 ...
wraping quantizers in layers.2.blocks.3 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.576 (rec:1.576, round:0.000)	b=0.00	count=500
Total loss:	1.465 (rec:1.465, round:0.000)	b=0.00	count=1000
Total loss:	1.367 (rec:1.367, round:0.000)	b=0.00	count=1500
Total loss:	1.331 (rec:1.331, round:0.000)	b=0.00	count=2000
Total loss:	1.280 (rec:1.280, round:0.000)	b=0.00	count=2500
Total loss:	1.226 (rec:1.226, round:0.000)	b=0.00	count=3000
Total loss:	1.215 (rec:1.215, round:0.000)	b=0.00	count=3500
Total loss:	28033.703 (rec:1.217, round:28032.486)	b=20.00	count=4000
Total loss:	14131.926 (rec:1.187, round:14130.738)	b=19.44	count=4500
Total loss:	12999.109 (rec:1.191, round:12997.918)	b=18.88	count=5000
Total loss:	12217.462 (rec:1.188, round:12216.273)	b=18.31	count=5500
Total loss:	11529.557 (rec:1.196, round:11528.361)	b=17.75	count=6000
Total loss:	10890.975 (rec:1.154, round:10889.820)	b=17.19	count=6500
Total loss:	10281.436 (rec:1.160, round:10280.275)	b=16.62	count=7000
Total loss:	9690.706 (rec:1.172, round:9689.534)	b=16.06	count=7500
Total loss:	9118.855 (rec:1.151, round:9117.705)	b=15.50	count=8000
Total loss:	8562.976 (rec:1.165, round:8561.811)	b=14.94	count=8500
Total loss:	8014.073 (rec:1.173, round:8012.899)	b=14.38	count=9000
Total loss:	7475.599 (rec:1.164, round:7474.435)	b=13.81	count=9500
Total loss:	6954.946 (rec:1.184, round:6953.762)	b=13.25	count=10000
Total loss:	6437.849 (rec:1.183, round:6436.666)	b=12.69	count=10500
Total loss:	5927.925 (rec:1.184, round:5926.741)	b=12.12	count=11000
Total loss:	5431.058 (rec:1.159, round:5429.898)	b=11.56	count=11500
Total loss:	4934.909 (rec:1.182, round:4933.728)	b=11.00	count=12000
Total loss:	4448.590 (rec:1.182, round:4447.408)	b=10.44	count=12500
Total loss:	3964.827 (rec:1.183, round:3963.645)	b=9.88	count=13000
Total loss:	3489.027 (rec:1.175, round:3487.852)	b=9.31	count=13500
Total loss:	3020.032 (rec:1.184, round:3018.849)	b=8.75	count=14000
Total loss:	2559.011 (rec:1.188, round:2557.823)	b=8.19	count=14500
Total loss:	2112.282 (rec:1.200, round:2111.082)	b=7.62	count=15000
Total loss:	1671.767 (rec:1.200, round:1670.567)	b=7.06	count=15500
Total loss:	1250.037 (rec:1.210, round:1248.827)	b=6.50	count=16000
Total loss:	853.545 (rec:1.215, round:852.330)	b=5.94	count=16500
Total loss:	487.849 (rec:1.207, round:486.642)	b=5.38	count=17000
Total loss:	193.338 (rec:1.270, round:192.068)	b=4.81	count=17500
Total loss:	53.720 (rec:1.264, round:52.456)	b=4.25	count=18000
Total loss:	11.640 (rec:1.227, round:10.413)	b=3.69	count=18500
Total loss:	2.411 (rec:1.231, round:1.180)	b=3.12	count=19000
Total loss:	1.284 (rec:1.238, round:0.046)	b=2.56	count=19500
Total loss:	1.244 (rec:1.244, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.3.
reconstructing layers.2.blocks.4 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.4 ...
wraping quantizers in layers.2.blocks.4 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.569 (rec:1.569, round:0.000)	b=0.00	count=500
Total loss:	1.461 (rec:1.461, round:0.000)	b=0.00	count=1000
Total loss:	1.408 (rec:1.408, round:0.000)	b=0.00	count=1500
Total loss:	1.364 (rec:1.364, round:0.000)	b=0.00	count=2000
Total loss:	1.322 (rec:1.322, round:0.000)	b=0.00	count=2500
Total loss:	1.290 (rec:1.290, round:0.000)	b=0.00	count=3000
Total loss:	1.273 (rec:1.273, round:0.000)	b=0.00	count=3500
Total loss:	28202.068 (rec:1.254, round:28200.814)	b=20.00	count=4000
Total loss:	14385.269 (rec:1.257, round:14384.012)	b=19.44	count=4500
Total loss:	13263.957 (rec:1.239, round:13262.719)	b=18.88	count=5000
Total loss:	12496.263 (rec:1.245, round:12495.018)	b=18.31	count=5500
Total loss:	11821.162 (rec:1.240, round:11819.922)	b=17.75	count=6000
Total loss:	11193.210 (rec:1.245, round:11191.965)	b=17.19	count=6500
Total loss:	10593.359 (rec:1.224, round:10592.135)	b=16.62	count=7000
Total loss:	10010.390 (rec:1.234, round:10009.155)	b=16.06	count=7500
Total loss:	9446.569 (rec:1.241, round:9445.328)	b=15.50	count=8000
Total loss:	8892.133 (rec:1.221, round:8890.912)	b=14.94	count=8500
Total loss:	8347.338 (rec:1.220, round:8346.118)	b=14.38	count=9000
Total loss:	7812.897 (rec:1.234, round:7811.663)	b=13.81	count=9500
Total loss:	7279.550 (rec:1.216, round:7278.334)	b=13.25	count=10000
Total loss:	6752.911 (rec:1.256, round:6751.655)	b=12.69	count=10500
Total loss:	6230.438 (rec:1.200, round:6229.238)	b=12.12	count=11000
Total loss:	5717.815 (rec:1.218, round:5716.597)	b=11.56	count=11500
Total loss:	5208.486 (rec:1.199, round:5207.288)	b=11.00	count=12000
Total loss:	4704.127 (rec:1.226, round:4702.901)	b=10.44	count=12500
Total loss:	4202.121 (rec:1.205, round:4200.916)	b=9.88	count=13000
Total loss:	3705.106 (rec:1.226, round:3703.880)	b=9.31	count=13500
Total loss:	3218.167 (rec:1.240, round:3216.927)	b=8.75	count=14000
Total loss:	2734.405 (rec:1.226, round:2733.178)	b=8.19	count=14500
Total loss:	2259.193 (rec:1.246, round:2257.947)	b=7.62	count=15000
Total loss:	1798.563 (rec:1.244, round:1797.319)	b=7.06	count=15500
Total loss:	1356.856 (rec:1.247, round:1355.609)	b=6.50	count=16000
Total loss:	933.147 (rec:1.285, round:931.862)	b=5.94	count=16500
Total loss:	540.042 (rec:1.301, round:538.741)	b=5.38	count=17000
Total loss:	227.628 (rec:1.278, round:226.349)	b=4.81	count=17500
Total loss:	68.481 (rec:1.275, round:67.206)	b=4.25	count=18000
Total loss:	14.354 (rec:1.269, round:13.085)	b=3.69	count=18500
Total loss:	2.637 (rec:1.296, round:1.341)	b=3.12	count=19000
Total loss:	1.348 (rec:1.299, round:0.049)	b=2.56	count=19500
Total loss:	1.271 (rec:1.271, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.4.
reconstructing layers.2.blocks.5 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.5 ...
wraping quantizers in layers.2.blocks.5 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.497 (rec:1.497, round:0.000)	b=0.00	count=500
Total loss:	1.443 (rec:1.443, round:0.000)	b=0.00	count=1000
Total loss:	1.375 (rec:1.375, round:0.000)	b=0.00	count=1500
Total loss:	1.326 (rec:1.326, round:0.000)	b=0.00	count=2000
Total loss:	1.311 (rec:1.311, round:0.000)	b=0.00	count=2500
Total loss:	1.241 (rec:1.241, round:0.000)	b=0.00	count=3000
Total loss:	1.259 (rec:1.259, round:0.000)	b=0.00	count=3500
Total loss:	28100.193 (rec:1.252, round:28098.941)	b=20.00	count=4000
Total loss:	14317.583 (rec:1.209, round:14316.374)	b=19.44	count=4500
Total loss:	13191.330 (rec:1.212, round:13190.117)	b=18.88	count=5000
Total loss:	12407.202 (rec:1.193, round:12406.009)	b=18.31	count=5500
Total loss:	11725.741 (rec:1.207, round:11724.534)	b=17.75	count=6000
Total loss:	11094.621 (rec:1.222, round:11093.398)	b=17.19	count=6500
Total loss:	10485.096 (rec:1.196, round:10483.900)	b=16.62	count=7000
Total loss:	9898.319 (rec:1.202, round:9897.117)	b=16.06	count=7500
Total loss:	9329.884 (rec:1.221, round:9328.662)	b=15.50	count=8000
Total loss:	8774.847 (rec:1.192, round:8773.654)	b=14.94	count=8500
Total loss:	8231.943 (rec:1.184, round:8230.760)	b=14.38	count=9000
Total loss:	7694.870 (rec:1.218, round:7693.651)	b=13.81	count=9500
Total loss:	7162.522 (rec:1.212, round:7161.310)	b=13.25	count=10000
Total loss:	6643.341 (rec:1.231, round:6642.109)	b=12.69	count=10500
Total loss:	6127.224 (rec:1.207, round:6126.017)	b=12.12	count=11000
Total loss:	5619.278 (rec:1.194, round:5618.083)	b=11.56	count=11500
Total loss:	5113.283 (rec:1.207, round:5112.076)	b=11.00	count=12000
Total loss:	4611.607 (rec:1.205, round:4610.402)	b=10.44	count=12500
Total loss:	4112.547 (rec:1.225, round:4111.322)	b=9.88	count=13000
Total loss:	3619.083 (rec:1.226, round:3617.857)	b=9.31	count=13500
Total loss:	3132.926 (rec:1.232, round:3131.694)	b=8.75	count=14000
Total loss:	2656.218 (rec:1.226, round:2654.991)	b=8.19	count=14500
Total loss:	2183.824 (rec:1.226, round:2182.598)	b=7.62	count=15000
Total loss:	1723.241 (rec:1.238, round:1722.003)	b=7.06	count=15500
Total loss:	1278.700 (rec:1.254, round:1277.445)	b=6.50	count=16000
Total loss:	866.019 (rec:1.232, round:864.787)	b=5.94	count=16500
Total loss:	503.999 (rec:1.282, round:502.717)	b=5.38	count=17000
Total loss:	230.675 (rec:1.266, round:229.408)	b=4.81	count=17500
Total loss:	78.361 (rec:1.271, round:77.090)	b=4.25	count=18000
Total loss:	17.232 (rec:1.281, round:15.951)	b=3.69	count=18500
Total loss:	3.004 (rec:1.279, round:1.724)	b=3.12	count=19000
Total loss:	1.331 (rec:1.271, round:0.060)	b=2.56	count=19500
Total loss:	1.278 (rec:1.278, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.5.
reconstructing layers.2.blocks.6 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.6 ...
wraping quantizers in layers.2.blocks.6 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.579 (rec:1.579, round:0.000)	b=0.00	count=500
Total loss:	1.469 (rec:1.469, round:0.000)	b=0.00	count=1000
Total loss:	1.428 (rec:1.428, round:0.000)	b=0.00	count=1500
Total loss:	1.382 (rec:1.382, round:0.000)	b=0.00	count=2000
Total loss:	1.339 (rec:1.339, round:0.000)	b=0.00	count=2500
Total loss:	1.321 (rec:1.321, round:0.000)	b=0.00	count=3000
Total loss:	1.283 (rec:1.283, round:0.000)	b=0.00	count=3500
Total loss:	28019.680 (rec:1.273, round:28018.406)	b=20.00	count=4000
Total loss:	14277.604 (rec:1.264, round:14276.340)	b=19.44	count=4500
Total loss:	13152.778 (rec:1.272, round:13151.506)	b=18.88	count=5000
Total loss:	12374.940 (rec:1.281, round:12373.659)	b=18.31	count=5500
Total loss:	11693.262 (rec:1.248, round:11692.014)	b=17.75	count=6000
Total loss:	11064.457 (rec:1.264, round:11063.193)	b=17.19	count=6500
Total loss:	10464.746 (rec:1.249, round:10463.497)	b=16.62	count=7000
Total loss:	9885.301 (rec:1.207, round:9884.094)	b=16.06	count=7500
Total loss:	9317.719 (rec:1.238, round:9316.480)	b=15.50	count=8000
Total loss:	8761.348 (rec:1.254, round:8760.094)	b=14.94	count=8500
Total loss:	8218.900 (rec:1.257, round:8217.644)	b=14.38	count=9000
Total loss:	7683.700 (rec:1.219, round:7682.481)	b=13.81	count=9500
Total loss:	7159.713 (rec:1.242, round:7158.472)	b=13.25	count=10000
Total loss:	6640.045 (rec:1.230, round:6638.815)	b=12.69	count=10500
Total loss:	6125.812 (rec:1.252, round:6124.561)	b=12.12	count=11000
Total loss:	5616.369 (rec:1.213, round:5615.156)	b=11.56	count=11500
Total loss:	5113.037 (rec:1.251, round:5111.786)	b=11.00	count=12000
Total loss:	4614.378 (rec:1.274, round:4613.104)	b=10.44	count=12500
Total loss:	4116.967 (rec:1.258, round:4115.709)	b=9.88	count=13000
Total loss:	3625.291 (rec:1.265, round:3624.025)	b=9.31	count=13500
Total loss:	3135.652 (rec:1.276, round:3134.376)	b=8.75	count=14000
Total loss:	2654.466 (rec:1.282, round:2653.184)	b=8.19	count=14500
Total loss:	2177.564 (rec:1.320, round:2176.244)	b=7.62	count=15000
Total loss:	1712.779 (rec:1.293, round:1711.487)	b=7.06	count=15500
Total loss:	1270.192 (rec:1.324, round:1268.868)	b=6.50	count=16000
Total loss:	858.643 (rec:1.305, round:857.338)	b=5.94	count=16500
Total loss:	501.291 (rec:1.353, round:499.938)	b=5.38	count=17000
Total loss:	233.506 (rec:1.343, round:232.164)	b=4.81	count=17500
Total loss:	77.812 (rec:1.341, round:76.471)	b=4.25	count=18000
Total loss:	15.926 (rec:1.328, round:14.598)	b=3.69	count=18500
Total loss:	2.625 (rec:1.324, round:1.301)	b=3.12	count=19000
Total loss:	1.360 (rec:1.314, round:0.046)	b=2.56	count=19500
Total loss:	1.350 (rec:1.350, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.6.
reconstructing layers.2.blocks.7 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.7 ...
wraping quantizers in layers.2.blocks.7 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.057 (rec:1.057, round:0.000)	b=0.00	count=500
Total loss:	0.884 (rec:0.884, round:0.000)	b=0.00	count=1000
Total loss:	0.811 (rec:0.811, round:0.000)	b=0.00	count=1500
Total loss:	0.766 (rec:0.766, round:0.000)	b=0.00	count=2000
Total loss:	0.730 (rec:0.730, round:0.000)	b=0.00	count=2500
Total loss:	0.684 (rec:0.684, round:0.000)	b=0.00	count=3000
Total loss:	0.666 (rec:0.666, round:0.000)	b=0.00	count=3500
Total loss:	27837.863 (rec:0.650, round:27837.213)	b=20.00	count=4000
Total loss:	13690.869 (rec:0.655, round:13690.214)	b=19.44	count=4500
Total loss:	12547.169 (rec:0.633, round:12546.536)	b=18.88	count=5000
Total loss:	11742.728 (rec:0.624, round:11742.104)	b=18.31	count=5500
Total loss:	11029.324 (rec:0.606, round:11028.718)	b=17.75	count=6000
Total loss:	10359.204 (rec:0.614, round:10358.590)	b=17.19	count=6500
Total loss:	9724.271 (rec:0.602, round:9723.668)	b=16.62	count=7000
Total loss:	9110.832 (rec:0.602, round:9110.230)	b=16.06	count=7500
Total loss:	8520.195 (rec:0.607, round:8519.588)	b=15.50	count=8000
Total loss:	7946.787 (rec:0.595, round:7946.192)	b=14.94	count=8500
Total loss:	7389.830 (rec:0.607, round:7389.224)	b=14.38	count=9000
Total loss:	6846.842 (rec:0.608, round:6846.235)	b=13.81	count=9500
Total loss:	6318.646 (rec:0.617, round:6318.029)	b=13.25	count=10000
Total loss:	5807.654 (rec:0.595, round:5807.059)	b=12.69	count=10500
Total loss:	5311.031 (rec:0.600, round:5310.431)	b=12.12	count=11000
Total loss:	4824.382 (rec:0.593, round:4823.789)	b=11.56	count=11500
Total loss:	4344.822 (rec:0.602, round:4344.220)	b=11.00	count=12000
Total loss:	3876.590 (rec:0.601, round:3875.989)	b=10.44	count=12500
Total loss:	3420.135 (rec:0.614, round:3419.521)	b=9.88	count=13000
Total loss:	2973.372 (rec:0.616, round:2972.756)	b=9.31	count=13500
Total loss:	2536.851 (rec:0.600, round:2536.251)	b=8.75	count=14000
Total loss:	2111.336 (rec:0.623, round:2110.713)	b=8.19	count=14500
Total loss:	1700.057 (rec:0.612, round:1699.445)	b=7.62	count=15000
Total loss:	1304.779 (rec:0.628, round:1304.152)	b=7.06	count=15500
Total loss:	933.973 (rec:0.624, round:933.349)	b=6.50	count=16000
Total loss:	603.813 (rec:0.633, round:603.180)	b=5.94	count=16500
Total loss:	332.479 (rec:0.624, round:331.855)	b=5.38	count=17000
Total loss:	149.417 (rec:0.623, round:148.794)	b=4.81	count=17500
Total loss:	49.013 (rec:0.634, round:48.380)	b=4.25	count=18000
Total loss:	8.965 (rec:0.628, round:8.336)	b=3.69	count=18500
Total loss:	1.185 (rec:0.643, round:0.543)	b=3.12	count=19000
Total loss:	0.659 (rec:0.627, round:0.032)	b=2.56	count=19500
Total loss:	0.632 (rec:0.630, round:0.002)	b=2.00	count=20000
finished reconstructing layers.2.blocks.7.
reconstructing layers.2.blocks.8 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.8 ...
wraping quantizers in layers.2.blocks.8 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.400 (rec:1.400, round:0.000)	b=0.00	count=500
Total loss:	1.122 (rec:1.122, round:0.000)	b=0.00	count=1000
Total loss:	0.925 (rec:0.925, round:0.000)	b=0.00	count=1500
Total loss:	0.832 (rec:0.832, round:0.000)	b=0.00	count=2000
Total loss:	0.801 (rec:0.801, round:0.000)	b=0.00	count=2500
Total loss:	0.726 (rec:0.726, round:0.000)	b=0.00	count=3000
Total loss:	0.681 (rec:0.681, round:0.000)	b=0.00	count=3500
Total loss:	27245.008 (rec:0.660, round:27244.348)	b=20.00	count=4000
Total loss:	13051.935 (rec:0.628, round:13051.307)	b=19.44	count=4500
Total loss:	11933.012 (rec:0.627, round:11932.385)	b=18.88	count=5000
Total loss:	11116.295 (rec:0.615, round:11115.680)	b=18.31	count=5500
Total loss:	10391.587 (rec:0.609, round:10390.979)	b=17.75	count=6000
Total loss:	9721.050 (rec:0.596, round:9720.454)	b=17.19	count=6500
Total loss:	9086.660 (rec:0.582, round:9086.078)	b=16.62	count=7000
Total loss:	8479.439 (rec:0.583, round:8478.856)	b=16.06	count=7500
Total loss:	7900.542 (rec:0.576, round:7899.966)	b=15.50	count=8000
Total loss:	7345.359 (rec:0.593, round:7344.766)	b=14.94	count=8500
Total loss:	6812.681 (rec:0.596, round:6812.085)	b=14.38	count=9000
Total loss:	6303.512 (rec:0.582, round:6302.930)	b=13.81	count=9500
Total loss:	5811.513 (rec:0.596, round:5810.917)	b=13.25	count=10000
Total loss:	5337.700 (rec:0.583, round:5337.117)	b=12.69	count=10500
Total loss:	4876.399 (rec:0.581, round:4875.818)	b=12.12	count=11000
Total loss:	4429.202 (rec:0.593, round:4428.609)	b=11.56	count=11500
Total loss:	3996.276 (rec:0.587, round:3995.689)	b=11.00	count=12000
Total loss:	3569.974 (rec:0.600, round:3569.374)	b=10.44	count=12500
Total loss:	3156.349 (rec:0.581, round:3155.769)	b=9.88	count=13000
Total loss:	2751.665 (rec:0.580, round:2751.085)	b=9.31	count=13500
Total loss:	2362.352 (rec:0.574, round:2361.778)	b=8.75	count=14000
Total loss:	1981.122 (rec:0.592, round:1980.530)	b=8.19	count=14500
Total loss:	1615.253 (rec:0.594, round:1614.659)	b=7.62	count=15000
Total loss:	1262.672 (rec:0.589, round:1262.083)	b=7.06	count=15500
Total loss:	931.218 (rec:0.599, round:930.619)	b=6.50	count=16000
Total loss:	630.229 (rec:0.608, round:629.621)	b=5.94	count=16500
Total loss:	378.373 (rec:0.598, round:377.775)	b=5.38	count=17000
Total loss:	190.486 (rec:0.590, round:189.895)	b=4.81	count=17500
Total loss:	73.440 (rec:0.607, round:72.833)	b=4.25	count=18000
Total loss:	16.087 (rec:0.617, round:15.469)	b=3.69	count=18500
Total loss:	2.153 (rec:0.604, round:1.549)	b=3.12	count=19000
Total loss:	0.665 (rec:0.602, round:0.063)	b=2.56	count=19500
Total loss:	0.594 (rec:0.593, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.8.
reconstructing layers.2.blocks.9 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.9 ...
wraping quantizers in layers.2.blocks.9 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.836 (rec:0.836, round:0.000)	b=0.00	count=500
Total loss:	0.768 (rec:0.768, round:0.000)	b=0.00	count=1000
Total loss:	0.671 (rec:0.671, round:0.000)	b=0.00	count=1500
Total loss:	0.683 (rec:0.683, round:0.000)	b=0.00	count=2000
Total loss:	0.599 (rec:0.599, round:0.000)	b=0.00	count=2500
Total loss:	0.579 (rec:0.579, round:0.000)	b=0.00	count=3000
Total loss:	0.571 (rec:0.571, round:0.000)	b=0.00	count=3500
Total loss:	27310.594 (rec:0.519, round:27310.074)	b=20.00	count=4000
Total loss:	12945.446 (rec:0.512, round:12944.934)	b=19.44	count=4500
Total loss:	11781.733 (rec:0.511, round:11781.223)	b=18.88	count=5000
Total loss:	10924.934 (rec:0.474, round:10924.459)	b=18.31	count=5500
Total loss:	10159.957 (rec:0.456, round:10159.501)	b=17.75	count=6000
Total loss:	9442.630 (rec:0.459, round:9442.171)	b=17.19	count=6500
Total loss:	8767.151 (rec:0.456, round:8766.695)	b=16.62	count=7000
Total loss:	8126.635 (rec:0.448, round:8126.188)	b=16.06	count=7500
Total loss:	7515.900 (rec:0.452, round:7515.448)	b=15.50	count=8000
Total loss:	6937.064 (rec:0.449, round:6936.615)	b=14.94	count=8500
Total loss:	6388.333 (rec:0.452, round:6387.881)	b=14.38	count=9000
Total loss:	5865.103 (rec:0.448, round:5864.655)	b=13.81	count=9500
Total loss:	5362.837 (rec:0.442, round:5362.395)	b=13.25	count=10000
Total loss:	4881.161 (rec:0.462, round:4880.699)	b=12.69	count=10500
Total loss:	4425.599 (rec:0.441, round:4425.158)	b=12.12	count=11000
Total loss:	3989.260 (rec:0.441, round:3988.819)	b=11.56	count=11500
Total loss:	3568.355 (rec:0.452, round:3567.903)	b=11.00	count=12000
Total loss:	3161.952 (rec:0.451, round:3161.501)	b=10.44	count=12500
Total loss:	2769.643 (rec:0.448, round:2769.195)	b=9.88	count=13000
Total loss:	2396.332 (rec:0.453, round:2395.879)	b=9.31	count=13500
Total loss:	2037.595 (rec:0.457, round:2037.139)	b=8.75	count=14000
Total loss:	1690.718 (rec:0.457, round:1690.261)	b=8.19	count=14500
Total loss:	1357.738 (rec:0.446, round:1357.292)	b=7.62	count=15000
Total loss:	1045.782 (rec:0.466, round:1045.316)	b=7.06	count=15500
Total loss:	753.436 (rec:0.467, round:752.969)	b=6.50	count=16000
Total loss:	493.677 (rec:0.456, round:493.221)	b=5.94	count=16500
Total loss:	282.955 (rec:0.459, round:282.497)	b=5.38	count=17000
Total loss:	131.564 (rec:0.465, round:131.099)	b=4.81	count=17500
Total loss:	43.886 (rec:0.454, round:43.433)	b=4.25	count=18000
Total loss:	8.714 (rec:0.471, round:8.243)	b=3.69	count=18500
Total loss:	1.325 (rec:0.460, round:0.866)	b=3.12	count=19000
Total loss:	0.503 (rec:0.452, round:0.051)	b=2.56	count=19500
Total loss:	0.464 (rec:0.464, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.9.
reconstructing layers.2.blocks.10 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.10 ...
wraping quantizers in layers.2.blocks.10 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.990 (rec:0.990, round:0.000)	b=0.00	count=500
Total loss:	0.851 (rec:0.851, round:0.000)	b=0.00	count=1000
Total loss:	0.791 (rec:0.791, round:0.000)	b=0.00	count=1500
Total loss:	0.732 (rec:0.732, round:0.000)	b=0.00	count=2000
Total loss:	0.709 (rec:0.709, round:0.000)	b=0.00	count=2500
Total loss:	0.673 (rec:0.673, round:0.000)	b=0.00	count=3000
Total loss:	0.667 (rec:0.667, round:0.000)	b=0.00	count=3500
Total loss:	27417.842 (rec:0.616, round:27417.227)	b=20.00	count=4000
Total loss:	13143.506 (rec:0.607, round:13142.899)	b=19.44	count=4500
Total loss:	11993.959 (rec:0.573, round:11993.387)	b=18.88	count=5000
Total loss:	11163.037 (rec:0.574, round:11162.463)	b=18.31	count=5500
Total loss:	10422.969 (rec:0.549, round:10422.420)	b=17.75	count=6000
Total loss:	9732.253 (rec:0.561, round:9731.692)	b=17.19	count=6500
Total loss:	9070.531 (rec:0.532, round:9069.999)	b=16.62	count=7000
Total loss:	8441.119 (rec:0.550, round:8440.568)	b=16.06	count=7500
Total loss:	7836.305 (rec:0.528, round:7835.777)	b=15.50	count=8000
Total loss:	7257.066 (rec:0.516, round:7256.550)	b=14.94	count=8500
Total loss:	6692.784 (rec:0.527, round:6692.257)	b=14.38	count=9000
Total loss:	6154.483 (rec:0.523, round:6153.959)	b=13.81	count=9500
Total loss:	5633.620 (rec:0.502, round:5633.118)	b=13.25	count=10000
Total loss:	5134.956 (rec:0.517, round:5134.438)	b=12.69	count=10500
Total loss:	4652.243 (rec:0.528, round:4651.715)	b=12.12	count=11000
Total loss:	4185.404 (rec:0.505, round:4184.898)	b=11.56	count=11500
Total loss:	3734.977 (rec:0.524, round:3734.453)	b=11.00	count=12000
Total loss:	3299.927 (rec:0.520, round:3299.407)	b=10.44	count=12500
Total loss:	2883.776 (rec:0.521, round:2883.255)	b=9.88	count=13000
Total loss:	2485.176 (rec:0.499, round:2484.677)	b=9.31	count=13500
Total loss:	2098.563 (rec:0.518, round:2098.046)	b=8.75	count=14000
Total loss:	1732.177 (rec:0.512, round:1731.665)	b=8.19	count=14500
Total loss:	1385.436 (rec:0.512, round:1384.923)	b=7.62	count=15000
Total loss:	1060.644 (rec:0.514, round:1060.130)	b=7.06	count=15500
Total loss:	764.464 (rec:0.526, round:763.938)	b=6.50	count=16000
Total loss:	502.700 (rec:0.508, round:502.192)	b=5.94	count=16500
Total loss:	288.550 (rec:0.545, round:288.005)	b=5.38	count=17000
Total loss:	133.655 (rec:0.516, round:133.139)	b=4.81	count=17500
Total loss:	43.367 (rec:0.531, round:42.836)	b=4.25	count=18000
Total loss:	8.753 (rec:0.539, round:8.214)	b=3.69	count=18500
Total loss:	1.429 (rec:0.526, round:0.903)	b=3.12	count=19000
Total loss:	0.568 (rec:0.517, round:0.051)	b=2.56	count=19500
Total loss:	0.536 (rec:0.534, round:0.002)	b=2.00	count=20000
finished reconstructing layers.2.blocks.10.
reconstructing layers.2.blocks.11 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.11 ...
wraping quantizers in layers.2.blocks.11 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.309 (rec:1.309, round:0.000)	b=0.00	count=500
Total loss:	1.102 (rec:1.102, round:0.000)	b=0.00	count=1000
Total loss:	1.010 (rec:1.010, round:0.000)	b=0.00	count=1500
Total loss:	0.930 (rec:0.930, round:0.000)	b=0.00	count=2000
Total loss:	0.884 (rec:0.884, round:0.000)	b=0.00	count=2500
Total loss:	0.857 (rec:0.857, round:0.000)	b=0.00	count=3000
Total loss:	0.820 (rec:0.820, round:0.000)	b=0.00	count=3500
Total loss:	27300.152 (rec:0.804, round:27299.348)	b=20.00	count=4000
Total loss:	13262.348 (rec:0.775, round:13261.572)	b=19.44	count=4500
Total loss:	12136.584 (rec:0.757, round:12135.827)	b=18.88	count=5000
Total loss:	11324.130 (rec:0.725, round:11323.405)	b=18.31	count=5500
Total loss:	10601.225 (rec:0.723, round:10600.502)	b=17.75	count=6000
Total loss:	9925.080 (rec:0.696, round:9924.385)	b=17.19	count=6500
Total loss:	9281.378 (rec:0.692, round:9280.687)	b=16.62	count=7000
Total loss:	8666.646 (rec:0.672, round:8665.975)	b=16.06	count=7500
Total loss:	8077.343 (rec:0.669, round:8076.674)	b=15.50	count=8000
Total loss:	7509.053 (rec:0.644, round:7508.409)	b=14.94	count=8500
Total loss:	6965.261 (rec:0.651, round:6964.610)	b=14.38	count=9000
Total loss:	6439.477 (rec:0.647, round:6438.830)	b=13.81	count=9500
Total loss:	5930.147 (rec:0.669, round:5929.478)	b=13.25	count=10000
Total loss:	5429.881 (rec:0.628, round:5429.253)	b=12.69	count=10500
Total loss:	4948.062 (rec:0.656, round:4947.405)	b=12.12	count=11000
Total loss:	4483.630 (rec:0.631, round:4482.999)	b=11.56	count=11500
Total loss:	4032.568 (rec:0.642, round:4031.925)	b=11.00	count=12000
Total loss:	3595.678 (rec:0.638, round:3595.040)	b=10.44	count=12500
Total loss:	3172.140 (rec:0.634, round:3171.506)	b=9.88	count=13000
Total loss:	2760.202 (rec:0.642, round:2759.561)	b=9.31	count=13500
Total loss:	2356.389 (rec:0.656, round:2355.732)	b=8.75	count=14000
Total loss:	1969.281 (rec:0.634, round:1968.647)	b=8.19	count=14500
Total loss:	1594.953 (rec:0.654, round:1594.299)	b=7.62	count=15000
Total loss:	1242.013 (rec:0.661, round:1241.352)	b=7.06	count=15500
Total loss:	909.838 (rec:0.649, round:909.188)	b=6.50	count=16000
Total loss:	609.565 (rec:0.656, round:608.909)	b=5.94	count=16500
Total loss:	355.659 (rec:0.659, round:355.001)	b=5.38	count=17000
Total loss:	167.974 (rec:0.656, round:167.318)	b=4.81	count=17500
Total loss:	57.156 (rec:0.661, round:56.495)	b=4.25	count=18000
Total loss:	12.518 (rec:0.675, round:11.842)	b=3.69	count=18500
Total loss:	1.934 (rec:0.668, round:1.266)	b=3.12	count=19000
Total loss:	0.724 (rec:0.659, round:0.065)	b=2.56	count=19500
Total loss:	0.671 (rec:0.659, round:0.012)	b=2.00	count=20000
finished reconstructing layers.2.blocks.11.
reconstructing layers.2.blocks.12 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.12 ...
wraping quantizers in layers.2.blocks.12 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.176 (rec:1.176, round:0.000)	b=0.00	count=500
Total loss:	0.927 (rec:0.927, round:0.000)	b=0.00	count=1000
Total loss:	0.831 (rec:0.831, round:0.000)	b=0.00	count=1500
Total loss:	0.773 (rec:0.773, round:0.000)	b=0.00	count=2000
Total loss:	0.770 (rec:0.770, round:0.000)	b=0.00	count=2500
Total loss:	0.719 (rec:0.719, round:0.000)	b=0.00	count=3000
Total loss:	0.672 (rec:0.672, round:0.000)	b=0.00	count=3500
Total loss:	27601.018 (rec:0.653, round:27600.365)	b=20.00	count=4000
Total loss:	13541.334 (rec:0.641, round:13540.693)	b=19.44	count=4500
Total loss:	12429.345 (rec:0.647, round:12428.697)	b=18.88	count=5000
Total loss:	11637.054 (rec:0.609, round:11636.445)	b=18.31	count=5500
Total loss:	10931.102 (rec:0.591, round:10930.510)	b=17.75	count=6000
Total loss:	10270.275 (rec:0.602, round:10269.674)	b=17.19	count=6500
Total loss:	9636.581 (rec:0.579, round:9636.002)	b=16.62	count=7000
Total loss:	9025.415 (rec:0.571, round:9024.844)	b=16.06	count=7500
Total loss:	8431.989 (rec:0.579, round:8431.410)	b=15.50	count=8000
Total loss:	7862.181 (rec:0.589, round:7861.592)	b=14.94	count=8500
Total loss:	7303.897 (rec:0.581, round:7303.316)	b=14.38	count=9000
Total loss:	6764.623 (rec:0.574, round:6764.049)	b=13.81	count=9500
Total loss:	6246.370 (rec:0.565, round:6245.804)	b=13.25	count=10000
Total loss:	5740.921 (rec:0.578, round:5740.343)	b=12.69	count=10500
Total loss:	5247.746 (rec:0.580, round:5247.166)	b=12.12	count=11000
Total loss:	4769.833 (rec:0.569, round:4769.264)	b=11.56	count=11500
Total loss:	4301.389 (rec:0.583, round:4300.806)	b=11.00	count=12000
Total loss:	3850.347 (rec:0.581, round:3849.767)	b=10.44	count=12500
Total loss:	3411.252 (rec:0.603, round:3410.649)	b=9.88	count=13000
Total loss:	2977.656 (rec:0.581, round:2977.075)	b=9.31	count=13500
Total loss:	2557.285 (rec:0.595, round:2556.690)	b=8.75	count=14000
Total loss:	2145.736 (rec:0.599, round:2145.137)	b=8.19	count=14500
Total loss:	1751.730 (rec:0.587, round:1751.143)	b=7.62	count=15000
Total loss:	1371.613 (rec:0.585, round:1371.028)	b=7.06	count=15500
Total loss:	1011.175 (rec:0.594, round:1010.581)	b=6.50	count=16000
Total loss:	682.047 (rec:0.608, round:681.440)	b=5.94	count=16500
Total loss:	397.514 (rec:0.609, round:396.906)	b=5.38	count=17000
Total loss:	185.496 (rec:0.593, round:184.903)	b=4.81	count=17500
Total loss:	62.472 (rec:0.618, round:61.855)	b=4.25	count=18000
Total loss:	13.781 (rec:0.609, round:13.172)	b=3.69	count=18500
Total loss:	2.066 (rec:0.611, round:1.454)	b=3.12	count=19000
Total loss:	0.685 (rec:0.614, round:0.071)	b=2.56	count=19500
Total loss:	0.604 (rec:0.603, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.12.
reconstructing layers.2.blocks.13 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.13 ...
wraping quantizers in layers.2.blocks.13 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.244 (rec:1.244, round:0.000)	b=0.00	count=500
Total loss:	1.142 (rec:1.142, round:0.000)	b=0.00	count=1000
Total loss:	1.049 (rec:1.049, round:0.000)	b=0.00	count=1500
Total loss:	0.975 (rec:0.975, round:0.000)	b=0.00	count=2000
Total loss:	0.984 (rec:0.984, round:0.000)	b=0.00	count=2500
Total loss:	0.937 (rec:0.937, round:0.000)	b=0.00	count=3000
Total loss:	0.907 (rec:0.907, round:0.000)	b=0.00	count=3500
Total loss:	28078.510 (rec:0.892, round:28077.617)	b=20.00	count=4000
Total loss:	14188.107 (rec:0.880, round:14187.228)	b=19.44	count=4500
Total loss:	13073.742 (rec:0.889, round:13072.854)	b=18.88	count=5000
Total loss:	12296.136 (rec:0.868, round:12295.269)	b=18.31	count=5500
Total loss:	11612.433 (rec:0.886, round:11611.546)	b=17.75	count=6000
Total loss:	10973.009 (rec:0.863, round:10972.146)	b=17.19	count=6500
Total loss:	10364.441 (rec:0.857, round:10363.584)	b=16.62	count=7000
Total loss:	9775.506 (rec:0.856, round:9774.650)	b=16.06	count=7500
Total loss:	9201.217 (rec:0.867, round:9200.350)	b=15.50	count=8000
Total loss:	8640.207 (rec:0.838, round:8639.369)	b=14.94	count=8500
Total loss:	8089.809 (rec:0.857, round:8088.953)	b=14.38	count=9000
Total loss:	7554.857 (rec:0.875, round:7553.982)	b=13.81	count=9500
Total loss:	7026.148 (rec:0.849, round:7025.300)	b=13.25	count=10000
Total loss:	6508.550 (rec:0.865, round:6507.685)	b=12.69	count=10500
Total loss:	6004.324 (rec:0.872, round:6003.453)	b=12.12	count=11000
Total loss:	5510.277 (rec:0.871, round:5509.406)	b=11.56	count=11500
Total loss:	5020.722 (rec:0.884, round:5019.838)	b=11.00	count=12000
Total loss:	4534.985 (rec:0.881, round:4534.104)	b=10.44	count=12500
Total loss:	4056.247 (rec:0.867, round:4055.379)	b=9.88	count=13000
Total loss:	3581.054 (rec:0.863, round:3580.190)	b=9.31	count=13500
Total loss:	3113.731 (rec:0.928, round:3112.802)	b=8.75	count=14000
Total loss:	2652.694 (rec:0.869, round:2651.825)	b=8.19	count=14500
Total loss:	2201.823 (rec:0.907, round:2200.916)	b=7.62	count=15000
Total loss:	1761.553 (rec:0.911, round:1760.641)	b=7.06	count=15500
Total loss:	1337.342 (rec:0.925, round:1336.417)	b=6.50	count=16000
Total loss:	936.053 (rec:0.918, round:935.135)	b=5.94	count=16500
Total loss:	577.733 (rec:0.956, round:576.778)	b=5.38	count=17000
Total loss:	290.034 (rec:0.928, round:289.107)	b=4.81	count=17500
Total loss:	104.557 (rec:0.920, round:103.637)	b=4.25	count=18000
Total loss:	23.266 (rec:0.925, round:22.342)	b=3.69	count=18500
Total loss:	3.106 (rec:0.945, round:2.161)	b=3.12	count=19000
Total loss:	0.987 (rec:0.923, round:0.064)	b=2.56	count=19500
Total loss:	0.938 (rec:0.938, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.13.
reconstructing layers.2.blocks.14 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.14 ...
wraping quantizers in layers.2.blocks.14 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.457 (rec:1.457, round:0.000)	b=0.00	count=500
Total loss:	1.290 (rec:1.290, round:0.000)	b=0.00	count=1000
Total loss:	1.213 (rec:1.213, round:0.000)	b=0.00	count=1500
Total loss:	1.158 (rec:1.158, round:0.000)	b=0.00	count=2000
Total loss:	1.065 (rec:1.065, round:0.000)	b=0.00	count=2500
Total loss:	1.077 (rec:1.077, round:0.000)	b=0.00	count=3000
Total loss:	1.020 (rec:1.020, round:0.000)	b=0.00	count=3500
Total loss:	28117.447 (rec:0.998, round:28116.449)	b=20.00	count=4000
Total loss:	14361.251 (rec:0.999, round:14360.252)	b=19.44	count=4500
Total loss:	13261.470 (rec:0.999, round:13260.471)	b=18.88	count=5000
Total loss:	12504.938 (rec:1.009, round:12503.929)	b=18.31	count=5500
Total loss:	11844.549 (rec:0.961, round:11843.588)	b=17.75	count=6000
Total loss:	11224.057 (rec:0.933, round:11223.124)	b=17.19	count=6500
Total loss:	10634.050 (rec:1.007, round:10633.043)	b=16.62	count=7000
Total loss:	10059.538 (rec:0.960, round:10058.578)	b=16.06	count=7500
Total loss:	9493.631 (rec:0.964, round:9492.667)	b=15.50	count=8000
Total loss:	8938.472 (rec:0.975, round:8937.497)	b=14.94	count=8500
Total loss:	8387.896 (rec:0.969, round:8386.927)	b=14.38	count=9000
Total loss:	7843.792 (rec:0.957, round:7842.835)	b=13.81	count=9500
Total loss:	7307.985 (rec:0.960, round:7307.025)	b=13.25	count=10000
Total loss:	6773.535 (rec:0.951, round:6772.584)	b=12.69	count=10500
Total loss:	6248.344 (rec:0.972, round:6247.372)	b=12.12	count=11000
Total loss:	5726.223 (rec:0.973, round:5725.250)	b=11.56	count=11500
Total loss:	5205.906 (rec:0.943, round:5204.964)	b=11.00	count=12000
Total loss:	4699.352 (rec:0.960, round:4698.392)	b=10.44	count=12500
Total loss:	4196.302 (rec:0.987, round:4195.315)	b=9.88	count=13000
Total loss:	3701.349 (rec:0.961, round:3700.388)	b=9.31	count=13500
Total loss:	3212.688 (rec:0.970, round:3211.718)	b=8.75	count=14000
Total loss:	2729.180 (rec:0.997, round:2728.183)	b=8.19	count=14500
Total loss:	2261.490 (rec:0.978, round:2260.512)	b=7.62	count=15000
Total loss:	1808.146 (rec:0.959, round:1807.187)	b=7.06	count=15500
Total loss:	1376.770 (rec:0.979, round:1375.791)	b=6.50	count=16000
Total loss:	973.093 (rec:0.985, round:972.107)	b=5.94	count=16500
Total loss:	613.383 (rec:0.990, round:612.393)	b=5.38	count=17000
Total loss:	323.405 (rec:1.012, round:322.393)	b=4.81	count=17500
Total loss:	124.839 (rec:1.049, round:123.790)	b=4.25	count=18000
Total loss:	28.755 (rec:1.002, round:27.753)	b=3.69	count=18500
Total loss:	3.943 (rec:1.023, round:2.920)	b=3.12	count=19000
Total loss:	1.141 (rec:1.049, round:0.092)	b=2.56	count=19500
Total loss:	0.997 (rec:0.997, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.14.
reconstructing layers.2.blocks.15 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.15 ...
wraping quantizers in layers.2.blocks.15 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.449 (rec:1.449, round:0.000)	b=0.00	count=500
Total loss:	1.326 (rec:1.326, round:0.000)	b=0.00	count=1000
Total loss:	1.300 (rec:1.300, round:0.000)	b=0.00	count=1500
Total loss:	1.220 (rec:1.220, round:0.000)	b=0.00	count=2000
Total loss:	1.206 (rec:1.206, round:0.000)	b=0.00	count=2500
Total loss:	1.128 (rec:1.128, round:0.000)	b=0.00	count=3000
Total loss:	1.117 (rec:1.117, round:0.000)	b=0.00	count=3500
Total loss:	28294.814 (rec:1.068, round:28293.746)	b=20.00	count=4000
Total loss:	14555.515 (rec:1.105, round:14554.410)	b=19.44	count=4500
Total loss:	13447.234 (rec:1.092, round:13446.142)	b=18.88	count=5000
Total loss:	12679.359 (rec:1.072, round:12678.287)	b=18.31	count=5500
Total loss:	12009.987 (rec:1.063, round:12008.924)	b=17.75	count=6000
Total loss:	11384.730 (rec:1.047, round:11383.684)	b=17.19	count=6500
Total loss:	10796.644 (rec:1.008, round:10795.636)	b=16.62	count=7000
Total loss:	10220.744 (rec:1.051, round:10219.693)	b=16.06	count=7500
Total loss:	9660.724 (rec:0.997, round:9659.727)	b=15.50	count=8000
Total loss:	9108.446 (rec:1.001, round:9107.445)	b=14.94	count=8500
Total loss:	8558.908 (rec:0.996, round:8557.912)	b=14.38	count=9000
Total loss:	8014.198 (rec:0.995, round:8013.203)	b=13.81	count=9500
Total loss:	7478.870 (rec:1.004, round:7477.866)	b=13.25	count=10000
Total loss:	6948.403 (rec:0.997, round:6947.406)	b=12.69	count=10500
Total loss:	6425.498 (rec:0.988, round:6424.510)	b=12.12	count=11000
Total loss:	5898.637 (rec:0.980, round:5897.657)	b=11.56	count=11500
Total loss:	5383.656 (rec:0.959, round:5382.696)	b=11.00	count=12000
Total loss:	4873.902 (rec:1.004, round:4872.898)	b=10.44	count=12500
Total loss:	4366.809 (rec:1.035, round:4365.773)	b=9.88	count=13000
Total loss:	3863.970 (rec:1.000, round:3862.970)	b=9.31	count=13500
Total loss:	3364.219 (rec:0.997, round:3363.221)	b=8.75	count=14000
Total loss:	2871.844 (rec:1.031, round:2870.813)	b=8.19	count=14500
Total loss:	2384.437 (rec:1.042, round:2383.395)	b=7.62	count=15000
Total loss:	1911.887 (rec:1.039, round:1910.848)	b=7.06	count=15500
Total loss:	1463.236 (rec:1.037, round:1462.199)	b=6.50	count=16000
Total loss:	1043.597 (rec:1.027, round:1042.570)	b=5.94	count=16500
Total loss:	669.012 (rec:1.026, round:667.986)	b=5.38	count=17000
Total loss:	361.858 (rec:1.054, round:360.804)	b=4.81	count=17500
Total loss:	148.466 (rec:1.063, round:147.403)	b=4.25	count=18000
Total loss:	39.107 (rec:1.072, round:38.035)	b=3.69	count=18500
Total loss:	5.493 (rec:1.089, round:4.404)	b=3.12	count=19000
Total loss:	1.207 (rec:1.067, round:0.139)	b=2.56	count=19500
Total loss:	1.059 (rec:1.059, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.15.
reconstructing layers.2.blocks.16 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.16 ...
wraping quantizers in layers.2.blocks.16 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.277 (rec:1.277, round:0.000)	b=0.00	count=500
Total loss:	1.130 (rec:1.130, round:0.000)	b=0.00	count=1000
Total loss:	1.037 (rec:1.037, round:0.000)	b=0.00	count=1500
Total loss:	0.995 (rec:0.995, round:0.000)	b=0.00	count=2000
Total loss:	0.916 (rec:0.916, round:0.000)	b=0.00	count=2500
Total loss:	0.933 (rec:0.933, round:0.000)	b=0.00	count=3000
Total loss:	0.861 (rec:0.861, round:0.000)	b=0.00	count=3500
Total loss:	28043.980 (rec:0.910, round:28043.070)	b=20.00	count=4000
Total loss:	14040.442 (rec:0.882, round:14039.561)	b=19.44	count=4500
Total loss:	12918.558 (rec:0.824, round:12917.734)	b=18.88	count=5000
Total loss:	12134.247 (rec:0.844, round:12133.402)	b=18.31	count=5500
Total loss:	11437.658 (rec:0.824, round:11436.834)	b=17.75	count=6000
Total loss:	10798.919 (rec:0.794, round:10798.125)	b=17.19	count=6500
Total loss:	10184.553 (rec:0.805, round:10183.748)	b=16.62	count=7000
Total loss:	9598.066 (rec:0.786, round:9597.280)	b=16.06	count=7500
Total loss:	9030.020 (rec:0.807, round:9029.213)	b=15.50	count=8000
Total loss:	8475.395 (rec:0.810, round:8474.584)	b=14.94	count=8500
Total loss:	7928.105 (rec:0.806, round:7927.300)	b=14.38	count=9000
Total loss:	7395.296 (rec:0.796, round:7394.500)	b=13.81	count=9500
Total loss:	6876.134 (rec:0.787, round:6875.347)	b=13.25	count=10000
Total loss:	6366.598 (rec:0.778, round:6365.819)	b=12.69	count=10500
Total loss:	5861.620 (rec:0.813, round:5860.807)	b=12.12	count=11000
Total loss:	5366.279 (rec:0.767, round:5365.512)	b=11.56	count=11500
Total loss:	4875.625 (rec:0.746, round:4874.879)	b=11.00	count=12000
Total loss:	4394.755 (rec:0.763, round:4393.991)	b=10.44	count=12500
Total loss:	3916.748 (rec:0.808, round:3915.940)	b=9.88	count=13000
Total loss:	3447.768 (rec:0.774, round:3446.994)	b=9.31	count=13500
Total loss:	2980.981 (rec:0.781, round:2980.201)	b=8.75	count=14000
Total loss:	2526.262 (rec:0.771, round:2525.490)	b=8.19	count=14500
Total loss:	2080.901 (rec:0.762, round:2080.139)	b=7.62	count=15000
Total loss:	1657.938 (rec:0.774, round:1657.164)	b=7.06	count=15500
Total loss:	1253.942 (rec:0.773, round:1253.168)	b=6.50	count=16000
Total loss:	884.341 (rec:0.780, round:883.562)	b=5.94	count=16500
Total loss:	561.923 (rec:0.845, round:561.078)	b=5.38	count=17000
Total loss:	302.840 (rec:0.777, round:302.063)	b=4.81	count=17500
Total loss:	123.884 (rec:0.808, round:123.076)	b=4.25	count=18000
Total loss:	33.789 (rec:0.783, round:33.006)	b=3.69	count=18500
Total loss:	5.560 (rec:0.794, round:4.766)	b=3.12	count=19000
Total loss:	1.146 (rec:0.796, round:0.349)	b=2.56	count=19500
Total loss:	0.792 (rec:0.783, round:0.009)	b=2.00	count=20000
finished reconstructing layers.2.blocks.16.
reconstructing layers.2.blocks.17 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.17 ...
wraping quantizers in layers.2.blocks.17 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.869 (rec:0.869, round:0.000)	b=0.00	count=500
Total loss:	0.669 (rec:0.669, round:0.000)	b=0.00	count=1000
Total loss:	0.617 (rec:0.617, round:0.000)	b=0.00	count=1500
Total loss:	0.597 (rec:0.597, round:0.000)	b=0.00	count=2000
Total loss:	0.553 (rec:0.553, round:0.000)	b=0.00	count=2500
Total loss:	0.536 (rec:0.536, round:0.000)	b=0.00	count=3000
Total loss:	0.536 (rec:0.536, round:0.000)	b=0.00	count=3500
Total loss:	27636.514 (rec:0.502, round:27636.012)	b=20.00	count=4000
Total loss:	12912.991 (rec:0.500, round:12912.491)	b=19.44	count=4500
Total loss:	11774.521 (rec:0.482, round:11774.038)	b=18.88	count=5000
Total loss:	10964.780 (rec:0.491, round:10964.290)	b=18.31	count=5500
Total loss:	10258.289 (rec:0.475, round:10257.814)	b=17.75	count=6000
Total loss:	9599.559 (rec:0.476, round:9599.082)	b=17.19	count=6500
Total loss:	8978.707 (rec:0.455, round:8978.252)	b=16.62	count=7000
Total loss:	8383.919 (rec:0.482, round:8383.438)	b=16.06	count=7500
Total loss:	7808.003 (rec:0.460, round:7807.543)	b=15.50	count=8000
Total loss:	7252.680 (rec:0.468, round:7252.212)	b=14.94	count=8500
Total loss:	6710.215 (rec:0.439, round:6709.776)	b=14.38	count=9000
Total loss:	6184.171 (rec:0.462, round:6183.709)	b=13.81	count=9500
Total loss:	5676.659 (rec:0.451, round:5676.208)	b=13.25	count=10000
Total loss:	5185.531 (rec:0.450, round:5185.081)	b=12.69	count=10500
Total loss:	4710.074 (rec:0.460, round:4709.614)	b=12.12	count=11000
Total loss:	4244.701 (rec:0.445, round:4244.256)	b=11.56	count=11500
Total loss:	3790.773 (rec:0.450, round:3790.323)	b=11.00	count=12000
Total loss:	3357.603 (rec:0.425, round:3357.178)	b=10.44	count=12500
Total loss:	2936.851 (rec:0.440, round:2936.411)	b=9.88	count=13000
Total loss:	2532.698 (rec:0.442, round:2532.256)	b=9.31	count=13500
Total loss:	2145.627 (rec:0.452, round:2145.176)	b=8.75	count=14000
Total loss:	1776.112 (rec:0.454, round:1775.658)	b=8.19	count=14500
Total loss:	1424.562 (rec:0.444, round:1424.119)	b=7.62	count=15000
Total loss:	1096.253 (rec:0.449, round:1095.805)	b=7.06	count=15500
Total loss:	798.092 (rec:0.442, round:797.650)	b=6.50	count=16000
Total loss:	537.539 (rec:0.437, round:537.103)	b=5.94	count=16500
Total loss:	318.624 (rec:0.453, round:318.170)	b=5.38	count=17000
Total loss:	152.088 (rec:0.446, round:151.642)	b=4.81	count=17500
Total loss:	50.147 (rec:0.457, round:49.690)	b=4.25	count=18000
Total loss:	10.246 (rec:0.476, round:9.771)	b=3.69	count=18500
Total loss:	1.628 (rec:0.449, round:1.178)	b=3.12	count=19000
Total loss:	0.542 (rec:0.456, round:0.086)	b=2.56	count=19500
Total loss:	0.448 (rec:0.446, round:0.003)	b=2.00	count=20000
finished reconstructing layers.2.blocks.17.
reconstructing layers.3.downsample ...
initializing raw input and raw output ...
adaround training for layers.3.downsample ...
wraping quantizers in layers.3.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.409 (rec:1.409, round:0.000)	b=0.00	count=500
Total loss:	1.076 (rec:1.076, round:0.000)	b=0.00	count=1000
Total loss:	1.011 (rec:1.011, round:0.000)	b=0.00	count=1500
Total loss:	0.880 (rec:0.880, round:0.000)	b=0.00	count=2000
Total loss:	0.783 (rec:0.783, round:0.000)	b=0.00	count=2500
Total loss:	0.711 (rec:0.711, round:0.000)	b=0.00	count=3000
Total loss:	0.734 (rec:0.734, round:0.000)	b=0.00	count=3500
Total loss:	18366.055 (rec:0.678, round:18365.377)	b=20.00	count=4000
Total loss:	8469.191 (rec:0.683, round:8468.509)	b=19.44	count=4500
Total loss:	7749.748 (rec:0.642, round:7749.106)	b=18.88	count=5000
Total loss:	7252.031 (rec:0.663, round:7251.368)	b=18.31	count=5500
Total loss:	6809.698 (rec:0.633, round:6809.065)	b=17.75	count=6000
Total loss:	6394.386 (rec:0.602, round:6393.784)	b=17.19	count=6500
Total loss:	6006.196 (rec:0.630, round:6005.565)	b=16.62	count=7000
Total loss:	5630.086 (rec:0.661, round:5629.425)	b=16.06	count=7500
Total loss:	5271.964 (rec:0.613, round:5271.351)	b=15.50	count=8000
Total loss:	4925.676 (rec:0.637, round:4925.040)	b=14.94	count=8500
Total loss:	4587.389 (rec:0.656, round:4586.732)	b=14.38	count=9000
Total loss:	4263.475 (rec:0.660, round:4262.815)	b=13.81	count=9500
Total loss:	3946.491 (rec:0.632, round:3945.859)	b=13.25	count=10000
Total loss:	3635.815 (rec:0.671, round:3635.144)	b=12.69	count=10500
Total loss:	3334.251 (rec:0.663, round:3333.587)	b=12.12	count=11000
Total loss:	3037.993 (rec:0.620, round:3037.373)	b=11.56	count=11500
Total loss:	2748.769 (rec:0.643, round:2748.126)	b=11.00	count=12000
Total loss:	2464.564 (rec:0.648, round:2463.916)	b=10.44	count=12500
Total loss:	2184.256 (rec:0.650, round:2183.606)	b=9.88	count=13000
Total loss:	1909.169 (rec:0.651, round:1908.518)	b=9.31	count=13500
Total loss:	1641.760 (rec:0.614, round:1641.145)	b=8.75	count=14000
Total loss:	1374.579 (rec:0.662, round:1373.917)	b=8.19	count=14500
Total loss:	1113.346 (rec:0.628, round:1112.718)	b=7.62	count=15000
Total loss:	857.562 (rec:0.667, round:856.895)	b=7.06	count=15500
Total loss:	614.091 (rec:0.612, round:613.479)	b=6.50	count=16000
Total loss:	388.856 (rec:0.628, round:388.228)	b=5.94	count=16500
Total loss:	208.378 (rec:0.625, round:207.753)	b=5.38	count=17000
Total loss:	87.854 (rec:0.656, round:87.198)	b=4.81	count=17500
Total loss:	26.641 (rec:0.652, round:25.989)	b=4.25	count=18000
Total loss:	5.287 (rec:0.636, round:4.651)	b=3.69	count=18500
Total loss:	1.089 (rec:0.664, round:0.424)	b=3.12	count=19000
Total loss:	0.650 (rec:0.625, round:0.025)	b=2.56	count=19500
Total loss:	0.668 (rec:0.668, round:0.001)	b=2.00	count=20000
finished reconstructing layers.3.downsample.
reconstructing layers.3.blocks.0 ...
initializing raw input and raw output ...
adaround training for layers.3.blocks.0 ...
wraping quantizers in layers.3.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.841 (rec:0.841, round:0.000)	b=0.00	count=500
Total loss:	0.677 (rec:0.677, round:0.000)	b=0.00	count=1000
Total loss:	0.575 (rec:0.575, round:0.000)	b=0.00	count=1500
Total loss:	0.525 (rec:0.525, round:0.000)	b=0.00	count=2000
Total loss:	0.528 (rec:0.528, round:0.000)	b=0.00	count=2500
Total loss:	0.447 (rec:0.447, round:0.000)	b=0.00	count=3000
Total loss:	0.424 (rec:0.424, round:0.000)	b=0.00	count=3500
Total loss:	115920.961 (rec:0.435, round:115920.523)	b=20.00	count=4000
Total loss:	55104.824 (rec:0.418, round:55104.406)	b=19.44	count=4500
Total loss:	50832.730 (rec:0.380, round:50832.352)	b=18.88	count=5000
Total loss:	47928.883 (rec:0.374, round:47928.508)	b=18.31	count=5500
Total loss:	45347.816 (rec:0.363, round:45347.453)	b=17.75	count=6000
Total loss:	42884.906 (rec:0.361, round:42884.547)	b=17.19	count=6500
Total loss:	40483.887 (rec:0.362, round:40483.523)	b=16.62	count=7000
Total loss:	38106.953 (rec:0.354, round:38106.602)	b=16.06	count=7500
Total loss:	35760.035 (rec:0.345, round:35759.691)	b=15.50	count=8000
Total loss:	33444.637 (rec:0.380, round:33444.258)	b=14.94	count=8500
Total loss:	31141.586 (rec:0.325, round:31141.262)	b=14.38	count=9000
Total loss:	28858.119 (rec:0.345, round:28857.773)	b=13.81	count=9500
Total loss:	26579.375 (rec:0.338, round:26579.037)	b=13.25	count=10000
Total loss:	24323.035 (rec:0.341, round:24322.693)	b=12.69	count=10500
Total loss:	22106.289 (rec:0.313, round:22105.977)	b=12.12	count=11000
Total loss:	19929.508 (rec:0.328, round:19929.180)	b=11.56	count=11500
Total loss:	17804.467 (rec:0.306, round:17804.162)	b=11.00	count=12000
Total loss:	15742.616 (rec:0.337, round:15742.279)	b=10.44	count=12500
Total loss:	13745.017 (rec:0.313, round:13744.703)	b=9.88	count=13000
Total loss:	11815.639 (rec:0.328, round:11815.311)	b=9.31	count=13500
Total loss:	9956.014 (rec:0.308, round:9955.705)	b=8.75	count=14000
Total loss:	8195.116 (rec:0.316, round:8194.800)	b=8.19	count=14500
Total loss:	6536.331 (rec:0.350, round:6535.981)	b=7.62	count=15000
Total loss:	4996.310 (rec:0.309, round:4996.001)	b=7.06	count=15500
Total loss:	3594.937 (rec:0.333, round:3594.604)	b=6.50	count=16000
Total loss:	2372.491 (rec:0.340, round:2372.151)	b=5.94	count=16500
Total loss:	1370.036 (rec:0.314, round:1369.722)	b=5.38	count=17000
Total loss:	617.634 (rec:0.309, round:617.326)	b=4.81	count=17500
Total loss:	162.845 (rec:0.341, round:162.504)	b=4.25	count=18000
Total loss:	26.641 (rec:0.324, round:26.317)	b=3.69	count=18500
Total loss:	2.881 (rec:0.352, round:2.530)	b=3.12	count=19000
Total loss:	0.460 (rec:0.329, round:0.131)	b=2.56	count=19500
Total loss:	0.330 (rec:0.328, round:0.003)	b=2.00	count=20000
finished reconstructing layers.3.blocks.0.
reconstructing layers.3.blocks.1 ...
initializing raw input and raw output ...
adaround training for layers.3.blocks.1 ...
wraping quantizers in layers.3.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.889 (rec:0.889, round:0.000)	b=0.00	count=500
Total loss:	0.813 (rec:0.813, round:0.000)	b=0.00	count=1000
Total loss:	0.662 (rec:0.662, round:0.000)	b=0.00	count=1500
Total loss:	0.565 (rec:0.565, round:0.000)	b=0.00	count=2000
Total loss:	0.520 (rec:0.520, round:0.000)	b=0.00	count=2500
Total loss:	0.496 (rec:0.496, round:0.000)	b=0.00	count=3000
Total loss:	0.446 (rec:0.446, round:0.000)	b=0.00	count=3500
Total loss:	115875.047 (rec:0.468, round:115874.578)	b=20.00	count=4000
Total loss:	54476.465 (rec:0.410, round:54476.055)	b=19.44	count=4500
Total loss:	50103.039 (rec:0.383, round:50102.656)	b=18.88	count=5000
Total loss:	47039.605 (rec:0.374, round:47039.230)	b=18.31	count=5500
Total loss:	44267.379 (rec:0.351, round:44267.027)	b=17.75	count=6000
Total loss:	41612.758 (rec:0.377, round:41612.383)	b=17.19	count=6500
Total loss:	39017.129 (rec:0.347, round:39016.781)	b=16.62	count=7000
Total loss:	36449.078 (rec:0.336, round:36448.742)	b=16.06	count=7500
Total loss:	33905.891 (rec:0.329, round:33905.562)	b=15.50	count=8000
Total loss:	31411.939 (rec:0.303, round:31411.637)	b=14.94	count=8500
Total loss:	28969.389 (rec:0.294, round:28969.094)	b=14.38	count=9000
Total loss:	26572.998 (rec:0.299, round:26572.699)	b=13.81	count=9500
Total loss:	24235.758 (rec:0.280, round:24235.477)	b=13.25	count=10000
Total loss:	21964.830 (rec:0.279, round:21964.551)	b=12.69	count=10500
Total loss:	19771.484 (rec:0.283, round:19771.201)	b=12.12	count=11000
Total loss:	17649.340 (rec:0.265, round:17649.074)	b=11.56	count=11500
Total loss:	15601.397 (rec:0.259, round:15601.139)	b=11.00	count=12000
Total loss:	13630.083 (rec:0.325, round:13629.759)	b=10.44	count=12500
Total loss:	11759.127 (rec:0.271, round:11758.855)	b=9.88	count=13000
Total loss:	9985.044 (rec:0.286, round:9984.758)	b=9.31	count=13500
Total loss:	8312.272 (rec:0.253, round:8312.020)	b=8.75	count=14000
Total loss:	6747.108 (rec:0.255, round:6746.853)	b=8.19	count=14500
Total loss:	5297.450 (rec:0.284, round:5297.166)	b=7.62	count=15000
Total loss:	3980.930 (rec:0.274, round:3980.656)	b=7.06	count=15500
Total loss:	2806.553 (rec:0.259, round:2806.294)	b=6.50	count=16000
Total loss:	1790.245 (rec:0.280, round:1789.965)	b=5.94	count=16500
Total loss:	980.270 (rec:0.287, round:979.983)	b=5.38	count=17000
Total loss:	421.768 (rec:0.263, round:421.505)	b=4.81	count=17500
Total loss:	126.782 (rec:0.258, round:126.524)	b=4.25	count=18000
Total loss:	20.509 (rec:0.263, round:20.246)	b=3.69	count=18500
Total loss:	2.303 (rec:0.283, round:2.021)	b=3.12	count=19000
Total loss:	0.383 (rec:0.262, round:0.121)	b=2.56	count=19500
Total loss:	0.271 (rec:0.270, round:0.001)	b=2.00	count=20000
finished reconstructing layers.3.blocks.1.
reconstructing head ...
initializing raw input and raw output ...
adaround training for head ...
wraping quantizers in head ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.127 (rec:1.127, round:0.000)	b=0.00	count=500
Total loss:	0.841 (rec:0.841, round:0.000)	b=0.00	count=1000
Total loss:	0.427 (rec:0.427, round:0.000)	b=0.00	count=1500
Total loss:	0.411 (rec:0.411, round:0.000)	b=0.00	count=2000
Total loss:	0.407 (rec:0.407, round:0.000)	b=0.00	count=2500
Total loss:	0.404 (rec:0.404, round:0.000)	b=0.00	count=3000
Total loss:	0.390 (rec:0.390, round:0.000)	b=0.00	count=3500
Total loss:	9602.718 (rec:0.450, round:9602.268)	b=20.00	count=4000
Total loss:	5781.433 (rec:0.464, round:5780.969)	b=19.44	count=4500
Total loss:	5410.980 (rec:0.464, round:5410.516)	b=18.88	count=5000
Total loss:	5166.854 (rec:0.365, round:5166.489)	b=18.31	count=5500
Total loss:	4962.125 (rec:0.369, round:4961.756)	b=17.75	count=6000
Total loss:	4776.789 (rec:0.334, round:4776.455)	b=17.19	count=6500
Total loss:	4596.494 (rec:0.437, round:4596.057)	b=16.62	count=7000
Total loss:	4421.334 (rec:0.458, round:4420.876)	b=16.06	count=7500
Total loss:	4246.133 (rec:0.410, round:4245.723)	b=15.50	count=8000
Total loss:	4070.717 (rec:0.323, round:4070.394)	b=14.94	count=8500
Total loss:	3893.628 (rec:0.309, round:3893.320)	b=14.38	count=9000
Total loss:	3713.863 (rec:0.362, round:3713.501)	b=13.81	count=9500
Total loss:	3529.391 (rec:0.305, round:3529.086)	b=13.25	count=10000
Total loss:	3343.341 (rec:0.318, round:3343.023)	b=12.69	count=10500
Total loss:	3154.021 (rec:0.327, round:3153.695)	b=12.12	count=11000
Total loss:	2957.867 (rec:0.320, round:2957.547)	b=11.56	count=11500
Total loss:	2760.021 (rec:0.339, round:2759.682)	b=11.00	count=12000
Total loss:	2562.681 (rec:0.283, round:2562.398)	b=10.44	count=12500
Total loss:	2362.848 (rec:0.364, round:2362.484)	b=9.88	count=13000
Total loss:	2157.754 (rec:0.263, round:2157.491)	b=9.31	count=13500
Total loss:	1951.221 (rec:0.236, round:1950.985)	b=8.75	count=14000
Total loss:	1742.544 (rec:0.281, round:1742.263)	b=8.19	count=14500
Total loss:	1528.811 (rec:0.319, round:1528.492)	b=7.62	count=15000
Total loss:	1313.739 (rec:0.294, round:1313.446)	b=7.06	count=15500
Total loss:	1095.966 (rec:0.282, round:1095.684)	b=6.50	count=16000
Total loss:	881.982 (rec:0.316, round:881.666)	b=5.94	count=16500
Total loss:	673.171 (rec:0.309, round:672.862)	b=5.38	count=17000
Total loss:	474.939 (rec:0.368, round:474.571)	b=4.81	count=17500
Total loss:	292.774 (rec:0.317, round:292.457)	b=4.25	count=18000
Total loss:	137.650 (rec:0.337, round:137.312)	b=3.69	count=18500
Total loss:	39.241 (rec:0.376, round:38.865)	b=3.12	count=19000
Total loss:	5.470 (rec:0.386, round:5.084)	b=2.56	count=19500
Total loss:	0.705 (rec:0.414, round:0.291)	b=2.00	count=20000
finished reconstructing head.
2025-09-08 20:36:27 - mse guided block reconstruction finished.
Saving checkpoint to ./checkpoint/quant_result/20250908_1606/swin_base_w4_a4_optimsize_1024_mse_rinp.pth
Validating on calibration set after block reconstruction ...
Test: [0/32]	Time 0.599 (0.599)	Loss 0.7065 (0.7065)	Prec@1 84.375 (84.375)	Prec@5 96.875 (96.875)
Test: [10/32]	Time 0.097 (0.143)	Loss 0.7023 (0.8899)	Prec@1 90.625 (83.523)	Prec@5 96.875 (95.170)
Test: [20/32]	Time 0.097 (0.121)	Loss 0.6656 (0.8296)	Prec@1 84.375 (84.970)	Prec@5 100.000 (96.577)
Test: [30/32]	Time 0.097 (0.113)	Loss 0.9041 (0.8417)	Prec@1 81.250 (85.081)	Prec@5 93.750 (96.270)
 * Prec@1 85.156 Prec@5 96.289 Loss 0.837 Time 3.741
Validating on test set after block reconstruction ...
Test: [0/100]	Time 5.674 (5.674)	Loss 0.6154 (0.6154)	Prec@1 90.000 (90.000)	Prec@5 98.400 (98.400)
Test: [10/100]	Time 2.369 (2.669)	Loss 0.6254 (0.7055)	Prec@1 90.000 (86.673)	Prec@5 98.200 (97.927)
Test: [20/100]	Time 2.375 (2.527)	Loss 0.7418 (0.7233)	Prec@1 82.400 (85.667)	Prec@5 97.800 (97.581)
Test: [30/100]	Time 2.373 (2.478)	Loss 0.6444 (0.7356)	Prec@1 87.600 (84.981)	Prec@5 99.000 (97.548)
Test: [40/100]	Time 2.376 (2.453)	Loss 0.9177 (0.7395)	Prec@1 78.600 (85.180)	Prec@5 96.000 (97.590)
Test: [50/100]	Time 2.381 (2.438)	Loss 1.1092 (0.7781)	Prec@1 75.600 (83.945)	Prec@5 93.400 (97.216)
Test: [60/100]	Time 2.382 (2.429)	Loss 0.8363 (0.7824)	Prec@1 84.400 (83.807)	Prec@5 95.600 (97.102)
Test: [70/100]	Time 2.374 (2.422)	Loss 0.8698 (0.7981)	Prec@1 81.800 (83.262)	Prec@5 96.600 (96.901)
Test: [80/100]	Time 2.377 (2.417)	Loss 0.6992 (0.8043)	Prec@1 85.600 (83.126)	Prec@5 98.400 (96.751)
Test: [90/100]	Time 2.378 (2.412)	Loss 1.1356 (0.8212)	Prec@1 73.400 (82.560)	Prec@5 93.600 (96.633)
 * Prec@1 82.606 Prec@5 96.666 Loss 0.819 Time 241.199
2025-09-08 20:40:32 - finished the process.
Extracting logits from quantized and full-precision models...
Testing combinations:
  Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
  Cluster numbers: [8, 16, 32, 64, 128, 256]
  PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.61%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.61%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.62%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.62%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.61%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.61%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.62%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.62%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.61%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.61%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.62%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.62%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.60%
[Alpha=0.10] Top-5 Accuracy: 96.66%
Result: Top-1: 82.60%, Top-5: 96.66%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.61%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.61%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.62%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.62%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.61%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.61%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.59%
[Alpha=0.10] Top-5 Accuracy: 96.71%
Result: Top-1: 82.59%, Top-5: 96.71%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.58%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.58%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.58%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.58%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.62%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.62%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.65%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.65%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.62%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.62%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.61%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.61%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.58%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.58%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.65%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.65%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.63%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.63%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.56%
[Alpha=0.10] Top-5 Accuracy: 96.70%
Result: Top-1: 82.56%, Top-5: 96.70%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.63%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.63%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.60%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.60%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.58%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.58%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.62%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.62%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.64%
[Alpha=0.10] Top-5 Accuracy: 96.69%
Result: Top-1: 82.64%, Top-5: 96.69%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.63%
[Alpha=0.10] Top-5 Accuracy: 96.69%
Result: Top-1: 82.63%, Top-5: 96.69%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.63%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.63%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.60%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.60%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.63%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.63%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.47%
[Alpha=0.10] Top-5 Accuracy: 96.69%
Result: Top-1: 82.47%, Top-5: 96.69%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.60%
[Alpha=0.10] Top-5 Accuracy: 96.69%
Result: Top-1: 82.60%, Top-5: 96.69%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.60%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.60%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.57%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.57%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.58%
[Alpha=0.10] Top-5 Accuracy: 96.71%
Result: Top-1: 82.58%, Top-5: 96.71%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.57%
[Alpha=0.10] Top-5 Accuracy: 96.69%
Result: Top-1: 82.57%, Top-5: 96.69%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.58%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.58%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.58%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.58%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.56%
[Alpha=0.10] Top-5 Accuracy: 96.69%
Result: Top-1: 82.56%, Top-5: 96.69%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.58%
[Alpha=0.10] Top-5 Accuracy: 96.66%
Result: Top-1: 82.58%, Top-5: 96.66%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.12%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.12%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.48%
[Alpha=0.10] Top-5 Accuracy: 96.63%
Result: Top-1: 82.48%, Top-5: 96.63%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.56%
[Alpha=0.10] Top-5 Accuracy: 96.65%
Result: Top-1: 82.56%, Top-5: 96.65%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.53%
[Alpha=0.10] Top-5 Accuracy: 96.63%
Result: Top-1: 82.53%, Top-5: 96.63%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.43%
[Alpha=0.10] Top-5 Accuracy: 96.65%
Result: Top-1: 82.43%, Top-5: 96.65%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.53%
[Alpha=0.10] Top-5 Accuracy: 96.66%
Result: Top-1: 82.53%, Top-5: 96.66%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.51%
[Alpha=0.10] Top-5 Accuracy: 96.65%
Result: Top-1: 82.51%, Top-5: 96.65%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.51%
[Alpha=0.10] Top-5 Accuracy: 96.62%
Result: Top-1: 82.51%, Top-5: 96.62%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.58%
[Alpha=0.10] Top-5 Accuracy: 96.67%
Result: Top-1: 82.58%, Top-5: 96.67%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.58%
[Alpha=0.10] Top-5 Accuracy: 96.68%
Result: Top-1: 82.58%, Top-5: 96.68%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 77.78%
[Alpha=0.10] Top-5 Accuracy: 95.68%
Result: Top-1: 77.78%, Top-5: 95.68%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 81.42%
[Alpha=0.10] Top-5 Accuracy: 96.18%
Result: Top-1: 81.42%, Top-5: 96.18%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 81.91%
[Alpha=0.10] Top-5 Accuracy: 96.49%
Result: Top-1: 81.91%, Top-5: 96.49%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.32%
[Alpha=0.10] Top-5 Accuracy: 96.58%
Result: Top-1: 82.32%, Top-5: 96.58%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.29%
[Alpha=0.10] Top-5 Accuracy: 96.55%
Result: Top-1: 82.29%, Top-5: 96.55%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.13%
[Alpha=0.10] Top-5 Accuracy: 96.57%
Result: Top-1: 82.13%, Top-5: 96.57%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.26%
[Alpha=0.10] Top-5 Accuracy: 96.58%
Result: Top-1: 82.26%, Top-5: 96.58%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.48%
[Alpha=0.10] Top-5 Accuracy: 96.65%
Result: Top-1: 82.48%, Top-5: 96.65%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.46%
[Alpha=0.10] Top-5 Accuracy: 96.66%
Result: Top-1: 82.46%, Top-5: 96.66%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.41%
[Alpha=0.10] Top-5 Accuracy: 96.64%
Result: Top-1: 82.41%, Top-5: 96.64%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.63%
[Alpha=0.20] Top-5 Accuracy: 96.70%
Result: Top-1: 82.63%, Top-5: 96.70%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.61%
[Alpha=0.20] Top-5 Accuracy: 96.68%
Result: Top-1: 82.61%, Top-5: 96.68%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.58%
[Alpha=0.20] Top-5 Accuracy: 96.68%
Result: Top-1: 82.58%, Top-5: 96.68%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.60%
[Alpha=0.20] Top-5 Accuracy: 96.70%
Result: Top-1: 82.60%, Top-5: 96.70%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.58%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.58%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.60%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.60%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.60%
[Alpha=0.20] Top-5 Accuracy: 96.68%
Result: Top-1: 82.60%, Top-5: 96.68%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.58%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.58%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.61%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.61%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.59%
[Alpha=0.20] Top-5 Accuracy: 96.68%
Result: Top-1: 82.59%, Top-5: 96.68%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.47%
[Alpha=0.20] Top-5 Accuracy: 96.67%
Result: Top-1: 82.47%, Top-5: 96.67%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.61%
[Alpha=0.20] Top-5 Accuracy: 96.65%
Result: Top-1: 82.61%, Top-5: 96.65%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.60%
[Alpha=0.20] Top-5 Accuracy: 96.67%
Result: Top-1: 82.60%, Top-5: 96.67%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.56%
[Alpha=0.20] Top-5 Accuracy: 96.68%
Result: Top-1: 82.56%, Top-5: 96.68%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.64%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.64%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.65%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.65%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.60%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.60%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.59%
[Alpha=0.20] Top-5 Accuracy: 96.66%
Result: Top-1: 82.59%, Top-5: 96.66%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.62%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.62%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.58%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.58%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.49%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.49%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.59%
[Alpha=0.20] Top-5 Accuracy: 96.66%
Result: Top-1: 82.59%, Top-5: 96.66%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.59%
[Alpha=0.20] Top-5 Accuracy: 96.67%
Result: Top-1: 82.59%, Top-5: 96.67%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.56%
[Alpha=0.20] Top-5 Accuracy: 96.68%
Result: Top-1: 82.56%, Top-5: 96.68%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.58%
[Alpha=0.20] Top-5 Accuracy: 96.69%
Result: Top-1: 82.58%, Top-5: 96.69%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.52%
[Alpha=0.20] Top-5 Accuracy: 96.70%
Result: Top-1: 82.52%, Top-5: 96.70%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.56%
[Alpha=0.20] Top-5 Accuracy: 96.70%
Result: Top-1: 82.56%, Top-5: 96.70%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.63%
[Alpha=0.20] Top-5 Accuracy: 96.67%
Result: Top-1: 82.63%, Top-5: 96.67%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.56%
[Alpha=0.20] Top-5 Accuracy: 96.66%
Result: Top-1: 82.56%, Top-5: 96.66%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.54%
[Alpha=0.20] Top-5 Accuracy: 96.68%
Result: Top-1: 82.54%, Top-5: 96.68%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.11%
[Alpha=0.20] Top-5 Accuracy: 96.68%
Result: Top-1: 82.11%, Top-5: 96.68%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.52%
[Alpha=0.20] Top-5 Accuracy: 96.64%
Result: Top-1: 82.52%, Top-5: 96.64%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.53%
[Alpha=0.20] Top-5 Accuracy: 96.67%
Result: Top-1: 82.53%, Top-5: 96.67%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.53%
[Alpha=0.20] Top-5 Accuracy: 96.67%
Result: Top-1: 82.53%, Top-5: 96.67%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.48%
[Alpha=0.20] Top-5 Accuracy: 96.70%
Result: Top-1: 82.48%, Top-5: 96.70%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.48%
[Alpha=0.20] Top-5 Accuracy: 96.66%
Result: Top-1: 82.48%, Top-5: 96.66%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.51%
[Alpha=0.20] Top-5 Accuracy: 96.66%
Result: Top-1: 82.51%, Top-5: 96.66%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.54%
[Alpha=0.20] Top-5 Accuracy: 96.65%
Result: Top-1: 82.54%, Top-5: 96.65%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.50%
[Alpha=0.20] Top-5 Accuracy: 96.66%
Result: Top-1: 82.50%, Top-5: 96.66%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.50%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.50%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.13%
[Alpha=0.20] Top-5 Accuracy: 96.35%
Result: Top-1: 81.13%, Top-5: 96.35%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.35%
[Alpha=0.20] Top-5 Accuracy: 96.55%
Result: Top-1: 82.35%, Top-5: 96.55%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.49%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.49%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.43%
[Alpha=0.20] Top-5 Accuracy: 96.58%
Result: Top-1: 82.43%, Top-5: 96.58%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.36%
[Alpha=0.20] Top-5 Accuracy: 96.51%
Result: Top-1: 82.36%, Top-5: 96.51%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.40%
[Alpha=0.20] Top-5 Accuracy: 96.56%
Result: Top-1: 82.40%, Top-5: 96.56%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.38%
[Alpha=0.20] Top-5 Accuracy: 96.56%
Result: Top-1: 82.38%, Top-5: 96.56%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.44%
[Alpha=0.20] Top-5 Accuracy: 96.57%
Result: Top-1: 82.44%, Top-5: 96.57%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.43%
[Alpha=0.20] Top-5 Accuracy: 96.65%
Result: Top-1: 82.43%, Top-5: 96.65%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.51%
[Alpha=0.20] Top-5 Accuracy: 96.62%
Result: Top-1: 82.51%, Top-5: 96.62%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 73.64%
[Alpha=0.20] Top-5 Accuracy: 93.56%
Result: Top-1: 73.64%, Top-5: 93.56%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.91%
[Alpha=0.20] Top-5 Accuracy: 95.51%
Result: Top-1: 80.91%, Top-5: 95.51%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.43%
[Alpha=0.20] Top-5 Accuracy: 96.15%
Result: Top-1: 81.43%, Top-5: 96.15%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.94%
[Alpha=0.20] Top-5 Accuracy: 96.35%
Result: Top-1: 81.94%, Top-5: 96.35%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.07%
[Alpha=0.20] Top-5 Accuracy: 96.32%
Result: Top-1: 82.07%, Top-5: 96.32%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.82%
[Alpha=0.20] Top-5 Accuracy: 96.28%
Result: Top-1: 81.82%, Top-5: 96.28%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.02%
[Alpha=0.20] Top-5 Accuracy: 96.37%
Result: Top-1: 82.02%, Top-5: 96.37%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.32%
[Alpha=0.20] Top-5 Accuracy: 96.56%
Result: Top-1: 82.32%, Top-5: 96.56%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.28%
[Alpha=0.20] Top-5 Accuracy: 96.57%
Result: Top-1: 82.28%, Top-5: 96.57%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.19%
[Alpha=0.20] Top-5 Accuracy: 96.55%
Result: Top-1: 82.19%, Top-5: 96.55%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.53%
[Alpha=0.30] Top-5 Accuracy: 96.67%
Result: Top-1: 82.53%, Top-5: 96.67%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.50%
[Alpha=0.30] Top-5 Accuracy: 96.69%
Result: Top-1: 82.50%, Top-5: 96.69%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.53%
[Alpha=0.30] Top-5 Accuracy: 96.69%
Result: Top-1: 82.53%, Top-5: 96.69%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.61%
[Alpha=0.30] Top-5 Accuracy: 96.68%
Result: Top-1: 82.61%, Top-5: 96.68%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.53%
[Alpha=0.30] Top-5 Accuracy: 96.70%
Result: Top-1: 82.53%, Top-5: 96.70%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.55%
[Alpha=0.30] Top-5 Accuracy: 96.69%
Result: Top-1: 82.55%, Top-5: 96.69%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.58%
[Alpha=0.30] Top-5 Accuracy: 96.69%
Result: Top-1: 82.58%, Top-5: 96.69%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.52%
[Alpha=0.30] Top-5 Accuracy: 96.69%
Result: Top-1: 82.52%, Top-5: 96.69%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.55%
[Alpha=0.30] Top-5 Accuracy: 96.68%
Result: Top-1: 82.55%, Top-5: 96.68%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.48%
[Alpha=0.30] Top-5 Accuracy: 96.69%
Result: Top-1: 82.48%, Top-5: 96.69%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.30%
[Alpha=0.30] Top-5 Accuracy: 96.67%
Result: Top-1: 82.30%, Top-5: 96.67%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.59%
[Alpha=0.30] Top-5 Accuracy: 96.66%
Result: Top-1: 82.59%, Top-5: 96.66%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.56%
[Alpha=0.30] Top-5 Accuracy: 96.66%
Result: Top-1: 82.56%, Top-5: 96.66%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.52%
[Alpha=0.30] Top-5 Accuracy: 96.67%
Result: Top-1: 82.52%, Top-5: 96.67%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.61%
[Alpha=0.30] Top-5 Accuracy: 96.68%
Result: Top-1: 82.61%, Top-5: 96.68%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.56%
[Alpha=0.30] Top-5 Accuracy: 96.68%
Result: Top-1: 82.56%, Top-5: 96.68%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.53%
[Alpha=0.30] Top-5 Accuracy: 96.70%
Result: Top-1: 82.53%, Top-5: 96.70%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.57%
[Alpha=0.30] Top-5 Accuracy: 96.65%
Result: Top-1: 82.57%, Top-5: 96.65%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.57%
[Alpha=0.30] Top-5 Accuracy: 96.67%
Result: Top-1: 82.57%, Top-5: 96.67%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.51%
[Alpha=0.30] Top-5 Accuracy: 96.67%
Result: Top-1: 82.51%, Top-5: 96.67%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.23%
[Alpha=0.30] Top-5 Accuracy: 96.65%
Result: Top-1: 82.23%, Top-5: 96.65%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.53%
[Alpha=0.30] Top-5 Accuracy: 96.63%
Result: Top-1: 82.53%, Top-5: 96.63%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.49%
[Alpha=0.30] Top-5 Accuracy: 96.66%
Result: Top-1: 82.49%, Top-5: 96.66%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.56%
[Alpha=0.30] Top-5 Accuracy: 96.65%
Result: Top-1: 82.56%, Top-5: 96.65%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.51%
[Alpha=0.30] Top-5 Accuracy: 96.65%
Result: Top-1: 82.51%, Top-5: 96.65%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.48%
[Alpha=0.30] Top-5 Accuracy: 96.68%
Result: Top-1: 82.48%, Top-5: 96.68%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.50%
[Alpha=0.30] Top-5 Accuracy: 96.66%
Result: Top-1: 82.50%, Top-5: 96.66%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.55%
[Alpha=0.30] Top-5 Accuracy: 96.65%
Result: Top-1: 82.55%, Top-5: 96.65%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.51%
[Alpha=0.30] Top-5 Accuracy: 96.62%
Result: Top-1: 82.51%, Top-5: 96.62%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.46%
[Alpha=0.30] Top-5 Accuracy: 96.65%
Result: Top-1: 82.46%, Top-5: 96.65%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.39%
[Alpha=0.30] Top-5 Accuracy: 96.54%
Result: Top-1: 81.39%, Top-5: 96.54%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.39%
[Alpha=0.30] Top-5 Accuracy: 96.58%
Result: Top-1: 82.39%, Top-5: 96.58%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.35%
[Alpha=0.30] Top-5 Accuracy: 96.62%
Result: Top-1: 82.35%, Top-5: 96.62%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.38%
[Alpha=0.30] Top-5 Accuracy: 96.66%
Result: Top-1: 82.38%, Top-5: 96.66%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.29%
[Alpha=0.30] Top-5 Accuracy: 96.65%
Result: Top-1: 82.29%, Top-5: 96.65%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.33%
[Alpha=0.30] Top-5 Accuracy: 96.65%
Result: Top-1: 82.33%, Top-5: 96.65%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.37%
[Alpha=0.30] Top-5 Accuracy: 96.62%
Result: Top-1: 82.37%, Top-5: 96.62%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.38%
[Alpha=0.30] Top-5 Accuracy: 96.62%
Result: Top-1: 82.38%, Top-5: 96.62%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.40%
[Alpha=0.30] Top-5 Accuracy: 96.61%
Result: Top-1: 82.40%, Top-5: 96.61%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.35%
[Alpha=0.30] Top-5 Accuracy: 96.58%
Result: Top-1: 82.35%, Top-5: 96.58%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 79.34%
[Alpha=0.30] Top-5 Accuracy: 95.86%
Result: Top-1: 79.34%, Top-5: 95.86%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.12%
[Alpha=0.30] Top-5 Accuracy: 96.46%
Result: Top-1: 82.12%, Top-5: 96.46%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.22%
[Alpha=0.30] Top-5 Accuracy: 96.51%
Result: Top-1: 82.22%, Top-5: 96.51%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.22%
[Alpha=0.30] Top-5 Accuracy: 96.55%
Result: Top-1: 82.22%, Top-5: 96.55%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.01%
[Alpha=0.30] Top-5 Accuracy: 96.47%
Result: Top-1: 82.01%, Top-5: 96.47%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.16%
[Alpha=0.30] Top-5 Accuracy: 96.46%
Result: Top-1: 82.16%, Top-5: 96.46%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.15%
[Alpha=0.30] Top-5 Accuracy: 96.49%
Result: Top-1: 82.15%, Top-5: 96.49%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.24%
[Alpha=0.30] Top-5 Accuracy: 96.52%
Result: Top-1: 82.24%, Top-5: 96.52%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.17%
[Alpha=0.30] Top-5 Accuracy: 96.57%
Result: Top-1: 82.17%, Top-5: 96.57%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.32%
[Alpha=0.30] Top-5 Accuracy: 96.59%
Result: Top-1: 82.32%, Top-5: 96.59%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 70.26%
[Alpha=0.30] Top-5 Accuracy: 90.81%
Result: Top-1: 70.26%, Top-5: 90.81%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.40%
[Alpha=0.30] Top-5 Accuracy: 95.09%
Result: Top-1: 80.40%, Top-5: 95.09%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.04%
[Alpha=0.30] Top-5 Accuracy: 95.77%
Result: Top-1: 81.04%, Top-5: 95.77%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.41%
[Alpha=0.30] Top-5 Accuracy: 96.17%
Result: Top-1: 81.41%, Top-5: 96.17%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.83%
[Alpha=0.30] Top-5 Accuracy: 96.12%
Result: Top-1: 81.83%, Top-5: 96.12%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.50%
[Alpha=0.30] Top-5 Accuracy: 96.01%
Result: Top-1: 81.50%, Top-5: 96.01%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.70%
[Alpha=0.30] Top-5 Accuracy: 96.12%
Result: Top-1: 81.70%, Top-5: 96.12%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.95%
[Alpha=0.30] Top-5 Accuracy: 96.41%
Result: Top-1: 81.95%, Top-5: 96.41%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.92%
[Alpha=0.30] Top-5 Accuracy: 96.45%
Result: Top-1: 81.92%, Top-5: 96.45%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.91%
[Alpha=0.30] Top-5 Accuracy: 96.44%
Result: Top-1: 81.91%, Top-5: 96.44%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.37%
[Alpha=0.40] Top-5 Accuracy: 96.66%
Result: Top-1: 82.37%, Top-5: 96.66%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.45%
[Alpha=0.40] Top-5 Accuracy: 96.69%
Result: Top-1: 82.45%, Top-5: 96.69%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.48%
[Alpha=0.40] Top-5 Accuracy: 96.66%
Result: Top-1: 82.48%, Top-5: 96.66%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.52%
[Alpha=0.40] Top-5 Accuracy: 96.66%
Result: Top-1: 82.52%, Top-5: 96.66%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.49%
[Alpha=0.40] Top-5 Accuracy: 96.66%
Result: Top-1: 82.49%, Top-5: 96.66%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.49%
[Alpha=0.40] Top-5 Accuracy: 96.67%
Result: Top-1: 82.49%, Top-5: 96.67%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.57%
[Alpha=0.40] Top-5 Accuracy: 96.70%
Result: Top-1: 82.57%, Top-5: 96.70%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.51%
[Alpha=0.40] Top-5 Accuracy: 96.66%
Result: Top-1: 82.51%, Top-5: 96.66%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.47%
[Alpha=0.40] Top-5 Accuracy: 96.68%
Result: Top-1: 82.47%, Top-5: 96.68%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.43%
[Alpha=0.40] Top-5 Accuracy: 96.69%
Result: Top-1: 82.43%, Top-5: 96.69%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.95%
[Alpha=0.40] Top-5 Accuracy: 96.64%
Result: Top-1: 81.95%, Top-5: 96.64%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.49%
[Alpha=0.40] Top-5 Accuracy: 96.64%
Result: Top-1: 82.49%, Top-5: 96.64%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.44%
[Alpha=0.40] Top-5 Accuracy: 96.64%
Result: Top-1: 82.44%, Top-5: 96.64%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.47%
[Alpha=0.40] Top-5 Accuracy: 96.65%
Result: Top-1: 82.47%, Top-5: 96.65%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.49%
[Alpha=0.40] Top-5 Accuracy: 96.64%
Result: Top-1: 82.49%, Top-5: 96.64%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.47%
[Alpha=0.40] Top-5 Accuracy: 96.65%
Result: Top-1: 82.47%, Top-5: 96.65%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.47%
[Alpha=0.40] Top-5 Accuracy: 96.67%
Result: Top-1: 82.47%, Top-5: 96.67%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.48%
[Alpha=0.40] Top-5 Accuracy: 96.63%
Result: Top-1: 82.48%, Top-5: 96.63%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.50%
[Alpha=0.40] Top-5 Accuracy: 96.65%
Result: Top-1: 82.50%, Top-5: 96.65%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.44%
[Alpha=0.40] Top-5 Accuracy: 96.64%
Result: Top-1: 82.44%, Top-5: 96.64%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.87%
[Alpha=0.40] Top-5 Accuracy: 96.60%
Result: Top-1: 81.87%, Top-5: 96.60%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.28%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.28%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.30%
[Alpha=0.40] Top-5 Accuracy: 96.59%
Result: Top-1: 82.30%, Top-5: 96.59%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.43%
[Alpha=0.40] Top-5 Accuracy: 96.62%
Result: Top-1: 82.43%, Top-5: 96.62%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.37%
[Alpha=0.40] Top-5 Accuracy: 96.60%
Result: Top-1: 82.37%, Top-5: 96.60%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.29%
[Alpha=0.40] Top-5 Accuracy: 96.66%
Result: Top-1: 82.29%, Top-5: 96.66%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.45%
[Alpha=0.40] Top-5 Accuracy: 96.66%
Result: Top-1: 82.45%, Top-5: 96.66%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.40%
[Alpha=0.40] Top-5 Accuracy: 96.61%
Result: Top-1: 82.40%, Top-5: 96.61%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.33%
[Alpha=0.40] Top-5 Accuracy: 96.60%
Result: Top-1: 82.33%, Top-5: 96.60%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.37%
[Alpha=0.40] Top-5 Accuracy: 96.62%
Result: Top-1: 82.37%, Top-5: 96.62%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.48%
[Alpha=0.40] Top-5 Accuracy: 96.34%
Result: Top-1: 80.48%, Top-5: 96.34%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.22%
[Alpha=0.40] Top-5 Accuracy: 96.52%
Result: Top-1: 82.22%, Top-5: 96.52%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.12%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.12%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.22%
[Alpha=0.40] Top-5 Accuracy: 96.60%
Result: Top-1: 82.22%, Top-5: 96.60%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.09%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.09%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.03%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.03%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.23%
[Alpha=0.40] Top-5 Accuracy: 96.55%
Result: Top-1: 82.23%, Top-5: 96.55%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.16%
[Alpha=0.40] Top-5 Accuracy: 96.53%
Result: Top-1: 82.16%, Top-5: 96.53%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.23%
[Alpha=0.40] Top-5 Accuracy: 96.57%
Result: Top-1: 82.23%, Top-5: 96.57%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.16%
[Alpha=0.40] Top-5 Accuracy: 96.51%
Result: Top-1: 82.16%, Top-5: 96.51%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 76.80%
[Alpha=0.40] Top-5 Accuracy: 94.98%
Result: Top-1: 76.80%, Top-5: 94.98%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.85%
[Alpha=0.40] Top-5 Accuracy: 96.36%
Result: Top-1: 81.85%, Top-5: 96.36%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.85%
[Alpha=0.40] Top-5 Accuracy: 96.41%
Result: Top-1: 81.85%, Top-5: 96.41%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.88%
[Alpha=0.40] Top-5 Accuracy: 96.41%
Result: Top-1: 81.88%, Top-5: 96.41%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.56%
[Alpha=0.40] Top-5 Accuracy: 96.35%
Result: Top-1: 81.56%, Top-5: 96.35%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.75%
[Alpha=0.40] Top-5 Accuracy: 96.35%
Result: Top-1: 81.75%, Top-5: 96.35%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.89%
[Alpha=0.40] Top-5 Accuracy: 96.41%
Result: Top-1: 81.89%, Top-5: 96.41%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.98%
[Alpha=0.40] Top-5 Accuracy: 96.45%
Result: Top-1: 81.98%, Top-5: 96.45%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.82%
[Alpha=0.40] Top-5 Accuracy: 96.46%
Result: Top-1: 81.82%, Top-5: 96.46%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.89%
[Alpha=0.40] Top-5 Accuracy: 96.45%
Result: Top-1: 81.89%, Top-5: 96.45%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 66.36%
[Alpha=0.40] Top-5 Accuracy: 87.69%
Result: Top-1: 66.36%, Top-5: 87.69%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.82%
[Alpha=0.40] Top-5 Accuracy: 94.65%
Result: Top-1: 79.82%, Top-5: 94.65%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.36%
[Alpha=0.40] Top-5 Accuracy: 95.35%
Result: Top-1: 80.36%, Top-5: 95.35%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.65%
[Alpha=0.40] Top-5 Accuracy: 95.90%
Result: Top-1: 80.65%, Top-5: 95.90%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.34%
[Alpha=0.40] Top-5 Accuracy: 95.89%
Result: Top-1: 81.34%, Top-5: 95.89%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.93%
[Alpha=0.40] Top-5 Accuracy: 95.71%
Result: Top-1: 80.93%, Top-5: 95.71%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.23%
[Alpha=0.40] Top-5 Accuracy: 95.91%
Result: Top-1: 81.23%, Top-5: 95.91%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.31%
[Alpha=0.40] Top-5 Accuracy: 96.21%
Result: Top-1: 81.31%, Top-5: 96.21%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.50%
[Alpha=0.40] Top-5 Accuracy: 96.27%
Result: Top-1: 81.50%, Top-5: 96.27%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.46%
[Alpha=0.40] Top-5 Accuracy: 96.29%
Result: Top-1: 81.46%, Top-5: 96.29%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.14%
[Alpha=0.50] Top-5 Accuracy: 96.63%
Result: Top-1: 82.14%, Top-5: 96.63%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.40%
[Alpha=0.50] Top-5 Accuracy: 96.66%
Result: Top-1: 82.40%, Top-5: 96.66%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.42%
[Alpha=0.50] Top-5 Accuracy: 96.66%
Result: Top-1: 82.42%, Top-5: 96.66%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.41%
[Alpha=0.50] Top-5 Accuracy: 96.65%
Result: Top-1: 82.41%, Top-5: 96.65%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.42%
[Alpha=0.50] Top-5 Accuracy: 96.65%
Result: Top-1: 82.42%, Top-5: 96.65%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.40%
[Alpha=0.50] Top-5 Accuracy: 96.63%
Result: Top-1: 82.40%, Top-5: 96.63%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.48%
[Alpha=0.50] Top-5 Accuracy: 96.70%
Result: Top-1: 82.48%, Top-5: 96.70%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.41%
[Alpha=0.50] Top-5 Accuracy: 96.66%
Result: Top-1: 82.41%, Top-5: 96.66%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.39%
[Alpha=0.50] Top-5 Accuracy: 96.63%
Result: Top-1: 82.39%, Top-5: 96.63%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.37%
[Alpha=0.50] Top-5 Accuracy: 96.67%
Result: Top-1: 82.37%, Top-5: 96.67%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.54%
[Alpha=0.50] Top-5 Accuracy: 96.59%
Result: Top-1: 81.54%, Top-5: 96.59%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.36%
[Alpha=0.50] Top-5 Accuracy: 96.60%
Result: Top-1: 82.36%, Top-5: 96.60%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.30%
[Alpha=0.50] Top-5 Accuracy: 96.62%
Result: Top-1: 82.30%, Top-5: 96.62%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.27%
[Alpha=0.50] Top-5 Accuracy: 96.59%
Result: Top-1: 82.27%, Top-5: 96.59%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.35%
[Alpha=0.50] Top-5 Accuracy: 96.58%
Result: Top-1: 82.35%, Top-5: 96.58%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.37%
[Alpha=0.50] Top-5 Accuracy: 96.62%
Result: Top-1: 82.37%, Top-5: 96.62%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.42%
[Alpha=0.50] Top-5 Accuracy: 96.63%
Result: Top-1: 82.42%, Top-5: 96.63%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.34%
[Alpha=0.50] Top-5 Accuracy: 96.62%
Result: Top-1: 82.34%, Top-5: 96.62%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.31%
[Alpha=0.50] Top-5 Accuracy: 96.59%
Result: Top-1: 82.31%, Top-5: 96.59%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.31%
[Alpha=0.50] Top-5 Accuracy: 96.60%
Result: Top-1: 82.31%, Top-5: 96.60%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.35%
[Alpha=0.50] Top-5 Accuracy: 96.50%
Result: Top-1: 81.35%, Top-5: 96.50%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.03%
[Alpha=0.50] Top-5 Accuracy: 96.50%
Result: Top-1: 82.03%, Top-5: 96.50%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.11%
[Alpha=0.50] Top-5 Accuracy: 96.49%
Result: Top-1: 82.11%, Top-5: 96.49%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.24%
[Alpha=0.50] Top-5 Accuracy: 96.55%
Result: Top-1: 82.24%, Top-5: 96.55%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.21%
[Alpha=0.50] Top-5 Accuracy: 96.52%
Result: Top-1: 82.21%, Top-5: 96.52%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.08%
[Alpha=0.50] Top-5 Accuracy: 96.56%
Result: Top-1: 82.08%, Top-5: 96.56%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.27%
[Alpha=0.50] Top-5 Accuracy: 96.59%
Result: Top-1: 82.27%, Top-5: 96.59%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.26%
[Alpha=0.50] Top-5 Accuracy: 96.53%
Result: Top-1: 82.26%, Top-5: 96.53%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.12%
[Alpha=0.50] Top-5 Accuracy: 96.50%
Result: Top-1: 82.12%, Top-5: 96.50%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.23%
[Alpha=0.50] Top-5 Accuracy: 96.57%
Result: Top-1: 82.23%, Top-5: 96.57%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.17%
[Alpha=0.50] Top-5 Accuracy: 96.04%
Result: Top-1: 79.17%, Top-5: 96.04%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.91%
[Alpha=0.50] Top-5 Accuracy: 96.40%
Result: Top-1: 81.91%, Top-5: 96.40%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.85%
[Alpha=0.50] Top-5 Accuracy: 96.46%
Result: Top-1: 81.85%, Top-5: 96.46%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.92%
[Alpha=0.50] Top-5 Accuracy: 96.52%
Result: Top-1: 81.92%, Top-5: 96.52%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.75%
[Alpha=0.50] Top-5 Accuracy: 96.43%
Result: Top-1: 81.75%, Top-5: 96.43%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.66%
[Alpha=0.50] Top-5 Accuracy: 96.49%
Result: Top-1: 81.66%, Top-5: 96.49%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 82.03%
[Alpha=0.50] Top-5 Accuracy: 96.45%
Result: Top-1: 82.03%, Top-5: 96.45%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.84%
[Alpha=0.50] Top-5 Accuracy: 96.41%
Result: Top-1: 81.84%, Top-5: 96.41%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.88%
[Alpha=0.50] Top-5 Accuracy: 96.42%
Result: Top-1: 81.88%, Top-5: 96.42%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.84%
[Alpha=0.50] Top-5 Accuracy: 96.38%
Result: Top-1: 81.84%, Top-5: 96.38%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 73.90%
[Alpha=0.50] Top-5 Accuracy: 93.56%
Result: Top-1: 73.90%, Top-5: 93.56%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.23%
[Alpha=0.50] Top-5 Accuracy: 96.14%
Result: Top-1: 81.23%, Top-5: 96.14%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.33%
[Alpha=0.50] Top-5 Accuracy: 96.21%
Result: Top-1: 81.33%, Top-5: 96.21%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.38%
[Alpha=0.50] Top-5 Accuracy: 96.22%
Result: Top-1: 81.38%, Top-5: 96.22%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.95%
[Alpha=0.50] Top-5 Accuracy: 96.16%
Result: Top-1: 80.95%, Top-5: 96.16%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.25%
[Alpha=0.50] Top-5 Accuracy: 96.16%
Result: Top-1: 81.25%, Top-5: 96.16%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.55%
[Alpha=0.50] Top-5 Accuracy: 96.26%
Result: Top-1: 81.55%, Top-5: 96.26%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.61%
[Alpha=0.50] Top-5 Accuracy: 96.30%
Result: Top-1: 81.61%, Top-5: 96.30%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.31%
[Alpha=0.50] Top-5 Accuracy: 96.23%
Result: Top-1: 81.31%, Top-5: 96.23%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 81.49%
[Alpha=0.50] Top-5 Accuracy: 96.27%
Result: Top-1: 81.49%, Top-5: 96.27%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 60.95%
[Alpha=0.50] Top-5 Accuracy: 84.38%
Result: Top-1: 60.95%, Top-5: 84.38%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.02%
[Alpha=0.50] Top-5 Accuracy: 94.27%
Result: Top-1: 79.02%, Top-5: 94.27%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.47%
[Alpha=0.50] Top-5 Accuracy: 94.89%
Result: Top-1: 79.47%, Top-5: 94.89%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.66%
[Alpha=0.50] Top-5 Accuracy: 95.47%
Result: Top-1: 79.66%, Top-5: 95.47%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.66%
[Alpha=0.50] Top-5 Accuracy: 95.55%
Result: Top-1: 80.66%, Top-5: 95.55%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.15%
[Alpha=0.50] Top-5 Accuracy: 95.31%
Result: Top-1: 80.15%, Top-5: 95.31%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.55%
[Alpha=0.50] Top-5 Accuracy: 95.64%
Result: Top-1: 80.55%, Top-5: 95.64%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.57%
[Alpha=0.50] Top-5 Accuracy: 95.85%
Result: Top-1: 80.57%, Top-5: 95.85%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.81%
[Alpha=0.50] Top-5 Accuracy: 95.95%
Result: Top-1: 80.81%, Top-5: 95.95%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.74%
[Alpha=0.50] Top-5 Accuracy: 96.04%
Result: Top-1: 80.74%, Top-5: 96.04%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.75%
[Alpha=0.60] Top-5 Accuracy: 96.57%
Result: Top-1: 81.75%, Top-5: 96.57%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.27%
[Alpha=0.60] Top-5 Accuracy: 96.60%
Result: Top-1: 82.27%, Top-5: 96.60%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.23%
[Alpha=0.60] Top-5 Accuracy: 96.60%
Result: Top-1: 82.23%, Top-5: 96.60%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.28%
[Alpha=0.60] Top-5 Accuracy: 96.58%
Result: Top-1: 82.28%, Top-5: 96.58%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.24%
[Alpha=0.60] Top-5 Accuracy: 96.59%
Result: Top-1: 82.24%, Top-5: 96.59%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.22%
[Alpha=0.60] Top-5 Accuracy: 96.58%
Result: Top-1: 82.22%, Top-5: 96.58%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.39%
[Alpha=0.60] Top-5 Accuracy: 96.66%
Result: Top-1: 82.39%, Top-5: 96.66%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.23%
[Alpha=0.60] Top-5 Accuracy: 96.59%
Result: Top-1: 82.23%, Top-5: 96.59%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.22%
[Alpha=0.60] Top-5 Accuracy: 96.58%
Result: Top-1: 82.22%, Top-5: 96.58%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.23%
[Alpha=0.60] Top-5 Accuracy: 96.59%
Result: Top-1: 82.23%, Top-5: 96.59%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.87%
[Alpha=0.60] Top-5 Accuracy: 96.50%
Result: Top-1: 80.87%, Top-5: 96.50%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.20%
[Alpha=0.60] Top-5 Accuracy: 96.52%
Result: Top-1: 82.20%, Top-5: 96.52%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.15%
[Alpha=0.60] Top-5 Accuracy: 96.54%
Result: Top-1: 82.15%, Top-5: 96.54%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.08%
[Alpha=0.60] Top-5 Accuracy: 96.49%
Result: Top-1: 82.08%, Top-5: 96.49%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.26%
[Alpha=0.60] Top-5 Accuracy: 96.48%
Result: Top-1: 82.26%, Top-5: 96.48%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.13%
[Alpha=0.60] Top-5 Accuracy: 96.49%
Result: Top-1: 82.13%, Top-5: 96.49%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.26%
[Alpha=0.60] Top-5 Accuracy: 96.54%
Result: Top-1: 82.26%, Top-5: 96.54%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.17%
[Alpha=0.60] Top-5 Accuracy: 96.52%
Result: Top-1: 82.17%, Top-5: 96.52%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.15%
[Alpha=0.60] Top-5 Accuracy: 96.47%
Result: Top-1: 82.15%, Top-5: 96.47%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.07%
[Alpha=0.60] Top-5 Accuracy: 96.49%
Result: Top-1: 82.07%, Top-5: 96.49%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.61%
[Alpha=0.60] Top-5 Accuracy: 96.35%
Result: Top-1: 80.61%, Top-5: 96.35%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.72%
[Alpha=0.60] Top-5 Accuracy: 96.33%
Result: Top-1: 81.72%, Top-5: 96.33%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.88%
[Alpha=0.60] Top-5 Accuracy: 96.36%
Result: Top-1: 81.88%, Top-5: 96.36%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.03%
[Alpha=0.60] Top-5 Accuracy: 96.44%
Result: Top-1: 82.03%, Top-5: 96.44%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.00%
[Alpha=0.60] Top-5 Accuracy: 96.40%
Result: Top-1: 82.00%, Top-5: 96.40%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.78%
[Alpha=0.60] Top-5 Accuracy: 96.38%
Result: Top-1: 81.78%, Top-5: 96.38%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.04%
[Alpha=0.60] Top-5 Accuracy: 96.51%
Result: Top-1: 82.04%, Top-5: 96.51%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.00%
[Alpha=0.60] Top-5 Accuracy: 96.44%
Result: Top-1: 82.00%, Top-5: 96.44%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.78%
[Alpha=0.60] Top-5 Accuracy: 96.40%
Result: Top-1: 81.78%, Top-5: 96.40%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 82.04%
[Alpha=0.60] Top-5 Accuracy: 96.44%
Result: Top-1: 82.04%, Top-5: 96.44%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 77.52%
[Alpha=0.60] Top-5 Accuracy: 95.41%
Result: Top-1: 77.52%, Top-5: 95.41%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.48%
[Alpha=0.60] Top-5 Accuracy: 96.22%
Result: Top-1: 81.48%, Top-5: 96.22%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.46%
[Alpha=0.60] Top-5 Accuracy: 96.28%
Result: Top-1: 81.46%, Top-5: 96.28%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.51%
[Alpha=0.60] Top-5 Accuracy: 96.33%
Result: Top-1: 81.51%, Top-5: 96.33%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.31%
[Alpha=0.60] Top-5 Accuracy: 96.26%
Result: Top-1: 81.31%, Top-5: 96.26%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.25%
[Alpha=0.60] Top-5 Accuracy: 96.34%
Result: Top-1: 81.25%, Top-5: 96.34%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.62%
[Alpha=0.60] Top-5 Accuracy: 96.24%
Result: Top-1: 81.62%, Top-5: 96.24%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.39%
[Alpha=0.60] Top-5 Accuracy: 96.24%
Result: Top-1: 81.39%, Top-5: 96.24%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.40%
[Alpha=0.60] Top-5 Accuracy: 96.22%
Result: Top-1: 81.40%, Top-5: 96.22%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.33%
[Alpha=0.60] Top-5 Accuracy: 96.20%
Result: Top-1: 81.33%, Top-5: 96.20%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 70.60%
[Alpha=0.60] Top-5 Accuracy: 91.65%
Result: Top-1: 70.60%, Top-5: 91.65%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.52%
[Alpha=0.60] Top-5 Accuracy: 95.82%
Result: Top-1: 80.52%, Top-5: 95.82%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.60%
[Alpha=0.60] Top-5 Accuracy: 95.89%
Result: Top-1: 80.60%, Top-5: 95.89%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.61%
[Alpha=0.60] Top-5 Accuracy: 95.96%
Result: Top-1: 80.61%, Top-5: 95.96%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.08%
[Alpha=0.60] Top-5 Accuracy: 95.85%
Result: Top-1: 80.08%, Top-5: 95.85%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.51%
[Alpha=0.60] Top-5 Accuracy: 95.87%
Result: Top-1: 80.51%, Top-5: 95.87%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.05%
[Alpha=0.60] Top-5 Accuracy: 96.02%
Result: Top-1: 81.05%, Top-5: 96.02%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 81.05%
[Alpha=0.60] Top-5 Accuracy: 96.04%
Result: Top-1: 81.05%, Top-5: 96.04%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.61%
[Alpha=0.60] Top-5 Accuracy: 95.95%
Result: Top-1: 80.61%, Top-5: 95.95%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.71%
[Alpha=0.60] Top-5 Accuracy: 95.96%
Result: Top-1: 80.71%, Top-5: 95.96%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 51.58%
[Alpha=0.60] Top-5 Accuracy: 80.13%
Result: Top-1: 51.58%, Top-5: 80.13%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 77.73%
[Alpha=0.60] Top-5 Accuracy: 93.74%
Result: Top-1: 77.73%, Top-5: 93.74%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 78.17%
[Alpha=0.60] Top-5 Accuracy: 94.29%
Result: Top-1: 78.17%, Top-5: 94.29%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 78.22%
[Alpha=0.60] Top-5 Accuracy: 94.93%
Result: Top-1: 78.22%, Top-5: 94.93%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 79.59%
[Alpha=0.60] Top-5 Accuracy: 95.08%
Result: Top-1: 79.59%, Top-5: 95.08%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 79.15%
[Alpha=0.60] Top-5 Accuracy: 94.81%
Result: Top-1: 79.15%, Top-5: 94.81%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 79.61%
[Alpha=0.60] Top-5 Accuracy: 95.16%
Result: Top-1: 79.61%, Top-5: 95.16%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 79.53%
[Alpha=0.60] Top-5 Accuracy: 95.36%
Result: Top-1: 79.53%, Top-5: 95.36%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 79.72%
[Alpha=0.60] Top-5 Accuracy: 95.56%
Result: Top-1: 79.72%, Top-5: 95.56%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 79.44%
[Alpha=0.60] Top-5 Accuracy: 95.59%
Result: Top-1: 79.44%, Top-5: 95.59%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.30%
[Alpha=0.70] Top-5 Accuracy: 96.50%
Result: Top-1: 81.30%, Top-5: 96.50%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 82.03%
[Alpha=0.70] Top-5 Accuracy: 96.47%
Result: Top-1: 82.03%, Top-5: 96.47%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 82.02%
[Alpha=0.70] Top-5 Accuracy: 96.46%
Result: Top-1: 82.02%, Top-5: 96.46%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 82.07%
[Alpha=0.70] Top-5 Accuracy: 96.48%
Result: Top-1: 82.07%, Top-5: 96.48%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 82.02%
[Alpha=0.70] Top-5 Accuracy: 96.46%
Result: Top-1: 82.02%, Top-5: 96.46%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 82.03%
[Alpha=0.70] Top-5 Accuracy: 96.45%
Result: Top-1: 82.03%, Top-5: 96.45%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 82.20%
[Alpha=0.70] Top-5 Accuracy: 96.61%
Result: Top-1: 82.20%, Top-5: 96.61%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 82.01%
[Alpha=0.70] Top-5 Accuracy: 96.46%
Result: Top-1: 82.01%, Top-5: 96.46%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 82.03%
[Alpha=0.70] Top-5 Accuracy: 96.46%
Result: Top-1: 82.03%, Top-5: 96.46%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.99%
[Alpha=0.70] Top-5 Accuracy: 96.47%
Result: Top-1: 81.99%, Top-5: 96.47%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.08%
[Alpha=0.70] Top-5 Accuracy: 96.37%
Result: Top-1: 80.08%, Top-5: 96.37%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.94%
[Alpha=0.70] Top-5 Accuracy: 96.40%
Result: Top-1: 81.94%, Top-5: 96.40%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.91%
[Alpha=0.70] Top-5 Accuracy: 96.42%
Result: Top-1: 81.91%, Top-5: 96.42%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.73%
[Alpha=0.70] Top-5 Accuracy: 96.38%
Result: Top-1: 81.73%, Top-5: 96.38%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 82.02%
[Alpha=0.70] Top-5 Accuracy: 96.38%
Result: Top-1: 82.02%, Top-5: 96.38%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.79%
[Alpha=0.70] Top-5 Accuracy: 96.36%
Result: Top-1: 81.79%, Top-5: 96.36%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.99%
[Alpha=0.70] Top-5 Accuracy: 96.41%
Result: Top-1: 81.99%, Top-5: 96.41%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.92%
[Alpha=0.70] Top-5 Accuracy: 96.41%
Result: Top-1: 81.92%, Top-5: 96.41%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.78%
[Alpha=0.70] Top-5 Accuracy: 96.35%
Result: Top-1: 81.78%, Top-5: 96.35%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.78%
[Alpha=0.70] Top-5 Accuracy: 96.35%
Result: Top-1: 81.78%, Top-5: 96.35%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 79.67%
[Alpha=0.70] Top-5 Accuracy: 96.18%
Result: Top-1: 79.67%, Top-5: 96.18%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.15%
[Alpha=0.70] Top-5 Accuracy: 96.10%
Result: Top-1: 81.15%, Top-5: 96.10%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.09%
[Alpha=0.70] Top-5 Accuracy: 96.19%
Result: Top-1: 81.09%, Top-5: 96.19%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.59%
[Alpha=0.70] Top-5 Accuracy: 96.27%
Result: Top-1: 81.59%, Top-5: 96.27%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.51%
[Alpha=0.70] Top-5 Accuracy: 96.28%
Result: Top-1: 81.51%, Top-5: 96.28%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.32%
[Alpha=0.70] Top-5 Accuracy: 96.23%
Result: Top-1: 81.32%, Top-5: 96.23%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.64%
[Alpha=0.70] Top-5 Accuracy: 96.32%
Result: Top-1: 81.64%, Top-5: 96.32%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.53%
[Alpha=0.70] Top-5 Accuracy: 96.30%
Result: Top-1: 81.53%, Top-5: 96.30%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.29%
[Alpha=0.70] Top-5 Accuracy: 96.27%
Result: Top-1: 81.29%, Top-5: 96.27%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 81.58%
[Alpha=0.70] Top-5 Accuracy: 96.26%
Result: Top-1: 81.58%, Top-5: 96.26%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 75.54%
[Alpha=0.70] Top-5 Accuracy: 94.58%
Result: Top-1: 75.54%, Top-5: 94.58%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.31%
[Alpha=0.70] Top-5 Accuracy: 95.94%
Result: Top-1: 80.31%, Top-5: 95.94%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.39%
[Alpha=0.70] Top-5 Accuracy: 96.06%
Result: Top-1: 80.39%, Top-5: 96.06%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.75%
[Alpha=0.70] Top-5 Accuracy: 96.14%
Result: Top-1: 80.75%, Top-5: 96.14%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.41%
[Alpha=0.70] Top-5 Accuracy: 95.99%
Result: Top-1: 80.41%, Top-5: 95.99%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.48%
[Alpha=0.70] Top-5 Accuracy: 96.11%
Result: Top-1: 80.48%, Top-5: 96.11%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.48%
[Alpha=0.70] Top-5 Accuracy: 96.02%
Result: Top-1: 80.48%, Top-5: 96.02%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.66%
[Alpha=0.70] Top-5 Accuracy: 96.05%
Result: Top-1: 80.66%, Top-5: 96.05%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.69%
[Alpha=0.70] Top-5 Accuracy: 95.92%
Result: Top-1: 80.69%, Top-5: 95.92%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.52%
[Alpha=0.70] Top-5 Accuracy: 96.01%
Result: Top-1: 80.52%, Top-5: 96.01%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 66.49%
[Alpha=0.70] Top-5 Accuracy: 89.33%
Result: Top-1: 66.49%, Top-5: 89.33%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 78.44%
[Alpha=0.70] Top-5 Accuracy: 95.40%
Result: Top-1: 78.44%, Top-5: 95.40%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 78.50%
[Alpha=0.70] Top-5 Accuracy: 95.53%
Result: Top-1: 78.50%, Top-5: 95.53%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 79.31%
[Alpha=0.70] Top-5 Accuracy: 95.58%
Result: Top-1: 79.31%, Top-5: 95.58%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 78.53%
[Alpha=0.70] Top-5 Accuracy: 95.40%
Result: Top-1: 78.53%, Top-5: 95.40%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 79.44%
[Alpha=0.70] Top-5 Accuracy: 95.48%
Result: Top-1: 79.44%, Top-5: 95.48%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 79.89%
[Alpha=0.70] Top-5 Accuracy: 95.71%
Result: Top-1: 79.89%, Top-5: 95.71%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 80.01%
[Alpha=0.70] Top-5 Accuracy: 95.70%
Result: Top-1: 80.01%, Top-5: 95.70%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 78.76%
[Alpha=0.70] Top-5 Accuracy: 95.54%
Result: Top-1: 78.76%, Top-5: 95.54%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 79.45%
[Alpha=0.70] Top-5 Accuracy: 95.58%
Result: Top-1: 79.45%, Top-5: 95.58%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 46.45%
[Alpha=0.70] Top-5 Accuracy: 72.85%
Result: Top-1: 46.45%, Top-5: 72.85%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 75.54%
[Alpha=0.70] Top-5 Accuracy: 93.11%
Result: Top-1: 75.54%, Top-5: 93.11%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 75.18%
[Alpha=0.70] Top-5 Accuracy: 93.61%
Result: Top-1: 75.18%, Top-5: 93.61%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 74.63%
[Alpha=0.70] Top-5 Accuracy: 94.23%
Result: Top-1: 74.63%, Top-5: 94.23%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 76.72%
[Alpha=0.70] Top-5 Accuracy: 94.61%
Result: Top-1: 76.72%, Top-5: 94.61%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 77.07%
[Alpha=0.70] Top-5 Accuracy: 94.15%
Result: Top-1: 77.07%, Top-5: 94.15%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 77.85%
[Alpha=0.70] Top-5 Accuracy: 94.56%
Result: Top-1: 77.85%, Top-5: 94.56%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 77.37%
[Alpha=0.70] Top-5 Accuracy: 94.77%
Result: Top-1: 77.37%, Top-5: 94.77%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 77.09%
[Alpha=0.70] Top-5 Accuracy: 94.98%
Result: Top-1: 77.09%, Top-5: 94.98%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 77.58%
[Alpha=0.70] Top-5 Accuracy: 95.05%
Result: Top-1: 77.58%, Top-5: 95.05%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 80.81%
[Alpha=0.80] Top-5 Accuracy: 96.38%
Result: Top-1: 80.81%, Top-5: 96.38%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.77%
[Alpha=0.80] Top-5 Accuracy: 96.29%
Result: Top-1: 81.77%, Top-5: 96.29%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.73%
[Alpha=0.80] Top-5 Accuracy: 96.30%
Result: Top-1: 81.73%, Top-5: 96.30%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.78%
[Alpha=0.80] Top-5 Accuracy: 96.34%
Result: Top-1: 81.78%, Top-5: 96.34%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.73%
[Alpha=0.80] Top-5 Accuracy: 96.29%
Result: Top-1: 81.73%, Top-5: 96.29%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.75%
[Alpha=0.80] Top-5 Accuracy: 96.28%
Result: Top-1: 81.75%, Top-5: 96.28%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 82.03%
[Alpha=0.80] Top-5 Accuracy: 96.56%
Result: Top-1: 82.03%, Top-5: 96.56%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.75%
[Alpha=0.80] Top-5 Accuracy: 96.28%
Result: Top-1: 81.75%, Top-5: 96.28%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.77%
[Alpha=0.80] Top-5 Accuracy: 96.28%
Result: Top-1: 81.77%, Top-5: 96.28%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.73%
[Alpha=0.80] Top-5 Accuracy: 96.27%
Result: Top-1: 81.73%, Top-5: 96.27%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 79.11%
[Alpha=0.80] Top-5 Accuracy: 96.14%
Result: Top-1: 79.11%, Top-5: 96.14%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.52%
[Alpha=0.80] Top-5 Accuracy: 96.19%
Result: Top-1: 81.52%, Top-5: 96.19%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.57%
[Alpha=0.80] Top-5 Accuracy: 96.22%
Result: Top-1: 81.57%, Top-5: 96.22%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 80.44%
[Alpha=0.80] Top-5 Accuracy: 96.22%
Result: Top-1: 80.44%, Top-5: 96.22%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.62%
[Alpha=0.80] Top-5 Accuracy: 96.22%
Result: Top-1: 81.62%, Top-5: 96.22%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 80.53%
[Alpha=0.80] Top-5 Accuracy: 96.22%
Result: Top-1: 80.53%, Top-5: 96.22%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.67%
[Alpha=0.80] Top-5 Accuracy: 96.19%
Result: Top-1: 81.67%, Top-5: 96.19%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.57%
[Alpha=0.80] Top-5 Accuracy: 96.18%
Result: Top-1: 81.57%, Top-5: 96.18%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 80.10%
[Alpha=0.80] Top-5 Accuracy: 96.21%
Result: Top-1: 80.10%, Top-5: 96.21%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 81.42%
[Alpha=0.80] Top-5 Accuracy: 96.09%
Result: Top-1: 81.42%, Top-5: 96.09%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 78.45%
[Alpha=0.80] Top-5 Accuracy: 95.86%
Result: Top-1: 78.45%, Top-5: 95.86%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 78.31%
[Alpha=0.80] Top-5 Accuracy: 95.86%
Result: Top-1: 78.31%, Top-5: 95.86%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 77.02%
[Alpha=0.80] Top-5 Accuracy: 95.98%
Result: Top-1: 77.02%, Top-5: 95.98%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 80.00%
[Alpha=0.80] Top-5 Accuracy: 96.11%
Result: Top-1: 80.00%, Top-5: 96.11%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 80.22%
[Alpha=0.80] Top-5 Accuracy: 96.06%
Result: Top-1: 80.22%, Top-5: 96.06%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 79.83%
[Alpha=0.80] Top-5 Accuracy: 96.01%
Result: Top-1: 79.83%, Top-5: 96.01%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 80.59%
[Alpha=0.80] Top-5 Accuracy: 96.09%
Result: Top-1: 80.59%, Top-5: 96.09%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 79.40%
[Alpha=0.80] Top-5 Accuracy: 96.12%
Result: Top-1: 79.40%, Top-5: 96.12%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 79.67%
[Alpha=0.80] Top-5 Accuracy: 96.09%
Result: Top-1: 79.67%, Top-5: 96.09%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 80.22%
[Alpha=0.80] Top-5 Accuracy: 96.09%
Result: Top-1: 80.22%, Top-5: 96.09%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 73.36%
[Alpha=0.80] Top-5 Accuracy: 93.45%
Result: Top-1: 73.36%, Top-5: 93.45%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 75.95%
[Alpha=0.80] Top-5 Accuracy: 95.67%
Result: Top-1: 75.95%, Top-5: 95.67%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 76.58%
[Alpha=0.80] Top-5 Accuracy: 95.72%
Result: Top-1: 76.58%, Top-5: 95.72%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 77.54%
[Alpha=0.80] Top-5 Accuracy: 95.91%
Result: Top-1: 77.54%, Top-5: 95.91%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 75.83%
[Alpha=0.80] Top-5 Accuracy: 95.67%
Result: Top-1: 75.83%, Top-5: 95.67%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 78.41%
[Alpha=0.80] Top-5 Accuracy: 95.89%
Result: Top-1: 78.41%, Top-5: 95.89%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 76.20%
[Alpha=0.80] Top-5 Accuracy: 95.78%
Result: Top-1: 76.20%, Top-5: 95.78%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 77.80%
[Alpha=0.80] Top-5 Accuracy: 95.70%
Result: Top-1: 77.80%, Top-5: 95.70%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 77.47%
[Alpha=0.80] Top-5 Accuracy: 95.64%
Result: Top-1: 77.47%, Top-5: 95.64%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 77.22%
[Alpha=0.80] Top-5 Accuracy: 95.80%
Result: Top-1: 77.22%, Top-5: 95.80%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 61.88%
[Alpha=0.80] Top-5 Accuracy: 86.49%
Result: Top-1: 61.88%, Top-5: 86.49%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 71.51%
[Alpha=0.80] Top-5 Accuracy: 94.95%
Result: Top-1: 71.51%, Top-5: 94.95%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 70.40%
[Alpha=0.80] Top-5 Accuracy: 95.13%
Result: Top-1: 70.40%, Top-5: 95.13%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 76.59%
[Alpha=0.80] Top-5 Accuracy: 95.26%
Result: Top-1: 76.59%, Top-5: 95.26%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 74.59%
[Alpha=0.80] Top-5 Accuracy: 94.84%
Result: Top-1: 74.59%, Top-5: 94.84%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 77.22%
[Alpha=0.80] Top-5 Accuracy: 94.94%
Result: Top-1: 77.22%, Top-5: 94.94%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 77.32%
[Alpha=0.80] Top-5 Accuracy: 95.38%
Result: Top-1: 77.32%, Top-5: 95.38%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 77.96%
[Alpha=0.80] Top-5 Accuracy: 95.32%
Result: Top-1: 77.96%, Top-5: 95.32%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 71.53%
[Alpha=0.80] Top-5 Accuracy: 94.98%
Result: Top-1: 71.53%, Top-5: 94.98%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 75.12%
[Alpha=0.80] Top-5 Accuracy: 95.11%
Result: Top-1: 75.12%, Top-5: 95.11%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 42.06%
[Alpha=0.80] Top-5 Accuracy: 67.19%
Result: Top-1: 42.06%, Top-5: 67.19%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 71.20%
[Alpha=0.80] Top-5 Accuracy: 92.39%
Result: Top-1: 71.20%, Top-5: 92.39%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 67.76%
[Alpha=0.80] Top-5 Accuracy: 92.69%
Result: Top-1: 67.76%, Top-5: 92.69%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 65.45%
[Alpha=0.80] Top-5 Accuracy: 93.30%
Result: Top-1: 65.45%, Top-5: 93.30%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 69.28%
[Alpha=0.80] Top-5 Accuracy: 93.95%
Result: Top-1: 69.28%, Top-5: 93.95%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 71.96%
[Alpha=0.80] Top-5 Accuracy: 93.46%
Result: Top-1: 71.96%, Top-5: 93.46%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 72.85%
[Alpha=0.80] Top-5 Accuracy: 93.92%
Result: Top-1: 72.85%, Top-5: 93.92%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 71.35%
[Alpha=0.80] Top-5 Accuracy: 94.09%
Result: Top-1: 71.35%, Top-5: 94.09%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 69.15%
[Alpha=0.80] Top-5 Accuracy: 94.19%
Result: Top-1: 69.15%, Top-5: 94.19%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 72.38%
[Alpha=0.80] Top-5 Accuracy: 94.28%
Result: Top-1: 72.38%, Top-5: 94.28%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 80.15%
[Alpha=0.90] Top-5 Accuracy: 96.18%
Result: Top-1: 80.15%, Top-5: 96.18%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 81.29%
[Alpha=0.90] Top-5 Accuracy: 96.07%
Result: Top-1: 81.29%, Top-5: 96.07%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 81.18%
[Alpha=0.90] Top-5 Accuracy: 96.14%
Result: Top-1: 81.18%, Top-5: 96.14%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 80.72%
[Alpha=0.90] Top-5 Accuracy: 96.21%
Result: Top-1: 80.72%, Top-5: 96.21%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 81.18%
[Alpha=0.90] Top-5 Accuracy: 96.14%
Result: Top-1: 81.18%, Top-5: 96.14%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 81.30%
[Alpha=0.90] Top-5 Accuracy: 96.09%
Result: Top-1: 81.30%, Top-5: 96.09%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 81.77%
[Alpha=0.90] Top-5 Accuracy: 96.40%
Result: Top-1: 81.77%, Top-5: 96.40%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 81.19%
[Alpha=0.90] Top-5 Accuracy: 96.13%
Result: Top-1: 81.19%, Top-5: 96.13%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 81.33%
[Alpha=0.90] Top-5 Accuracy: 96.09%
Result: Top-1: 81.33%, Top-5: 96.09%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 81.22%
[Alpha=0.90] Top-5 Accuracy: 96.06%
Result: Top-1: 81.22%, Top-5: 96.06%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 77.88%
[Alpha=0.90] Top-5 Accuracy: 95.77%
Result: Top-1: 77.88%, Top-5: 95.77%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 80.70%
[Alpha=0.90] Top-5 Accuracy: 95.97%
Result: Top-1: 80.70%, Top-5: 95.97%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 80.67%
[Alpha=0.90] Top-5 Accuracy: 96.02%
Result: Top-1: 80.67%, Top-5: 96.02%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 76.04%
[Alpha=0.90] Top-5 Accuracy: 96.00%
Result: Top-1: 76.04%, Top-5: 96.00%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 79.82%
[Alpha=0.90] Top-5 Accuracy: 96.11%
Result: Top-1: 79.82%, Top-5: 96.11%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 75.61%
[Alpha=0.90] Top-5 Accuracy: 95.99%
Result: Top-1: 75.61%, Top-5: 95.99%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 80.79%
[Alpha=0.90] Top-5 Accuracy: 95.97%
Result: Top-1: 80.79%, Top-5: 95.97%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 80.85%
[Alpha=0.90] Top-5 Accuracy: 95.95%
Result: Top-1: 80.85%, Top-5: 95.95%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 77.16%
[Alpha=0.90] Top-5 Accuracy: 96.01%
Result: Top-1: 77.16%, Top-5: 96.01%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 80.48%
[Alpha=0.90] Top-5 Accuracy: 95.81%
Result: Top-1: 80.48%, Top-5: 95.81%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 77.02%
[Alpha=0.90] Top-5 Accuracy: 95.42%
Result: Top-1: 77.02%, Top-5: 95.42%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 70.29%
[Alpha=0.90] Top-5 Accuracy: 95.54%
Result: Top-1: 70.29%, Top-5: 95.54%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 69.34%
[Alpha=0.90] Top-5 Accuracy: 95.71%
Result: Top-1: 69.34%, Top-5: 95.71%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 74.25%
[Alpha=0.90] Top-5 Accuracy: 95.85%
Result: Top-1: 74.25%, Top-5: 95.85%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 75.34%
[Alpha=0.90] Top-5 Accuracy: 95.78%
Result: Top-1: 75.34%, Top-5: 95.78%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 74.81%
[Alpha=0.90] Top-5 Accuracy: 95.73%
Result: Top-1: 74.81%, Top-5: 95.73%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 76.63%
[Alpha=0.90] Top-5 Accuracy: 95.86%
Result: Top-1: 76.63%, Top-5: 95.86%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 75.02%
[Alpha=0.90] Top-5 Accuracy: 95.88%
Result: Top-1: 75.02%, Top-5: 95.88%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 74.90%
[Alpha=0.90] Top-5 Accuracy: 95.85%
Result: Top-1: 74.90%, Top-5: 95.85%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 76.12%
[Alpha=0.90] Top-5 Accuracy: 95.90%
Result: Top-1: 76.12%, Top-5: 95.90%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 70.48%
[Alpha=0.90] Top-5 Accuracy: 92.06%
Result: Top-1: 70.48%, Top-5: 92.06%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 69.29%
[Alpha=0.90] Top-5 Accuracy: 95.35%
Result: Top-1: 69.29%, Top-5: 95.35%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 70.65%
[Alpha=0.90] Top-5 Accuracy: 95.41%
Result: Top-1: 70.65%, Top-5: 95.41%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 72.49%
[Alpha=0.90] Top-5 Accuracy: 95.67%
Result: Top-1: 72.49%, Top-5: 95.67%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 64.49%
[Alpha=0.90] Top-5 Accuracy: 95.38%
Result: Top-1: 64.49%, Top-5: 95.38%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 73.96%
[Alpha=0.90] Top-5 Accuracy: 95.51%
Result: Top-1: 73.96%, Top-5: 95.51%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 69.56%
[Alpha=0.90] Top-5 Accuracy: 95.43%
Result: Top-1: 69.56%, Top-5: 95.43%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 70.44%
[Alpha=0.90] Top-5 Accuracy: 95.28%
Result: Top-1: 70.44%, Top-5: 95.28%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 65.13%
[Alpha=0.90] Top-5 Accuracy: 95.34%
Result: Top-1: 65.13%, Top-5: 95.34%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 70.41%
[Alpha=0.90] Top-5 Accuracy: 95.52%
Result: Top-1: 70.41%, Top-5: 95.52%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 57.48%
[Alpha=0.90] Top-5 Accuracy: 83.09%
Result: Top-1: 57.48%, Top-5: 83.09%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 64.11%
[Alpha=0.90] Top-5 Accuracy: 94.29%
Result: Top-1: 64.11%, Top-5: 94.29%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 59.42%
[Alpha=0.90] Top-5 Accuracy: 94.54%
Result: Top-1: 59.42%, Top-5: 94.54%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 71.03%
[Alpha=0.90] Top-5 Accuracy: 94.79%
Result: Top-1: 71.03%, Top-5: 94.79%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 65.60%
[Alpha=0.90] Top-5 Accuracy: 94.12%
Result: Top-1: 65.60%, Top-5: 94.12%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 71.30%
[Alpha=0.90] Top-5 Accuracy: 94.37%
Result: Top-1: 71.30%, Top-5: 94.37%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 71.52%
[Alpha=0.90] Top-5 Accuracy: 95.03%
Result: Top-1: 71.52%, Top-5: 95.03%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 74.25%
[Alpha=0.90] Top-5 Accuracy: 94.90%
Result: Top-1: 74.25%, Top-5: 94.90%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 59.00%
[Alpha=0.90] Top-5 Accuracy: 94.23%
Result: Top-1: 59.00%, Top-5: 94.23%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 65.10%
[Alpha=0.90] Top-5 Accuracy: 94.55%
Result: Top-1: 65.10%, Top-5: 94.55%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 38.14%
[Alpha=0.90] Top-5 Accuracy: 63.45%
Result: Top-1: 38.14%, Top-5: 63.45%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 65.64%
[Alpha=0.90] Top-5 Accuracy: 91.45%
Result: Top-1: 65.64%, Top-5: 91.45%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 58.33%
[Alpha=0.90] Top-5 Accuracy: 91.62%
Result: Top-1: 58.33%, Top-5: 91.62%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 56.19%
[Alpha=0.90] Top-5 Accuracy: 92.12%
Result: Top-1: 56.19%, Top-5: 92.12%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 60.41%
[Alpha=0.90] Top-5 Accuracy: 93.18%
Result: Top-1: 60.41%, Top-5: 93.18%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 63.10%
[Alpha=0.90] Top-5 Accuracy: 92.66%
Result: Top-1: 63.10%, Top-5: 92.66%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 61.55%
[Alpha=0.90] Top-5 Accuracy: 93.16%
Result: Top-1: 61.55%, Top-5: 93.16%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 61.70%
[Alpha=0.90] Top-5 Accuracy: 93.25%
Result: Top-1: 61.70%, Top-5: 93.25%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 59.97%
[Alpha=0.90] Top-5 Accuracy: 93.40%
Result: Top-1: 59.97%, Top-5: 93.40%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 64.59%
[Alpha=0.90] Top-5 Accuracy: 93.46%
Result: Top-1: 64.59%, Top-5: 93.46%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 79.35%
[Alpha=1.00] Top-5 Accuracy: 96.05%
Result: Top-1: 79.35%, Top-5: 96.05%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 78.82%
[Alpha=1.00] Top-5 Accuracy: 95.89%
Result: Top-1: 78.82%, Top-5: 95.89%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 78.40%
[Alpha=1.00] Top-5 Accuracy: 95.96%
Result: Top-1: 78.40%, Top-5: 95.96%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 74.64%
[Alpha=1.00] Top-5 Accuracy: 96.06%
Result: Top-1: 74.64%, Top-5: 96.06%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 78.39%
[Alpha=1.00] Top-5 Accuracy: 95.96%
Result: Top-1: 78.39%, Top-5: 95.96%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 79.31%
[Alpha=1.00] Top-5 Accuracy: 95.92%
Result: Top-1: 79.31%, Top-5: 95.92%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 81.35%
[Alpha=1.00] Top-5 Accuracy: 96.17%
Result: Top-1: 81.35%, Top-5: 96.17%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 78.31%
[Alpha=1.00] Top-5 Accuracy: 95.96%
Result: Top-1: 78.31%, Top-5: 95.96%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 79.35%
[Alpha=1.00] Top-5 Accuracy: 95.92%
Result: Top-1: 79.35%, Top-5: 95.92%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 78.73%
[Alpha=1.00] Top-5 Accuracy: 95.90%
Result: Top-1: 78.73%, Top-5: 95.90%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 76.49%
[Alpha=1.00] Top-5 Accuracy: 95.27%
Result: Top-1: 76.49%, Top-5: 95.27%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 77.46%
[Alpha=1.00] Top-5 Accuracy: 95.75%
Result: Top-1: 77.46%, Top-5: 95.75%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 77.46%
[Alpha=1.00] Top-5 Accuracy: 95.79%
Result: Top-1: 77.46%, Top-5: 95.79%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 71.15%
[Alpha=1.00] Top-5 Accuracy: 95.76%
Result: Top-1: 71.15%, Top-5: 95.76%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 70.49%
[Alpha=1.00] Top-5 Accuracy: 95.97%
Result: Top-1: 70.49%, Top-5: 95.97%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 69.81%
[Alpha=1.00] Top-5 Accuracy: 95.77%
Result: Top-1: 69.81%, Top-5: 95.77%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 77.42%
[Alpha=1.00] Top-5 Accuracy: 95.80%
Result: Top-1: 77.42%, Top-5: 95.80%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 77.89%
[Alpha=1.00] Top-5 Accuracy: 95.75%
Result: Top-1: 77.89%, Top-5: 95.75%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 73.78%
[Alpha=1.00] Top-5 Accuracy: 95.78%
Result: Top-1: 73.78%, Top-5: 95.78%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 76.39%
[Alpha=1.00] Top-5 Accuracy: 95.57%
Result: Top-1: 76.39%, Top-5: 95.57%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 75.37%
[Alpha=1.00] Top-5 Accuracy: 94.81%
Result: Top-1: 75.37%, Top-5: 94.81%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 58.98%
[Alpha=1.00] Top-5 Accuracy: 95.17%
Result: Top-1: 58.98%, Top-5: 95.17%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 60.05%
[Alpha=1.00] Top-5 Accuracy: 95.45%
Result: Top-1: 60.05%, Top-5: 95.45%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 68.13%
[Alpha=1.00] Top-5 Accuracy: 95.54%
Result: Top-1: 68.13%, Top-5: 95.54%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 68.24%
[Alpha=1.00] Top-5 Accuracy: 95.44%
Result: Top-1: 68.24%, Top-5: 95.44%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 67.11%
[Alpha=1.00] Top-5 Accuracy: 95.46%
Result: Top-1: 67.11%, Top-5: 95.46%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 69.72%
[Alpha=1.00] Top-5 Accuracy: 95.61%
Result: Top-1: 69.72%, Top-5: 95.61%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 71.62%
[Alpha=1.00] Top-5 Accuracy: 95.55%
Result: Top-1: 71.62%, Top-5: 95.55%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 71.04%
[Alpha=1.00] Top-5 Accuracy: 95.56%
Result: Top-1: 71.04%, Top-5: 95.56%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 69.66%
[Alpha=1.00] Top-5 Accuracy: 95.64%
Result: Top-1: 69.66%, Top-5: 95.64%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 67.63%
[Alpha=1.00] Top-5 Accuracy: 90.16%
Result: Top-1: 67.63%, Top-5: 90.16%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 61.93%
[Alpha=1.00] Top-5 Accuracy: 94.99%
Result: Top-1: 61.93%, Top-5: 94.99%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 62.44%
[Alpha=1.00] Top-5 Accuracy: 94.98%
Result: Top-1: 62.44%, Top-5: 94.98%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 68.44%
[Alpha=1.00] Top-5 Accuracy: 95.32%
Result: Top-1: 68.44%, Top-5: 95.32%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 53.98%
[Alpha=1.00] Top-5 Accuracy: 94.97%
Result: Top-1: 53.98%, Top-5: 94.97%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 67.88%
[Alpha=1.00] Top-5 Accuracy: 95.09%
Result: Top-1: 67.88%, Top-5: 95.09%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 63.21%
[Alpha=1.00] Top-5 Accuracy: 95.00%
Result: Top-1: 63.21%, Top-5: 95.00%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 60.95%
[Alpha=1.00] Top-5 Accuracy: 94.76%
Result: Top-1: 60.95%, Top-5: 94.76%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 45.82%
[Alpha=1.00] Top-5 Accuracy: 94.91%
Result: Top-1: 45.82%, Top-5: 94.91%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 63.95%
[Alpha=1.00] Top-5 Accuracy: 95.13%
Result: Top-1: 63.95%, Top-5: 95.13%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 53.46%
[Alpha=1.00] Top-5 Accuracy: 79.29%
Result: Top-1: 53.46%, Top-5: 79.29%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 56.50%
[Alpha=1.00] Top-5 Accuracy: 93.61%
Result: Top-1: 56.50%, Top-5: 93.61%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 49.15%
[Alpha=1.00] Top-5 Accuracy: 93.78%
Result: Top-1: 49.15%, Top-5: 93.78%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 59.55%
[Alpha=1.00] Top-5 Accuracy: 94.17%
Result: Top-1: 59.55%, Top-5: 94.17%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 52.86%
[Alpha=1.00] Top-5 Accuracy: 93.01%
Result: Top-1: 52.86%, Top-5: 93.01%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 60.06%
[Alpha=1.00] Top-5 Accuracy: 93.53%
Result: Top-1: 60.06%, Top-5: 93.53%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 58.50%
[Alpha=1.00] Top-5 Accuracy: 94.53%
Result: Top-1: 58.50%, Top-5: 94.53%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 67.49%
[Alpha=1.00] Top-5 Accuracy: 94.44%
Result: Top-1: 67.49%, Top-5: 94.44%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 47.99%
[Alpha=1.00] Top-5 Accuracy: 93.19%
Result: Top-1: 47.99%, Top-5: 93.19%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 54.44%
[Alpha=1.00] Top-5 Accuracy: 93.88%
Result: Top-1: 54.44%, Top-5: 93.88%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 34.82%
[Alpha=1.00] Top-5 Accuracy: 59.73%
Result: Top-1: 34.82%, Top-5: 59.73%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 60.51%
[Alpha=1.00] Top-5 Accuracy: 90.34%
Result: Top-1: 60.51%, Top-5: 90.34%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 48.60%
[Alpha=1.00] Top-5 Accuracy: 90.28%
Result: Top-1: 48.60%, Top-5: 90.28%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 48.45%
[Alpha=1.00] Top-5 Accuracy: 90.76%
Result: Top-1: 48.45%, Top-5: 90.76%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 50.91%
[Alpha=1.00] Top-5 Accuracy: 92.27%
Result: Top-1: 50.91%, Top-5: 92.27%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 52.09%
[Alpha=1.00] Top-5 Accuracy: 91.73%
Result: Top-1: 52.09%, Top-5: 91.73%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 48.39%
[Alpha=1.00] Top-5 Accuracy: 92.22%
Result: Top-1: 48.39%, Top-5: 92.22%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 50.11%
[Alpha=1.00] Top-5 Accuracy: 92.29%
Result: Top-1: 50.11%, Top-5: 92.29%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 50.67%
[Alpha=1.00] Top-5 Accuracy: 92.28%
Result: Top-1: 50.67%, Top-5: 92.28%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 56.84%
[Alpha=1.00] Top-5 Accuracy: 92.31%
Result: Top-1: 56.84%, Top-5: 92.31%

================================================================================
SUMMARY OF ALL RESULTS
================================================================================
Alpha    Clusters   PCA_dim    Top-1      Top-5     
--------------------------------------------------
0.10     8          1          82.61      96.68     
0.10     8          25         82.62      96.68     
0.10     8          50         82.61      96.67     
0.10     8          75         82.62      96.68     
0.10     8          100        82.61      96.68     
0.10     8          125        82.62      96.68     
0.10     8          150        82.60      96.66     
0.10     8          175        82.61      96.68     
0.10     8          200        82.62      96.68     
0.10     8          220        82.61      96.67     
0.10     16         1          82.59      96.71     
0.10     16         25         82.58      96.67     
0.10     16         50         82.58      96.68     
0.10     16         75         82.62      96.67     
0.10     16         100        82.65      96.68     
0.10     16         125        82.62      96.68     
0.10     16         150        82.61      96.68     
0.10     16         175        82.58      96.68     
0.10     16         200        82.65      96.67     
0.10     16         220        82.63      96.68     
0.10     32         1          82.56      96.70     
0.10     32         25         82.63      96.67     
0.10     32         50         82.60      96.67     
0.10     32         75         82.58      96.68     
0.10     32         100        82.62      96.68     
0.10     32         125        82.64      96.69     
0.10     32         150        82.63      96.69     
0.10     32         175        82.63      96.67     
0.10     32         200        82.60      96.67     
0.10     32         220        82.63      96.68     
0.10     64         1          82.47      96.69     
0.10     64         25         82.60      96.69     
0.10     64         50         82.60      96.67     
0.10     64         75         82.57      96.68     
0.10     64         100        82.58      96.71     
0.10     64         125        82.57      96.69     
0.10     64         150        82.58      96.67     
0.10     64         175        82.58      96.68     
0.10     64         200        82.56      96.69     
0.10     64         220        82.58      96.66     
0.10     128        1          82.12      96.61     
0.10     128        25         82.48      96.63     
0.10     128        50         82.56      96.65     
0.10     128        75         82.53      96.63     
0.10     128        100        82.43      96.65     
0.10     128        125        82.53      96.66     
0.10     128        150        82.51      96.65     
0.10     128        175        82.51      96.62     
0.10     128        200        82.58      96.67     
0.10     128        220        82.58      96.68     
0.10     256        1          77.78      95.68     
0.10     256        25         81.42      96.18     
0.10     256        50         81.91      96.49     
0.10     256        75         82.32      96.58     
0.10     256        100        82.29      96.55     
0.10     256        125        82.13      96.57     
0.10     256        150        82.26      96.58     
0.10     256        175        82.48      96.65     
0.10     256        200        82.46      96.66     
0.10     256        220        82.41      96.64     
0.20     8          1          82.63      96.70     
0.20     8          25         82.61      96.68     
0.20     8          50         82.58      96.68     
0.20     8          75         82.60      96.70     
0.20     8          100        82.58      96.69     
0.20     8          125        82.60      96.69     
0.20     8          150        82.60      96.68     
0.20     8          175        82.58      96.69     
0.20     8          200        82.61      96.69     
0.20     8          220        82.59      96.68     
0.20     16         1          82.47      96.67     
0.20     16         25         82.61      96.65     
0.20     16         50         82.60      96.67     
0.20     16         75         82.56      96.68     
0.20     16         100        82.64      96.69     
0.20     16         125        82.65      96.69     
0.20     16         150        82.60      96.69     
0.20     16         175        82.59      96.66     
0.20     16         200        82.62      96.69     
0.20     16         220        82.58      96.69     
0.20     32         1          82.49      96.69     
0.20     32         25         82.59      96.66     
0.20     32         50         82.59      96.67     
0.20     32         75         82.56      96.68     
0.20     32         100        82.58      96.69     
0.20     32         125        82.52      96.70     
0.20     32         150        82.56      96.70     
0.20     32         175        82.63      96.67     
0.20     32         200        82.56      96.66     
0.20     32         220        82.54      96.68     
0.20     64         1          82.11      96.68     
0.20     64         25         82.52      96.64     
0.20     64         50         82.53      96.67     
0.20     64         75         82.53      96.67     
0.20     64         100        82.48      96.70     
0.20     64         125        82.48      96.66     
0.20     64         150        82.51      96.66     
0.20     64         175        82.54      96.65     
0.20     64         200        82.50      96.66     
0.20     64         220        82.50      96.61     
0.20     128        1          81.13      96.35     
0.20     128        25         82.35      96.55     
0.20     128        50         82.49      96.61     
0.20     128        75         82.43      96.58     
0.20     128        100        82.36      96.51     
0.20     128        125        82.40      96.56     
0.20     128        150        82.38      96.56     
0.20     128        175        82.44      96.57     
0.20     128        200        82.43      96.65     
0.20     128        220        82.51      96.62     
0.20     256        1          73.64      93.56     
0.20     256        25         80.91      95.51     
0.20     256        50         81.43      96.15     
0.20     256        75         81.94      96.35     
0.20     256        100        82.07      96.32     
0.20     256        125        81.82      96.28     
0.20     256        150        82.02      96.37     
0.20     256        175        82.32      96.56     
0.20     256        200        82.28      96.57     
0.20     256        220        82.19      96.55     
0.30     8          1          82.53      96.67     
0.30     8          25         82.50      96.69     
0.30     8          50         82.53      96.69     
0.30     8          75         82.61      96.68     
0.30     8          100        82.53      96.70     
0.30     8          125        82.55      96.69     
0.30     8          150        82.58      96.69     
0.30     8          175        82.52      96.69     
0.30     8          200        82.55      96.68     
0.30     8          220        82.48      96.69     
0.30     16         1          82.30      96.67     
0.30     16         25         82.59      96.66     
0.30     16         50         82.56      96.66     
0.30     16         75         82.52      96.67     
0.30     16         100        82.61      96.68     
0.30     16         125        82.56      96.68     
0.30     16         150        82.53      96.70     
0.30     16         175        82.57      96.65     
0.30     16         200        82.57      96.67     
0.30     16         220        82.51      96.67     
0.30     32         1          82.23      96.65     
0.30     32         25         82.53      96.63     
0.30     32         50         82.49      96.66     
0.30     32         75         82.56      96.65     
0.30     32         100        82.51      96.65     
0.30     32         125        82.48      96.68     
0.30     32         150        82.50      96.66     
0.30     32         175        82.55      96.65     
0.30     32         200        82.51      96.62     
0.30     32         220        82.46      96.65     
0.30     64         1          81.39      96.54     
0.30     64         25         82.39      96.58     
0.30     64         50         82.35      96.62     
0.30     64         75         82.38      96.66     
0.30     64         100        82.29      96.65     
0.30     64         125        82.33      96.65     
0.30     64         150        82.37      96.62     
0.30     64         175        82.38      96.62     
0.30     64         200        82.40      96.61     
0.30     64         220        82.35      96.58     
0.30     128        1          79.34      95.86     
0.30     128        25         82.12      96.46     
0.30     128        50         82.22      96.51     
0.30     128        75         82.22      96.55     
0.30     128        100        82.01      96.47     
0.30     128        125        82.16      96.46     
0.30     128        150        82.15      96.49     
0.30     128        175        82.24      96.52     
0.30     128        200        82.17      96.57     
0.30     128        220        82.32      96.59     
0.30     256        1          70.26      90.81     
0.30     256        25         80.40      95.09     
0.30     256        50         81.04      95.77     
0.30     256        75         81.41      96.17     
0.30     256        100        81.83      96.12     
0.30     256        125        81.50      96.01     
0.30     256        150        81.70      96.12     
0.30     256        175        81.95      96.41     
0.30     256        200        81.92      96.45     
0.30     256        220        81.91      96.44     
0.40     8          1          82.37      96.66     
0.40     8          25         82.45      96.69     
0.40     8          50         82.48      96.66     
0.40     8          75         82.52      96.66     
0.40     8          100        82.49      96.66     
0.40     8          125        82.49      96.67     
0.40     8          150        82.57      96.70     
0.40     8          175        82.51      96.66     
0.40     8          200        82.47      96.68     
0.40     8          220        82.43      96.69     
0.40     16         1          81.95      96.64     
0.40     16         25         82.49      96.64     
0.40     16         50         82.44      96.64     
0.40     16         75         82.47      96.65     
0.40     16         100        82.49      96.64     
0.40     16         125        82.47      96.65     
0.40     16         150        82.47      96.67     
0.40     16         175        82.48      96.63     
0.40     16         200        82.50      96.65     
0.40     16         220        82.44      96.64     
0.40     32         1          81.87      96.60     
0.40     32         25         82.28      96.58     
0.40     32         50         82.30      96.59     
0.40     32         75         82.43      96.62     
0.40     32         100        82.37      96.60     
0.40     32         125        82.29      96.66     
0.40     32         150        82.45      96.66     
0.40     32         175        82.40      96.61     
0.40     32         200        82.33      96.60     
0.40     32         220        82.37      96.62     
0.40     64         1          80.48      96.34     
0.40     64         25         82.22      96.52     
0.40     64         50         82.12      96.58     
0.40     64         75         82.22      96.60     
0.40     64         100        82.09      96.58     
0.40     64         125        82.03      96.58     
0.40     64         150        82.23      96.55     
0.40     64         175        82.16      96.53     
0.40     64         200        82.23      96.57     
0.40     64         220        82.16      96.51     
0.40     128        1          76.80      94.98     
0.40     128        25         81.85      96.36     
0.40     128        50         81.85      96.41     
0.40     128        75         81.88      96.41     
0.40     128        100        81.56      96.35     
0.40     128        125        81.75      96.35     
0.40     128        150        81.89      96.41     
0.40     128        175        81.98      96.45     
0.40     128        200        81.82      96.46     
0.40     128        220        81.89      96.45     
0.40     256        1          66.36      87.69     
0.40     256        25         79.82      94.65     
0.40     256        50         80.36      95.35     
0.40     256        75         80.65      95.90     
0.40     256        100        81.34      95.89     
0.40     256        125        80.93      95.71     
0.40     256        150        81.23      95.91     
0.40     256        175        81.31      96.21     
0.40     256        200        81.50      96.27     
0.40     256        220        81.46      96.29     
0.50     8          1          82.14      96.63     
0.50     8          25         82.40      96.66     
0.50     8          50         82.42      96.66     
0.50     8          75         82.41      96.65     
0.50     8          100        82.42      96.65     
0.50     8          125        82.40      96.63     
0.50     8          150        82.48      96.70     
0.50     8          175        82.41      96.66     
0.50     8          200        82.39      96.63     
0.50     8          220        82.37      96.67     
0.50     16         1          81.54      96.59     
0.50     16         25         82.36      96.60     
0.50     16         50         82.30      96.62     
0.50     16         75         82.27      96.59     
0.50     16         100        82.35      96.58     
0.50     16         125        82.37      96.62     
0.50     16         150        82.42      96.63     
0.50     16         175        82.34      96.62     
0.50     16         200        82.31      96.59     
0.50     16         220        82.31      96.60     
0.50     32         1          81.35      96.50     
0.50     32         25         82.03      96.50     
0.50     32         50         82.11      96.49     
0.50     32         75         82.24      96.55     
0.50     32         100        82.21      96.52     
0.50     32         125        82.08      96.56     
0.50     32         150        82.27      96.59     
0.50     32         175        82.26      96.53     
0.50     32         200        82.12      96.50     
0.50     32         220        82.23      96.57     
0.50     64         1          79.17      96.04     
0.50     64         25         81.91      96.40     
0.50     64         50         81.85      96.46     
0.50     64         75         81.92      96.52     
0.50     64         100        81.75      96.43     
0.50     64         125        81.66      96.49     
0.50     64         150        82.03      96.45     
0.50     64         175        81.84      96.41     
0.50     64         200        81.88      96.42     
0.50     64         220        81.84      96.38     
0.50     128        1          73.90      93.56     
0.50     128        25         81.23      96.14     
0.50     128        50         81.33      96.21     
0.50     128        75         81.38      96.22     
0.50     128        100        80.95      96.16     
0.50     128        125        81.25      96.16     
0.50     128        150        81.55      96.26     
0.50     128        175        81.61      96.30     
0.50     128        200        81.31      96.23     
0.50     128        220        81.49      96.27     
0.50     256        1          60.95      84.38     
0.50     256        25         79.02      94.27     
0.50     256        50         79.47      94.89     
0.50     256        75         79.66      95.47     
0.50     256        100        80.66      95.55     
0.50     256        125        80.15      95.31     
0.50     256        150        80.55      95.64     
0.50     256        175        80.57      95.85     
0.50     256        200        80.81      95.95     
0.50     256        220        80.74      96.04     
0.60     8          1          81.75      96.57     
0.60     8          25         82.27      96.60     
0.60     8          50         82.23      96.60     
0.60     8          75         82.28      96.58     
0.60     8          100        82.24      96.59     
0.60     8          125        82.22      96.58     
0.60     8          150        82.39      96.66     
0.60     8          175        82.23      96.59     
0.60     8          200        82.22      96.58     
0.60     8          220        82.23      96.59     
0.60     16         1          80.87      96.50     
0.60     16         25         82.20      96.52     
0.60     16         50         82.15      96.54     
0.60     16         75         82.08      96.49     
0.60     16         100        82.26      96.48     
0.60     16         125        82.13      96.49     
0.60     16         150        82.26      96.54     
0.60     16         175        82.17      96.52     
0.60     16         200        82.15      96.47     
0.60     16         220        82.07      96.49     
0.60     32         1          80.61      96.35     
0.60     32         25         81.72      96.33     
0.60     32         50         81.88      96.36     
0.60     32         75         82.03      96.44     
0.60     32         100        82.00      96.40     
0.60     32         125        81.78      96.38     
0.60     32         150        82.04      96.51     
0.60     32         175        82.00      96.44     
0.60     32         200        81.78      96.40     
0.60     32         220        82.04      96.44     
0.60     64         1          77.52      95.41     
0.60     64         25         81.48      96.22     
0.60     64         50         81.46      96.28     
0.60     64         75         81.51      96.33     
0.60     64         100        81.31      96.26     
0.60     64         125        81.25      96.34     
0.60     64         150        81.62      96.24     
0.60     64         175        81.39      96.24     
0.60     64         200        81.40      96.22     
0.60     64         220        81.33      96.20     
0.60     128        1          70.60      91.65     
0.60     128        25         80.52      95.82     
0.60     128        50         80.60      95.89     
0.60     128        75         80.61      95.96     
0.60     128        100        80.08      95.85     
0.60     128        125        80.51      95.87     
0.60     128        150        81.05      96.02     
0.60     128        175        81.05      96.04     
0.60     128        200        80.61      95.95     
0.60     128        220        80.71      95.96     
0.60     256        1          51.58      80.13     
0.60     256        25         77.73      93.74     
0.60     256        50         78.17      94.29     
0.60     256        75         78.22      94.93     
0.60     256        100        79.59      95.08     
0.60     256        125        79.15      94.81     
0.60     256        150        79.61      95.16     
0.60     256        175        79.53      95.36     
0.60     256        200        79.72      95.56     
0.60     256        220        79.44      95.59     
0.70     8          1          81.30      96.50     
0.70     8          25         82.03      96.47     
0.70     8          50         82.02      96.46     
0.70     8          75         82.07      96.48     
0.70     8          100        82.02      96.46     
0.70     8          125        82.03      96.45     
0.70     8          150        82.20      96.61     
0.70     8          175        82.01      96.46     
0.70     8          200        82.03      96.46     
0.70     8          220        81.99      96.47     
0.70     16         1          80.08      96.37     
0.70     16         25         81.94      96.40     
0.70     16         50         81.91      96.42     
0.70     16         75         81.73      96.38     
0.70     16         100        82.02      96.38     
0.70     16         125        81.79      96.36     
0.70     16         150        81.99      96.41     
0.70     16         175        81.92      96.41     
0.70     16         200        81.78      96.35     
0.70     16         220        81.78      96.35     
0.70     32         1          79.67      96.18     
0.70     32         25         81.15      96.10     
0.70     32         50         81.09      96.19     
0.70     32         75         81.59      96.27     
0.70     32         100        81.51      96.28     
0.70     32         125        81.32      96.23     
0.70     32         150        81.64      96.32     
0.70     32         175        81.53      96.30     
0.70     32         200        81.29      96.27     
0.70     32         220        81.58      96.26     
0.70     64         1          75.54      94.58     
0.70     64         25         80.31      95.94     
0.70     64         50         80.39      96.06     
0.70     64         75         80.75      96.14     
0.70     64         100        80.41      95.99     
0.70     64         125        80.48      96.11     
0.70     64         150        80.48      96.02     
0.70     64         175        80.66      96.05     
0.70     64         200        80.69      95.92     
0.70     64         220        80.52      96.01     
0.70     128        1          66.49      89.33     
0.70     128        25         78.44      95.40     
0.70     128        50         78.50      95.53     
0.70     128        75         79.31      95.58     
0.70     128        100        78.53      95.40     
0.70     128        125        79.44      95.48     
0.70     128        150        79.89      95.71     
0.70     128        175        80.01      95.70     
0.70     128        200        78.76      95.54     
0.70     128        220        79.45      95.58     
0.70     256        1          46.45      72.85     
0.70     256        25         75.54      93.11     
0.70     256        50         75.18      93.61     
0.70     256        75         74.63      94.23     
0.70     256        100        76.72      94.61     
0.70     256        125        77.07      94.15     
0.70     256        150        77.85      94.56     
0.70     256        175        77.37      94.77     
0.70     256        200        77.09      94.98     
0.70     256        220        77.58      95.05     
0.80     8          1          80.81      96.38     
0.80     8          25         81.77      96.29     
0.80     8          50         81.73      96.30     
0.80     8          75         81.78      96.34     
0.80     8          100        81.73      96.29     
0.80     8          125        81.75      96.28     
0.80     8          150        82.03      96.56     
0.80     8          175        81.75      96.28     
0.80     8          200        81.77      96.28     
0.80     8          220        81.73      96.27     
0.80     16         1          79.11      96.14     
0.80     16         25         81.52      96.19     
0.80     16         50         81.57      96.22     
0.80     16         75         80.44      96.22     
0.80     16         100        81.62      96.22     
0.80     16         125        80.53      96.22     
0.80     16         150        81.67      96.19     
0.80     16         175        81.57      96.18     
0.80     16         200        80.10      96.21     
0.80     16         220        81.42      96.09     
0.80     32         1          78.45      95.86     
0.80     32         25         78.31      95.86     
0.80     32         50         77.02      95.98     
0.80     32         75         80.00      96.11     
0.80     32         100        80.22      96.06     
0.80     32         125        79.83      96.01     
0.80     32         150        80.59      96.09     
0.80     32         175        79.40      96.12     
0.80     32         200        79.67      96.09     
0.80     32         220        80.22      96.09     
0.80     64         1          73.36      93.45     
0.80     64         25         75.95      95.67     
0.80     64         50         76.58      95.72     
0.80     64         75         77.54      95.91     
0.80     64         100        75.83      95.67     
0.80     64         125        78.41      95.89     
0.80     64         150        76.20      95.78     
0.80     64         175        77.80      95.70     
0.80     64         200        77.47      95.64     
0.80     64         220        77.22      95.80     
0.80     128        1          61.88      86.49     
0.80     128        25         71.51      94.95     
0.80     128        50         70.40      95.13     
0.80     128        75         76.59      95.26     
0.80     128        100        74.59      94.84     
0.80     128        125        77.22      94.94     
0.80     128        150        77.32      95.38     
0.80     128        175        77.96      95.32     
0.80     128        200        71.53      94.98     
0.80     128        220        75.12      95.11     
0.80     256        1          42.06      67.19     
0.80     256        25         71.20      92.39     
0.80     256        50         67.76      92.69     
0.80     256        75         65.45      93.30     
0.80     256        100        69.28      93.95     
0.80     256        125        71.96      93.46     
0.80     256        150        72.85      93.92     
0.80     256        175        71.35      94.09     
0.80     256        200        69.15      94.19     
0.80     256        220        72.38      94.28     
0.90     8          1          80.15      96.18     
0.90     8          25         81.29      96.07     
0.90     8          50         81.18      96.14     
0.90     8          75         80.72      96.21     
0.90     8          100        81.18      96.14     
0.90     8          125        81.30      96.09     
0.90     8          150        81.77      96.40     
0.90     8          175        81.19      96.13     
0.90     8          200        81.33      96.09     
0.90     8          220        81.22      96.06     
0.90     16         1          77.88      95.77     
0.90     16         25         80.70      95.97     
0.90     16         50         80.67      96.02     
0.90     16         75         76.04      96.00     
0.90     16         100        79.82      96.11     
0.90     16         125        75.61      95.99     
0.90     16         150        80.79      95.97     
0.90     16         175        80.85      95.95     
0.90     16         200        77.16      96.01     
0.90     16         220        80.48      95.81     
0.90     32         1          77.02      95.42     
0.90     32         25         70.29      95.54     
0.90     32         50         69.34      95.71     
0.90     32         75         74.25      95.85     
0.90     32         100        75.34      95.78     
0.90     32         125        74.81      95.73     
0.90     32         150        76.63      95.86     
0.90     32         175        75.02      95.88     
0.90     32         200        74.90      95.85     
0.90     32         220        76.12      95.90     
0.90     64         1          70.48      92.06     
0.90     64         25         69.29      95.35     
0.90     64         50         70.65      95.41     
0.90     64         75         72.49      95.67     
0.90     64         100        64.49      95.38     
0.90     64         125        73.96      95.51     
0.90     64         150        69.56      95.43     
0.90     64         175        70.44      95.28     
0.90     64         200        65.13      95.34     
0.90     64         220        70.41      95.52     
0.90     128        1          57.48      83.09     
0.90     128        25         64.11      94.29     
0.90     128        50         59.42      94.54     
0.90     128        75         71.03      94.79     
0.90     128        100        65.60      94.12     
0.90     128        125        71.30      94.37     
0.90     128        150        71.52      95.03     
0.90     128        175        74.25      94.90     
0.90     128        200        59.00      94.23     
0.90     128        220        65.10      94.55     
0.90     256        1          38.14      63.45     
0.90     256        25         65.64      91.45     
0.90     256        50         58.33      91.62     
0.90     256        75         56.19      92.12     
0.90     256        100        60.41      93.18     
0.90     256        125        63.10      92.66     
0.90     256        150        61.55      93.16     
0.90     256        175        61.70      93.25     
0.90     256        200        59.97      93.40     
0.90     256        220        64.59      93.46     
1.00     8          1          79.35      96.05     
1.00     8          25         78.82      95.89     
1.00     8          50         78.40      95.96     
1.00     8          75         74.64      96.06     
1.00     8          100        78.39      95.96     
1.00     8          125        79.31      95.92     
1.00     8          150        81.35      96.17     
1.00     8          175        78.31      95.96     
1.00     8          200        79.35      95.92     
1.00     8          220        78.73      95.90     
1.00     16         1          76.49      95.27     
1.00     16         25         77.46      95.75     
1.00     16         50         77.46      95.79     
1.00     16         75         71.15      95.76     
1.00     16         100        70.49      95.97     
1.00     16         125        69.81      95.77     
1.00     16         150        77.42      95.80     
1.00     16         175        77.89      95.75     
1.00     16         200        73.78      95.78     
1.00     16         220        76.39      95.57     
1.00     32         1          75.37      94.81     
1.00     32         25         58.98      95.17     
1.00     32         50         60.05      95.45     
1.00     32         75         68.13      95.54     
1.00     32         100        68.24      95.44     
1.00     32         125        67.11      95.46     
1.00     32         150        69.72      95.61     
1.00     32         175        71.62      95.55     
1.00     32         200        71.04      95.56     
1.00     32         220        69.66      95.64     
1.00     64         1          67.63      90.16     
1.00     64         25         61.93      94.99     
1.00     64         50         62.44      94.98     
1.00     64         75         68.44      95.32     
1.00     64         100        53.98      94.97     
1.00     64         125        67.88      95.09     
1.00     64         150        63.21      95.00     
1.00     64         175        60.95      94.76     
1.00     64         200        45.82      94.91     
1.00     64         220        63.95      95.13     
1.00     128        1          53.46      79.29     
1.00     128        25         56.50      93.61     
1.00     128        50         49.15      93.78     
1.00     128        75         59.55      94.17     
1.00     128        100        52.86      93.01     
1.00     128        125        60.06      93.53     
1.00     128        150        58.50      94.53     
1.00     128        175        67.49      94.44     
1.00     128        200        47.99      93.19     
1.00     128        220        54.44      93.88     
1.00     256        1          34.82      59.73     
1.00     256        25         60.51      90.34     
1.00     256        50         48.60      90.28     
1.00     256        75         48.45      90.76     
1.00     256        100        50.91      92.27     
1.00     256        125        52.09      91.73     
1.00     256        150        48.39      92.22     
1.00     256        175        50.11      92.29     
1.00     256        200        50.67      92.28     
1.00     256        220        56.84      92.31     

BEST RESULT:
  Alpha: 0.1
  Clusters: 16
  PCA_dim: 200
  Top-1 Accuracy: 82.65%
  Top-5 Accuracy: 96.67%
2025-09-11 06:36:16,136 - INFO - Experiment for seed 1001 completed in 225520.59 seconds
2025-09-11 06:36:16,384 - INFO - SUCCESS: Experiment for seed 1001 completed successfully
2025-09-11 06:36:16,410 - INFO - Looking for results in: ./checkpoint/quant_result/20250910_1419
2025-09-11 06:36:16,416 - INFO - Parsed 0 reconstructed results from log file for seed 1001
2025-09-11 06:36:16,416 - INFO - Parsed 0 baseline results from log file for seed 1001
2025-09-11 06:36:16,416 - INFO - Seed 1001 completed successfully
2025-09-11 06:36:16,416 - INFO - Sleeping for 0.5 seconds before next seed...
2025-09-11 06:36:16,916 - INFO - 
============================================================
2025-09-11 06:36:16,917 - INFO - Running experiment 2/3 for seed 1002
2025-09-11 06:36:16,917 - INFO - ============================================================
2025-09-11 06:36:16,921 - INFO - Running experiment for seed 1002
2025-09-11 06:36:16,921 - INFO - Command: /home/alz07xz/project/PD-Quant/pd_quant/bin/python ../test_quant.py --model swin_base --w_bit 4 --a_bit 4 --seed 1002 --config ../configs/4bit/brecq_baseline.py --dataset /home/alz07xz/imagenet --calib-size 1000 --calib-batch-size 32 --val-batch-size 500 --num-workers 8 --device cuda --alpha-list 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 --num-clusters-list 8 16 32 64 128 256 --pca-dim-list 1 25 50 75 100 125 150 175 200 220 --calibrate --optimize
2025-09-11 06:36:16,921 - INFO - Working directory: /home/alz07xz/project/APHQ_CAT/brecq_base
2025-09-11 06:38:03 - start the process.
Namespace(model='swin_base', config='../configs/4bit/brecq_baseline.py', dataset='/home/alz07xz/imagenet', val_batch_size=500, num_workers=8, device='cuda', reconstruct_mlp=False, load_reconstruct_checkpoint=False, test_reconstruct_checkpoint=False, calibrate=True, load_calibrate_checkpoint=None, test_calibrate_checkpoint=False, optimize=True, load_optimize_checkpoint='', test_optimize_checkpoint=False, print_freq=10, seed=1002, alpha_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], num_clusters_list=[8, 16, 32, 64, 128, 256], pca_dim_list=[1, 25, 50, 75, 100, 125, 150, 175, 200, 220], w_bit=4, a_bit=4, calib_size=1000, calib_batch_size=32)
deefe /home/alz07xz/project/APHQ_CAT/configs/4bit
Successfully imported Config class!
optim_size: 1024
calib_size: 1000
optim_batch_size: 32
calib_batch_size: 32
w_bit: 4
a_bit: 4
qconv_a_bit: 8
qhead_a_bit: 4
calib_metric: mse
matmul_head_channel_wise: True
token_channel_wise: False
eq_n: 128
search_round: 3
keep_gpu: True
optim_metric: mse
use_mean_hessian: False
temp: 20
recon_metric: hessian_perturb
pct: 0.99
optim_mode: rinp
drop_prob: 1.0
reconstruct_mlp: False
Building model ...
/home/alz07xz/project/PD-Quant/pd_quant/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
No checkpoint found at './checkpoint/vit_raw/swin_base_patch4_window7_224.bin'
Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)
[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Building validation dataloader ...
Validating on test set on fp ...
Test: [0/100]	Time 16.999 (16.999)	Loss 0.4076 (0.4076)	Prec@1 91.800 (91.800)	Prec@5 98.400 (98.400)
Test: [10/100]	Time 1.053 (2.583)	Loss 0.4707 (0.5107)	Prec@1 91.600 (88.745)	Prec@5 98.800 (98.491)
Test: [20/100]	Time 1.059 (1.857)	Loss 0.5991 (0.5373)	Prec@1 86.000 (88.381)	Prec@5 98.000 (98.171)
Test: [30/100]	Time 1.067 (1.600)	Loss 0.4928 (0.5636)	Prec@1 88.200 (87.555)	Prec@5 99.400 (98.129)
Test: [40/100]	Time 1.061 (1.469)	Loss 0.7451 (0.5610)	Prec@1 82.400 (87.663)	Prec@5 97.000 (98.185)
Test: [50/100]	Time 1.071 (1.390)	Loss 0.9181 (0.6040)	Prec@1 77.800 (86.451)	Prec@5 94.800 (97.808)
Test: [60/100]	Time 1.070 (1.337)	Loss 0.5948 (0.6094)	Prec@1 87.200 (86.338)	Prec@5 96.600 (97.764)
Test: [70/100]	Time 1.066 (1.300)	Loss 0.6936 (0.6248)	Prec@1 84.200 (85.859)	Prec@5 97.800 (97.668)
Test: [80/100]	Time 1.069 (1.270)	Loss 0.4770 (0.6272)	Prec@1 88.400 (85.780)	Prec@5 99.200 (97.602)
Test: [90/100]	Time 1.060 (1.248)	Loss 0.9203 (0.6428)	Prec@1 77.000 (85.305)	Prec@5 95.400 (97.525)
 * Prec@1 85.274 Prec@5 97.568 Loss 0.641 Time 123.284
Wraping quantiztion modules (reparam: False, recon: False) ...
2025-09-11 06:40:57 - start mse guided calibration
  0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   1%|          | 1/149 [00:15<39:06, 15.85s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|          | 1/149 [00:15<39:06, 15.85s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|▏         | 2/149 [01:35<2:10:22, 53.21s/it]calibrating layers.0.blocks.0.attn.proj:   1%|▏         | 2/149 [01:35<2:10:22, 53.21s/it]calibrating layers.0.blocks.0.attn.proj:   2%|▏         | 3/149 [02:15<1:54:41, 47.14s/it]calibrating layers.0.blocks.0.attn.matmul1:   2%|▏         | 3/149 [02:15<1:54:41, 47.14s/it]calibrating layers.0.blocks.0.attn.matmul1:   3%|▎         | 4/149 [07:43<6:22:16, 158.18s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 4/149 [07:43<6:22:16, 158.18s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 5/149 [13:12<8:47:09, 219.65s/it]calibrating layers.0.blocks.0.mlp.fc1:   3%|▎         | 5/149 [13:12<8:47:09, 219.65s/it]     calibrating layers.0.blocks.0.mlp.fc1:   4%|▍         | 6/149 [16:14<8:13:15, 206.96s/it]calibrating layers.0.blocks.0.mlp.fc2:   4%|▍         | 6/149 [16:14<8:13:15, 206.96s/it]calibrating layers.0.blocks.0.mlp.fc2:   5%|▍         | 7/149 [19:23<7:56:13, 201.22s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▍         | 7/149 [19:23<7:56:13, 201.22s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▌         | 8/149 [20:42<6:21:20, 162.28s/it]calibrating layers.0.blocks.1.attn.proj:   5%|▌         | 8/149 [20:42<6:21:20, 162.28s/it]calibrating layers.0.blocks.1.attn.proj:   6%|▌         | 9/149 [21:22<4:49:23, 124.02s/it]calibrating layers.0.blocks.1.attn.matmul1:   6%|▌         | 9/149 [21:22<4:49:23, 124.02s/it]calibrating layers.0.blocks.1.attn.matmul1:   7%|▋         | 10/149 [26:49<7:12:15, 186.59s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 10/149 [26:49<7:12:15, 186.59s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 11/149 [32:15<8:47:32, 229.37s/it]calibrating layers.0.blocks.1.mlp.fc1:   7%|▋         | 11/149 [32:15<8:47:32, 229.37s/it]     calibrating layers.0.blocks.1.mlp.fc1:   8%|▊         | 12/149 [35:18<8:11:10, 215.11s/it]calibrating layers.0.blocks.1.mlp.fc2:   8%|▊         | 12/149 [35:18<8:11:10, 215.11s/it]calibrating layers.0.blocks.1.mlp.fc2:   9%|▊         | 13/149 [38:27<7:49:56, 207.32s/it]calibrating layers.1.downsample.reduction:   9%|▊         | 13/149 [38:27<7:49:56, 207.32s/it]calibrating layers.1.downsample.reduction:   9%|▉         | 14/149 [39:09<5:54:02, 157.35s/it]calibrating layers.1.blocks.0.attn.qkv:   9%|▉         | 14/149 [39:09<5:54:02, 157.35s/it]   calibrating layers.1.blocks.0.attn.qkv:  10%|█         | 15/149 [39:55<4:36:14, 123.69s/it]calibrating layers.1.blocks.0.attn.proj:  10%|█         | 15/149 [39:55<4:36:14, 123.69s/it]calibrating layers.1.blocks.0.attn.proj:  11%|█         | 16/149 [40:18<3:26:51, 93.32s/it] calibrating layers.1.blocks.0.attn.matmul1:  11%|█         | 16/149 [40:18<3:26:51, 93.32s/it]calibrating layers.1.blocks.0.attn.matmul1:  11%|█▏        | 17/149 [40:56<2:49:00, 76.82s/it]calibrating layers.1.blocks.0.attn.matmul2:  11%|█▏        | 17/149 [40:56<2:49:00, 76.82s/it]calibrating layers.1.blocks.0.attn.matmul2:  12%|█▏        | 18/149 [41:50<2:32:35, 69.89s/it]calibrating layers.1.blocks.0.mlp.fc1:  12%|█▏        | 18/149 [41:50<2:32:35, 69.89s/it]     calibrating layers.1.blocks.0.mlp.fc1:  13%|█▎        | 19/149 [43:20<2:44:46, 76.05s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 19/149 [43:20<2:44:46, 76.05s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 20/149 [44:54<2:54:43, 81.27s/it]calibrating layers.1.blocks.1.attn.qkv:  13%|█▎        | 20/149 [44:54<2:54:43, 81.27s/it]calibrating layers.1.blocks.1.attn.qkv:  14%|█▍        | 21/149 [45:41<2:31:44, 71.13s/it]calibrating layers.1.blocks.1.attn.proj:  14%|█▍        | 21/149 [45:41<2:31:44, 71.13s/it]calibrating layers.1.blocks.1.attn.proj:  15%|█▍        | 22/149 [46:05<2:00:44, 57.04s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▍        | 22/149 [46:05<2:00:44, 57.04s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▌        | 23/149 [46:44<1:48:30, 51.67s/it]calibrating layers.1.blocks.1.attn.matmul2:  15%|█▌        | 23/149 [46:44<1:48:30, 51.67s/it]calibrating layers.1.blocks.1.attn.matmul2:  16%|█▌        | 24/149 [47:38<1:48:57, 52.30s/it]calibrating layers.1.blocks.1.mlp.fc1:  16%|█▌        | 24/149 [47:38<1:48:57, 52.30s/it]     calibrating layers.1.blocks.1.mlp.fc1:  17%|█▋        | 25/149 [49:09<2:12:03, 63.90s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 25/149 [49:09<2:12:03, 63.90s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 26/149 [50:43<2:29:31, 72.94s/it]calibrating layers.2.downsample.reduction:  17%|█▋        | 26/149 [50:43<2:29:31, 72.94s/it]calibrating layers.2.downsample.reduction:  18%|█▊        | 27/149 [51:09<1:59:29, 58.77s/it]calibrating layers.2.blocks.0.attn.qkv:  18%|█▊        | 27/149 [51:09<1:59:29, 58.77s/it]   calibrating layers.2.blocks.0.attn.qkv:  19%|█▉        | 28/149 [51:42<1:42:59, 51.07s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 28/149 [51:42<1:42:59, 51.07s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 29/149 [51:57<1:20:25, 40.21s/it]calibrating layers.2.blocks.0.attn.matmul1:  19%|█▉        | 29/149 [51:57<1:20:25, 40.21s/it]calibrating layers.2.blocks.0.attn.matmul1:  20%|██        | 30/149 [52:15<1:06:44, 33.66s/it]calibrating layers.2.blocks.0.attn.matmul2:  20%|██        | 30/149 [52:15<1:06:44, 33.66s/it]calibrating layers.2.blocks.0.attn.matmul2:  21%|██        | 31/149 [52:34<57:13, 29.10s/it]  calibrating layers.2.blocks.0.mlp.fc1:  21%|██        | 31/149 [52:34<57:13, 29.10s/it]     calibrating layers.2.blocks.0.mlp.fc1:  21%|██▏       | 32/149 [53:23<1:08:51, 35.31s/it]calibrating layers.2.blocks.0.mlp.fc2:  21%|██▏       | 32/149 [53:23<1:08:51, 35.31s/it]calibrating layers.2.blocks.0.mlp.fc2:  22%|██▏       | 33/149 [54:15<1:17:30, 40.09s/it]calibrating layers.2.blocks.1.attn.qkv:  22%|██▏       | 33/149 [54:15<1:17:30, 40.09s/it]calibrating layers.2.blocks.1.attn.qkv:  23%|██▎       | 34/149 [54:48<1:12:54, 38.04s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 34/149 [54:48<1:12:54, 38.04s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 35/149 [55:03<59:05, 31.10s/it]  calibrating layers.2.blocks.1.attn.matmul1:  23%|██▎       | 35/149 [55:03<59:05, 31.10s/it]calibrating layers.2.blocks.1.attn.matmul1:  24%|██▍       | 36/149 [55:21<51:24, 27.29s/it]calibrating layers.2.blocks.1.attn.matmul2:  24%|██▍       | 36/149 [55:21<51:24, 27.29s/it]calibrating layers.2.blocks.1.attn.matmul2:  25%|██▍       | 37/149 [55:40<46:10, 24.74s/it]calibrating layers.2.blocks.1.mlp.fc1:  25%|██▍       | 37/149 [55:40<46:10, 24.74s/it]     calibrating layers.2.blocks.1.mlp.fc1:  26%|██▌       | 38/149 [56:30<59:45, 32.30s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 38/149 [56:30<59:45, 32.30s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 39/149 [57:21<1:09:27, 37.89s/it]calibrating layers.2.blocks.2.attn.qkv:  26%|██▌       | 39/149 [57:21<1:09:27, 37.89s/it]calibrating layers.2.blocks.2.attn.qkv:  27%|██▋       | 40/149 [57:54<1:06:17, 36.49s/it]calibrating layers.2.blocks.2.attn.proj:  27%|██▋       | 40/149 [57:54<1:06:17, 36.49s/it]calibrating layers.2.blocks.2.attn.proj:  28%|██▊       | 41/149 [58:09<54:04, 30.04s/it]  calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 41/149 [58:09<54:04, 30.04s/it]calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 42/149 [58:28<47:23, 26.57s/it]calibrating layers.2.blocks.2.attn.matmul2:  28%|██▊       | 42/149 [58:28<47:23, 26.57s/it]calibrating layers.2.blocks.2.attn.matmul2:  29%|██▉       | 43/149 [58:46<42:41, 24.17s/it]calibrating layers.2.blocks.2.mlp.fc1:  29%|██▉       | 43/149 [58:46<42:41, 24.17s/it]     calibrating layers.2.blocks.2.mlp.fc1:  30%|██▉       | 44/149 [59:36<55:47, 31.88s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|██▉       | 44/149 [59:36<55:47, 31.88s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|███       | 45/149 [1:00:27<1:05:18, 37.68s/it]calibrating layers.2.blocks.3.attn.qkv:  30%|███       | 45/149 [1:00:27<1:05:18, 37.68s/it]calibrating layers.2.blocks.3.attn.qkv:  31%|███       | 46/149 [1:01:01<1:02:25, 36.37s/it]calibrating layers.2.blocks.3.attn.proj:  31%|███       | 46/149 [1:01:01<1:02:25, 36.37s/it]calibrating layers.2.blocks.3.attn.proj:  32%|███▏      | 47/149 [1:01:16<51:01, 30.01s/it]  calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 47/149 [1:01:16<51:01, 30.01s/it]calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 48/149 [1:01:34<44:33, 26.47s/it]calibrating layers.2.blocks.3.attn.matmul2:  32%|███▏      | 48/149 [1:01:34<44:33, 26.47s/it]calibrating layers.2.blocks.3.attn.matmul2:  33%|███▎      | 49/149 [1:01:52<40:06, 24.07s/it]calibrating layers.2.blocks.3.mlp.fc1:  33%|███▎      | 49/149 [1:01:52<40:06, 24.07s/it]     calibrating layers.2.blocks.3.mlp.fc1:  34%|███▎      | 50/149 [1:02:43<52:34, 31.87s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▎      | 50/149 [1:02:43<52:34, 31.87s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▍      | 51/149 [1:03:34<1:01:32, 37.68s/it]calibrating layers.2.blocks.4.attn.qkv:  34%|███▍      | 51/149 [1:03:34<1:01:32, 37.68s/it]calibrating layers.2.blocks.4.attn.qkv:  35%|███▍      | 52/149 [1:04:07<58:44, 36.33s/it]  calibrating layers.2.blocks.4.attn.proj:  35%|███▍      | 52/149 [1:04:07<58:44, 36.33s/it]calibrating layers.2.blocks.4.attn.proj:  36%|███▌      | 53/149 [1:04:22<47:53, 29.94s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 53/149 [1:04:22<47:53, 29.94s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 54/149 [1:04:40<41:51, 26.44s/it]calibrating layers.2.blocks.4.attn.matmul2:  36%|███▌      | 54/149 [1:04:40<41:51, 26.44s/it]calibrating layers.2.blocks.4.attn.matmul2:  37%|███▋      | 55/149 [1:04:59<37:42, 24.07s/it]calibrating layers.2.blocks.4.mlp.fc1:  37%|███▋      | 55/149 [1:04:59<37:42, 24.07s/it]     calibrating layers.2.blocks.4.mlp.fc1:  38%|███▊      | 56/149 [1:05:49<49:20, 31.83s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 56/149 [1:05:49<49:20, 31.83s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 57/149 [1:06:40<57:33, 37.54s/it]calibrating layers.2.blocks.5.attn.qkv:  38%|███▊      | 57/149 [1:06:40<57:33, 37.54s/it]calibrating layers.2.blocks.5.attn.qkv:  39%|███▉      | 58/149 [1:07:13<54:57, 36.23s/it]calibrating layers.2.blocks.5.attn.proj:  39%|███▉      | 58/149 [1:07:13<54:57, 36.23s/it]calibrating layers.2.blocks.5.attn.proj:  40%|███▉      | 59/149 [1:07:28<44:48, 29.87s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|███▉      | 59/149 [1:07:28<44:48, 29.87s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|████      | 60/149 [1:07:46<39:12, 26.43s/it]calibrating layers.2.blocks.5.attn.matmul2:  40%|████      | 60/149 [1:07:46<39:12, 26.43s/it]calibrating layers.2.blocks.5.attn.matmul2:  41%|████      | 61/149 [1:08:05<35:19, 24.08s/it]calibrating layers.2.blocks.5.mlp.fc1:  41%|████      | 61/149 [1:08:05<35:19, 24.08s/it]     calibrating layers.2.blocks.5.mlp.fc1:  42%|████▏     | 62/149 [1:08:55<46:06, 31.80s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 62/149 [1:08:55<46:06, 31.80s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 63/149 [1:09:46<53:54, 37.61s/it]calibrating layers.2.blocks.6.attn.qkv:  42%|████▏     | 63/149 [1:09:46<53:54, 37.61s/it]calibrating layers.2.blocks.6.attn.qkv:  43%|████▎     | 64/149 [1:10:19<51:26, 36.31s/it]calibrating layers.2.blocks.6.attn.proj:  43%|████▎     | 64/149 [1:10:19<51:26, 36.31s/it]calibrating layers.2.blocks.6.attn.proj:  44%|████▎     | 65/149 [1:10:34<41:55, 29.95s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▎     | 65/149 [1:10:34<41:55, 29.95s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▍     | 66/149 [1:10:52<36:33, 26.43s/it]calibrating layers.2.blocks.6.attn.matmul2:  44%|████▍     | 66/149 [1:10:52<36:33, 26.43s/it]calibrating layers.2.blocks.6.attn.matmul2:  45%|████▍     | 67/149 [1:11:11<32:50, 24.03s/it]calibrating layers.2.blocks.6.mlp.fc1:  45%|████▍     | 67/149 [1:11:11<32:50, 24.03s/it]     calibrating layers.2.blocks.6.mlp.fc1:  46%|████▌     | 68/149 [1:12:01<42:57, 31.82s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▌     | 68/149 [1:12:01<42:57, 31.82s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▋     | 69/149 [1:12:52<50:10, 37.63s/it]calibrating layers.2.blocks.7.attn.qkv:  46%|████▋     | 69/149 [1:12:52<50:10, 37.63s/it]calibrating layers.2.blocks.7.attn.qkv:  47%|████▋     | 70/149 [1:13:25<47:49, 36.32s/it]calibrating layers.2.blocks.7.attn.proj:  47%|████▋     | 70/149 [1:13:25<47:49, 36.32s/it]calibrating layers.2.blocks.7.attn.proj:  48%|████▊     | 71/149 [1:13:40<38:57, 29.97s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 71/149 [1:13:40<38:57, 29.97s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 72/149 [1:13:59<33:53, 26.41s/it]calibrating layers.2.blocks.7.attn.matmul2:  48%|████▊     | 72/149 [1:13:59<33:53, 26.41s/it]calibrating layers.2.blocks.7.attn.matmul2:  49%|████▉     | 73/149 [1:14:17<30:28, 24.06s/it]calibrating layers.2.blocks.7.mlp.fc1:  49%|████▉     | 73/149 [1:14:17<30:28, 24.06s/it]     calibrating layers.2.blocks.7.mlp.fc1:  50%|████▉     | 74/149 [1:15:07<39:49, 31.87s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|████▉     | 74/149 [1:15:07<39:49, 31.87s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|█████     | 75/149 [1:15:58<46:26, 37.66s/it]calibrating layers.2.blocks.8.attn.qkv:  50%|█████     | 75/149 [1:15:58<46:26, 37.66s/it]calibrating layers.2.blocks.8.attn.qkv:  51%|█████     | 76/149 [1:16:32<44:12, 36.34s/it]calibrating layers.2.blocks.8.attn.proj:  51%|█████     | 76/149 [1:16:32<44:12, 36.34s/it]calibrating layers.2.blocks.8.attn.proj:  52%|█████▏    | 77/149 [1:16:47<35:57, 29.96s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 77/149 [1:16:47<35:57, 29.96s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 78/149 [1:17:05<31:18, 26.45s/it]calibrating layers.2.blocks.8.attn.matmul2:  52%|█████▏    | 78/149 [1:17:05<31:18, 26.45s/it]calibrating layers.2.blocks.8.attn.matmul2:  53%|█████▎    | 79/149 [1:17:24<28:07, 24.11s/it]calibrating layers.2.blocks.8.mlp.fc1:  53%|█████▎    | 79/149 [1:17:24<28:07, 24.11s/it]     calibrating layers.2.blocks.8.mlp.fc1:  54%|█████▎    | 80/149 [1:18:14<36:42, 31.92s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▎    | 80/149 [1:18:14<36:42, 31.92s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▍    | 81/149 [1:19:05<42:44, 37.71s/it]calibrating layers.2.blocks.9.attn.qkv:  54%|█████▍    | 81/149 [1:19:05<42:44, 37.71s/it]calibrating layers.2.blocks.9.attn.qkv:  55%|█████▌    | 82/149 [1:19:38<40:36, 36.36s/it]calibrating layers.2.blocks.9.attn.proj:  55%|█████▌    | 82/149 [1:19:38<40:36, 36.36s/it]calibrating layers.2.blocks.9.attn.proj:  56%|█████▌    | 83/149 [1:19:53<32:58, 29.98s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▌    | 83/149 [1:19:53<32:58, 29.98s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▋    | 84/149 [1:20:11<28:37, 26.43s/it]calibrating layers.2.blocks.9.attn.matmul2:  56%|█████▋    | 84/149 [1:20:11<28:37, 26.43s/it]calibrating layers.2.blocks.9.attn.matmul2:  57%|█████▋    | 85/149 [1:20:30<25:38, 24.04s/it]calibrating layers.2.blocks.9.mlp.fc1:  57%|█████▋    | 85/149 [1:20:30<25:38, 24.04s/it]     calibrating layers.2.blocks.9.mlp.fc1:  58%|█████▊    | 86/149 [1:21:20<33:24, 31.82s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 86/149 [1:21:20<33:24, 31.82s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 87/149 [1:22:11<38:48, 37.56s/it]calibrating layers.2.blocks.10.attn.qkv:  58%|█████▊    | 87/149 [1:22:11<38:48, 37.56s/it]calibrating layers.2.blocks.10.attn.qkv:  59%|█████▉    | 88/149 [1:22:44<36:51, 36.26s/it]calibrating layers.2.blocks.10.attn.proj:  59%|█████▉    | 88/149 [1:22:44<36:51, 36.26s/it]calibrating layers.2.blocks.10.attn.proj:  60%|█████▉    | 89/149 [1:22:59<29:54, 29.90s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|█████▉    | 89/149 [1:22:59<29:54, 29.90s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|██████    | 90/149 [1:23:17<25:57, 26.40s/it]calibrating layers.2.blocks.10.attn.matmul2:  60%|██████    | 90/149 [1:23:17<25:57, 26.40s/it]calibrating layers.2.blocks.10.attn.matmul2:  61%|██████    | 91/149 [1:23:36<23:13, 24.03s/it]calibrating layers.2.blocks.10.mlp.fc1:  61%|██████    | 91/149 [1:23:36<23:13, 24.03s/it]     calibrating layers.2.blocks.10.mlp.fc1:  62%|██████▏   | 92/149 [1:24:26<30:11, 31.78s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 92/149 [1:24:26<30:11, 31.78s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 93/149 [1:25:17<35:07, 37.63s/it]calibrating layers.2.blocks.11.attn.qkv:  62%|██████▏   | 93/149 [1:25:17<35:07, 37.63s/it]calibrating layers.2.blocks.11.attn.qkv:  63%|██████▎   | 94/149 [1:25:50<33:16, 36.30s/it]calibrating layers.2.blocks.11.attn.proj:  63%|██████▎   | 94/149 [1:25:50<33:16, 36.30s/it]calibrating layers.2.blocks.11.attn.proj:  64%|██████▍   | 95/149 [1:26:05<26:55, 29.91s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 95/149 [1:26:05<26:55, 29.91s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 96/149 [1:26:23<23:18, 26.38s/it]calibrating layers.2.blocks.11.attn.matmul2:  64%|██████▍   | 96/149 [1:26:23<23:18, 26.38s/it]calibrating layers.2.blocks.11.attn.matmul2:  65%|██████▌   | 97/149 [1:26:42<20:48, 24.01s/it]calibrating layers.2.blocks.11.mlp.fc1:  65%|██████▌   | 97/149 [1:26:42<20:48, 24.01s/it]     calibrating layers.2.blocks.11.mlp.fc1:  66%|██████▌   | 98/149 [1:27:32<27:00, 31.78s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▌   | 98/149 [1:27:32<27:00, 31.78s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▋   | 99/149 [1:28:23<31:21, 37.64s/it]calibrating layers.2.blocks.12.attn.qkv:  66%|██████▋   | 99/149 [1:28:23<31:21, 37.64s/it]calibrating layers.2.blocks.12.attn.qkv:  67%|██████▋   | 100/149 [1:28:56<29:39, 36.33s/it]calibrating layers.2.blocks.12.attn.proj:  67%|██████▋   | 100/149 [1:28:56<29:39, 36.33s/it]calibrating layers.2.blocks.12.attn.proj:  68%|██████▊   | 101/149 [1:29:11<23:57, 29.95s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 101/149 [1:29:11<23:57, 29.95s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 102/149 [1:29:29<20:41, 26.42s/it]calibrating layers.2.blocks.12.attn.matmul2:  68%|██████▊   | 102/149 [1:29:29<20:41, 26.42s/it]calibrating layers.2.blocks.12.attn.matmul2:  69%|██████▉   | 103/149 [1:29:48<18:24, 24.02s/it]calibrating layers.2.blocks.12.mlp.fc1:  69%|██████▉   | 103/149 [1:29:48<18:24, 24.02s/it]     calibrating layers.2.blocks.12.mlp.fc1:  70%|██████▉   | 104/149 [1:30:38<23:50, 31.80s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|██████▉   | 104/149 [1:30:38<23:50, 31.80s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|███████   | 105/149 [1:31:29<27:35, 37.64s/it]calibrating layers.2.blocks.13.attn.qkv:  70%|███████   | 105/149 [1:31:29<27:35, 37.64s/it]calibrating layers.2.blocks.13.attn.qkv:  71%|███████   | 106/149 [1:32:02<26:02, 36.35s/it]calibrating layers.2.blocks.13.attn.proj:  71%|███████   | 106/149 [1:32:02<26:02, 36.35s/it]calibrating layers.2.blocks.13.attn.proj:  72%|███████▏  | 107/149 [1:32:18<20:58, 29.97s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 107/149 [1:32:18<20:58, 29.97s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 108/149 [1:32:36<18:03, 26.44s/it]calibrating layers.2.blocks.13.attn.matmul2:  72%|███████▏  | 108/149 [1:32:36<18:03, 26.44s/it]calibrating layers.2.blocks.13.attn.matmul2:  73%|███████▎  | 109/149 [1:32:54<16:03, 24.08s/it]calibrating layers.2.blocks.13.mlp.fc1:  73%|███████▎  | 109/149 [1:32:54<16:03, 24.08s/it]     calibrating layers.2.blocks.13.mlp.fc1:  74%|███████▍  | 110/149 [1:33:44<20:42, 31.85s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 110/149 [1:33:44<20:42, 31.85s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 111/149 [1:34:36<23:51, 37.67s/it]calibrating layers.2.blocks.14.attn.qkv:  74%|███████▍  | 111/149 [1:34:36<23:51, 37.67s/it]calibrating layers.2.blocks.14.attn.qkv:  75%|███████▌  | 112/149 [1:35:09<22:24, 36.35s/it]calibrating layers.2.blocks.14.attn.proj:  75%|███████▌  | 112/149 [1:35:09<22:24, 36.35s/it]calibrating layers.2.blocks.14.attn.proj:  76%|███████▌  | 113/149 [1:35:24<17:58, 29.97s/it]calibrating layers.2.blocks.14.attn.matmul1:  76%|███████▌  | 113/149 [1:35:24<17:58, 29.97s/it]calibrating layers.2.blocks.14.attn.matmul1:  77%|███████▋  | 114/149 [1:35:42<15:25, 26.44s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 114/149 [1:35:42<15:25, 26.44s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 115/149 [1:36:01<13:36, 24.02s/it]calibrating layers.2.blocks.14.mlp.fc1:  77%|███████▋  | 115/149 [1:36:01<13:36, 24.02s/it]     calibrating layers.2.blocks.14.mlp.fc1:  78%|███████▊  | 116/149 [1:36:51<17:31, 31.85s/it]calibrating layers.2.blocks.14.mlp.fc2:  78%|███████▊  | 116/149 [1:36:51<17:31, 31.85s/it]calibrating layers.2.blocks.14.mlp.fc2:  79%|███████▊  | 117/149 [1:37:42<20:05, 37.68s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▊  | 117/149 [1:37:42<20:05, 37.68s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▉  | 118/149 [1:38:15<18:46, 36.34s/it]calibrating layers.2.blocks.15.attn.proj:  79%|███████▉  | 118/149 [1:38:15<18:46, 36.34s/it]calibrating layers.2.blocks.15.attn.proj:  80%|███████▉  | 119/149 [1:38:30<14:57, 29.90s/it]calibrating layers.2.blocks.15.attn.matmul1:  80%|███████▉  | 119/149 [1:38:30<14:57, 29.90s/it]calibrating layers.2.blocks.15.attn.matmul1:  81%|████████  | 120/149 [1:38:48<12:45, 26.40s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 120/149 [1:38:48<12:45, 26.40s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 121/149 [1:39:07<11:14, 24.10s/it]calibrating layers.2.blocks.15.mlp.fc1:  81%|████████  | 121/149 [1:39:07<11:14, 24.10s/it]     calibrating layers.2.blocks.15.mlp.fc1:  82%|████████▏ | 122/149 [1:39:57<14:20, 31.88s/it]calibrating layers.2.blocks.15.mlp.fc2:  82%|████████▏ | 122/149 [1:39:57<14:20, 31.88s/it]calibrating layers.2.blocks.15.mlp.fc2:  83%|████████▎ | 123/149 [1:40:48<16:18, 37.64s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 123/149 [1:40:48<16:18, 37.64s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 124/149 [1:41:21<15:08, 36.33s/it]calibrating layers.2.blocks.16.attn.proj:  83%|████████▎ | 124/149 [1:41:21<15:08, 36.33s/it]calibrating layers.2.blocks.16.attn.proj:  84%|████████▍ | 125/149 [1:41:36<11:58, 29.94s/it]calibrating layers.2.blocks.16.attn.matmul1:  84%|████████▍ | 125/149 [1:41:36<11:58, 29.94s/it]calibrating layers.2.blocks.16.attn.matmul1:  85%|████████▍ | 126/149 [1:41:55<10:08, 26.46s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▍ | 126/149 [1:41:55<10:08, 26.46s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▌ | 127/149 [1:42:13<08:49, 24.06s/it]calibrating layers.2.blocks.16.mlp.fc1:  85%|████████▌ | 127/149 [1:42:13<08:49, 24.06s/it]     calibrating layers.2.blocks.16.mlp.fc1:  86%|████████▌ | 128/149 [1:43:03<11:08, 31.85s/it]calibrating layers.2.blocks.16.mlp.fc2:  86%|████████▌ | 128/149 [1:43:03<11:08, 31.85s/it]calibrating layers.2.blocks.16.mlp.fc2:  87%|████████▋ | 129/149 [1:43:55<12:33, 37.68s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 129/149 [1:43:55<12:33, 37.68s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 130/149 [1:44:28<11:30, 36.34s/it]calibrating layers.2.blocks.17.attn.proj:  87%|████████▋ | 130/149 [1:44:28<11:30, 36.34s/it]calibrating layers.2.blocks.17.attn.proj:  88%|████████▊ | 131/149 [1:44:43<08:59, 29.97s/it]calibrating layers.2.blocks.17.attn.matmul1:  88%|████████▊ | 131/149 [1:44:43<08:59, 29.97s/it]calibrating layers.2.blocks.17.attn.matmul1:  89%|████████▊ | 132/149 [1:45:01<07:29, 26.45s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▊ | 132/149 [1:45:01<07:29, 26.45s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▉ | 133/149 [1:45:20<06:25, 24.08s/it]calibrating layers.2.blocks.17.mlp.fc1:  89%|████████▉ | 133/149 [1:45:20<06:25, 24.08s/it]     calibrating layers.2.blocks.17.mlp.fc1:  90%|████████▉ | 134/149 [1:46:10<07:58, 31.91s/it]calibrating layers.2.blocks.17.mlp.fc2:  90%|████████▉ | 134/149 [1:46:10<07:58, 31.91s/it]calibrating layers.2.blocks.17.mlp.fc2:  91%|█████████ | 135/149 [1:47:01<08:48, 37.72s/it]calibrating layers.3.downsample.reduction:  91%|█████████ | 135/149 [1:47:01<08:48, 37.72s/it]calibrating layers.3.downsample.reduction:  91%|█████████▏| 136/149 [1:47:18<06:51, 31.62s/it]calibrating layers.3.blocks.0.attn.qkv:  91%|█████████▏| 136/149 [1:47:18<06:51, 31.62s/it]   calibrating layers.3.blocks.0.attn.qkv:  92%|█████████▏| 137/149 [1:47:45<05:59, 29.96s/it]calibrating layers.3.blocks.0.attn.proj:  92%|█████████▏| 137/149 [1:47:45<05:59, 29.96s/it]calibrating layers.3.blocks.0.attn.proj:  93%|█████████▎| 138/149 [1:47:56<04:28, 24.42s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 138/149 [1:47:56<04:28, 24.42s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 139/149 [1:48:13<03:43, 22.30s/it]calibrating layers.3.blocks.0.attn.matmul2:  93%|█████████▎| 139/149 [1:48:13<03:43, 22.30s/it]calibrating layers.3.blocks.0.attn.matmul2:  94%|█████████▍| 140/149 [1:48:30<03:05, 20.60s/it]calibrating layers.3.blocks.0.mlp.fc1:  94%|█████████▍| 140/149 [1:48:30<03:05, 20.60s/it]     calibrating layers.3.blocks.0.mlp.fc1:  95%|█████████▍| 141/149 [1:49:03<03:14, 24.29s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▍| 141/149 [1:49:03<03:14, 24.29s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▌| 142/149 [1:49:37<03:09, 27.09s/it]calibrating layers.3.blocks.1.attn.qkv:  95%|█████████▌| 142/149 [1:49:37<03:09, 27.09s/it]calibrating layers.3.blocks.1.attn.qkv:  96%|█████████▌| 143/149 [1:50:03<02:40, 26.75s/it]calibrating layers.3.blocks.1.attn.proj:  96%|█████████▌| 143/149 [1:50:03<02:40, 26.75s/it]calibrating layers.3.blocks.1.attn.proj:  97%|█████████▋| 144/149 [1:50:14<01:50, 22.19s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 144/149 [1:50:14<01:50, 22.19s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 145/149 [1:50:31<01:22, 20.75s/it]calibrating layers.3.blocks.1.attn.matmul2:  97%|█████████▋| 145/149 [1:50:31<01:22, 20.75s/it]calibrating layers.3.blocks.1.attn.matmul2:  98%|█████████▊| 146/149 [1:50:48<00:58, 19.51s/it]calibrating layers.3.blocks.1.mlp.fc1:  98%|█████████▊| 146/149 [1:50:48<00:58, 19.51s/it]     calibrating layers.3.blocks.1.mlp.fc1:  99%|█████████▊| 147/149 [1:51:21<00:47, 23.50s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▊| 147/149 [1:51:21<00:47, 23.50s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▉| 148/149 [1:51:54<00:26, 26.47s/it]calibrating head.fc:  99%|█████████▉| 148/149 [1:51:54<00:26, 26.47s/it]                  calibrating head.fc: 100%|██████████| 149/149 [1:51:59<00:00, 19.81s/it]calibrating head.fc: 100%|██████████| 149/149 [1:51:59<00:00, 45.09s/it]
2025-09-11 08:33:13 - mse guided calibration finished.
Saving checkpoint to ./checkpoint/quant_result/20250911_0638/swin_base_w4_a4_calibsize_1000_mse.pth
Validating after calibration ...
Test: [0/100]	Time 6.016 (6.016)	Loss 1.0495 (1.0495)	Prec@1 88.800 (88.800)	Prec@5 97.400 (97.400)
Test: [10/100]	Time 2.377 (2.712)	Loss 1.0411 (1.1542)	Prec@1 86.400 (84.527)	Prec@5 96.000 (97.073)
Test: [20/100]	Time 2.378 (2.554)	Loss 1.5026 (1.2256)	Prec@1 79.600 (83.038)	Prec@5 97.400 (96.638)
Test: [30/100]	Time 2.379 (2.498)	Loss 1.0975 (1.2627)	Prec@1 84.400 (82.277)	Prec@5 98.200 (96.594)
Test: [40/100]	Time 2.382 (2.469)	Loss 1.2569 (1.2328)	Prec@1 78.200 (82.683)	Prec@5 95.800 (96.644)
Test: [50/100]	Time 2.376 (2.451)	Loss 1.5486 (1.2611)	Prec@1 70.400 (81.204)	Prec@5 90.800 (96.071)
Test: [60/100]	Time 2.376 (2.439)	Loss 1.1948 (1.2571)	Prec@1 82.200 (81.049)	Prec@5 94.400 (95.862)
Test: [70/100]	Time 2.377 (2.431)	Loss 1.4821 (1.2756)	Prec@1 74.000 (80.217)	Prec@5 93.200 (95.575)
Test: [80/100]	Time 2.379 (2.425)	Loss 1.1150 (1.2808)	Prec@1 83.800 (80.052)	Prec@5 97.400 (95.370)
Test: [90/100]	Time 2.378 (2.420)	Loss 1.4597 (1.2988)	Prec@1 71.800 (79.431)	Prec@5 92.000 (95.178)
 * Prec@1 79.570 Prec@5 95.308 Loss 1.282 Time 241.955
Building calibrator ...
2025-09-11 08:37:19 - start mse guided block reconstruction
reconstructing patch_embed ...
initializing raw input and raw output ...
adaround training for patch_embed ...
wraping quantizers in patch_embed ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.127 (rec:0.127, round:0.000)	b=0.00	count=500
Total loss:	0.074 (rec:0.074, round:0.000)	b=0.00	count=1000
Total loss:	0.042 (rec:0.042, round:0.000)	b=0.00	count=1500
Total loss:	0.036 (rec:0.036, round:0.000)	b=0.00	count=2000
Total loss:	0.032 (rec:0.032, round:0.000)	b=0.00	count=2500
Total loss:	0.027 (rec:0.027, round:0.000)	b=0.00	count=3000
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=3500
Total loss:	57.803 (rec:0.019, round:57.784)	b=20.00	count=4000
Total loss:	37.606 (rec:0.021, round:37.585)	b=19.44	count=4500
Total loss:	34.864 (rec:0.026, round:34.838)	b=18.88	count=5000
Total loss:	33.380 (rec:0.024, round:33.356)	b=18.31	count=5500
Total loss:	31.898 (rec:0.025, round:31.873)	b=17.75	count=6000
Total loss:	30.553 (rec:0.019, round:30.534)	b=17.19	count=6500
Total loss:	29.055 (rec:0.023, round:29.031)	b=16.62	count=7000
Total loss:	27.593 (rec:0.016, round:27.577)	b=16.06	count=7500
Total loss:	26.242 (rec:0.021, round:26.222)	b=15.50	count=8000
Total loss:	24.616 (rec:0.027, round:24.589)	b=14.94	count=8500
Total loss:	23.018 (rec:0.020, round:22.998)	b=14.38	count=9000
Total loss:	21.461 (rec:0.025, round:21.436)	b=13.81	count=9500
Total loss:	20.121 (rec:0.029, round:20.091)	b=13.25	count=10000
Total loss:	18.594 (rec:0.040, round:18.554)	b=12.69	count=10500
Total loss:	16.891 (rec:0.035, round:16.855)	b=12.12	count=11000
Total loss:	15.275 (rec:0.036, round:15.239)	b=11.56	count=11500
Total loss:	13.564 (rec:0.052, round:13.511)	b=11.00	count=12000
Total loss:	12.113 (rec:0.051, round:12.062)	b=10.44	count=12500
Total loss:	10.708 (rec:0.058, round:10.650)	b=9.88	count=13000
Total loss:	9.170 (rec:0.073, round:9.097)	b=9.31	count=13500
Total loss:	7.711 (rec:0.097, round:7.615)	b=8.75	count=14000
Total loss:	6.367 (rec:0.092, round:6.276)	b=8.19	count=14500
Total loss:	5.253 (rec:0.116, round:5.138)	b=7.62	count=15000
Total loss:	4.285 (rec:0.111, round:4.174)	b=7.06	count=15500
Total loss:	3.384 (rec:0.127, round:3.257)	b=6.50	count=16000
Total loss:	2.693 (rec:0.167, round:2.526)	b=5.94	count=16500
Total loss:	2.171 (rec:0.199, round:1.972)	b=5.38	count=17000
Total loss:	1.692 (rec:0.208, round:1.484)	b=4.81	count=17500
Total loss:	1.311 (rec:0.260, round:1.052)	b=4.25	count=18000
Total loss:	0.929 (rec:0.266, round:0.662)	b=3.69	count=18500
Total loss:	0.798 (rec:0.367, round:0.430)	b=3.12	count=19000
Total loss:	0.736 (rec:0.414, round:0.322)	b=2.56	count=19500
Total loss:	0.623 (rec:0.415, round:0.208)	b=2.00	count=20000
finished reconstructing patch_embed.
reconstructing layers.0.blocks.0 ...
initializing raw input and raw output ...
adaround training for layers.0.blocks.0 ...
wraping quantizers in layers.0.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.011 (rec:1.011, round:0.000)	b=0.00	count=500
Total loss:	0.942 (rec:0.942, round:0.000)	b=0.00	count=1000
Total loss:	0.756 (rec:0.756, round:0.000)	b=0.00	count=1500
Total loss:	0.721 (rec:0.721, round:0.000)	b=0.00	count=2000
Total loss:	0.673 (rec:0.673, round:0.000)	b=0.00	count=2500
Total loss:	0.637 (rec:0.637, round:0.000)	b=0.00	count=3000
Total loss:	0.620 (rec:0.620, round:0.000)	b=0.00	count=3500
Total loss:	1533.105 (rec:0.599, round:1532.505)	b=20.00	count=4000
Total loss:	649.357 (rec:0.527, round:648.830)	b=19.44	count=4500
Total loss:	566.341 (rec:0.512, round:565.829)	b=18.88	count=5000
Total loss:	503.583 (rec:0.549, round:503.034)	b=18.31	count=5500
Total loss:	447.610 (rec:0.543, round:447.067)	b=17.75	count=6000
Total loss:	397.554 (rec:0.501, round:397.053)	b=17.19	count=6500
Total loss:	354.882 (rec:0.481, round:354.400)	b=16.62	count=7000
Total loss:	317.110 (rec:0.480, round:316.630)	b=16.06	count=7500
Total loss:	285.156 (rec:0.529, round:284.627)	b=15.50	count=8000
Total loss:	255.431 (rec:0.524, round:254.907)	b=14.94	count=8500
Total loss:	229.533 (rec:0.500, round:229.033)	b=14.38	count=9000
Total loss:	206.304 (rec:0.504, round:205.799)	b=13.81	count=9500
Total loss:	186.543 (rec:0.530, round:186.013)	b=13.25	count=10000
Total loss:	167.384 (rec:0.477, round:166.907)	b=12.69	count=10500
Total loss:	150.174 (rec:0.479, round:149.695)	b=12.12	count=11000
Total loss:	133.875 (rec:0.485, round:133.390)	b=11.56	count=11500
Total loss:	118.525 (rec:0.516, round:118.009)	b=11.00	count=12000
Total loss:	103.530 (rec:0.519, round:103.011)	b=10.44	count=12500
Total loss:	89.813 (rec:0.484, round:89.328)	b=9.88	count=13000
Total loss:	76.389 (rec:0.523, round:75.867)	b=9.31	count=13500
Total loss:	64.000 (rec:0.487, round:63.513)	b=8.75	count=14000
Total loss:	52.504 (rec:0.498, round:52.006)	b=8.19	count=14500
Total loss:	41.490 (rec:0.501, round:40.989)	b=7.62	count=15000
Total loss:	31.582 (rec:0.481, round:31.101)	b=7.06	count=15500
Total loss:	23.408 (rec:0.485, round:22.922)	b=6.50	count=16000
Total loss:	15.960 (rec:0.493, round:15.468)	b=5.94	count=16500
Total loss:	9.766 (rec:0.479, round:9.287)	b=5.38	count=17000
Total loss:	5.389 (rec:0.498, round:4.891)	b=4.81	count=17500
Total loss:	2.592 (rec:0.487, round:2.105)	b=4.25	count=18000
Total loss:	1.179 (rec:0.506, round:0.673)	b=3.69	count=18500
Total loss:	0.642 (rec:0.529, round:0.114)	b=3.12	count=19000
Total loss:	0.494 (rec:0.481, round:0.013)	b=2.56	count=19500
Total loss:	0.507 (rec:0.504, round:0.003)	b=2.00	count=20000
finished reconstructing layers.0.blocks.0.
reconstructing layers.0.blocks.1 ...
initializing raw input and raw output ...
adaround training for layers.0.blocks.1 ...
wraping quantizers in layers.0.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.540 (rec:1.540, round:0.000)	b=0.00	count=500
Total loss:	1.411 (rec:1.411, round:0.000)	b=0.00	count=1000
Total loss:	1.289 (rec:1.289, round:0.000)	b=0.00	count=1500
Total loss:	1.366 (rec:1.366, round:0.000)	b=0.00	count=2000
Total loss:	1.253 (rec:1.253, round:0.000)	b=0.00	count=2500
Total loss:	1.274 (rec:1.274, round:0.000)	b=0.00	count=3000
Total loss:	1.233 (rec:1.233, round:0.000)	b=0.00	count=3500
Total loss:	1622.249 (rec:1.306, round:1620.943)	b=20.00	count=4000
Total loss:	876.295 (rec:1.239, round:875.056)	b=19.44	count=4500
Total loss:	796.314 (rec:1.303, round:795.011)	b=18.88	count=5000
Total loss:	736.124 (rec:1.163, round:734.961)	b=18.31	count=5500
Total loss:	685.701 (rec:1.204, round:684.497)	b=17.75	count=6000
Total loss:	638.898 (rec:1.211, round:637.687)	b=17.19	count=6500
Total loss:	596.334 (rec:1.233, round:595.101)	b=16.62	count=7000
Total loss:	557.201 (rec:1.176, round:556.025)	b=16.06	count=7500
Total loss:	520.933 (rec:1.155, round:519.778)	b=15.50	count=8000
Total loss:	486.764 (rec:1.226, round:485.539)	b=14.94	count=8500
Total loss:	453.278 (rec:1.132, round:452.146)	b=14.38	count=9000
Total loss:	423.201 (rec:1.188, round:422.013)	b=13.81	count=9500
Total loss:	393.594 (rec:1.100, round:392.494)	b=13.25	count=10000
Total loss:	364.994 (rec:1.181, round:363.813)	b=12.69	count=10500
Total loss:	337.069 (rec:1.199, round:335.870)	b=12.12	count=11000
Total loss:	309.634 (rec:1.220, round:308.414)	b=11.56	count=11500
Total loss:	283.424 (rec:1.193, round:282.231)	b=11.00	count=12000
Total loss:	257.013 (rec:1.167, round:255.846)	b=10.44	count=12500
Total loss:	230.443 (rec:1.254, round:229.189)	b=9.88	count=13000
Total loss:	203.562 (rec:1.188, round:202.374)	b=9.31	count=13500
Total loss:	176.487 (rec:1.189, round:175.298)	b=8.75	count=14000
Total loss:	150.355 (rec:1.241, round:149.114)	b=8.19	count=14500
Total loss:	122.693 (rec:1.116, round:121.577)	b=7.62	count=15000
Total loss:	96.309 (rec:1.285, round:95.024)	b=7.06	count=15500
Total loss:	70.788 (rec:1.182, round:69.607)	b=6.50	count=16000
Total loss:	48.092 (rec:1.235, round:46.857)	b=5.94	count=16500
Total loss:	29.166 (rec:1.237, round:27.930)	b=5.38	count=17000
Total loss:	14.944 (rec:1.201, round:13.743)	b=4.81	count=17500
Total loss:	6.494 (rec:1.344, round:5.150)	b=4.25	count=18000
Total loss:	2.648 (rec:1.281, round:1.367)	b=3.69	count=18500
Total loss:	1.562 (rec:1.293, round:0.269)	b=3.12	count=19000
Total loss:	1.309 (rec:1.266, round:0.043)	b=2.56	count=19500
Total loss:	1.209 (rec:1.206, round:0.002)	b=2.00	count=20000
finished reconstructing layers.0.blocks.1.
reconstructing layers.1.downsample ...
initializing raw input and raw output ...
adaround training for layers.1.downsample ...
wraping quantizers in layers.1.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.668 (rec:1.668, round:0.000)	b=0.00	count=500
Total loss:	1.779 (rec:1.779, round:0.000)	b=0.00	count=1000
Total loss:	1.673 (rec:1.673, round:0.000)	b=0.00	count=1500
Total loss:	1.690 (rec:1.690, round:0.000)	b=0.00	count=2000
Total loss:	1.518 (rec:1.518, round:0.000)	b=0.00	count=2500
Total loss:	1.704 (rec:1.704, round:0.000)	b=0.00	count=3000
Total loss:	1.547 (rec:1.547, round:0.000)	b=0.00	count=3500
Total loss:	1007.250 (rec:1.573, round:1005.677)	b=20.00	count=4000
Total loss:	529.196 (rec:1.620, round:527.576)	b=19.44	count=4500
Total loss:	478.765 (rec:1.548, round:477.217)	b=18.88	count=5000
Total loss:	444.291 (rec:1.431, round:442.860)	b=18.31	count=5500
Total loss:	416.301 (rec:1.428, round:414.873)	b=17.75	count=6000
Total loss:	392.140 (rec:1.516, round:390.624)	b=17.19	count=6500
Total loss:	370.930 (rec:1.541, round:369.390)	b=16.62	count=7000
Total loss:	351.084 (rec:1.533, round:349.550)	b=16.06	count=7500
Total loss:	332.984 (rec:1.443, round:331.542)	b=15.50	count=8000
Total loss:	315.566 (rec:1.510, round:314.057)	b=14.94	count=8500
Total loss:	297.922 (rec:1.464, round:296.458)	b=14.38	count=9000
Total loss:	280.741 (rec:1.586, round:279.155)	b=13.81	count=9500
Total loss:	263.330 (rec:1.418, round:261.912)	b=13.25	count=10000
Total loss:	246.180 (rec:1.536, round:244.645)	b=12.69	count=10500
Total loss:	228.857 (rec:1.589, round:227.268)	b=12.12	count=11000
Total loss:	210.907 (rec:1.611, round:209.297)	b=11.56	count=11500
Total loss:	193.269 (rec:1.549, round:191.721)	b=11.00	count=12000
Total loss:	174.496 (rec:1.478, round:173.018)	b=10.44	count=12500
Total loss:	156.206 (rec:1.538, round:154.668)	b=9.88	count=13000
Total loss:	137.553 (rec:1.408, round:136.145)	b=9.31	count=13500
Total loss:	117.989 (rec:1.559, round:116.430)	b=8.75	count=14000
Total loss:	98.532 (rec:1.549, round:96.983)	b=8.19	count=14500
Total loss:	79.228 (rec:1.570, round:77.658)	b=7.62	count=15000
Total loss:	61.451 (rec:1.717, round:59.734)	b=7.06	count=15500
Total loss:	44.732 (rec:1.571, round:43.162)	b=6.50	count=16000
Total loss:	30.238 (rec:1.561, round:28.678)	b=5.94	count=16500
Total loss:	19.028 (rec:1.569, round:17.459)	b=5.38	count=17000
Total loss:	11.172 (rec:1.635, round:9.537)	b=4.81	count=17500
Total loss:	5.912 (rec:1.526, round:4.386)	b=4.25	count=18000
Total loss:	3.317 (rec:1.559, round:1.758)	b=3.69	count=18500
Total loss:	2.188 (rec:1.650, round:0.539)	b=3.12	count=19000
Total loss:	1.942 (rec:1.840, round:0.102)	b=2.56	count=19500
Total loss:	1.573 (rec:1.563, round:0.010)	b=2.00	count=20000
finished reconstructing layers.1.downsample.
reconstructing layers.1.blocks.0 ...
initializing raw input and raw output ...
adaround training for layers.1.blocks.0 ...
wraping quantizers in layers.1.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.505 (rec:1.505, round:0.000)	b=0.00	count=500
Total loss:	1.419 (rec:1.419, round:0.000)	b=0.00	count=1000
Total loss:	1.459 (rec:1.459, round:0.000)	b=0.00	count=1500
Total loss:	1.441 (rec:1.441, round:0.000)	b=0.00	count=2000
Total loss:	1.310 (rec:1.310, round:0.000)	b=0.00	count=2500
Total loss:	1.336 (rec:1.336, round:0.000)	b=0.00	count=3000
Total loss:	1.367 (rec:1.367, round:0.000)	b=0.00	count=3500
Total loss:	6755.115 (rec:1.272, round:6753.843)	b=20.00	count=4000
Total loss:	3523.037 (rec:1.318, round:3521.719)	b=19.44	count=4500
Total loss:	3219.399 (rec:1.320, round:3218.079)	b=18.88	count=5000
Total loss:	3007.682 (rec:1.336, round:3006.346)	b=18.31	count=5500
Total loss:	2826.526 (rec:1.310, round:2825.216)	b=17.75	count=6000
Total loss:	2660.927 (rec:1.258, round:2659.669)	b=17.19	count=6500
Total loss:	2507.877 (rec:1.307, round:2506.569)	b=16.62	count=7000
Total loss:	2361.989 (rec:1.331, round:2360.658)	b=16.06	count=7500
Total loss:	2224.262 (rec:1.344, round:2222.918)	b=15.50	count=8000
Total loss:	2090.594 (rec:1.372, round:2089.222)	b=14.94	count=8500
Total loss:	1961.102 (rec:1.310, round:1959.792)	b=14.38	count=9000
Total loss:	1834.392 (rec:1.383, round:1833.010)	b=13.81	count=9500
Total loss:	1710.163 (rec:1.363, round:1708.800)	b=13.25	count=10000
Total loss:	1590.267 (rec:1.300, round:1588.967)	b=12.69	count=10500
Total loss:	1470.370 (rec:1.347, round:1469.023)	b=12.12	count=11000
Total loss:	1348.937 (rec:1.321, round:1347.615)	b=11.56	count=11500
Total loss:	1227.006 (rec:1.354, round:1225.652)	b=11.00	count=12000
Total loss:	1103.808 (rec:1.393, round:1102.415)	b=10.44	count=12500
Total loss:	978.288 (rec:1.373, round:976.915)	b=9.88	count=13000
Total loss:	852.914 (rec:1.402, round:851.512)	b=9.31	count=13500
Total loss:	725.488 (rec:1.403, round:724.085)	b=8.75	count=14000
Total loss:	599.780 (rec:1.352, round:598.428)	b=8.19	count=14500
Total loss:	476.150 (rec:1.389, round:474.761)	b=7.62	count=15000
Total loss:	358.299 (rec:1.406, round:356.893)	b=7.06	count=15500
Total loss:	250.615 (rec:1.488, round:249.128)	b=6.50	count=16000
Total loss:	157.838 (rec:1.482, round:156.356)	b=5.94	count=16500
Total loss:	86.502 (rec:1.457, round:85.045)	b=5.38	count=17000
Total loss:	40.264 (rec:1.426, round:38.839)	b=4.81	count=17500
Total loss:	15.486 (rec:1.448, round:14.038)	b=4.25	count=18000
Total loss:	4.931 (rec:1.431, round:3.500)	b=3.69	count=18500
Total loss:	1.908 (rec:1.446, round:0.462)	b=3.12	count=19000
Total loss:	1.476 (rec:1.457, round:0.020)	b=2.56	count=19500
Total loss:	1.448 (rec:1.448, round:0.000)	b=2.00	count=20000
finished reconstructing layers.1.blocks.0.
reconstructing layers.1.blocks.1 ...
initializing raw input and raw output ...
adaround training for layers.1.blocks.1 ...
wraping quantizers in layers.1.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.437 (rec:1.437, round:0.000)	b=0.00	count=500
Total loss:	1.319 (rec:1.319, round:0.000)	b=0.00	count=1000
Total loss:	1.273 (rec:1.273, round:0.000)	b=0.00	count=1500
Total loss:	1.277 (rec:1.277, round:0.000)	b=0.00	count=2000
Total loss:	1.244 (rec:1.244, round:0.000)	b=0.00	count=2500
Total loss:	1.209 (rec:1.209, round:0.000)	b=0.00	count=3000
Total loss:	1.158 (rec:1.158, round:0.000)	b=0.00	count=3500
Total loss:	6789.764 (rec:1.124, round:6788.641)	b=20.00	count=4000
Total loss:	3575.218 (rec:1.161, round:3574.057)	b=19.44	count=4500
Total loss:	3277.923 (rec:1.180, round:3276.743)	b=18.88	count=5000
Total loss:	3066.663 (rec:1.148, round:3065.515)	b=18.31	count=5500
Total loss:	2883.649 (rec:1.154, round:2882.494)	b=17.75	count=6000
Total loss:	2715.833 (rec:1.161, round:2714.671)	b=17.19	count=6500
Total loss:	2559.811 (rec:1.192, round:2558.618)	b=16.62	count=7000
Total loss:	2412.548 (rec:1.163, round:2411.385)	b=16.06	count=7500
Total loss:	2271.193 (rec:1.155, round:2270.038)	b=15.50	count=8000
Total loss:	2138.548 (rec:1.123, round:2137.425)	b=14.94	count=8500
Total loss:	2010.304 (rec:1.138, round:2009.167)	b=14.38	count=9000
Total loss:	1882.379 (rec:1.183, round:1881.196)	b=13.81	count=9500
Total loss:	1760.866 (rec:1.193, round:1759.673)	b=13.25	count=10000
Total loss:	1638.868 (rec:1.193, round:1637.675)	b=12.69	count=10500
Total loss:	1517.559 (rec:1.158, round:1516.401)	b=12.12	count=11000
Total loss:	1396.242 (rec:1.133, round:1395.108)	b=11.56	count=11500
Total loss:	1275.515 (rec:1.171, round:1274.343)	b=11.00	count=12000
Total loss:	1154.642 (rec:1.149, round:1153.493)	b=10.44	count=12500
Total loss:	1031.763 (rec:1.206, round:1030.557)	b=9.88	count=13000
Total loss:	908.275 (rec:1.220, round:907.055)	b=9.31	count=13500
Total loss:	784.333 (rec:1.168, round:783.166)	b=8.75	count=14000
Total loss:	657.499 (rec:1.217, round:656.282)	b=8.19	count=14500
Total loss:	532.302 (rec:1.218, round:531.085)	b=7.62	count=15000
Total loss:	409.555 (rec:1.239, round:408.317)	b=7.06	count=15500
Total loss:	293.318 (rec:1.226, round:292.093)	b=6.50	count=16000
Total loss:	190.058 (rec:1.262, round:188.795)	b=5.94	count=16500
Total loss:	105.426 (rec:1.222, round:104.204)	b=5.38	count=17000
Total loss:	47.847 (rec:1.241, round:46.606)	b=4.81	count=17500
Total loss:	17.052 (rec:1.258, round:15.794)	b=4.25	count=18000
Total loss:	4.814 (rec:1.258, round:3.556)	b=3.69	count=18500
Total loss:	1.629 (rec:1.219, round:0.409)	b=3.12	count=19000
Total loss:	1.266 (rec:1.249, round:0.017)	b=2.56	count=19500
Total loss:	1.237 (rec:1.237, round:0.000)	b=2.00	count=20000
finished reconstructing layers.1.blocks.1.
reconstructing layers.2.downsample ...
initializing raw input and raw output ...
adaround training for layers.2.downsample ...
wraping quantizers in layers.2.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.454 (rec:1.454, round:0.000)	b=0.00	count=500
Total loss:	1.377 (rec:1.377, round:0.000)	b=0.00	count=1000
Total loss:	1.409 (rec:1.409, round:0.000)	b=0.00	count=1500
Total loss:	1.320 (rec:1.320, round:0.000)	b=0.00	count=2000
Total loss:	1.247 (rec:1.247, round:0.000)	b=0.00	count=2500
Total loss:	1.177 (rec:1.177, round:0.000)	b=0.00	count=3000
Total loss:	1.215 (rec:1.215, round:0.000)	b=0.00	count=3500
Total loss:	4240.134 (rec:1.275, round:4238.859)	b=20.00	count=4000
Total loss:	2044.494 (rec:1.155, round:2043.339)	b=19.44	count=4500
Total loss:	1846.274 (rec:1.136, round:1845.138)	b=18.88	count=5000
Total loss:	1704.042 (rec:1.237, round:1702.805)	b=18.31	count=5500
Total loss:	1584.343 (rec:1.323, round:1583.020)	b=17.75	count=6000
Total loss:	1478.271 (rec:1.244, round:1477.027)	b=17.19	count=6500
Total loss:	1382.124 (rec:1.165, round:1380.959)	b=16.62	count=7000
Total loss:	1293.334 (rec:1.144, round:1292.190)	b=16.06	count=7500
Total loss:	1210.215 (rec:1.205, round:1209.011)	b=15.50	count=8000
Total loss:	1132.130 (rec:1.225, round:1130.906)	b=14.94	count=8500
Total loss:	1058.072 (rec:1.157, round:1056.915)	b=14.38	count=9000
Total loss:	985.510 (rec:1.273, round:984.237)	b=13.81	count=9500
Total loss:	913.925 (rec:1.243, round:912.683)	b=13.25	count=10000
Total loss:	843.457 (rec:1.251, round:842.206)	b=12.69	count=10500
Total loss:	775.012 (rec:1.189, round:773.823)	b=12.12	count=11000
Total loss:	707.057 (rec:1.188, round:705.869)	b=11.56	count=11500
Total loss:	638.543 (rec:1.258, round:637.284)	b=11.00	count=12000
Total loss:	571.857 (rec:1.143, round:570.714)	b=10.44	count=12500
Total loss:	505.732 (rec:1.234, round:504.497)	b=9.88	count=13000
Total loss:	438.080 (rec:1.130, round:436.949)	b=9.31	count=13500
Total loss:	371.170 (rec:1.231, round:369.939)	b=8.75	count=14000
Total loss:	304.568 (rec:1.226, round:303.342)	b=8.19	count=14500
Total loss:	239.995 (rec:1.205, round:238.790)	b=7.62	count=15000
Total loss:	179.279 (rec:1.204, round:178.076)	b=7.06	count=15500
Total loss:	126.400 (rec:1.145, round:125.255)	b=6.50	count=16000
Total loss:	81.330 (rec:1.258, round:80.072)	b=5.94	count=16500
Total loss:	47.652 (rec:1.234, round:46.418)	b=5.38	count=17000
Total loss:	24.583 (rec:1.128, round:23.455)	b=4.81	count=17500
Total loss:	10.790 (rec:1.398, round:9.392)	b=4.25	count=18000
Total loss:	4.122 (rec:1.158, round:2.965)	b=3.69	count=18500
Total loss:	1.907 (rec:1.228, round:0.679)	b=3.12	count=19000
Total loss:	1.295 (rec:1.208, round:0.086)	b=2.56	count=19500
Total loss:	1.272 (rec:1.266, round:0.006)	b=2.00	count=20000
finished reconstructing layers.2.downsample.
reconstructing layers.2.blocks.0 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.0 ...
wraping quantizers in layers.2.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.196 (rec:1.196, round:0.000)	b=0.00	count=500
Total loss:	1.094 (rec:1.094, round:0.000)	b=0.00	count=1000
Total loss:	1.048 (rec:1.048, round:0.000)	b=0.00	count=1500
Total loss:	1.012 (rec:1.012, round:0.000)	b=0.00	count=2000
Total loss:	0.965 (rec:0.965, round:0.000)	b=0.00	count=2500
Total loss:	0.924 (rec:0.924, round:0.000)	b=0.00	count=3000
Total loss:	0.933 (rec:0.933, round:0.000)	b=0.00	count=3500
Total loss:	28496.998 (rec:0.910, round:28496.088)	b=20.00	count=4000
Total loss:	14201.502 (rec:0.887, round:14200.615)	b=19.44	count=4500
Total loss:	13084.729 (rec:0.888, round:13083.841)	b=18.88	count=5000
Total loss:	12314.885 (rec:0.863, round:12314.021)	b=18.31	count=5500
Total loss:	11638.771 (rec:0.863, round:11637.908)	b=17.75	count=6000
Total loss:	11006.335 (rec:0.860, round:11005.476)	b=17.19	count=6500
Total loss:	10396.864 (rec:0.855, round:10396.009)	b=16.62	count=7000
Total loss:	9801.647 (rec:0.866, round:9800.781)	b=16.06	count=7500
Total loss:	9223.825 (rec:0.846, round:9222.979)	b=15.50	count=8000
Total loss:	8654.302 (rec:0.859, round:8653.442)	b=14.94	count=8500
Total loss:	8097.372 (rec:0.869, round:8096.503)	b=14.38	count=9000
Total loss:	7550.064 (rec:0.843, round:7549.221)	b=13.81	count=9500
Total loss:	7009.305 (rec:0.849, round:7008.456)	b=13.25	count=10000
Total loss:	6480.605 (rec:0.873, round:6479.731)	b=12.69	count=10500
Total loss:	5963.430 (rec:0.894, round:5962.536)	b=12.12	count=11000
Total loss:	5451.096 (rec:0.849, round:5450.246)	b=11.56	count=11500
Total loss:	4945.465 (rec:0.880, round:4944.585)	b=11.00	count=12000
Total loss:	4449.630 (rec:0.874, round:4448.756)	b=10.44	count=12500
Total loss:	3961.423 (rec:0.893, round:3960.530)	b=9.88	count=13000
Total loss:	3483.969 (rec:0.847, round:3483.122)	b=9.31	count=13500
Total loss:	3011.987 (rec:0.882, round:3011.105)	b=8.75	count=14000
Total loss:	2550.878 (rec:0.852, round:2550.026)	b=8.19	count=14500
Total loss:	2104.364 (rec:0.867, round:2103.497)	b=7.62	count=15000
Total loss:	1675.660 (rec:0.877, round:1674.782)	b=7.06	count=15500
Total loss:	1265.556 (rec:0.896, round:1264.659)	b=6.50	count=16000
Total loss:	876.086 (rec:0.915, round:875.171)	b=5.94	count=16500
Total loss:	510.399 (rec:0.909, round:509.490)	b=5.38	count=17000
Total loss:	200.755 (rec:0.910, round:199.845)	b=4.81	count=17500
Total loss:	51.342 (rec:0.900, round:50.442)	b=4.25	count=18000
Total loss:	10.889 (rec:0.898, round:9.991)	b=3.69	count=18500
Total loss:	2.236 (rec:0.950, round:1.286)	b=3.12	count=19000
Total loss:	0.972 (rec:0.915, round:0.057)	b=2.56	count=19500
Total loss:	0.913 (rec:0.912, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.0.
reconstructing layers.2.blocks.1 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.1 ...
wraping quantizers in layers.2.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.536 (rec:1.536, round:0.000)	b=0.00	count=500
Total loss:	1.438 (rec:1.438, round:0.000)	b=0.00	count=1000
Total loss:	1.363 (rec:1.363, round:0.000)	b=0.00	count=1500
Total loss:	1.336 (rec:1.336, round:0.000)	b=0.00	count=2000
Total loss:	1.355 (rec:1.355, round:0.000)	b=0.00	count=2500
Total loss:	1.304 (rec:1.304, round:0.000)	b=0.00	count=3000
Total loss:	1.290 (rec:1.290, round:0.000)	b=0.00	count=3500
Total loss:	28414.615 (rec:1.259, round:28413.355)	b=20.00	count=4000
Total loss:	14517.546 (rec:1.293, round:14516.253)	b=19.44	count=4500
Total loss:	13395.636 (rec:1.269, round:13394.367)	b=18.88	count=5000
Total loss:	12636.654 (rec:1.276, round:12635.379)	b=18.31	count=5500
Total loss:	11977.627 (rec:1.270, round:11976.356)	b=17.75	count=6000
Total loss:	11365.174 (rec:1.296, round:11363.878)	b=17.19	count=6500
Total loss:	10782.334 (rec:1.270, round:10781.064)	b=16.62	count=7000
Total loss:	10214.688 (rec:1.252, round:10213.437)	b=16.06	count=7500
Total loss:	9659.871 (rec:1.274, round:9658.597)	b=15.50	count=8000
Total loss:	9115.416 (rec:1.255, round:9114.161)	b=14.94	count=8500
Total loss:	8578.059 (rec:1.281, round:8576.778)	b=14.38	count=9000
Total loss:	8042.322 (rec:1.281, round:8041.041)	b=13.81	count=9500
Total loss:	7511.885 (rec:1.263, round:7510.622)	b=13.25	count=10000
Total loss:	6985.590 (rec:1.272, round:6984.318)	b=12.69	count=10500
Total loss:	6459.081 (rec:1.250, round:6457.831)	b=12.12	count=11000
Total loss:	5937.779 (rec:1.271, round:5936.508)	b=11.56	count=11500
Total loss:	5417.507 (rec:1.299, round:5416.209)	b=11.00	count=12000
Total loss:	4895.674 (rec:1.324, round:4894.351)	b=10.44	count=12500
Total loss:	4382.888 (rec:1.297, round:4381.592)	b=9.88	count=13000
Total loss:	3867.823 (rec:1.316, round:3866.507)	b=9.31	count=13500
Total loss:	3356.716 (rec:1.298, round:3355.418)	b=8.75	count=14000
Total loss:	2852.521 (rec:1.311, round:2851.210)	b=8.19	count=14500
Total loss:	2357.690 (rec:1.314, round:2356.377)	b=7.62	count=15000
Total loss:	1871.368 (rec:1.336, round:1870.032)	b=7.06	count=15500
Total loss:	1405.035 (rec:1.312, round:1403.723)	b=6.50	count=16000
Total loss:	966.556 (rec:1.347, round:965.209)	b=5.94	count=16500
Total loss:	564.095 (rec:1.374, round:562.721)	b=5.38	count=17000
Total loss:	227.741 (rec:1.375, round:226.367)	b=4.81	count=17500
Total loss:	55.135 (rec:1.384, round:53.751)	b=4.25	count=18000
Total loss:	10.935 (rec:1.407, round:9.528)	b=3.69	count=18500
Total loss:	2.385 (rec:1.356, round:1.029)	b=3.12	count=19000
Total loss:	1.446 (rec:1.393, round:0.053)	b=2.56	count=19500
Total loss:	1.388 (rec:1.388, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.1.
reconstructing layers.2.blocks.2 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.2 ...
wraping quantizers in layers.2.blocks.2 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.541 (rec:1.541, round:0.000)	b=0.00	count=500
Total loss:	1.424 (rec:1.424, round:0.000)	b=0.00	count=1000
Total loss:	1.344 (rec:1.344, round:0.000)	b=0.00	count=1500
Total loss:	1.309 (rec:1.309, round:0.000)	b=0.00	count=2000
Total loss:	1.276 (rec:1.276, round:0.000)	b=0.00	count=2500
Total loss:	1.245 (rec:1.245, round:0.000)	b=0.00	count=3000
Total loss:	1.224 (rec:1.224, round:0.000)	b=0.00	count=3500
Total loss:	28185.951 (rec:1.205, round:28184.746)	b=20.00	count=4000
Total loss:	14314.103 (rec:1.191, round:14312.911)	b=19.44	count=4500
Total loss:	13191.675 (rec:1.198, round:13190.477)	b=18.88	count=5000
Total loss:	12424.385 (rec:1.185, round:12423.200)	b=18.31	count=5500
Total loss:	11756.873 (rec:1.199, round:11755.674)	b=17.75	count=6000
Total loss:	11132.048 (rec:1.199, round:11130.849)	b=17.19	count=6500
Total loss:	10536.020 (rec:1.185, round:10534.834)	b=16.62	count=7000
Total loss:	9963.066 (rec:1.209, round:9961.857)	b=16.06	count=7500
Total loss:	9406.389 (rec:1.197, round:9405.191)	b=15.50	count=8000
Total loss:	8861.386 (rec:1.196, round:8860.189)	b=14.94	count=8500
Total loss:	8320.620 (rec:1.172, round:8319.447)	b=14.38	count=9000
Total loss:	7788.268 (rec:1.198, round:7787.070)	b=13.81	count=9500
Total loss:	7260.717 (rec:1.190, round:7259.526)	b=13.25	count=10000
Total loss:	6742.106 (rec:1.206, round:6740.899)	b=12.69	count=10500
Total loss:	6225.145 (rec:1.199, round:6223.945)	b=12.12	count=11000
Total loss:	5712.694 (rec:1.200, round:5711.494)	b=11.56	count=11500
Total loss:	5202.292 (rec:1.186, round:5201.106)	b=11.00	count=12000
Total loss:	4695.417 (rec:1.198, round:4694.219)	b=10.44	count=12500
Total loss:	4190.988 (rec:1.208, round:4189.780)	b=9.88	count=13000
Total loss:	3694.680 (rec:1.222, round:3693.458)	b=9.31	count=13500
Total loss:	3200.248 (rec:1.218, round:3199.029)	b=8.75	count=14000
Total loss:	2713.089 (rec:1.228, round:2711.861)	b=8.19	count=14500
Total loss:	2232.174 (rec:1.226, round:2230.948)	b=7.62	count=15000
Total loss:	1765.480 (rec:1.245, round:1764.235)	b=7.06	count=15500
Total loss:	1316.721 (rec:1.244, round:1315.477)	b=6.50	count=16000
Total loss:	899.294 (rec:1.232, round:898.062)	b=5.94	count=16500
Total loss:	514.523 (rec:1.246, round:513.278)	b=5.38	count=17000
Total loss:	203.916 (rec:1.275, round:202.641)	b=4.81	count=17500
Total loss:	52.724 (rec:1.266, round:51.458)	b=4.25	count=18000
Total loss:	10.990 (rec:1.276, round:9.714)	b=3.69	count=18500
Total loss:	2.246 (rec:1.277, round:0.969)	b=3.12	count=19000
Total loss:	1.306 (rec:1.278, round:0.028)	b=2.56	count=19500
Total loss:	1.269 (rec:1.269, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.2.
reconstructing layers.2.blocks.3 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.3 ...
wraping quantizers in layers.2.blocks.3 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.586 (rec:1.586, round:0.000)	b=0.00	count=500
Total loss:	1.462 (rec:1.462, round:0.000)	b=0.00	count=1000
Total loss:	1.383 (rec:1.383, round:0.000)	b=0.00	count=1500
Total loss:	1.345 (rec:1.345, round:0.000)	b=0.00	count=2000
Total loss:	1.302 (rec:1.302, round:0.000)	b=0.00	count=2500
Total loss:	1.233 (rec:1.233, round:0.000)	b=0.00	count=3000
Total loss:	1.222 (rec:1.222, round:0.000)	b=0.00	count=3500
Total loss:	28021.512 (rec:1.222, round:28020.289)	b=20.00	count=4000
Total loss:	14133.702 (rec:1.219, round:14132.483)	b=19.44	count=4500
Total loss:	13010.166 (rec:1.228, round:13008.938)	b=18.88	count=5000
Total loss:	12229.288 (rec:1.238, round:12228.051)	b=18.31	count=5500
Total loss:	11544.899 (rec:1.184, round:11543.715)	b=17.75	count=6000
Total loss:	10905.982 (rec:1.176, round:10904.807)	b=17.19	count=6500
Total loss:	10298.013 (rec:1.194, round:10296.819)	b=16.62	count=7000
Total loss:	9707.107 (rec:1.203, round:9705.904)	b=16.06	count=7500
Total loss:	9132.112 (rec:1.213, round:9130.899)	b=15.50	count=8000
Total loss:	8573.482 (rec:1.182, round:8572.301)	b=14.94	count=8500
Total loss:	8028.187 (rec:1.190, round:8026.997)	b=14.38	count=9000
Total loss:	7492.833 (rec:1.186, round:7491.647)	b=13.81	count=9500
Total loss:	6966.900 (rec:1.161, round:6965.739)	b=13.25	count=10000
Total loss:	6451.499 (rec:1.178, round:6450.321)	b=12.69	count=10500
Total loss:	5941.922 (rec:1.169, round:5940.752)	b=12.12	count=11000
Total loss:	5440.261 (rec:1.188, round:5439.073)	b=11.56	count=11500
Total loss:	4947.783 (rec:1.176, round:4946.607)	b=11.00	count=12000
Total loss:	4457.543 (rec:1.217, round:4456.326)	b=10.44	count=12500
Total loss:	3975.037 (rec:1.176, round:3973.861)	b=9.88	count=13000
Total loss:	3501.065 (rec:1.208, round:3499.857)	b=9.31	count=13500
Total loss:	3033.563 (rec:1.208, round:3032.356)	b=8.75	count=14000
Total loss:	2572.444 (rec:1.229, round:2571.215)	b=8.19	count=14500
Total loss:	2121.720 (rec:1.230, round:2120.490)	b=7.62	count=15000
Total loss:	1684.170 (rec:1.221, round:1682.949)	b=7.06	count=15500
Total loss:	1263.098 (rec:1.225, round:1261.873)	b=6.50	count=16000
Total loss:	863.868 (rec:1.245, round:862.623)	b=5.94	count=16500
Total loss:	496.404 (rec:1.251, round:495.154)	b=5.38	count=17000
Total loss:	197.162 (rec:1.246, round:195.916)	b=4.81	count=17500
Total loss:	53.301 (rec:1.244, round:52.057)	b=4.25	count=18000
Total loss:	11.258 (rec:1.268, round:9.990)	b=3.69	count=18500
Total loss:	2.350 (rec:1.262, round:1.088)	b=3.12	count=19000
Total loss:	1.295 (rec:1.252, round:0.044)	b=2.56	count=19500
Total loss:	1.238 (rec:1.238, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.3.
reconstructing layers.2.blocks.4 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.4 ...
wraping quantizers in layers.2.blocks.4 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.512 (rec:1.512, round:0.000)	b=0.00	count=500
Total loss:	1.423 (rec:1.423, round:0.000)	b=0.00	count=1000
Total loss:	1.354 (rec:1.354, round:0.000)	b=0.00	count=1500
Total loss:	1.308 (rec:1.308, round:0.000)	b=0.00	count=2000
Total loss:	1.247 (rec:1.247, round:0.000)	b=0.00	count=2500
Total loss:	1.265 (rec:1.265, round:0.000)	b=0.00	count=3000
Total loss:	1.219 (rec:1.219, round:0.000)	b=0.00	count=3500
Total loss:	28192.629 (rec:1.183, round:28191.445)	b=20.00	count=4000
Total loss:	14345.333 (rec:1.210, round:14344.122)	b=19.44	count=4500
Total loss:	13223.814 (rec:1.196, round:13222.619)	b=18.88	count=5000
Total loss:	12455.546 (rec:1.191, round:12454.355)	b=18.31	count=5500
Total loss:	11778.456 (rec:1.204, round:11777.252)	b=17.75	count=6000
Total loss:	11150.241 (rec:1.163, round:11149.078)	b=17.19	count=6500
Total loss:	10549.459 (rec:1.163, round:10548.296)	b=16.62	count=7000
Total loss:	9968.413 (rec:1.221, round:9967.191)	b=16.06	count=7500
Total loss:	9403.599 (rec:1.157, round:9402.441)	b=15.50	count=8000
Total loss:	8851.528 (rec:1.166, round:8850.362)	b=14.94	count=8500
Total loss:	8308.600 (rec:1.177, round:8307.422)	b=14.38	count=9000
Total loss:	7774.255 (rec:1.163, round:7773.092)	b=13.81	count=9500
Total loss:	7247.620 (rec:1.158, round:7246.462)	b=13.25	count=10000
Total loss:	6727.065 (rec:1.174, round:6725.891)	b=12.69	count=10500
Total loss:	6212.500 (rec:1.185, round:6211.315)	b=12.12	count=11000
Total loss:	5698.274 (rec:1.171, round:5697.103)	b=11.56	count=11500
Total loss:	5188.890 (rec:1.152, round:5187.738)	b=11.00	count=12000
Total loss:	4683.117 (rec:1.176, round:4681.941)	b=10.44	count=12500
Total loss:	4184.188 (rec:1.190, round:4182.998)	b=9.88	count=13000
Total loss:	3684.646 (rec:1.187, round:3683.459)	b=9.31	count=13500
Total loss:	3197.139 (rec:1.170, round:3195.969)	b=8.75	count=14000
Total loss:	2711.169 (rec:1.196, round:2709.973)	b=8.19	count=14500
Total loss:	2234.417 (rec:1.185, round:2233.232)	b=7.62	count=15000
Total loss:	1773.647 (rec:1.195, round:1772.452)	b=7.06	count=15500
Total loss:	1331.985 (rec:1.217, round:1330.768)	b=6.50	count=16000
Total loss:	910.609 (rec:1.197, round:909.412)	b=5.94	count=16500
Total loss:	519.793 (rec:1.199, round:518.594)	b=5.38	count=17000
Total loss:	217.915 (rec:1.242, round:216.673)	b=4.81	count=17500
Total loss:	65.271 (rec:1.206, round:64.065)	b=4.25	count=18000
Total loss:	14.227 (rec:1.243, round:12.984)	b=3.69	count=18500
Total loss:	2.491 (rec:1.248, round:1.244)	b=3.12	count=19000
Total loss:	1.268 (rec:1.231, round:0.036)	b=2.56	count=19500
Total loss:	1.197 (rec:1.197, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.4.
reconstructing layers.2.blocks.5 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.5 ...
wraping quantizers in layers.2.blocks.5 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.625 (rec:1.625, round:0.000)	b=0.00	count=500
Total loss:	1.511 (rec:1.511, round:0.000)	b=0.00	count=1000
Total loss:	1.413 (rec:1.413, round:0.000)	b=0.00	count=1500
Total loss:	1.394 (rec:1.394, round:0.000)	b=0.00	count=2000
Total loss:	1.362 (rec:1.362, round:0.000)	b=0.00	count=2500
Total loss:	1.349 (rec:1.349, round:0.000)	b=0.00	count=3000
Total loss:	1.295 (rec:1.295, round:0.000)	b=0.00	count=3500
Total loss:	28083.721 (rec:1.330, round:28082.391)	b=20.00	count=4000
Total loss:	14348.169 (rec:1.298, round:14346.871)	b=19.44	count=4500
Total loss:	13218.882 (rec:1.308, round:13217.574)	b=18.88	count=5000
Total loss:	12443.050 (rec:1.291, round:12441.759)	b=18.31	count=5500
Total loss:	11763.621 (rec:1.281, round:11762.340)	b=17.75	count=6000
Total loss:	11132.809 (rec:1.280, round:11131.529)	b=17.19	count=6500
Total loss:	10523.105 (rec:1.255, round:10521.850)	b=16.62	count=7000
Total loss:	9938.541 (rec:1.243, round:9937.298)	b=16.06	count=7500
Total loss:	9369.359 (rec:1.289, round:9368.070)	b=15.50	count=8000
Total loss:	8808.654 (rec:1.258, round:8807.396)	b=14.94	count=8500
Total loss:	8260.409 (rec:1.264, round:8259.146)	b=14.38	count=9000
Total loss:	7723.771 (rec:1.257, round:7722.514)	b=13.81	count=9500
Total loss:	7196.944 (rec:1.295, round:7195.648)	b=13.25	count=10000
Total loss:	6676.528 (rec:1.275, round:6675.253)	b=12.69	count=10500
Total loss:	6160.385 (rec:1.276, round:6159.109)	b=12.12	count=11000
Total loss:	5651.872 (rec:1.268, round:5650.604)	b=11.56	count=11500
Total loss:	5148.779 (rec:1.267, round:5147.513)	b=11.00	count=12000
Total loss:	4651.446 (rec:1.294, round:4650.151)	b=10.44	count=12500
Total loss:	4156.494 (rec:1.280, round:4155.214)	b=9.88	count=13000
Total loss:	3663.584 (rec:1.296, round:3662.289)	b=9.31	count=13500
Total loss:	3174.883 (rec:1.288, round:3173.595)	b=8.75	count=14000
Total loss:	2692.654 (rec:1.292, round:2691.362)	b=8.19	count=14500
Total loss:	2216.679 (rec:1.303, round:2215.376)	b=7.62	count=15000
Total loss:	1750.902 (rec:1.322, round:1749.581)	b=7.06	count=15500
Total loss:	1306.387 (rec:1.291, round:1305.096)	b=6.50	count=16000
Total loss:	889.255 (rec:1.317, round:887.938)	b=5.94	count=16500
Total loss:	520.434 (rec:1.328, round:519.105)	b=5.38	count=17000
Total loss:	240.311 (rec:1.349, round:238.962)	b=4.81	count=17500
Total loss:	79.523 (rec:1.358, round:78.166)	b=4.25	count=18000
Total loss:	17.074 (rec:1.349, round:15.725)	b=3.69	count=18500
Total loss:	2.941 (rec:1.339, round:1.602)	b=3.12	count=19000
Total loss:	1.440 (rec:1.370, round:0.070)	b=2.56	count=19500
Total loss:	1.355 (rec:1.355, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.5.
reconstructing layers.2.blocks.6 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.6 ...
wraping quantizers in layers.2.blocks.6 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.663 (rec:1.663, round:0.000)	b=0.00	count=500
Total loss:	1.526 (rec:1.526, round:0.000)	b=0.00	count=1000
Total loss:	1.446 (rec:1.446, round:0.000)	b=0.00	count=1500
Total loss:	1.400 (rec:1.400, round:0.000)	b=0.00	count=2000
Total loss:	1.354 (rec:1.354, round:0.000)	b=0.00	count=2500
Total loss:	1.334 (rec:1.334, round:0.000)	b=0.00	count=3000
Total loss:	1.329 (rec:1.329, round:0.000)	b=0.00	count=3500
Total loss:	28027.320 (rec:1.321, round:28026.000)	b=20.00	count=4000
Total loss:	14289.297 (rec:1.303, round:14287.994)	b=19.44	count=4500
Total loss:	13163.329 (rec:1.288, round:13162.041)	b=18.88	count=5000
Total loss:	12387.734 (rec:1.280, round:12386.455)	b=18.31	count=5500
Total loss:	11703.670 (rec:1.279, round:11702.392)	b=17.75	count=6000
Total loss:	11071.323 (rec:1.300, round:11070.023)	b=17.19	count=6500
Total loss:	10468.532 (rec:1.263, round:10467.269)	b=16.62	count=7000
Total loss:	9881.066 (rec:1.280, round:9879.787)	b=16.06	count=7500
Total loss:	9316.288 (rec:1.277, round:9315.012)	b=15.50	count=8000
Total loss:	8766.354 (rec:1.276, round:8765.078)	b=14.94	count=8500
Total loss:	8222.275 (rec:1.278, round:8220.997)	b=14.38	count=9000
Total loss:	7688.957 (rec:1.271, round:7687.686)	b=13.81	count=9500
Total loss:	7160.105 (rec:1.272, round:7158.833)	b=13.25	count=10000
Total loss:	6640.289 (rec:1.275, round:6639.014)	b=12.69	count=10500
Total loss:	6123.664 (rec:1.258, round:6122.406)	b=12.12	count=11000
Total loss:	5616.388 (rec:1.265, round:5615.123)	b=11.56	count=11500
Total loss:	5112.037 (rec:1.285, round:5110.751)	b=11.00	count=12000
Total loss:	4609.993 (rec:1.292, round:4608.701)	b=10.44	count=12500
Total loss:	4109.853 (rec:1.280, round:4108.573)	b=9.88	count=13000
Total loss:	3620.274 (rec:1.290, round:3618.984)	b=9.31	count=13500
Total loss:	3137.856 (rec:1.307, round:3136.549)	b=8.75	count=14000
Total loss:	2658.549 (rec:1.327, round:2657.223)	b=8.19	count=14500
Total loss:	2180.917 (rec:1.271, round:2179.646)	b=7.62	count=15000
Total loss:	1716.101 (rec:1.285, round:1714.816)	b=7.06	count=15500
Total loss:	1274.314 (rec:1.307, round:1273.007)	b=6.50	count=16000
Total loss:	863.954 (rec:1.342, round:862.612)	b=5.94	count=16500
Total loss:	508.867 (rec:1.338, round:507.529)	b=5.38	count=17000
Total loss:	236.254 (rec:1.333, round:234.921)	b=4.81	count=17500
Total loss:	75.624 (rec:1.362, round:74.261)	b=4.25	count=18000
Total loss:	15.092 (rec:1.339, round:13.753)	b=3.69	count=18500
Total loss:	2.658 (rec:1.348, round:1.310)	b=3.12	count=19000
Total loss:	1.386 (rec:1.338, round:0.049)	b=2.56	count=19500
Total loss:	1.351 (rec:1.351, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.6.
reconstructing layers.2.blocks.7 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.7 ...
wraping quantizers in layers.2.blocks.7 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.978 (rec:0.978, round:0.000)	b=0.00	count=500
Total loss:	0.796 (rec:0.796, round:0.000)	b=0.00	count=1000
Total loss:	0.711 (rec:0.711, round:0.000)	b=0.00	count=1500
Total loss:	0.673 (rec:0.673, round:0.000)	b=0.00	count=2000
Total loss:	0.655 (rec:0.655, round:0.000)	b=0.00	count=2500
Total loss:	0.615 (rec:0.615, round:0.000)	b=0.00	count=3000
Total loss:	0.586 (rec:0.586, round:0.000)	b=0.00	count=3500
Total loss:	27913.732 (rec:0.589, round:27913.145)	b=20.00	count=4000
Total loss:	13595.004 (rec:0.558, round:13594.446)	b=19.44	count=4500
Total loss:	12434.152 (rec:0.553, round:12433.600)	b=18.88	count=5000
Total loss:	11599.751 (rec:0.550, round:11599.201)	b=18.31	count=5500
Total loss:	10866.425 (rec:0.546, round:10865.879)	b=17.75	count=6000
Total loss:	10175.350 (rec:0.530, round:10174.820)	b=17.19	count=6500
Total loss:	9516.243 (rec:0.530, round:9515.714)	b=16.62	count=7000
Total loss:	8884.819 (rec:0.533, round:8884.286)	b=16.06	count=7500
Total loss:	8276.509 (rec:0.533, round:8275.976)	b=15.50	count=8000
Total loss:	7688.617 (rec:0.531, round:7688.086)	b=14.94	count=8500
Total loss:	7119.542 (rec:0.524, round:7119.019)	b=14.38	count=9000
Total loss:	6573.451 (rec:0.534, round:6572.917)	b=13.81	count=9500
Total loss:	6048.831 (rec:0.535, round:6048.296)	b=13.25	count=10000
Total loss:	5537.413 (rec:0.539, round:5536.874)	b=12.69	count=10500
Total loss:	5042.426 (rec:0.518, round:5041.908)	b=12.12	count=11000
Total loss:	4564.014 (rec:0.530, round:4563.483)	b=11.56	count=11500
Total loss:	4099.634 (rec:0.535, round:4099.100)	b=11.00	count=12000
Total loss:	3649.033 (rec:0.533, round:3648.501)	b=10.44	count=12500
Total loss:	3206.176 (rec:0.536, round:3205.639)	b=9.88	count=13000
Total loss:	2775.115 (rec:0.530, round:2774.585)	b=9.31	count=13500
Total loss:	2356.688 (rec:0.532, round:2356.156)	b=8.75	count=14000
Total loss:	1951.647 (rec:0.537, round:1951.110)	b=8.19	count=14500
Total loss:	1564.684 (rec:0.543, round:1564.141)	b=7.62	count=15000
Total loss:	1197.273 (rec:0.533, round:1196.740)	b=7.06	count=15500
Total loss:	853.892 (rec:0.563, round:853.329)	b=6.50	count=16000
Total loss:	549.664 (rec:0.549, round:549.115)	b=5.94	count=16500
Total loss:	305.838 (rec:0.550, round:305.288)	b=5.38	count=17000
Total loss:	138.492 (rec:0.550, round:137.942)	b=4.81	count=17500
Total loss:	46.934 (rec:0.552, round:46.382)	b=4.25	count=18000
Total loss:	10.151 (rec:0.565, round:9.585)	b=3.69	count=18500
Total loss:	1.467 (rec:0.562, round:0.905)	b=3.12	count=19000
Total loss:	0.594 (rec:0.554, round:0.040)	b=2.56	count=19500
Total loss:	0.552 (rec:0.549, round:0.003)	b=2.00	count=20000
finished reconstructing layers.2.blocks.7.
reconstructing layers.2.blocks.8 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.8 ...
wraping quantizers in layers.2.blocks.8 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.420 (rec:1.420, round:0.000)	b=0.00	count=500
Total loss:	1.177 (rec:1.177, round:0.000)	b=0.00	count=1000
Total loss:	1.020 (rec:1.020, round:0.000)	b=0.00	count=1500
Total loss:	0.900 (rec:0.900, round:0.000)	b=0.00	count=2000
Total loss:	0.791 (rec:0.791, round:0.000)	b=0.00	count=2500
Total loss:	0.723 (rec:0.723, round:0.000)	b=0.00	count=3000
Total loss:	0.716 (rec:0.716, round:0.000)	b=0.00	count=3500
Total loss:	27231.789 (rec:0.679, round:27231.109)	b=20.00	count=4000
Total loss:	13060.141 (rec:0.667, round:13059.474)	b=19.44	count=4500
Total loss:	11945.416 (rec:0.633, round:11944.783)	b=18.88	count=5000
Total loss:	11129.451 (rec:0.639, round:11128.812)	b=18.31	count=5500
Total loss:	10398.372 (rec:0.615, round:10397.757)	b=17.75	count=6000
Total loss:	9724.370 (rec:0.621, round:9723.749)	b=17.19	count=6500
Total loss:	9093.371 (rec:0.605, round:9092.767)	b=16.62	count=7000
Total loss:	8488.164 (rec:0.610, round:8487.555)	b=16.06	count=7500
Total loss:	7906.259 (rec:0.607, round:7905.652)	b=15.50	count=8000
Total loss:	7351.206 (rec:0.612, round:7350.594)	b=14.94	count=8500
Total loss:	6820.031 (rec:0.625, round:6819.406)	b=14.38	count=9000
Total loss:	6308.958 (rec:0.596, round:6308.363)	b=13.81	count=9500
Total loss:	5817.949 (rec:0.599, round:5817.350)	b=13.25	count=10000
Total loss:	5342.561 (rec:0.594, round:5341.967)	b=12.69	count=10500
Total loss:	4880.159 (rec:0.601, round:4879.559)	b=12.12	count=11000
Total loss:	4433.275 (rec:0.587, round:4432.688)	b=11.56	count=11500
Total loss:	3999.891 (rec:0.622, round:3999.269)	b=11.00	count=12000
Total loss:	3577.322 (rec:0.591, round:3576.730)	b=10.44	count=12500
Total loss:	3164.085 (rec:0.609, round:3163.476)	b=9.88	count=13000
Total loss:	2760.708 (rec:0.609, round:2760.099)	b=9.31	count=13500
Total loss:	2370.283 (rec:0.608, round:2369.675)	b=8.75	count=14000
Total loss:	1988.456 (rec:0.597, round:1987.859)	b=8.19	count=14500
Total loss:	1619.646 (rec:0.613, round:1619.033)	b=7.62	count=15000
Total loss:	1264.599 (rec:0.598, round:1264.001)	b=7.06	count=15500
Total loss:	933.752 (rec:0.608, round:933.144)	b=6.50	count=16000
Total loss:	633.102 (rec:0.595, round:632.507)	b=5.94	count=16500
Total loss:	380.960 (rec:0.629, round:380.331)	b=5.38	count=17000
Total loss:	191.379 (rec:0.607, round:190.772)	b=4.81	count=17500
Total loss:	71.738 (rec:0.612, round:71.126)	b=4.25	count=18000
Total loss:	15.140 (rec:0.623, round:14.517)	b=3.69	count=18500
Total loss:	1.840 (rec:0.619, round:1.221)	b=3.12	count=19000
Total loss:	0.657 (rec:0.623, round:0.034)	b=2.56	count=19500
Total loss:	0.613 (rec:0.613, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.8.
reconstructing layers.2.blocks.9 ...
initializing raw input and raw output ...
adaround training for layers.2.blocks.9 ...
wraping quantizers in layers.2.blocks.9 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.806 (rec:0.806, round:0.000)	b=0.00	count=500
Total loss:	0.668 (rec:0.668, round:0.000)	b=0.00	count=1000
Total loss:	0.667 (rec:0.667, round:0.000)	b=0.00	count=1500
Total loss:	0.610 (rec:0.610, round:0.000)	b=0.00	count=2000
Total loss:	0.573 (rec:0.573, round:0.000)	b=0.00	count=2500
Total loss:	0.537 (rec:0.537, round:0.000)	b=0.00	count=3000
Total loss:	0.525 (rec:0.525, round:0.000)	b=0.00	count=3500
Total loss:	27326.369 (rec:0.498, round:27325.871)	b=20.00	count=4000
Total loss:	12909.142 (rec:0.460, round:12908.682)	b=19.44	count=4500
Total loss:	11746.589 (rec:0.456, round:11746.133)	b=18.88	count=5000
Total loss:	10887.574 (rec:0.457, round:10887.117)	b=18.31	count=5500
Total loss:	10120.847 (rec:0.450, round:10120.396)	b=17.75	count=6000
Total loss:	9405.832 (rec:0.431, round:9405.400)	b=17.19	count=6500
Total loss:	8730.568 (rec:0.439, round:8730.129)	b=16.62	count=7000
Total loss:	8091.005 (rec:0.427, round:8090.578)	b=16.06	count=7500
Total loss:	7483.899 (rec:0.430, round:7483.469)	b=15.50	count=8000
Total loss:	6909.306 (rec:0.422, round:6908.884)	b=14.94	count=8500
Total loss:	6356.435 (rec:0.410, round:6356.024)	b=14.38	count=9000
Total loss:	5831.225 (rec:0.419, round:5830.806)	b=13.81	count=9500
Total loss:	5331.177 (rec:0.430, round:5330.747)	b=13.25	count=10000
Total loss:	4855.311 (rec:0.420, round:4854.892)	b=12.69	count=10500
Total loss:	4400.373 (rec:0.422, round:4399.950)	b=12.12	count=11000
Total loss:	3964.785 (rec:0.430, round:3964.355)	b=11.56	count=11500
Total loss:	3541.499 (rec:0.417, round:3541.082)	b=11.00	count=12000
Total loss:	3139.069 (rec:0.426, round:3138.642)	b=10.44	count=12500
slurmstepd-jnfat04: error: *** JOB 1643628 ON jnfat04 CANCELLED AT 2025-09-11T10:17:42 ***
