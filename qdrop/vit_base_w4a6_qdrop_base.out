Starting ViT-Base W4A6 QDROP experiment at Sun Sep 14 02:27:47 PM CEST 2025
2025-09-14 14:27:50,924 - INFO - Starting multi-seed experiment
2025-09-14 14:27:50,924 - INFO - Architecture: vit_base
2025-09-14 14:27:50,924 - INFO - Weight bits: 4
2025-09-14 14:27:50,924 - INFO - Activation bits: 6
2025-09-14 14:27:50,924 - INFO - Seeds: [1001, 1002, 1003]
2025-09-14 14:27:50,924 - INFO - Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
2025-09-14 14:27:50,924 - INFO - Cluster numbers: [8, 16, 32, 64, 128, 256]
2025-09-14 14:27:50,924 - INFO - PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]
2025-09-14 14:27:50,924 - INFO - Output directory: ./experiment_results/vit_base_w4_a6_20250914_142750
2025-09-14 14:27:50,925 - INFO - Checking basic requirements...
2025-09-14 14:27:50,925 - INFO - Basic checks passed
2025-09-14 14:27:50,925 - INFO - 
Starting experiments for 3 seeds...
2025-09-14 14:27:50,925 - INFO - Total parameter combinations: 600
2025-09-14 14:27:50,925 - INFO - Total experiments: 1800
2025-09-14 14:27:50,925 - INFO - 
============================================================
2025-09-14 14:27:50,925 - INFO - Running experiment 1/3 for seed 1001
2025-09-14 14:27:50,925 - INFO - ============================================================
2025-09-14 14:27:50,925 - INFO - Running experiment for seed 1001
2025-09-14 14:27:50,925 - INFO - Command: /home/alz07xz/project/PD-Quant/pd_quant/bin/python ../test_quant.py --model vit_base --w_bit 4 --a_bit 6 --seed 1001 --config ../configs/4bit/qdrop_baseline.py --dataset /home/alz07xz/imagenet --calib-size 1000 --calib-batch-size 32 --val-batch-size 500 --num-workers 8 --device cuda --alpha-list 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 --num-clusters-list 8 16 32 64 128 256 --pca-dim-list 1 25 50 75 100 125 150 175 200 220 --calibrate --optimize
2025-09-14 14:27:50,925 - INFO - Working directory: /home/alz07xz/project/APHQ_CAT/qdrop
2025-09-14 14:36:07 - start the process.
Namespace(model='vit_base', config='../configs/4bit/qdrop_baseline.py', dataset='/home/alz07xz/imagenet', val_batch_size=500, num_workers=8, device='cuda', reconstruct_mlp=False, load_reconstruct_checkpoint=False, test_reconstruct_checkpoint=False, calibrate=True, load_calibrate_checkpoint=None, test_calibrate_checkpoint=False, optimize=True, load_optimize_checkpoint='', test_optimize_checkpoint=False, print_freq=10, seed=1001, alpha_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], num_clusters_list=[8, 16, 32, 64, 128, 256], pca_dim_list=[1, 25, 50, 75, 100, 125, 150, 175, 200, 220], w_bit=4, a_bit=6, calib_size=1000, calib_batch_size=32)
deefe /home/alz07xz/project/APHQ_CAT/configs/4bit
Successfully imported Config class!
optim_size: 1024
calib_size: 1000
optim_batch_size: 32
calib_batch_size: 32
w_bit: 4
a_bit: 6
qconv_a_bit: 8
qhead_a_bit: 4
calib_metric: mse
matmul_head_channel_wise: True
token_channel_wise: False
eq_n: 128
search_round: 3
keep_gpu: True
optim_metric: mse
use_mean_hessian: False
temp: 20
recon_metric: hessian_perturb
pct: 0.99
optim_mode: qdrop
drop_prob: 0.5
reconstruct_mlp: False
Building model ...
No checkpoint found at './checkpoint/vit_raw/vit_base_patch16_224.bin'
Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
[timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Building validation dataloader ...
Validating on test set on fp ...
Test: [0/100]	Time 12.227 (12.227)	Loss 0.4459 (0.4459)	Prec@1 91.400 (91.400)	Prec@5 98.600 (98.600)
Test: [10/100]	Time 0.756 (2.174)	Loss 0.4659 (0.5379)	Prec@1 90.800 (88.455)	Prec@5 98.600 (98.345)
Test: [20/100]	Time 0.759 (1.529)	Loss 0.6057 (0.5588)	Prec@1 85.800 (88.124)	Prec@5 98.600 (98.095)
Test: [30/100]	Time 0.765 (1.617)	Loss 0.5066 (0.5820)	Prec@1 89.800 (87.471)	Prec@5 99.600 (98.045)
Test: [40/100]	Time 3.541 (1.549)	Loss 0.7571 (0.5772)	Prec@1 81.400 (87.532)	Prec@5 97.000 (98.088)
Test: [50/100]	Time 4.427 (1.563)	Loss 1.0069 (0.6165)	Prec@1 77.000 (86.384)	Prec@5 95.200 (97.827)
Test: [60/100]	Time 0.772 (1.499)	Loss 0.5700 (0.6205)	Prec@1 89.200 (86.285)	Prec@5 97.200 (97.751)
Test: [70/100]	Time 0.773 (1.430)	Loss 0.7296 (0.6361)	Prec@1 83.800 (85.673)	Prec@5 97.400 (97.654)
Test: [80/100]	Time 0.785 (1.359)	Loss 0.5101 (0.6392)	Prec@1 88.400 (85.605)	Prec@5 98.000 (97.580)
Test: [90/100]	Time 3.938 (1.330)	Loss 0.9420 (0.6541)	Prec@1 75.000 (85.062)	Prec@5 95.800 (97.495)
 * Prec@1 85.102 Prec@5 97.526 Loss 0.652 Time 130.389
Wraping quantiztion modules (reparam: False, recon: False) ...
2025-09-14 14:39:03 - start mse guided calibration
  0%|          | 0/74 [00:00<?, ?it/s]calibrating patch_embed.proj:   0%|          | 0/74 [00:00<?, ?it/s]calibrating patch_embed.proj:   1%|▏         | 1/74 [00:11<14:21, 11.80s/it]calibrating blocks.0.attn.qkv:   1%|▏         | 1/74 [00:11<14:21, 11.80s/it]calibrating blocks.0.attn.qkv:   3%|▎         | 2/74 [01:25<58:01, 48.36s/it]calibrating blocks.0.attn.proj:   3%|▎         | 2/74 [01:25<58:01, 48.36s/it]calibrating blocks.0.attn.proj:   4%|▍         | 3/74 [01:50<44:41, 37.77s/it]calibrating blocks.0.attn.matmul1:   4%|▍         | 3/74 [01:50<44:41, 37.77s/it]calibrating blocks.0.attn.matmul1:   5%|▌         | 4/74 [03:02<59:27, 50.97s/it]calibrating blocks.0.attn.matmul2:   5%|▌         | 4/74 [03:02<59:27, 50.97s/it]calibrating blocks.0.attn.matmul2:   7%|▋         | 5/74 [04:02<1:02:34, 54.41s/it]calibrating blocks.0.mlp.fc1:   7%|▋         | 5/74 [04:02<1:02:34, 54.41s/it]     calibrating blocks.0.mlp.fc1:   8%|▊         | 6/74 [05:56<1:24:32, 74.59s/it]calibrating blocks.0.mlp.fc2:   8%|▊         | 6/74 [05:56<1:24:32, 74.59s/it]calibrating blocks.0.mlp.fc2:   9%|▉         | 7/74 [07:54<1:39:07, 88.77s/it]calibrating blocks.1.attn.qkv:   9%|▉         | 7/74 [07:54<1:39:07, 88.77s/it]calibrating blocks.1.attn.qkv:  11%|█         | 8/74 [09:10<1:33:10, 84.70s/it]calibrating blocks.1.attn.proj:  11%|█         | 8/74 [09:10<1:33:10, 84.70s/it]calibrating blocks.1.attn.proj:  12%|█▏        | 9/74 [09:36<1:12:01, 66.48s/it]calibrating blocks.1.attn.matmul1:  12%|█▏        | 9/74 [09:36<1:12:01, 66.48s/it]calibrating blocks.1.attn.matmul1:  14%|█▎        | 10/74 [10:48<1:12:29, 67.97s/it]calibrating blocks.1.attn.matmul2:  14%|█▎        | 10/74 [10:48<1:12:29, 67.97s/it]calibrating blocks.1.attn.matmul2:  15%|█▍        | 11/74 [11:48<1:08:49, 65.55s/it]calibrating blocks.1.mlp.fc1:  15%|█▍        | 11/74 [11:48<1:08:49, 65.55s/it]     calibrating blocks.1.mlp.fc1:  16%|█▌        | 12/74 [13:42<1:22:56, 80.26s/it]calibrating blocks.1.mlp.fc2:  16%|█▌        | 12/74 [13:42<1:22:56, 80.26s/it]calibrating blocks.1.mlp.fc2:  18%|█▊        | 13/74 [15:39<1:33:05, 91.57s/it]calibrating blocks.2.attn.qkv:  18%|█▊        | 13/74 [15:39<1:33:05, 91.57s/it]calibrating blocks.2.attn.qkv:  19%|█▉        | 14/74 [16:55<1:26:57, 86.96s/it]calibrating blocks.2.attn.proj:  19%|█▉        | 14/74 [16:55<1:26:57, 86.96s/it]calibrating blocks.2.attn.proj:  20%|██        | 15/74 [17:22<1:07:33, 68.70s/it]calibrating blocks.2.attn.matmul1:  20%|██        | 15/74 [17:22<1:07:33, 68.70s/it]calibrating blocks.2.attn.matmul1:  22%|██▏       | 16/74 [18:34<1:07:25, 69.75s/it]calibrating blocks.2.attn.matmul2:  22%|██▏       | 16/74 [18:34<1:07:25, 69.75s/it]calibrating blocks.2.attn.matmul2:  23%|██▎       | 17/74 [19:35<1:03:43, 67.08s/it]calibrating blocks.2.mlp.fc1:  23%|██▎       | 17/74 [19:35<1:03:43, 67.08s/it]     calibrating blocks.2.mlp.fc1:  24%|██▍       | 18/74 [21:30<1:16:05, 81.52s/it]calibrating blocks.2.mlp.fc2:  24%|██▍       | 18/74 [21:30<1:16:05, 81.52s/it]calibrating blocks.2.mlp.fc2:  26%|██▌       | 19/74 [23:29<1:24:59, 92.71s/it]calibrating blocks.3.attn.qkv:  26%|██▌       | 19/74 [23:29<1:24:59, 92.71s/it]calibrating blocks.3.attn.qkv:  27%|██▋       | 20/74 [24:44<1:18:43, 87.48s/it]calibrating blocks.3.attn.proj:  27%|██▋       | 20/74 [24:44<1:18:43, 87.48s/it]calibrating blocks.3.attn.proj:  28%|██▊       | 21/74 [25:10<1:01:00, 69.07s/it]calibrating blocks.3.attn.matmul1:  28%|██▊       | 21/74 [25:10<1:01:00, 69.07s/it]calibrating blocks.3.attn.matmul1:  30%|██▉       | 22/74 [26:22<1:00:30, 69.82s/it]calibrating blocks.3.attn.matmul2:  30%|██▉       | 22/74 [26:22<1:00:30, 69.82s/it]calibrating blocks.3.attn.matmul2:  31%|███       | 23/74 [27:22<56:56, 66.99s/it]  calibrating blocks.3.mlp.fc1:  31%|███       | 23/74 [27:22<56:56, 66.99s/it]     calibrating blocks.3.mlp.fc1:  32%|███▏      | 24/74 [29:17<1:07:48, 81.37s/it]calibrating blocks.3.mlp.fc2:  32%|███▏      | 24/74 [29:17<1:07:48, 81.37s/it]calibrating blocks.3.mlp.fc2:  34%|███▍      | 25/74 [31:15<1:15:24, 92.33s/it]calibrating blocks.4.attn.qkv:  34%|███▍      | 25/74 [31:15<1:15:24, 92.33s/it]calibrating blocks.4.attn.qkv:  35%|███▌      | 26/74 [32:30<1:09:43, 87.15s/it]calibrating blocks.4.attn.proj:  35%|███▌      | 26/74 [32:30<1:09:43, 87.15s/it]calibrating blocks.4.attn.proj:  36%|███▋      | 27/74 [32:56<53:49, 68.71s/it]  calibrating blocks.4.attn.matmul1:  36%|███▋      | 27/74 [32:56<53:49, 68.71s/it]calibrating blocks.4.attn.matmul1:  38%|███▊      | 28/74 [34:07<53:18, 69.54s/it]calibrating blocks.4.attn.matmul2:  38%|███▊      | 28/74 [34:07<53:18, 69.54s/it]calibrating blocks.4.attn.matmul2:  39%|███▉      | 29/74 [35:08<50:08, 66.87s/it]calibrating blocks.4.mlp.fc1:  39%|███▉      | 29/74 [35:08<50:08, 66.87s/it]     calibrating blocks.4.mlp.fc1:  41%|████      | 30/74 [37:02<59:31, 81.16s/it]calibrating blocks.4.mlp.fc2:  41%|████      | 30/74 [37:02<59:31, 81.16s/it]calibrating blocks.4.mlp.fc2:  42%|████▏     | 31/74 [39:00<1:06:04, 92.20s/it]calibrating blocks.5.attn.qkv:  42%|████▏     | 31/74 [39:00<1:06:04, 92.20s/it]calibrating blocks.5.attn.qkv:  43%|████▎     | 32/74 [40:17<1:01:17, 87.55s/it]calibrating blocks.5.attn.proj:  43%|████▎     | 32/74 [40:17<1:01:17, 87.55s/it]calibrating blocks.5.attn.proj:  45%|████▍     | 33/74 [40:43<47:18, 69.22s/it]  calibrating blocks.5.attn.matmul1:  45%|████▍     | 33/74 [40:43<47:18, 69.22s/it]calibrating blocks.5.attn.matmul1:  46%|████▌     | 34/74 [41:56<46:45, 70.14s/it]calibrating blocks.5.attn.matmul2:  46%|████▌     | 34/74 [41:56<46:45, 70.14s/it]calibrating blocks.5.attn.matmul2:  47%|████▋     | 35/74 [42:57<43:49, 67.43s/it]calibrating blocks.5.mlp.fc1:  47%|████▋     | 35/74 [42:57<43:49, 67.43s/it]     calibrating blocks.5.mlp.fc1:  49%|████▊     | 36/74 [44:52<51:45, 81.73s/it]calibrating blocks.5.mlp.fc2:  49%|████▊     | 36/74 [44:52<51:45, 81.73s/it]calibrating blocks.5.mlp.fc2:  50%|█████     | 37/74 [46:50<57:05, 92.58s/it]calibrating blocks.6.attn.qkv:  50%|█████     | 37/74 [46:50<57:05, 92.58s/it]calibrating blocks.6.attn.qkv:  51%|█████▏    | 38/74 [48:06<52:34, 87.61s/it]calibrating blocks.6.attn.proj:  51%|█████▏    | 38/74 [48:06<52:34, 87.61s/it]calibrating blocks.6.attn.proj:  53%|█████▎    | 39/74 [48:32<40:23, 69.24s/it]calibrating blocks.6.attn.matmul1:  53%|█████▎    | 39/74 [48:32<40:23, 69.24s/it]calibrating blocks.6.attn.matmul1:  54%|█████▍    | 40/74 [49:44<39:38, 69.95s/it]calibrating blocks.6.attn.matmul2:  54%|█████▍    | 40/74 [49:44<39:38, 69.95s/it]calibrating blocks.6.attn.matmul2:  55%|█████▌    | 41/74 [50:44<36:53, 67.08s/it]calibrating blocks.6.mlp.fc1:  55%|█████▌    | 41/74 [50:44<36:53, 67.08s/it]     calibrating blocks.6.mlp.fc1:  57%|█████▋    | 42/74 [52:38<43:18, 81.21s/it]calibrating blocks.6.mlp.fc2:  57%|█████▋    | 42/74 [52:38<43:18, 81.21s/it]calibrating blocks.6.mlp.fc2:  58%|█████▊    | 43/74 [54:36<47:39, 92.25s/it]calibrating blocks.7.attn.qkv:  58%|█████▊    | 43/74 [54:36<47:39, 92.25s/it]calibrating blocks.7.attn.qkv:  59%|█████▉    | 44/74 [55:53<43:48, 87.60s/it]calibrating blocks.7.attn.proj:  59%|█████▉    | 44/74 [55:53<43:48, 87.60s/it]calibrating blocks.7.attn.proj:  61%|██████    | 45/74 [56:20<33:28, 69.25s/it]calibrating blocks.7.attn.matmul1:  61%|██████    | 45/74 [56:20<33:28, 69.25s/it]calibrating blocks.7.attn.matmul1:  62%|██████▏   | 46/74 [57:32<32:43, 70.12s/it]calibrating blocks.7.attn.matmul2:  62%|██████▏   | 46/74 [57:32<32:43, 70.12s/it]calibrating blocks.7.attn.matmul2:  64%|██████▎   | 47/74 [58:33<30:18, 67.36s/it]calibrating blocks.7.mlp.fc1:  64%|██████▎   | 47/74 [58:33<30:18, 67.36s/it]     calibrating blocks.7.mlp.fc1:  65%|██████▍   | 48/74 [1:00:27<35:19, 81.51s/it]calibrating blocks.7.mlp.fc2:  65%|██████▍   | 48/74 [1:00:27<35:19, 81.51s/it]calibrating blocks.7.mlp.fc2:  66%|██████▌   | 49/74 [1:02:24<38:22, 92.12s/it]calibrating blocks.8.attn.qkv:  66%|██████▌   | 49/74 [1:02:24<38:22, 92.12s/it]calibrating blocks.8.attn.qkv:  68%|██████▊   | 50/74 [1:03:40<34:55, 87.29s/it]calibrating blocks.8.attn.proj:  68%|██████▊   | 50/74 [1:03:40<34:55, 87.29s/it]calibrating blocks.8.attn.proj:  69%|██████▉   | 51/74 [1:04:06<26:24, 68.88s/it]calibrating blocks.8.attn.matmul1:  69%|██████▉   | 51/74 [1:04:06<26:24, 68.88s/it]calibrating blocks.8.attn.matmul1:  70%|███████   | 52/74 [1:05:17<25:31, 69.61s/it]calibrating blocks.8.attn.matmul2:  70%|███████   | 52/74 [1:05:17<25:31, 69.61s/it]calibrating blocks.8.attn.matmul2:  72%|███████▏  | 53/74 [1:06:18<23:22, 66.80s/it]calibrating blocks.8.mlp.fc1:  72%|███████▏  | 53/74 [1:06:18<23:22, 66.80s/it]     calibrating blocks.8.mlp.fc1:  73%|███████▎  | 54/74 [1:08:12<27:01, 81.06s/it]calibrating blocks.8.mlp.fc2:  73%|███████▎  | 54/74 [1:08:12<27:01, 81.06s/it]calibrating blocks.8.mlp.fc2:  74%|███████▍  | 55/74 [1:10:09<29:02, 91.72s/it]calibrating blocks.9.attn.qkv:  74%|███████▍  | 55/74 [1:10:09<29:02, 91.72s/it]calibrating blocks.9.attn.qkv:  76%|███████▌  | 56/74 [1:11:25<26:09, 87.22s/it]calibrating blocks.9.attn.proj:  76%|███████▌  | 56/74 [1:11:25<26:09, 87.22s/it]calibrating blocks.9.attn.proj:  77%|███████▋  | 57/74 [1:11:52<19:32, 68.98s/it]calibrating blocks.9.attn.matmul1:  77%|███████▋  | 57/74 [1:11:52<19:32, 68.98s/it]calibrating blocks.9.attn.matmul1:  78%|███████▊  | 58/74 [1:13:04<18:39, 69.99s/it]calibrating blocks.9.attn.matmul2:  78%|███████▊  | 58/74 [1:13:04<18:39, 69.99s/it]calibrating blocks.9.attn.matmul2:  80%|███████▉  | 59/74 [1:14:05<16:49, 67.29s/it]calibrating blocks.9.mlp.fc1:  80%|███████▉  | 59/74 [1:14:05<16:49, 67.29s/it]     calibrating blocks.9.mlp.fc1:  81%|████████  | 60/74 [1:16:00<19:04, 81.73s/it]calibrating blocks.9.mlp.fc2:  81%|████████  | 60/74 [1:16:00<19:04, 81.73s/it]calibrating blocks.9.mlp.fc2:  82%|████████▏ | 61/74 [1:17:58<20:00, 92.36s/it]calibrating blocks.10.attn.qkv:  82%|████████▏ | 61/74 [1:17:58<20:00, 92.36s/it]calibrating blocks.10.attn.qkv:  84%|████████▍ | 62/74 [1:19:15<17:33, 87.79s/it]calibrating blocks.10.attn.proj:  84%|████████▍ | 62/74 [1:19:15<17:33, 87.79s/it]calibrating blocks.10.attn.proj:  85%|████████▌ | 63/74 [1:19:41<12:43, 69.43s/it]calibrating blocks.10.attn.matmul1:  85%|████████▌ | 63/74 [1:19:41<12:43, 69.43s/it]calibrating blocks.10.attn.matmul1:  86%|████████▋ | 64/74 [1:20:54<11:43, 70.30s/it]calibrating blocks.10.attn.matmul2:  86%|████████▋ | 64/74 [1:20:54<11:43, 70.30s/it]calibrating blocks.10.attn.matmul2:  88%|████████▊ | 65/74 [1:21:55<10:07, 67.54s/it]calibrating blocks.10.mlp.fc1:  88%|████████▊ | 65/74 [1:21:55<10:07, 67.54s/it]     calibrating blocks.10.mlp.fc1:  89%|████████▉ | 66/74 [1:23:50<10:53, 81.75s/it]calibrating blocks.10.mlp.fc2:  89%|████████▉ | 66/74 [1:23:50<10:53, 81.75s/it]calibrating blocks.10.mlp.fc2:  91%|█████████ | 67/74 [1:25:47<10:46, 92.40s/it]calibrating blocks.11.attn.qkv:  91%|█████████ | 67/74 [1:25:47<10:46, 92.40s/it]calibrating blocks.11.attn.qkv:  92%|█████████▏| 68/74 [1:27:04<08:46, 87.80s/it]calibrating blocks.11.attn.proj:  92%|█████████▏| 68/74 [1:27:04<08:46, 87.80s/it]calibrating blocks.11.attn.proj:  93%|█████████▎| 69/74 [1:27:30<05:46, 69.38s/it]calibrating blocks.11.attn.matmul1:  93%|█████████▎| 69/74 [1:27:30<05:46, 69.38s/it]calibrating blocks.11.attn.matmul1:  95%|█████████▍| 70/74 [1:28:42<04:40, 70.15s/it]calibrating blocks.11.attn.matmul2:  95%|█████████▍| 70/74 [1:28:42<04:40, 70.15s/it]calibrating blocks.11.attn.matmul2:  96%|█████████▌| 71/74 [1:29:43<03:22, 67.41s/it]calibrating blocks.11.mlp.fc1:  96%|█████████▌| 71/74 [1:29:43<03:22, 67.41s/it]     calibrating blocks.11.mlp.fc1:  97%|█████████▋| 72/74 [1:31:38<02:43, 81.72s/it]calibrating blocks.11.mlp.fc2:  97%|█████████▋| 72/74 [1:31:38<02:43, 81.72s/it]calibrating blocks.11.mlp.fc2:  99%|█████████▊| 73/74 [1:33:37<01:32, 92.63s/it]calibrating head:  99%|█████████▊| 73/74 [1:33:37<01:32, 92.63s/it]             calibrating head: 100%|██████████| 74/74 [1:33:40<00:00, 65.91s/it]calibrating head: 100%|██████████| 74/74 [1:33:40<00:00, 75.95s/it]
2025-09-14 16:12:58 - mse guided calibration finished.
Saving checkpoint to ./checkpoint/quant_result/20250914_1436/vit_base_w4_a6_calibsize_1000_mse.pth
Validating after calibration ...
Test: [0/100]	Time 4.974 (4.974)	Loss 0.4648 (0.4648)	Prec@1 89.200 (89.200)	Prec@5 98.000 (98.000)
Test: [10/100]	Time 1.674 (1.983)	Loss 0.5355 (0.5931)	Prec@1 88.000 (86.345)	Prec@5 98.200 (97.273)
Test: [20/100]	Time 1.687 (1.839)	Loss 0.6797 (0.6320)	Prec@1 84.000 (85.667)	Prec@5 98.200 (97.133)
Test: [30/100]	Time 1.681 (1.789)	Loss 0.6168 (0.6576)	Prec@1 85.800 (84.910)	Prec@5 98.800 (97.161)
Test: [40/100]	Time 1.684 (1.762)	Loss 0.9311 (0.6480)	Prec@1 78.400 (85.244)	Prec@5 95.000 (97.234)
Test: [50/100]	Time 1.681 (1.747)	Loss 1.1256 (0.6990)	Prec@1 74.600 (83.988)	Prec@5 92.200 (96.855)
Test: [60/100]	Time 1.683 (1.736)	Loss 0.7005 (0.7101)	Prec@1 85.800 (83.875)	Prec@5 96.000 (96.731)
Test: [70/100]	Time 1.682 (1.728)	Loss 0.8302 (0.7316)	Prec@1 81.600 (83.259)	Prec@5 95.800 (96.589)
Test: [80/100]	Time 1.678 (1.722)	Loss 0.6212 (0.7412)	Prec@1 87.000 (83.106)	Prec@5 96.800 (96.437)
Test: [90/100]	Time 1.681 (1.718)	Loss 1.1514 (0.7640)	Prec@1 72.200 (82.488)	Prec@5 93.400 (96.292)
 * Prec@1 82.534 Prec@5 96.356 Loss 0.760 Time 171.724
Building calibrator ...
2025-09-14 16:15:55 - start mse guided block reconstruction
reconstructing patch_embed ...
initializing raw input and raw output ...
adaround training for patch_embed ...
wraping quantizers in patch_embed ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.195 (rec:0.195, round:0.000)	b=0.00	count=500
Total loss:	0.104 (rec:0.104, round:0.000)	b=0.00	count=1000
Total loss:	0.084 (rec:0.084, round:0.000)	b=0.00	count=1500
Total loss:	0.100 (rec:0.100, round:0.000)	b=0.00	count=2000
Total loss:	0.063 (rec:0.063, round:0.000)	b=0.00	count=2500
Total loss:	0.091 (rec:0.091, round:0.000)	b=0.00	count=3000
Total loss:	0.042 (rec:0.042, round:0.000)	b=0.00	count=3500
Total loss:	5577.735 (rec:0.043, round:5577.692)	b=20.00	count=4000
Total loss:	2963.248 (rec:0.101, round:2963.146)	b=19.44	count=4500
Total loss:	2719.665 (rec:0.085, round:2719.579)	b=18.88	count=5000
Total loss:	2560.497 (rec:0.059, round:2560.438)	b=18.31	count=5500
Total loss:	2424.148 (rec:0.082, round:2424.066)	b=17.75	count=6000
Total loss:	2299.934 (rec:0.071, round:2299.863)	b=17.19	count=6500
Total loss:	2179.701 (rec:0.082, round:2179.620)	b=16.62	count=7000
Total loss:	2062.494 (rec:0.086, round:2062.408)	b=16.06	count=7500
Total loss:	1945.542 (rec:0.035, round:1945.506)	b=15.50	count=8000
Total loss:	1828.179 (rec:0.077, round:1828.102)	b=14.94	count=8500
Total loss:	1710.580 (rec:0.070, round:1710.510)	b=14.38	count=9000
Total loss:	1592.257 (rec:0.071, round:1592.187)	b=13.81	count=9500
Total loss:	1471.401 (rec:0.088, round:1471.313)	b=13.25	count=10000
Total loss:	1347.724 (rec:0.115, round:1347.609)	b=12.69	count=10500
Total loss:	1221.566 (rec:0.073, round:1221.492)	b=12.12	count=11000
Total loss:	1093.355 (rec:0.101, round:1093.254)	b=11.56	count=11500
Total loss:	963.092 (rec:0.120, round:962.972)	b=11.00	count=12000
Total loss:	830.101 (rec:0.098, round:830.003)	b=10.44	count=12500
Total loss:	697.107 (rec:0.144, round:696.963)	b=9.88	count=13000
Total loss:	567.517 (rec:0.161, round:567.356)	b=9.31	count=13500
Total loss:	443.290 (rec:0.136, round:443.155)	b=8.75	count=14000
Total loss:	329.249 (rec:0.221, round:329.028)	b=8.19	count=14500
Total loss:	228.940 (rec:0.183, round:228.757)	b=7.62	count=15000
Total loss:	147.896 (rec:0.163, round:147.733)	b=7.06	count=15500
Total loss:	85.833 (rec:0.305, round:85.528)	b=6.50	count=16000
Total loss:	42.910 (rec:0.313, round:42.597)	b=5.94	count=16500
Total loss:	17.824 (rec:0.218, round:17.606)	b=5.38	count=17000
Total loss:	6.138 (rec:0.283, round:5.854)	b=4.81	count=17500
Total loss:	1.861 (rec:0.319, round:1.542)	b=4.25	count=18000
Total loss:	0.717 (rec:0.347, round:0.370)	b=3.69	count=18500
Total loss:	0.320 (rec:0.236, round:0.084)	b=3.12	count=19000
Total loss:	0.251 (rec:0.233, round:0.017)	b=2.56	count=19500
Total loss:	0.418 (rec:0.417, round:0.002)	b=2.00	count=20000
finished reconstructing patch_embed.
reconstructing blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.0 ...
wraping quantizers in blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.800 (rec:0.800, round:0.000)	b=0.00	count=500
Total loss:	0.765 (rec:0.765, round:0.000)	b=0.00	count=1000
Total loss:	0.666 (rec:0.666, round:0.000)	b=0.00	count=1500
Total loss:	0.624 (rec:0.624, round:0.000)	b=0.00	count=2000
Total loss:	0.559 (rec:0.559, round:0.000)	b=0.00	count=2500
Total loss:	0.594 (rec:0.594, round:0.000)	b=0.00	count=3000
Total loss:	0.573 (rec:0.573, round:0.000)	b=0.00	count=3500
Total loss:	64860.949 (rec:0.536, round:64860.414)	b=20.00	count=4000
Total loss:	29239.723 (rec:0.571, round:29239.152)	b=19.44	count=4500
Total loss:	26755.348 (rec:0.522, round:26754.824)	b=18.88	count=5000
Total loss:	25087.385 (rec:0.560, round:25086.824)	b=18.31	count=5500
Total loss:	23638.475 (rec:0.529, round:23637.945)	b=17.75	count=6000
Total loss:	22281.578 (rec:0.562, round:22281.016)	b=17.19	count=6500
Total loss:	20981.439 (rec:0.576, round:20980.863)	b=16.62	count=7000
Total loss:	19721.002 (rec:0.675, round:19720.328)	b=16.06	count=7500
Total loss:	18492.629 (rec:0.546, round:18492.082)	b=15.50	count=8000
Total loss:	17283.119 (rec:0.517, round:17282.602)	b=14.94	count=8500
Total loss:	16098.377 (rec:0.535, round:16097.842)	b=14.38	count=9000
Total loss:	14941.959 (rec:0.517, round:14941.441)	b=13.81	count=9500
Total loss:	13803.283 (rec:0.539, round:13802.744)	b=13.25	count=10000
Total loss:	12681.202 (rec:0.560, round:12680.643)	b=12.69	count=10500
Total loss:	11572.869 (rec:0.512, round:11572.356)	b=12.12	count=11000
Total loss:	10483.342 (rec:0.608, round:10482.734)	b=11.56	count=11500
Total loss:	9413.998 (rec:0.542, round:9413.456)	b=11.00	count=12000
Total loss:	8358.701 (rec:0.519, round:8358.182)	b=10.44	count=12500
Total loss:	7328.233 (rec:0.552, round:7327.681)	b=9.88	count=13000
Total loss:	6309.264 (rec:0.540, round:6308.724)	b=9.31	count=13500
Total loss:	5316.059 (rec:0.570, round:5315.489)	b=8.75	count=14000
Total loss:	4352.051 (rec:0.580, round:4351.471)	b=8.19	count=14500
Total loss:	3438.017 (rec:0.553, round:3437.464)	b=7.62	count=15000
Total loss:	2588.181 (rec:0.570, round:2587.611)	b=7.06	count=15500
Total loss:	1823.106 (rec:0.556, round:1822.550)	b=6.50	count=16000
Total loss:	1162.300 (rec:0.604, round:1161.696)	b=5.94	count=16500
Total loss:	649.775 (rec:0.563, round:649.211)	b=5.38	count=17000
Total loss:	310.705 (rec:0.645, round:310.059)	b=4.81	count=17500
Total loss:	115.883 (rec:0.588, round:115.295)	b=4.25	count=18000
Total loss:	28.635 (rec:0.613, round:28.022)	b=3.69	count=18500
Total loss:	4.333 (rec:0.634, round:3.699)	b=3.12	count=19000
Total loss:	0.982 (rec:0.670, round:0.312)	b=2.56	count=19500
Total loss:	0.701 (rec:0.666, round:0.035)	b=2.00	count=20000
finished reconstructing blocks.0.
reconstructing blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.1 ...
wraping quantizers in blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.321 (rec:1.321, round:0.000)	b=0.00	count=500
Total loss:	1.283 (rec:1.283, round:0.000)	b=0.00	count=1000
Total loss:	1.115 (rec:1.115, round:0.000)	b=0.00	count=1500
Total loss:	1.191 (rec:1.191, round:0.000)	b=0.00	count=2000
Total loss:	1.229 (rec:1.229, round:0.000)	b=0.00	count=2500
Total loss:	0.993 (rec:0.993, round:0.000)	b=0.00	count=3000
Total loss:	1.023 (rec:1.023, round:0.000)	b=0.00	count=3500
Total loss:	64644.434 (rec:1.058, round:64643.375)	b=20.00	count=4000
Total loss:	30182.061 (rec:1.050, round:30181.012)	b=19.44	count=4500
Total loss:	27727.500 (rec:1.017, round:27726.484)	b=18.88	count=5000
Total loss:	26075.672 (rec:1.004, round:26074.668)	b=18.31	count=5500
Total loss:	24637.938 (rec:0.972, round:24636.965)	b=17.75	count=6000
Total loss:	23299.432 (rec:0.965, round:23298.467)	b=17.19	count=6500
Total loss:	22007.508 (rec:0.991, round:22006.518)	b=16.62	count=7000
Total loss:	20757.484 (rec:1.003, round:20756.480)	b=16.06	count=7500
Total loss:	19521.154 (rec:1.006, round:19520.148)	b=15.50	count=8000
Total loss:	18309.223 (rec:0.999, round:18308.223)	b=14.94	count=8500
Total loss:	17109.469 (rec:0.984, round:17108.484)	b=14.38	count=9000
Total loss:	15926.685 (rec:0.987, round:15925.697)	b=13.81	count=9500
Total loss:	14756.639 (rec:0.988, round:14755.650)	b=13.25	count=10000
Total loss:	13596.074 (rec:0.973, round:13595.102)	b=12.69	count=10500
Total loss:	12452.203 (rec:0.996, round:12451.207)	b=12.12	count=11000
Total loss:	11317.763 (rec:0.939, round:11316.824)	b=11.56	count=11500
Total loss:	10201.991 (rec:1.021, round:10200.971)	b=11.00	count=12000
Total loss:	9098.553 (rec:1.000, round:9097.553)	b=10.44	count=12500
Total loss:	8013.590 (rec:1.042, round:8012.548)	b=9.88	count=13000
Total loss:	6942.550 (rec:0.969, round:6941.581)	b=9.31	count=13500
Total loss:	5891.283 (rec:0.942, round:5890.341)	b=8.75	count=14000
Total loss:	4869.455 (rec:0.969, round:4868.485)	b=8.19	count=14500
Total loss:	3882.551 (rec:0.962, round:3881.588)	b=7.62	count=15000
Total loss:	2937.672 (rec:0.966, round:2936.706)	b=7.06	count=15500
Total loss:	2060.928 (rec:1.075, round:2059.854)	b=6.50	count=16000
Total loss:	1270.572 (rec:1.061, round:1269.512)	b=5.94	count=16500
Total loss:	614.370 (rec:1.104, round:613.266)	b=5.38	count=17000
Total loss:	214.507 (rec:1.020, round:213.486)	b=4.81	count=17500
Total loss:	57.501 (rec:1.042, round:56.459)	b=4.25	count=18000
Total loss:	12.387 (rec:1.009, round:11.378)	b=3.69	count=18500
Total loss:	2.512 (rec:1.035, round:1.477)	b=3.12	count=19000
Total loss:	1.077 (rec:0.984, round:0.093)	b=2.56	count=19500
Total loss:	1.057 (rec:1.056, round:0.002)	b=2.00	count=20000
finished reconstructing blocks.1.
reconstructing blocks.2 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.2 ...
wraping quantizers in blocks.2 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.643 (rec:1.643, round:0.000)	b=0.00	count=500
Total loss:	1.648 (rec:1.648, round:0.000)	b=0.00	count=1000
Total loss:	1.539 (rec:1.539, round:0.000)	b=0.00	count=1500
Total loss:	1.520 (rec:1.520, round:0.000)	b=0.00	count=2000
Total loss:	1.513 (rec:1.513, round:0.000)	b=0.00	count=2500
Total loss:	1.469 (rec:1.469, round:0.000)	b=0.00	count=3000
Total loss:	1.545 (rec:1.545, round:0.000)	b=0.00	count=3500
Total loss:	65190.410 (rec:1.518, round:65188.891)	b=20.00	count=4000
Total loss:	31816.920 (rec:1.416, round:31815.504)	b=19.44	count=4500
Total loss:	29376.715 (rec:1.526, round:29375.189)	b=18.88	count=5000
Total loss:	27778.082 (rec:1.505, round:27776.576)	b=18.31	count=5500
Total loss:	26408.811 (rec:1.492, round:26407.318)	b=17.75	count=6000
Total loss:	25138.646 (rec:1.599, round:25137.047)	b=17.19	count=6500
Total loss:	23926.742 (rec:1.645, round:23925.098)	b=16.62	count=7000
Total loss:	22746.473 (rec:1.466, round:22745.006)	b=16.06	count=7500
Total loss:	21578.795 (rec:1.530, round:21577.266)	b=15.50	count=8000
Total loss:	20421.451 (rec:1.440, round:20420.012)	b=14.94	count=8500
Total loss:	19266.486 (rec:1.391, round:19265.096)	b=14.38	count=9000
Total loss:	18100.547 (rec:1.460, round:18099.088)	b=13.81	count=9500
Total loss:	16938.777 (rec:1.500, round:16937.277)	b=13.25	count=10000
Total loss:	15774.689 (rec:1.483, round:15773.206)	b=12.69	count=10500
Total loss:	14611.105 (rec:1.449, round:14609.656)	b=12.12	count=11000
Total loss:	13442.449 (rec:1.434, round:13441.016)	b=11.56	count=11500
Total loss:	12260.869 (rec:1.385, round:12259.484)	b=11.00	count=12000
Total loss:	11082.270 (rec:1.422, round:11080.848)	b=10.44	count=12500
Total loss:	9900.472 (rec:1.419, round:9899.053)	b=9.88	count=13000
Total loss:	8720.247 (rec:1.525, round:8718.722)	b=9.31	count=13500
Total loss:	7540.282 (rec:1.483, round:7538.799)	b=8.75	count=14000
Total loss:	6362.625 (rec:1.473, round:6361.152)	b=8.19	count=14500
Total loss:	5206.647 (rec:1.552, round:5205.095)	b=7.62	count=15000
Total loss:	4074.669 (rec:1.451, round:4073.218)	b=7.06	count=15500
Total loss:	2996.454 (rec:1.448, round:2995.006)	b=6.50	count=16000
Total loss:	1991.146 (rec:1.630, round:1989.516)	b=5.94	count=16500
Total loss:	1068.457 (rec:1.510, round:1066.946)	b=5.38	count=17000
Total loss:	391.828 (rec:1.429, round:390.399)	b=4.81	count=17500
Total loss:	98.816 (rec:1.523, round:97.293)	b=4.25	count=18000
Total loss:	17.394 (rec:1.561, round:15.833)	b=3.69	count=18500
Total loss:	2.846 (rec:1.523, round:1.323)	b=3.12	count=19000
Total loss:	1.625 (rec:1.584, round:0.041)	b=2.56	count=19500
Total loss:	1.535 (rec:1.535, round:0.000)	b=2.00	count=20000
finished reconstructing blocks.2.
reconstructing blocks.3 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.3 ...
wraping quantizers in blocks.3 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.273 (rec:1.273, round:0.000)	b=0.00	count=500
Total loss:	1.245 (rec:1.245, round:0.000)	b=0.00	count=1000
Total loss:	1.099 (rec:1.099, round:0.000)	b=0.00	count=1500
Total loss:	1.177 (rec:1.177, round:0.000)	b=0.00	count=2000
Total loss:	1.116 (rec:1.116, round:0.000)	b=0.00	count=2500
Total loss:	1.117 (rec:1.117, round:0.000)	b=0.00	count=3000
Total loss:	1.085 (rec:1.085, round:0.000)	b=0.00	count=3500
Total loss:	65213.500 (rec:1.053, round:65212.445)	b=20.00	count=4000
Total loss:	31604.906 (rec:1.109, round:31603.797)	b=19.44	count=4500
Total loss:	29181.461 (rec:1.133, round:29180.328)	b=18.88	count=5000
Total loss:	27581.213 (rec:1.173, round:27580.039)	b=18.31	count=5500
Total loss:	26206.059 (rec:1.051, round:26205.008)	b=17.75	count=6000
Total loss:	24929.160 (rec:1.012, round:24928.148)	b=17.19	count=6500
Total loss:	23694.771 (rec:1.010, round:23693.762)	b=16.62	count=7000
Total loss:	22484.801 (rec:1.067, round:22483.734)	b=16.06	count=7500
Total loss:	21299.510 (rec:1.027, round:21298.482)	b=15.50	count=8000
Total loss:	20119.809 (rec:1.066, round:20118.742)	b=14.94	count=8500
Total loss:	18943.117 (rec:1.112, round:18942.004)	b=14.38	count=9000
Total loss:	17772.217 (rec:1.074, round:17771.143)	b=13.81	count=9500
Total loss:	16602.980 (rec:1.105, round:16601.875)	b=13.25	count=10000
Total loss:	15436.838 (rec:1.113, round:15435.725)	b=12.69	count=10500
Total loss:	14259.329 (rec:1.014, round:14258.315)	b=12.12	count=11000
Total loss:	13081.452 (rec:1.031, round:13080.421)	b=11.56	count=11500
Total loss:	11903.696 (rec:1.003, round:11902.693)	b=11.00	count=12000
Total loss:	10728.950 (rec:1.143, round:10727.807)	b=10.44	count=12500
Total loss:	9557.296 (rec:0.996, round:9556.300)	b=9.88	count=13000
Total loss:	8395.203 (rec:1.070, round:8394.133)	b=9.31	count=13500
Total loss:	7239.914 (rec:1.023, round:7238.891)	b=8.75	count=14000
Total loss:	6103.971 (rec:1.101, round:6102.870)	b=8.19	count=14500
Total loss:	4990.572 (rec:1.038, round:4989.534)	b=7.62	count=15000
Total loss:	3903.382 (rec:1.050, round:3902.332)	b=7.06	count=15500
Total loss:	2867.214 (rec:1.051, round:2866.163)	b=6.50	count=16000
Total loss:	1897.090 (rec:1.062, round:1896.028)	b=5.94	count=16500
Total loss:	1005.648 (rec:1.092, round:1004.555)	b=5.38	count=17000
Total loss:	402.059 (rec:1.044, round:401.015)	b=4.81	count=17500
Total loss:	126.732 (rec:1.099, round:125.632)	b=4.25	count=18000
Total loss:	23.184 (rec:1.074, round:22.110)	b=3.69	count=18500
Total loss:	2.694 (rec:1.131, round:1.563)	b=3.12	count=19000
Total loss:	1.138 (rec:1.076, round:0.062)	b=2.56	count=19500
Total loss:	1.067 (rec:1.067, round:0.000)	b=2.00	count=20000
finished reconstructing blocks.3.
reconstructing blocks.4 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.4 ...
wraping quantizers in blocks.4 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.401 (rec:1.401, round:0.000)	b=0.00	count=500
Total loss:	1.252 (rec:1.252, round:0.000)	b=0.00	count=1000
Total loss:	1.295 (rec:1.295, round:0.000)	b=0.00	count=1500
Total loss:	1.192 (rec:1.192, round:0.000)	b=0.00	count=2000
Total loss:	1.069 (rec:1.069, round:0.000)	b=0.00	count=2500
Total loss:	1.112 (rec:1.112, round:0.000)	b=0.00	count=3000
Total loss:	1.238 (rec:1.238, round:0.000)	b=0.00	count=3500
Total loss:	64899.023 (rec:1.038, round:64897.984)	b=20.00	count=4000
Total loss:	31154.963 (rec:1.069, round:31153.895)	b=19.44	count=4500
Total loss:	28725.664 (rec:1.098, round:28724.566)	b=18.88	count=5000
Total loss:	27083.584 (rec:1.100, round:27082.484)	b=18.31	count=5500
Total loss:	25668.859 (rec:1.031, round:25667.828)	b=17.75	count=6000
Total loss:	24346.945 (rec:0.998, round:24345.947)	b=17.19	count=6500
Total loss:	23080.377 (rec:1.205, round:23079.172)	b=16.62	count=7000
Total loss:	21838.918 (rec:0.993, round:21837.924)	b=16.06	count=7500
Total loss:	20620.158 (rec:0.998, round:20619.160)	b=15.50	count=8000
Total loss:	19421.641 (rec:1.035, round:19420.605)	b=14.94	count=8500
Total loss:	18232.480 (rec:1.009, round:18231.471)	b=14.38	count=9000
Total loss:	17052.723 (rec:1.000, round:17051.723)	b=13.81	count=9500
Total loss:	15876.553 (rec:1.118, round:15875.435)	b=13.25	count=10000
Total loss:	14709.595 (rec:0.959, round:14708.636)	b=12.69	count=10500
Total loss:	13545.841 (rec:1.111, round:13544.730)	b=12.12	count=11000
Total loss:	12396.303 (rec:1.087, round:12395.215)	b=11.56	count=11500
Total loss:	11249.003 (rec:1.033, round:11247.970)	b=11.00	count=12000
Total loss:	10111.729 (rec:0.996, round:10110.732)	b=10.44	count=12500
Total loss:	8988.071 (rec:1.031, round:8987.040)	b=9.88	count=13000
Total loss:	7880.034 (rec:1.022, round:7879.012)	b=9.31	count=13500
Total loss:	6792.200 (rec:1.011, round:6791.189)	b=8.75	count=14000
Total loss:	5720.365 (rec:1.046, round:5719.318)	b=8.19	count=14500
Total loss:	4677.119 (rec:0.961, round:4676.158)	b=7.62	count=15000
Total loss:	3661.819 (rec:1.089, round:3660.730)	b=7.06	count=15500
Total loss:	2690.613 (rec:1.015, round:2689.599)	b=6.50	count=16000
Total loss:	1761.133 (rec:1.127, round:1760.006)	b=5.94	count=16500
Total loss:	954.495 (rec:1.079, round:953.416)	b=5.38	count=17000
Total loss:	439.341 (rec:1.126, round:438.215)	b=4.81	count=17500
Total loss:	170.340 (rec:1.107, round:169.233)	b=4.25	count=18000
Total loss:	37.095 (rec:1.103, round:35.991)	b=3.69	count=18500
Total loss:	3.727 (rec:1.094, round:2.633)	b=3.12	count=19000
Total loss:	1.132 (rec:1.025, round:0.107)	b=2.56	count=19500
Total loss:	1.008 (rec:1.005, round:0.003)	b=2.00	count=20000
finished reconstructing blocks.4.
reconstructing blocks.5 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.5 ...
wraping quantizers in blocks.5 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.704 (rec:1.704, round:0.000)	b=0.00	count=500
Total loss:	1.559 (rec:1.559, round:0.000)	b=0.00	count=1000
Total loss:	1.605 (rec:1.605, round:0.000)	b=0.00	count=1500
Total loss:	1.463 (rec:1.463, round:0.000)	b=0.00	count=2000
Total loss:	1.541 (rec:1.541, round:0.000)	b=0.00	count=2500
Total loss:	1.646 (rec:1.646, round:0.000)	b=0.00	count=3000
Total loss:	1.474 (rec:1.474, round:0.000)	b=0.00	count=3500
Total loss:	64755.977 (rec:1.384, round:64754.594)	b=20.00	count=4000
Total loss:	30919.082 (rec:1.368, round:30917.715)	b=19.44	count=4500
Total loss:	28516.830 (rec:1.352, round:28515.479)	b=18.88	count=5000
Total loss:	26905.752 (rec:1.487, round:26904.266)	b=18.31	count=5500
Total loss:	25513.490 (rec:1.424, round:25512.066)	b=17.75	count=6000
Total loss:	24215.900 (rec:1.290, round:24214.611)	b=17.19	count=6500
Total loss:	22958.535 (rec:1.558, round:22956.977)	b=16.62	count=7000
Total loss:	21727.238 (rec:1.436, round:21725.803)	b=16.06	count=7500
Total loss:	20522.062 (rec:1.377, round:20520.686)	b=15.50	count=8000
Total loss:	19330.941 (rec:1.452, round:19329.488)	b=14.94	count=8500
Total loss:	18143.715 (rec:1.304, round:18142.410)	b=14.38	count=9000
Total loss:	16969.641 (rec:1.369, round:16968.271)	b=13.81	count=9500
Total loss:	15798.464 (rec:1.289, round:15797.175)	b=13.25	count=10000
Total loss:	14631.770 (rec:1.319, round:14630.451)	b=12.69	count=10500
Total loss:	13470.585 (rec:1.265, round:13469.320)	b=12.12	count=11000
Total loss:	12320.562 (rec:1.417, round:12319.145)	b=11.56	count=11500
Total loss:	11181.089 (rec:1.521, round:11179.567)	b=11.00	count=12000
Total loss:	10047.214 (rec:1.273, round:10045.940)	b=10.44	count=12500
Total loss:	8924.775 (rec:1.418, round:8923.357)	b=9.88	count=13000
Total loss:	7818.778 (rec:1.198, round:7817.580)	b=9.31	count=13500
Total loss:	6739.241 (rec:1.328, round:6737.913)	b=8.75	count=14000
Total loss:	5677.540 (rec:1.354, round:5676.186)	b=8.19	count=14500
Total loss:	4644.086 (rec:1.386, round:4642.700)	b=7.62	count=15000
Total loss:	3647.604 (rec:1.293, round:3646.311)	b=7.06	count=15500
Total loss:	2689.857 (rec:1.518, round:2688.338)	b=6.50	count=16000
Total loss:	1785.233 (rec:1.486, round:1783.746)	b=5.94	count=16500
Total loss:	1010.625 (rec:1.258, round:1009.367)	b=5.38	count=17000
Total loss:	488.355 (rec:1.401, round:486.955)	b=4.81	count=17500
Total loss:	198.930 (rec:1.288, round:197.642)	b=4.25	count=18000
Total loss:	57.946 (rec:1.191, round:56.754)	b=3.69	count=18500
Total loss:	7.440 (rec:1.318, round:6.122)	b=3.12	count=19000
Total loss:	1.405 (rec:1.215, round:0.190)	b=2.56	count=19500
Total loss:	1.412 (rec:1.400, round:0.012)	b=2.00	count=20000
finished reconstructing blocks.5.
reconstructing blocks.6 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.6 ...
wraping quantizers in blocks.6 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.544 (rec:1.544, round:0.000)	b=0.00	count=500
Total loss:	1.630 (rec:1.630, round:0.000)	b=0.00	count=1000
Total loss:	1.366 (rec:1.366, round:0.000)	b=0.00	count=1500
Total loss:	1.411 (rec:1.411, round:0.000)	b=0.00	count=2000
Total loss:	1.435 (rec:1.435, round:0.000)	b=0.00	count=2500
Total loss:	1.128 (rec:1.128, round:0.000)	b=0.00	count=3000
Total loss:	1.346 (rec:1.346, round:0.000)	b=0.00	count=3500
Total loss:	64190.004 (rec:1.421, round:64188.582)	b=20.00	count=4000
Total loss:	29525.061 (rec:1.392, round:29523.668)	b=19.44	count=4500
Total loss:	27110.732 (rec:1.386, round:27109.346)	b=18.88	count=5000
Total loss:	25437.023 (rec:1.383, round:25435.641)	b=18.31	count=5500
Total loss:	23967.852 (rec:1.355, round:23966.496)	b=17.75	count=6000
Total loss:	22589.686 (rec:1.170, round:22588.516)	b=17.19	count=6500
Total loss:	21267.514 (rec:1.316, round:21266.197)	b=16.62	count=7000
Total loss:	19983.277 (rec:1.604, round:19981.674)	b=16.06	count=7500
Total loss:	18723.988 (rec:1.477, round:18722.512)	b=15.50	count=8000
Total loss:	17489.139 (rec:1.451, round:17487.688)	b=14.94	count=8500
Total loss:	16276.312 (rec:1.363, round:16274.949)	b=14.38	count=9000
Total loss:	15087.070 (rec:1.371, round:15085.699)	b=13.81	count=9500
Total loss:	13922.748 (rec:1.319, round:13921.430)	b=13.25	count=10000
Total loss:	12783.304 (rec:1.352, round:12781.951)	b=12.69	count=10500
Total loss:	11664.018 (rec:1.136, round:11662.882)	b=12.12	count=11000
Total loss:	10578.581 (rec:1.264, round:10577.317)	b=11.56	count=11500
Total loss:	9509.622 (rec:1.108, round:9508.515)	b=11.00	count=12000
Total loss:	8470.271 (rec:1.283, round:8468.987)	b=10.44	count=12500
Total loss:	7457.058 (rec:1.236, round:7455.821)	b=9.88	count=13000
Total loss:	6477.708 (rec:1.254, round:6476.454)	b=9.31	count=13500
Total loss:	5521.652 (rec:1.273, round:5520.379)	b=8.75	count=14000
Total loss:	4601.662 (rec:1.290, round:4600.372)	b=8.19	count=14500
Total loss:	3717.757 (rec:1.391, round:3716.366)	b=7.62	count=15000
Total loss:	2877.900 (rec:1.403, round:2876.497)	b=7.06	count=15500
Total loss:	2078.726 (rec:1.150, round:2077.575)	b=6.50	count=16000
Total loss:	1348.973 (rec:1.090, round:1347.883)	b=5.94	count=16500
Total loss:	742.607 (rec:1.387, round:741.220)	b=5.38	count=17000
Total loss:	333.235 (rec:1.200, round:332.035)	b=4.81	count=17500
Total loss:	115.801 (rec:1.247, round:114.554)	b=4.25	count=18000
Total loss:	28.970 (rec:1.279, round:27.692)	b=3.69	count=18500
Total loss:	5.269 (rec:1.337, round:3.932)	b=3.12	count=19000
Total loss:	1.537 (rec:1.284, round:0.253)	b=2.56	count=19500
Total loss:	1.296 (rec:1.282, round:0.015)	b=2.00	count=20000
finished reconstructing blocks.6.
reconstructing blocks.7 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.7 ...
wraping quantizers in blocks.7 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.780 (rec:1.780, round:0.000)	b=0.00	count=500
Total loss:	1.723 (rec:1.723, round:0.000)	b=0.00	count=1000
Total loss:	1.757 (rec:1.757, round:0.000)	b=0.00	count=1500
Total loss:	2.031 (rec:2.031, round:0.000)	b=0.00	count=2000
Total loss:	1.979 (rec:1.979, round:0.000)	b=0.00	count=2500
Total loss:	1.827 (rec:1.827, round:0.000)	b=0.00	count=3000
Total loss:	2.016 (rec:2.016, round:0.000)	b=0.00	count=3500
Total loss:	65107.984 (rec:1.682, round:65106.301)	b=20.00	count=4000
Total loss:	30380.031 (rec:1.920, round:30378.111)	b=19.44	count=4500
Total loss:	28008.348 (rec:2.043, round:28006.305)	b=18.88	count=5000
Total loss:	26423.121 (rec:1.671, round:26421.449)	b=18.31	count=5500
Total loss:	25040.373 (rec:2.068, round:25038.305)	b=17.75	count=6000
Total loss:	23752.744 (rec:1.939, round:23750.805)	b=17.19	count=6500
Total loss:	22506.139 (rec:1.774, round:22504.365)	b=16.62	count=7000
Total loss:	21288.900 (rec:1.528, round:21287.373)	b=16.06	count=7500
Total loss:	20083.482 (rec:1.736, round:20081.746)	b=15.50	count=8000
Total loss:	18880.984 (rec:1.636, round:18879.348)	b=14.94	count=8500
Total loss:	17690.119 (rec:2.031, round:17688.088)	b=14.38	count=9000
Total loss:	16511.885 (rec:1.467, round:16510.418)	b=13.81	count=9500
Total loss:	15334.238 (rec:1.653, round:15332.586)	b=13.25	count=10000
Total loss:	14167.601 (rec:1.572, round:14166.028)	b=12.69	count=10500
Total loss:	13007.720 (rec:1.614, round:13006.105)	b=12.12	count=11000
Total loss:	11861.846 (rec:2.069, round:11859.777)	b=11.56	count=11500
Total loss:	10716.217 (rec:1.834, round:10714.383)	b=11.00	count=12000
Total loss:	9597.075 (rec:1.725, round:9595.351)	b=10.44	count=12500
Total loss:	8488.053 (rec:1.938, round:8486.115)	b=9.88	count=13000
Total loss:	7400.349 (rec:1.779, round:7398.570)	b=9.31	count=13500
Total loss:	6338.417 (rec:1.709, round:6336.708)	b=8.75	count=14000
Total loss:	5303.489 (rec:1.914, round:5301.576)	b=8.19	count=14500
Total loss:	4297.737 (rec:1.912, round:4295.825)	b=7.62	count=15000
Total loss:	3333.070 (rec:1.543, round:3331.527)	b=7.06	count=15500
Total loss:	2422.272 (rec:1.777, round:2420.495)	b=6.50	count=16000
Total loss:	1583.416 (rec:1.672, round:1581.744)	b=5.94	count=16500
Total loss:	870.661 (rec:2.224, round:868.437)	b=5.38	count=17000
Total loss:	368.775 (rec:1.686, round:367.089)	b=4.81	count=17500
Total loss:	107.884 (rec:1.773, round:106.111)	b=4.25	count=18000
Total loss:	19.147 (rec:1.617, round:17.529)	b=3.69	count=18500
Total loss:	3.082 (rec:1.857, round:1.225)	b=3.12	count=19000
Total loss:	2.019 (rec:1.972, round:0.047)	b=2.56	count=19500
Total loss:	1.945 (rec:1.943, round:0.002)	b=2.00	count=20000
finished reconstructing blocks.7.
reconstructing blocks.8 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.8 ...
wraping quantizers in blocks.8 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.742 (rec:1.742, round:0.000)	b=0.00	count=500
Total loss:	1.877 (rec:1.877, round:0.000)	b=0.00	count=1000
Total loss:	1.771 (rec:1.771, round:0.000)	b=0.00	count=1500
Total loss:	1.760 (rec:1.760, round:0.000)	b=0.00	count=2000
Total loss:	1.618 (rec:1.618, round:0.000)	b=0.00	count=2500
Total loss:	1.613 (rec:1.613, round:0.000)	b=0.00	count=3000
Total loss:	1.814 (rec:1.814, round:0.000)	b=0.00	count=3500
Total loss:	65491.277 (rec:1.686, round:65489.590)	b=20.00	count=4000
Total loss:	30908.156 (rec:1.949, round:30906.207)	b=19.44	count=4500
Total loss:	28540.988 (rec:1.891, round:28539.098)	b=18.88	count=5000
Total loss:	26970.256 (rec:1.701, round:26968.555)	b=18.31	count=5500
Total loss:	25604.693 (rec:1.787, round:25602.906)	b=17.75	count=6000
Total loss:	24332.260 (rec:1.976, round:24330.283)	b=17.19	count=6500
Total loss:	23105.906 (rec:1.977, round:23103.930)	b=16.62	count=7000
Total loss:	21906.469 (rec:2.136, round:21904.332)	b=16.06	count=7500
Total loss:	20713.371 (rec:1.845, round:20711.525)	b=15.50	count=8000
Total loss:	19533.455 (rec:1.704, round:19531.750)	b=14.94	count=8500
Total loss:	18356.311 (rec:1.612, round:18354.697)	b=14.38	count=9000
Total loss:	17180.932 (rec:1.766, round:17179.166)	b=13.81	count=9500
Total loss:	16008.925 (rec:1.402, round:16007.523)	b=13.25	count=10000
Total loss:	14838.082 (rec:1.967, round:14836.115)	b=12.69	count=10500
Total loss:	13673.315 (rec:1.901, round:13671.414)	b=12.12	count=11000
Total loss:	12512.575 (rec:1.720, round:12510.855)	b=11.56	count=11500
Total loss:	11361.080 (rec:1.997, round:11359.084)	b=11.00	count=12000
Total loss:	10219.334 (rec:1.797, round:10217.537)	b=10.44	count=12500
Total loss:	9086.779 (rec:1.906, round:9084.873)	b=9.88	count=13000
Total loss:	7969.599 (rec:1.665, round:7967.934)	b=9.31	count=13500
Total loss:	6869.188 (rec:1.585, round:6867.604)	b=8.75	count=14000
Total loss:	5787.583 (rec:1.885, round:5785.697)	b=8.19	count=14500
Total loss:	4738.226 (rec:1.770, round:4736.456)	b=7.62	count=15000
Total loss:	3736.883 (rec:2.184, round:3734.700)	b=7.06	count=15500
Total loss:	2787.357 (rec:1.879, round:2785.479)	b=6.50	count=16000
Total loss:	1912.380 (rec:2.160, round:1910.219)	b=5.94	count=16500
Total loss:	1150.321 (rec:1.909, round:1148.411)	b=5.38	count=17000
Total loss:	561.841 (rec:1.704, round:560.138)	b=4.81	count=17500
Total loss:	195.882 (rec:1.970, round:193.911)	b=4.25	count=18000
Total loss:	38.799 (rec:1.843, round:36.956)	b=3.69	count=18500
Total loss:	4.548 (rec:1.878, round:2.670)	b=3.12	count=19000
Total loss:	1.873 (rec:1.783, round:0.090)	b=2.56	count=19500
Total loss:	1.959 (rec:1.958, round:0.001)	b=2.00	count=20000
finished reconstructing blocks.8.
reconstructing blocks.9 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.9 ...
wraping quantizers in blocks.9 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.832 (rec:1.832, round:0.000)	b=0.00	count=500
Total loss:	2.091 (rec:2.091, round:0.000)	b=0.00	count=1000
Total loss:	1.827 (rec:1.827, round:0.000)	b=0.00	count=1500
Total loss:	2.130 (rec:2.130, round:0.000)	b=0.00	count=2000
Total loss:	1.623 (rec:1.623, round:0.000)	b=0.00	count=2500
Total loss:	1.945 (rec:1.945, round:0.000)	b=0.00	count=3000
Total loss:	2.117 (rec:2.117, round:0.000)	b=0.00	count=3500
Total loss:	65877.102 (rec:1.924, round:65875.180)	b=20.00	count=4000
Total loss:	31589.812 (rec:2.001, round:31587.812)	b=19.44	count=4500
Total loss:	29237.836 (rec:2.109, round:29235.727)	b=18.88	count=5000
Total loss:	27702.457 (rec:1.786, round:27700.672)	b=18.31	count=5500
Total loss:	26374.715 (rec:2.094, round:26372.621)	b=17.75	count=6000
Total loss:	25137.650 (rec:1.952, round:25135.699)	b=17.19	count=6500
Total loss:	23944.490 (rec:1.847, round:23942.643)	b=16.62	count=7000
Total loss:	22773.447 (rec:2.144, round:22771.303)	b=16.06	count=7500
Total loss:	21613.391 (rec:1.921, round:21611.469)	b=15.50	count=8000
Total loss:	20453.633 (rec:2.084, round:20451.549)	b=14.94	count=8500
Total loss:	19297.422 (rec:1.863, round:19295.559)	b=14.38	count=9000
Total loss:	18133.662 (rec:1.896, round:18131.766)	b=13.81	count=9500
Total loss:	16965.887 (rec:1.892, round:16963.994)	b=13.25	count=10000
Total loss:	15799.875 (rec:1.961, round:15797.914)	b=12.69	count=10500
Total loss:	14625.062 (rec:1.863, round:14623.200)	b=12.12	count=11000
Total loss:	13444.985 (rec:2.114, round:13442.872)	b=11.56	count=11500
Total loss:	12263.174 (rec:1.722, round:12261.452)	b=11.00	count=12000
Total loss:	11076.110 (rec:1.718, round:11074.393)	b=10.44	count=12500
Total loss:	9894.333 (rec:1.832, round:9892.502)	b=9.88	count=13000
Total loss:	8719.797 (rec:1.996, round:8717.801)	b=9.31	count=13500
Total loss:	7554.375 (rec:1.861, round:7552.514)	b=8.75	count=14000
Total loss:	6413.742 (rec:2.048, round:6411.693)	b=8.19	count=14500
Total loss:	5295.322 (rec:1.894, round:5293.429)	b=7.62	count=15000
Total loss:	4213.510 (rec:1.724, round:4211.787)	b=7.06	count=15500
Total loss:	3188.509 (rec:1.920, round:3186.589)	b=6.50	count=16000
Total loss:	2239.858 (rec:1.865, round:2237.993)	b=5.94	count=16500
Total loss:	1402.341 (rec:1.768, round:1400.574)	b=5.38	count=17000
Total loss:	729.461 (rec:1.832, round:727.629)	b=4.81	count=17500
Total loss:	272.710 (rec:2.083, round:270.626)	b=4.25	count=18000
Total loss:	58.003 (rec:1.829, round:56.174)	b=3.69	count=18500
Total loss:	6.064 (rec:1.677, round:4.386)	b=3.12	count=19000
Total loss:	1.965 (rec:1.824, round:0.141)	b=2.56	count=19500
Total loss:	1.916 (rec:1.913, round:0.003)	b=2.00	count=20000
finished reconstructing blocks.9.
reconstructing blocks.10 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.10 ...
wraping quantizers in blocks.10 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.793 (rec:1.793, round:0.000)	b=0.00	count=500
Total loss:	1.588 (rec:1.588, round:0.000)	b=0.00	count=1000
Total loss:	1.598 (rec:1.598, round:0.000)	b=0.00	count=1500
Total loss:	1.547 (rec:1.547, round:0.000)	b=0.00	count=2000
Total loss:	1.627 (rec:1.627, round:0.000)	b=0.00	count=2500
Total loss:	1.496 (rec:1.496, round:0.000)	b=0.00	count=3000
Total loss:	1.774 (rec:1.774, round:0.000)	b=0.00	count=3500
Total loss:	66091.305 (rec:1.762, round:66089.547)	b=20.00	count=4000
Total loss:	32141.502 (rec:1.520, round:32139.982)	b=19.44	count=4500
Total loss:	29811.219 (rec:1.389, round:29809.830)	b=18.88	count=5000
Total loss:	28297.742 (rec:1.633, round:28296.109)	b=18.31	count=5500
Total loss:	27002.205 (rec:1.516, round:27000.689)	b=17.75	count=6000
Total loss:	25798.053 (rec:1.582, round:25796.471)	b=17.19	count=6500
Total loss:	24634.152 (rec:1.562, round:24632.590)	b=16.62	count=7000
Total loss:	23484.633 (rec:1.490, round:23483.143)	b=16.06	count=7500
Total loss:	22355.662 (rec:1.517, round:22354.145)	b=15.50	count=8000
Total loss:	21211.725 (rec:1.530, round:21210.195)	b=14.94	count=8500
Total loss:	20077.254 (rec:1.635, round:20075.619)	b=14.38	count=9000
Total loss:	18931.631 (rec:1.495, round:18930.135)	b=13.81	count=9500
Total loss:	17776.875 (rec:1.747, round:17775.129)	b=13.25	count=10000
Total loss:	16601.660 (rec:1.464, round:16600.195)	b=12.69	count=10500
Total loss:	15420.843 (rec:1.695, round:15419.147)	b=12.12	count=11000
Total loss:	14241.076 (rec:1.416, round:14239.660)	b=11.56	count=11500
Total loss:	13049.415 (rec:1.432, round:13047.983)	b=11.00	count=12000
Total loss:	11845.785 (rec:1.614, round:11844.171)	b=10.44	count=12500
Total loss:	10649.217 (rec:1.510, round:10647.707)	b=9.88	count=13000
Total loss:	9454.996 (rec:1.343, round:9453.652)	b=9.31	count=13500
Total loss:	8267.688 (rec:1.647, round:8266.042)	b=8.75	count=14000
Total loss:	7087.614 (rec:1.533, round:7086.081)	b=8.19	count=14500
Total loss:	5931.706 (rec:1.660, round:5930.045)	b=7.62	count=15000
Total loss:	4807.024 (rec:1.626, round:4805.398)	b=7.06	count=15500
Total loss:	3721.328 (rec:1.558, round:3719.770)	b=6.50	count=16000
Total loss:	2699.219 (rec:1.579, round:2697.641)	b=5.94	count=16500
Total loss:	1770.220 (rec:1.548, round:1768.672)	b=5.38	count=17000
Total loss:	983.430 (rec:1.358, round:982.072)	b=4.81	count=17500
Total loss:	409.362 (rec:1.545, round:407.817)	b=4.25	count=18000
Total loss:	97.821 (rec:1.446, round:96.374)	b=3.69	count=18500
Total loss:	11.259 (rec:1.756, round:9.504)	b=3.12	count=19000
Total loss:	1.819 (rec:1.485, round:0.333)	b=2.56	count=19500
Total loss:	1.558 (rec:1.549, round:0.008)	b=2.00	count=20000
finished reconstructing blocks.10.
reconstructing blocks.11 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.11 ...
wraping quantizers in blocks.11 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.924 (rec:1.924, round:0.000)	b=0.00	count=500
Total loss:	1.779 (rec:1.779, round:0.000)	b=0.00	count=1000
Total loss:	2.234 (rec:2.234, round:0.000)	b=0.00	count=1500
Total loss:	2.119 (rec:2.119, round:0.000)	b=0.00	count=2000
Total loss:	1.788 (rec:1.788, round:0.000)	b=0.00	count=2500
Total loss:	2.046 (rec:2.046, round:0.000)	b=0.00	count=3000
Total loss:	1.948 (rec:1.948, round:0.000)	b=0.00	count=3500
Total loss:	65506.598 (rec:2.140, round:65504.457)	b=20.00	count=4000
Total loss:	31309.014 (rec:1.956, round:31307.059)	b=19.44	count=4500
Total loss:	28917.021 (rec:1.968, round:28915.055)	b=18.88	count=5000
Total loss:	27316.852 (rec:1.972, round:27314.881)	b=18.31	count=5500
Total loss:	25934.908 (rec:1.926, round:25932.982)	b=17.75	count=6000
Total loss:	24634.656 (rec:2.094, round:24632.562)	b=17.19	count=6500
Total loss:	23376.221 (rec:1.926, round:23374.295)	b=16.62	count=7000
Total loss:	22134.479 (rec:1.842, round:22132.637)	b=16.06	count=7500
Total loss:	20914.658 (rec:1.958, round:20912.699)	b=15.50	count=8000
Total loss:	19700.316 (rec:2.097, round:19698.219)	b=14.94	count=8500
Total loss:	18495.824 (rec:1.979, round:18493.846)	b=14.38	count=9000
Total loss:	17291.424 (rec:1.893, round:17289.531)	b=13.81	count=9500
Total loss:	16103.312 (rec:1.886, round:16101.426)	b=13.25	count=10000
Total loss:	14915.402 (rec:1.802, round:14913.600)	b=12.69	count=10500
Total loss:	13739.228 (rec:1.739, round:13737.488)	b=12.12	count=11000
Total loss:	12574.073 (rec:1.948, round:12572.125)	b=11.56	count=11500
Total loss:	11417.604 (rec:2.121, round:11415.483)	b=11.00	count=12000
Total loss:	10279.047 (rec:1.727, round:10277.319)	b=10.44	count=12500
Total loss:	9157.502 (rec:1.885, round:9155.617)	b=9.88	count=13000
Total loss:	8045.595 (rec:1.871, round:8043.724)	b=9.31	count=13500
Total loss:	6955.151 (rec:1.885, round:6953.267)	b=8.75	count=14000
Total loss:	5890.685 (rec:2.147, round:5888.538)	b=8.19	count=14500
Total loss:	4859.478 (rec:2.136, round:4857.341)	b=7.62	count=15000
Total loss:	3864.602 (rec:1.861, round:3862.741)	b=7.06	count=15500
Total loss:	2919.902 (rec:1.687, round:2918.215)	b=6.50	count=16000
Total loss:	2033.226 (rec:1.952, round:2031.274)	b=5.94	count=16500
Total loss:	1219.016 (rec:2.140, round:1216.876)	b=5.38	count=17000
Total loss:	554.871 (rec:1.936, round:552.935)	b=4.81	count=17500
Total loss:	174.310 (rec:2.076, round:172.234)	b=4.25	count=18000
Total loss:	33.800 (rec:2.097, round:31.703)	b=3.69	count=18500
Total loss:	4.494 (rec:1.903, round:2.592)	b=3.12	count=19000
Total loss:	2.044 (rec:1.951, round:0.093)	b=2.56	count=19500
Total loss:	1.940 (rec:1.938, round:0.002)	b=2.00	count=20000
finished reconstructing blocks.11.
reconstructing head ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for head ...
wraping quantizers in head ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.245 (rec:1.245, round:0.000)	b=0.00	count=500
Total loss:	1.613 (rec:1.613, round:0.000)	b=0.00	count=1000
Total loss:	1.258 (rec:1.258, round:0.000)	b=0.00	count=1500
Total loss:	0.652 (rec:0.652, round:0.000)	b=0.00	count=2000
Total loss:	0.797 (rec:0.797, round:0.000)	b=0.00	count=2500
Total loss:	0.555 (rec:0.555, round:0.000)	b=0.00	count=3000
Total loss:	0.780 (rec:0.780, round:0.000)	b=0.00	count=3500
Total loss:	7223.956 (rec:1.061, round:7222.895)	b=20.00	count=4000
Total loss:	4521.995 (rec:0.726, round:4521.269)	b=19.44	count=4500
Total loss:	4245.993 (rec:0.616, round:4245.377)	b=18.88	count=5000
Total loss:	4082.430 (rec:0.996, round:4081.434)	b=18.31	count=5500
Total loss:	3956.188 (rec:0.815, round:3955.373)	b=17.75	count=6000
Total loss:	3844.076 (rec:0.891, round:3843.185)	b=17.19	count=6500
Total loss:	3738.433 (rec:0.939, round:3737.495)	b=16.62	count=7000
Total loss:	3635.125 (rec:0.641, round:3634.484)	b=16.06	count=7500
Total loss:	3535.945 (rec:0.791, round:3535.154)	b=15.50	count=8000
Total loss:	3435.104 (rec:0.621, round:3434.482)	b=14.94	count=8500
Total loss:	3332.770 (rec:1.218, round:3331.552)	b=14.38	count=9000
Total loss:	3229.083 (rec:0.674, round:3228.409)	b=13.81	count=9500
Total loss:	3123.359 (rec:1.011, round:3122.348)	b=13.25	count=10000
Total loss:	3013.419 (rec:0.754, round:3012.665)	b=12.69	count=10500
Total loss:	2900.240 (rec:0.783, round:2899.458)	b=12.12	count=11000
Total loss:	2781.950 (rec:0.983, round:2780.967)	b=11.56	count=11500
Total loss:	2658.884 (rec:0.606, round:2658.278)	b=11.00	count=12000
Total loss:	2533.500 (rec:0.843, round:2532.657)	b=10.44	count=12500
Total loss:	2398.664 (rec:0.907, round:2397.758)	b=9.88	count=13000
Total loss:	2256.474 (rec:0.780, round:2255.695)	b=9.31	count=13500
Total loss:	2107.754 (rec:0.878, round:2106.876)	b=8.75	count=14000
Total loss:	1951.082 (rec:0.677, round:1950.405)	b=8.19	count=14500
Total loss:	1785.898 (rec:0.860, round:1785.038)	b=7.62	count=15000
Total loss:	1609.198 (rec:0.552, round:1608.646)	b=7.06	count=15500
Total loss:	1426.203 (rec:0.657, round:1425.546)	b=6.50	count=16000
Total loss:	1235.714 (rec:0.857, round:1234.857)	b=5.94	count=16500
Total loss:	1037.077 (rec:0.732, round:1036.345)	b=5.38	count=17000
Total loss:	829.048 (rec:0.828, round:828.220)	b=4.81	count=17500
Total loss:	620.605 (rec:0.656, round:619.950)	b=4.25	count=18000
Total loss:	414.976 (rec:0.816, round:414.160)	b=3.69	count=18500
Total loss:	226.428 (rec:0.997, round:225.431)	b=3.12	count=19000
Total loss:	81.475 (rec:0.870, round:80.604)	b=2.56	count=19500
Total loss:	14.927 (rec:1.073, round:13.855)	b=2.00	count=20000
finished reconstructing head.
2025-09-14 18:10:46 - mse guided block reconstruction finished.
Saving checkpoint to ./checkpoint/quant_result/20250914_1436/vit_base_w4_a6_optimsize_1024_mse_qdrop.pth
Validating on calibration set after block reconstruction ...
Test: [0/32]	Time 0.457 (0.457)	Loss 0.3741 (0.3741)	Prec@1 93.750 (93.750)	Prec@5 100.000 (100.000)
Test: [10/32]	Time 0.077 (0.111)	Loss 0.7435 (0.7058)	Prec@1 87.500 (84.659)	Prec@5 93.750 (95.455)
Test: [20/32]	Time 0.077 (0.095)	Loss 0.7930 (0.6416)	Prec@1 78.125 (85.863)	Prec@5 93.750 (96.875)
Test: [30/32]	Time 0.077 (0.089)	Loss 0.8237 (0.6277)	Prec@1 78.125 (85.988)	Prec@5 96.875 (97.278)
 * Prec@1 86.133 Prec@5 97.363 Loss 0.621 Time 2.951
Validating on test set after block reconstruction ...
Test: [0/100]	Time 4.841 (4.841)	Loss 0.4442 (0.4442)	Prec@1 91.200 (91.200)	Prec@5 98.600 (98.600)
Test: [10/100]	Time 1.675 (1.961)	Loss 0.4986 (0.5504)	Prec@1 89.600 (88.073)	Prec@5 98.600 (98.145)
Test: [20/100]	Time 1.681 (1.826)	Loss 0.6338 (0.5781)	Prec@1 85.000 (87.495)	Prec@5 98.200 (98.010)
Test: [30/100]	Time 1.678 (1.779)	Loss 0.5291 (0.6016)	Prec@1 88.400 (86.748)	Prec@5 99.600 (97.942)
Test: [40/100]	Time 1.686 (1.756)	Loss 0.8042 (0.5961)	Prec@1 80.400 (86.888)	Prec@5 96.600 (97.980)
Test: [50/100]	Time 1.684 (1.742)	Loss 1.0544 (0.6400)	Prec@1 75.600 (85.651)	Prec@5 94.600 (97.624)
Test: [60/100]	Time 1.692 (1.733)	Loss 0.6285 (0.6466)	Prec@1 88.400 (85.574)	Prec@5 97.400 (97.511)
Test: [70/100]	Time 1.686 (1.726)	Loss 0.7540 (0.6633)	Prec@1 83.200 (85.073)	Prec@5 97.200 (97.400)
Test: [80/100]	Time 1.688 (1.721)	Loss 0.5618 (0.6699)	Prec@1 86.800 (84.923)	Prec@5 97.600 (97.291)
Test: [90/100]	Time 1.682 (1.717)	Loss 1.0170 (0.6872)	Prec@1 74.600 (84.354)	Prec@5 94.800 (97.196)
 * Prec@1 84.408 Prec@5 97.258 Loss 0.683 Time 171.641
2025-09-14 18:13:41 - finished the process.
Extracting logits from quantized and full-precision models...
Testing combinations:
  Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
  Cluster numbers: [8, 16, 32, 64, 128, 256]
  PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.41%
[Alpha=0.10] Top-5 Accuracy: 97.25%
Result: Top-1: 84.41%, Top-5: 97.25%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.40%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.40%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.42%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.42%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.39%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.39%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.41%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.41%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.39%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.39%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.42%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.42%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.39%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.39%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.39%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.39%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.39%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.39%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.42%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.42%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.40%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.40%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.42%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.42%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.41%
[Alpha=0.10] Top-5 Accuracy: 97.28%
Result: Top-1: 84.41%, Top-5: 97.28%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.38%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.38%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.40%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.40%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.41%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.41%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.38%
[Alpha=0.10] Top-5 Accuracy: 97.25%
Result: Top-1: 84.38%, Top-5: 97.25%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.42%
[Alpha=0.10] Top-5 Accuracy: 97.28%
Result: Top-1: 84.42%, Top-5: 97.28%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.39%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.39%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.41%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.41%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.39%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.39%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.41%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.41%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.40%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.40%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.39%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.39%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.41%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.41%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.38%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.38%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.40%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.40%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.37%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.37%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.37%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.37%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.35%
[Alpha=0.10] Top-5 Accuracy: 97.25%
Result: Top-1: 84.35%, Top-5: 97.25%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.38%
[Alpha=0.10] Top-5 Accuracy: 97.25%
Result: Top-1: 84.38%, Top-5: 97.25%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.32%
[Alpha=0.10] Top-5 Accuracy: 97.25%
Result: Top-1: 84.32%, Top-5: 97.25%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.39%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.39%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.32%
[Alpha=0.10] Top-5 Accuracy: 97.23%
Result: Top-1: 84.32%, Top-5: 97.23%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.39%
[Alpha=0.10] Top-5 Accuracy: 97.25%
Result: Top-1: 84.39%, Top-5: 97.25%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.35%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.35%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.40%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.40%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.36%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.36%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.35%
[Alpha=0.10] Top-5 Accuracy: 97.25%
Result: Top-1: 84.35%, Top-5: 97.25%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.93%
[Alpha=0.10] Top-5 Accuracy: 97.02%
Result: Top-1: 82.93%, Top-5: 97.02%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.15%
[Alpha=0.10] Top-5 Accuracy: 97.15%
Result: Top-1: 84.15%, Top-5: 97.15%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.26%
[Alpha=0.10] Top-5 Accuracy: 97.23%
Result: Top-1: 84.26%, Top-5: 97.23%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.32%
[Alpha=0.10] Top-5 Accuracy: 97.24%
Result: Top-1: 84.32%, Top-5: 97.24%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.23%
[Alpha=0.10] Top-5 Accuracy: 97.23%
Result: Top-1: 84.23%, Top-5: 97.23%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.36%
[Alpha=0.10] Top-5 Accuracy: 97.25%
Result: Top-1: 84.36%, Top-5: 97.25%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.14%
[Alpha=0.10] Top-5 Accuracy: 97.19%
Result: Top-1: 84.14%, Top-5: 97.19%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.38%
[Alpha=0.10] Top-5 Accuracy: 97.27%
Result: Top-1: 84.38%, Top-5: 97.27%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.31%
[Alpha=0.10] Top-5 Accuracy: 97.23%
Result: Top-1: 84.31%, Top-5: 97.23%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.33%
[Alpha=0.10] Top-5 Accuracy: 97.26%
Result: Top-1: 84.33%, Top-5: 97.26%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 68.15%
[Alpha=0.10] Top-5 Accuracy: 95.01%
Result: Top-1: 68.15%, Top-5: 95.01%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 64.41%
[Alpha=0.10] Top-5 Accuracy: 95.05%
Result: Top-1: 64.41%, Top-5: 95.05%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 81.32%
[Alpha=0.10] Top-5 Accuracy: 96.65%
Result: Top-1: 81.32%, Top-5: 96.65%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.24%
[Alpha=0.10] Top-5 Accuracy: 96.97%
Result: Top-1: 80.24%, Top-5: 96.97%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 83.74%
[Alpha=0.10] Top-5 Accuracy: 96.99%
Result: Top-1: 83.74%, Top-5: 96.99%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 83.76%
[Alpha=0.10] Top-5 Accuracy: 97.14%
Result: Top-1: 83.76%, Top-5: 97.14%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.08%
[Alpha=0.10] Top-5 Accuracy: 97.15%
Result: Top-1: 84.08%, Top-5: 97.15%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.04%
[Alpha=0.10] Top-5 Accuracy: 97.21%
Result: Top-1: 84.04%, Top-5: 97.21%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.11%
[Alpha=0.10] Top-5 Accuracy: 97.19%
Result: Top-1: 84.11%, Top-5: 97.19%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 84.32%
[Alpha=0.10] Top-5 Accuracy: 97.21%
Result: Top-1: 84.32%, Top-5: 97.21%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.47%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.47%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.40%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.40%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.43%
[Alpha=0.20] Top-5 Accuracy: 97.27%
Result: Top-1: 84.43%, Top-5: 97.27%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.41%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.41%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.36%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.36%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.39%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.39%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.40%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.40%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.38%
[Alpha=0.20] Top-5 Accuracy: 97.28%
Result: Top-1: 84.38%, Top-5: 97.28%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.40%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.40%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.39%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.39%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.35%
[Alpha=0.20] Top-5 Accuracy: 97.24%
Result: Top-1: 84.35%, Top-5: 97.24%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.33%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.33%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.41%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.41%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.36%
[Alpha=0.20] Top-5 Accuracy: 97.27%
Result: Top-1: 84.36%, Top-5: 97.27%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.36%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.36%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.36%
[Alpha=0.20] Top-5 Accuracy: 97.27%
Result: Top-1: 84.36%, Top-5: 97.27%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.38%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.38%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.39%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.39%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.39%
[Alpha=0.20] Top-5 Accuracy: 97.27%
Result: Top-1: 84.39%, Top-5: 97.27%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.39%
[Alpha=0.20] Top-5 Accuracy: 97.27%
Result: Top-1: 84.39%, Top-5: 97.27%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.21%
[Alpha=0.20] Top-5 Accuracy: 97.24%
Result: Top-1: 84.21%, Top-5: 97.24%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.35%
[Alpha=0.20] Top-5 Accuracy: 97.27%
Result: Top-1: 84.35%, Top-5: 97.27%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.32%
[Alpha=0.20] Top-5 Accuracy: 97.27%
Result: Top-1: 84.32%, Top-5: 97.27%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.37%
[Alpha=0.20] Top-5 Accuracy: 97.27%
Result: Top-1: 84.37%, Top-5: 97.27%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.38%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.38%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.35%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.35%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.33%
[Alpha=0.20] Top-5 Accuracy: 97.27%
Result: Top-1: 84.33%, Top-5: 97.27%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.33%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.33%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.36%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.36%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.36%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.36%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 83.97%
[Alpha=0.20] Top-5 Accuracy: 97.20%
Result: Top-1: 83.97%, Top-5: 97.20%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.25%
[Alpha=0.20] Top-5 Accuracy: 97.27%
Result: Top-1: 84.25%, Top-5: 97.27%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.25%
[Alpha=0.20] Top-5 Accuracy: 97.22%
Result: Top-1: 84.25%, Top-5: 97.22%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.34%
[Alpha=0.20] Top-5 Accuracy: 97.23%
Result: Top-1: 84.34%, Top-5: 97.23%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.29%
[Alpha=0.20] Top-5 Accuracy: 97.21%
Result: Top-1: 84.29%, Top-5: 97.21%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.34%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.34%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.38%
[Alpha=0.20] Top-5 Accuracy: 97.24%
Result: Top-1: 84.38%, Top-5: 97.24%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.36%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.36%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.33%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.33%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.30%
[Alpha=0.20] Top-5 Accuracy: 97.25%
Result: Top-1: 84.30%, Top-5: 97.25%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.46%
[Alpha=0.20] Top-5 Accuracy: 96.35%
Result: Top-1: 80.46%, Top-5: 96.35%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 83.69%
[Alpha=0.20] Top-5 Accuracy: 96.89%
Result: Top-1: 83.69%, Top-5: 96.89%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.07%
[Alpha=0.20] Top-5 Accuracy: 97.18%
Result: Top-1: 84.07%, Top-5: 97.18%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.15%
[Alpha=0.20] Top-5 Accuracy: 97.18%
Result: Top-1: 84.15%, Top-5: 97.18%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.07%
[Alpha=0.20] Top-5 Accuracy: 97.16%
Result: Top-1: 84.07%, Top-5: 97.16%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.21%
[Alpha=0.20] Top-5 Accuracy: 97.22%
Result: Top-1: 84.21%, Top-5: 97.22%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.10%
[Alpha=0.20] Top-5 Accuracy: 97.08%
Result: Top-1: 84.10%, Top-5: 97.08%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.31%
[Alpha=0.20] Top-5 Accuracy: 97.23%
Result: Top-1: 84.31%, Top-5: 97.23%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.15%
[Alpha=0.20] Top-5 Accuracy: 97.20%
Result: Top-1: 84.15%, Top-5: 97.20%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.25%
[Alpha=0.20] Top-5 Accuracy: 97.26%
Result: Top-1: 84.25%, Top-5: 97.26%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.51%
[Alpha=0.20] Top-5 Accuracy: 88.59%
Result: Top-1: 58.51%, Top-5: 88.59%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 45.59%
[Alpha=0.20] Top-5 Accuracy: 90.66%
Result: Top-1: 45.59%, Top-5: 90.66%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.06%
[Alpha=0.20] Top-5 Accuracy: 95.54%
Result: Top-1: 80.06%, Top-5: 95.54%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 71.84%
[Alpha=0.20] Top-5 Accuracy: 96.36%
Result: Top-1: 71.84%, Top-5: 96.36%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 83.48%
[Alpha=0.20] Top-5 Accuracy: 96.63%
Result: Top-1: 83.48%, Top-5: 96.63%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 83.42%
[Alpha=0.20] Top-5 Accuracy: 96.89%
Result: Top-1: 83.42%, Top-5: 96.89%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 83.90%
[Alpha=0.20] Top-5 Accuracy: 96.96%
Result: Top-1: 83.90%, Top-5: 96.96%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 83.81%
[Alpha=0.20] Top-5 Accuracy: 97.05%
Result: Top-1: 83.81%, Top-5: 97.05%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 83.67%
[Alpha=0.20] Top-5 Accuracy: 97.01%
Result: Top-1: 83.67%, Top-5: 97.01%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 84.03%
[Alpha=0.20] Top-5 Accuracy: 97.12%
Result: Top-1: 84.03%, Top-5: 97.12%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.40%
[Alpha=0.30] Top-5 Accuracy: 97.27%
Result: Top-1: 84.40%, Top-5: 97.27%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.29%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.29%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.37%
[Alpha=0.30] Top-5 Accuracy: 97.26%
Result: Top-1: 84.37%, Top-5: 97.26%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.33%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.33%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.31%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.31%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.32%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.32%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.40%
[Alpha=0.30] Top-5 Accuracy: 97.26%
Result: Top-1: 84.40%, Top-5: 97.26%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.32%
[Alpha=0.30] Top-5 Accuracy: 97.27%
Result: Top-1: 84.32%, Top-5: 97.27%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.32%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.32%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.34%
[Alpha=0.30] Top-5 Accuracy: 97.27%
Result: Top-1: 84.34%, Top-5: 97.27%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.29%
[Alpha=0.30] Top-5 Accuracy: 97.24%
Result: Top-1: 84.29%, Top-5: 97.24%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.26%
[Alpha=0.30] Top-5 Accuracy: 97.24%
Result: Top-1: 84.26%, Top-5: 97.24%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.37%
[Alpha=0.30] Top-5 Accuracy: 97.26%
Result: Top-1: 84.37%, Top-5: 97.26%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.29%
[Alpha=0.30] Top-5 Accuracy: 97.27%
Result: Top-1: 84.29%, Top-5: 97.27%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.32%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.32%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.32%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.32%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.36%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.36%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.37%
[Alpha=0.30] Top-5 Accuracy: 97.24%
Result: Top-1: 84.37%, Top-5: 97.24%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.34%
[Alpha=0.30] Top-5 Accuracy: 97.26%
Result: Top-1: 84.34%, Top-5: 97.26%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.32%
[Alpha=0.30] Top-5 Accuracy: 97.27%
Result: Top-1: 84.32%, Top-5: 97.27%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.83%
[Alpha=0.30] Top-5 Accuracy: 97.14%
Result: Top-1: 83.83%, Top-5: 97.14%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.28%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.28%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.26%
[Alpha=0.30] Top-5 Accuracy: 97.26%
Result: Top-1: 84.26%, Top-5: 97.26%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.29%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.29%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.34%
[Alpha=0.30] Top-5 Accuracy: 97.26%
Result: Top-1: 84.34%, Top-5: 97.26%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.31%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.31%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.26%
[Alpha=0.30] Top-5 Accuracy: 97.26%
Result: Top-1: 84.26%, Top-5: 97.26%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.28%
[Alpha=0.30] Top-5 Accuracy: 97.23%
Result: Top-1: 84.28%, Top-5: 97.23%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.30%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.30%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.27%
[Alpha=0.30] Top-5 Accuracy: 97.24%
Result: Top-1: 84.27%, Top-5: 97.24%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.29%
[Alpha=0.30] Top-5 Accuracy: 97.10%
Result: Top-1: 83.29%, Top-5: 97.10%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.11%
[Alpha=0.30] Top-5 Accuracy: 97.22%
Result: Top-1: 84.11%, Top-5: 97.22%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.10%
[Alpha=0.30] Top-5 Accuracy: 97.18%
Result: Top-1: 84.10%, Top-5: 97.18%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.27%
[Alpha=0.30] Top-5 Accuracy: 97.24%
Result: Top-1: 84.27%, Top-5: 97.24%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.22%
[Alpha=0.30] Top-5 Accuracy: 97.17%
Result: Top-1: 84.22%, Top-5: 97.17%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.23%
[Alpha=0.30] Top-5 Accuracy: 97.24%
Result: Top-1: 84.23%, Top-5: 97.24%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.26%
[Alpha=0.30] Top-5 Accuracy: 97.24%
Result: Top-1: 84.26%, Top-5: 97.24%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.31%
[Alpha=0.30] Top-5 Accuracy: 97.24%
Result: Top-1: 84.31%, Top-5: 97.24%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.18%
[Alpha=0.30] Top-5 Accuracy: 97.20%
Result: Top-1: 84.18%, Top-5: 97.20%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.20%
[Alpha=0.30] Top-5 Accuracy: 97.25%
Result: Top-1: 84.20%, Top-5: 97.25%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 76.90%
[Alpha=0.30] Top-5 Accuracy: 95.29%
Result: Top-1: 76.90%, Top-5: 95.29%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.35%
[Alpha=0.30] Top-5 Accuracy: 96.59%
Result: Top-1: 83.35%, Top-5: 96.59%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.86%
[Alpha=0.30] Top-5 Accuracy: 97.09%
Result: Top-1: 83.86%, Top-5: 97.09%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.96%
[Alpha=0.30] Top-5 Accuracy: 97.10%
Result: Top-1: 83.96%, Top-5: 97.10%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.88%
[Alpha=0.30] Top-5 Accuracy: 97.07%
Result: Top-1: 83.88%, Top-5: 97.07%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.04%
[Alpha=0.30] Top-5 Accuracy: 97.14%
Result: Top-1: 84.04%, Top-5: 97.14%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.90%
[Alpha=0.30] Top-5 Accuracy: 96.96%
Result: Top-1: 83.90%, Top-5: 96.96%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.13%
[Alpha=0.30] Top-5 Accuracy: 97.19%
Result: Top-1: 84.13%, Top-5: 97.19%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.01%
[Alpha=0.30] Top-5 Accuracy: 97.13%
Result: Top-1: 84.01%, Top-5: 97.13%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 84.13%
[Alpha=0.30] Top-5 Accuracy: 97.20%
Result: Top-1: 84.13%, Top-5: 97.20%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 55.39%
[Alpha=0.30] Top-5 Accuracy: 80.59%
Result: Top-1: 55.39%, Top-5: 80.59%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 38.35%
[Alpha=0.30] Top-5 Accuracy: 83.64%
Result: Top-1: 38.35%, Top-5: 83.64%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 79.45%
[Alpha=0.30] Top-5 Accuracy: 94.27%
Result: Top-1: 79.45%, Top-5: 94.27%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 64.61%
[Alpha=0.30] Top-5 Accuracy: 95.23%
Result: Top-1: 64.61%, Top-5: 95.23%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.23%
[Alpha=0.30] Top-5 Accuracy: 96.35%
Result: Top-1: 83.23%, Top-5: 96.35%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.20%
[Alpha=0.30] Top-5 Accuracy: 96.64%
Result: Top-1: 83.20%, Top-5: 96.64%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.66%
[Alpha=0.30] Top-5 Accuracy: 96.85%
Result: Top-1: 83.66%, Top-5: 96.85%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.57%
[Alpha=0.30] Top-5 Accuracy: 96.89%
Result: Top-1: 83.57%, Top-5: 96.89%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.37%
[Alpha=0.30] Top-5 Accuracy: 96.79%
Result: Top-1: 83.37%, Top-5: 96.79%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 83.75%
[Alpha=0.30] Top-5 Accuracy: 97.03%
Result: Top-1: 83.75%, Top-5: 97.03%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.33%
[Alpha=0.40] Top-5 Accuracy: 97.26%
Result: Top-1: 84.33%, Top-5: 97.26%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.28%
[Alpha=0.40] Top-5 Accuracy: 97.26%
Result: Top-1: 84.28%, Top-5: 97.26%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.37%
[Alpha=0.40] Top-5 Accuracy: 97.26%
Result: Top-1: 84.37%, Top-5: 97.26%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.31%
[Alpha=0.40] Top-5 Accuracy: 97.25%
Result: Top-1: 84.31%, Top-5: 97.25%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.28%
[Alpha=0.40] Top-5 Accuracy: 97.26%
Result: Top-1: 84.28%, Top-5: 97.26%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.28%
[Alpha=0.40] Top-5 Accuracy: 97.27%
Result: Top-1: 84.28%, Top-5: 97.27%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.34%
[Alpha=0.40] Top-5 Accuracy: 97.26%
Result: Top-1: 84.34%, Top-5: 97.26%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.28%
[Alpha=0.40] Top-5 Accuracy: 97.26%
Result: Top-1: 84.28%, Top-5: 97.26%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.31%
[Alpha=0.40] Top-5 Accuracy: 97.25%
Result: Top-1: 84.31%, Top-5: 97.25%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.30%
[Alpha=0.40] Top-5 Accuracy: 97.28%
Result: Top-1: 84.30%, Top-5: 97.28%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.17%
[Alpha=0.40] Top-5 Accuracy: 97.20%
Result: Top-1: 84.17%, Top-5: 97.20%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.22%
[Alpha=0.40] Top-5 Accuracy: 97.23%
Result: Top-1: 84.22%, Top-5: 97.23%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.33%
[Alpha=0.40] Top-5 Accuracy: 97.26%
Result: Top-1: 84.33%, Top-5: 97.26%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.26%
[Alpha=0.40] Top-5 Accuracy: 97.28%
Result: Top-1: 84.26%, Top-5: 97.28%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.27%
[Alpha=0.40] Top-5 Accuracy: 97.24%
Result: Top-1: 84.27%, Top-5: 97.24%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.28%
[Alpha=0.40] Top-5 Accuracy: 97.25%
Result: Top-1: 84.28%, Top-5: 97.25%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.30%
[Alpha=0.40] Top-5 Accuracy: 97.24%
Result: Top-1: 84.30%, Top-5: 97.24%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.31%
[Alpha=0.40] Top-5 Accuracy: 97.24%
Result: Top-1: 84.31%, Top-5: 97.24%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.31%
[Alpha=0.40] Top-5 Accuracy: 97.27%
Result: Top-1: 84.31%, Top-5: 97.27%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.30%
[Alpha=0.40] Top-5 Accuracy: 97.25%
Result: Top-1: 84.30%, Top-5: 97.25%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.29%
[Alpha=0.40] Top-5 Accuracy: 97.06%
Result: Top-1: 83.29%, Top-5: 97.06%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.19%
[Alpha=0.40] Top-5 Accuracy: 97.25%
Result: Top-1: 84.19%, Top-5: 97.25%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.18%
[Alpha=0.40] Top-5 Accuracy: 97.24%
Result: Top-1: 84.18%, Top-5: 97.24%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.21%
[Alpha=0.40] Top-5 Accuracy: 97.24%
Result: Top-1: 84.21%, Top-5: 97.24%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.26%
[Alpha=0.40] Top-5 Accuracy: 97.21%
Result: Top-1: 84.26%, Top-5: 97.21%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.30%
[Alpha=0.40] Top-5 Accuracy: 97.22%
Result: Top-1: 84.30%, Top-5: 97.22%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.18%
[Alpha=0.40] Top-5 Accuracy: 97.24%
Result: Top-1: 84.18%, Top-5: 97.24%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.19%
[Alpha=0.40] Top-5 Accuracy: 97.21%
Result: Top-1: 84.19%, Top-5: 97.21%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.23%
[Alpha=0.40] Top-5 Accuracy: 97.26%
Result: Top-1: 84.23%, Top-5: 97.26%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.15%
[Alpha=0.40] Top-5 Accuracy: 97.23%
Result: Top-1: 84.15%, Top-5: 97.23%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.51%
[Alpha=0.40] Top-5 Accuracy: 96.94%
Result: Top-1: 82.51%, Top-5: 96.94%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.92%
[Alpha=0.40] Top-5 Accuracy: 97.16%
Result: Top-1: 83.92%, Top-5: 97.16%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.00%
[Alpha=0.40] Top-5 Accuracy: 97.12%
Result: Top-1: 84.00%, Top-5: 97.12%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.13%
[Alpha=0.40] Top-5 Accuracy: 97.22%
Result: Top-1: 84.13%, Top-5: 97.22%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.07%
[Alpha=0.40] Top-5 Accuracy: 97.11%
Result: Top-1: 84.07%, Top-5: 97.11%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.09%
[Alpha=0.40] Top-5 Accuracy: 97.19%
Result: Top-1: 84.09%, Top-5: 97.19%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.10%
[Alpha=0.40] Top-5 Accuracy: 97.20%
Result: Top-1: 84.10%, Top-5: 97.20%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.22%
[Alpha=0.40] Top-5 Accuracy: 97.21%
Result: Top-1: 84.22%, Top-5: 97.21%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.04%
[Alpha=0.40] Top-5 Accuracy: 97.18%
Result: Top-1: 84.04%, Top-5: 97.18%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 84.02%
[Alpha=0.40] Top-5 Accuracy: 97.19%
Result: Top-1: 84.02%, Top-5: 97.19%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 72.72%
[Alpha=0.40] Top-5 Accuracy: 94.15%
Result: Top-1: 72.72%, Top-5: 94.15%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.99%
[Alpha=0.40] Top-5 Accuracy: 96.40%
Result: Top-1: 82.99%, Top-5: 96.40%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.57%
[Alpha=0.40] Top-5 Accuracy: 97.01%
Result: Top-1: 83.57%, Top-5: 97.01%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.67%
[Alpha=0.40] Top-5 Accuracy: 97.03%
Result: Top-1: 83.67%, Top-5: 97.03%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.64%
[Alpha=0.40] Top-5 Accuracy: 97.00%
Result: Top-1: 83.64%, Top-5: 97.00%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.77%
[Alpha=0.40] Top-5 Accuracy: 97.09%
Result: Top-1: 83.77%, Top-5: 97.09%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.64%
[Alpha=0.40] Top-5 Accuracy: 96.89%
Result: Top-1: 83.64%, Top-5: 96.89%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.95%
[Alpha=0.40] Top-5 Accuracy: 97.15%
Result: Top-1: 83.95%, Top-5: 97.15%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.74%
[Alpha=0.40] Top-5 Accuracy: 97.05%
Result: Top-1: 83.74%, Top-5: 97.05%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.78%
[Alpha=0.40] Top-5 Accuracy: 97.13%
Result: Top-1: 83.78%, Top-5: 97.13%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 53.71%
[Alpha=0.40] Top-5 Accuracy: 74.65%
Result: Top-1: 53.71%, Top-5: 74.65%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 35.47%
[Alpha=0.40] Top-5 Accuracy: 75.88%
Result: Top-1: 35.47%, Top-5: 75.88%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 78.94%
[Alpha=0.40] Top-5 Accuracy: 93.14%
Result: Top-1: 78.94%, Top-5: 93.14%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 59.31%
[Alpha=0.40] Top-5 Accuracy: 93.43%
Result: Top-1: 59.31%, Top-5: 93.43%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.84%
[Alpha=0.40] Top-5 Accuracy: 96.14%
Result: Top-1: 82.84%, Top-5: 96.14%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.88%
[Alpha=0.40] Top-5 Accuracy: 96.36%
Result: Top-1: 82.88%, Top-5: 96.36%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.41%
[Alpha=0.40] Top-5 Accuracy: 96.69%
Result: Top-1: 83.41%, Top-5: 96.69%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.26%
[Alpha=0.40] Top-5 Accuracy: 96.76%
Result: Top-1: 83.26%, Top-5: 96.76%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.92%
[Alpha=0.40] Top-5 Accuracy: 96.64%
Result: Top-1: 82.92%, Top-5: 96.64%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 83.29%
[Alpha=0.40] Top-5 Accuracy: 96.88%
Result: Top-1: 83.29%, Top-5: 96.88%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.25%
[Alpha=0.50] Top-5 Accuracy: 97.24%
Result: Top-1: 84.25%, Top-5: 97.24%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.26%
[Alpha=0.50] Top-5 Accuracy: 97.26%
Result: Top-1: 84.26%, Top-5: 97.26%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.32%
[Alpha=0.50] Top-5 Accuracy: 97.25%
Result: Top-1: 84.32%, Top-5: 97.25%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.27%
[Alpha=0.50] Top-5 Accuracy: 97.25%
Result: Top-1: 84.27%, Top-5: 97.25%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.24%
[Alpha=0.50] Top-5 Accuracy: 97.24%
Result: Top-1: 84.24%, Top-5: 97.24%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.24%
[Alpha=0.50] Top-5 Accuracy: 97.26%
Result: Top-1: 84.24%, Top-5: 97.26%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.27%
[Alpha=0.50] Top-5 Accuracy: 97.26%
Result: Top-1: 84.27%, Top-5: 97.26%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.26%
[Alpha=0.50] Top-5 Accuracy: 97.24%
Result: Top-1: 84.26%, Top-5: 97.24%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.27%
[Alpha=0.50] Top-5 Accuracy: 97.24%
Result: Top-1: 84.27%, Top-5: 97.24%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.26%
[Alpha=0.50] Top-5 Accuracy: 97.26%
Result: Top-1: 84.26%, Top-5: 97.26%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 83.98%
[Alpha=0.50] Top-5 Accuracy: 97.18%
Result: Top-1: 83.98%, Top-5: 97.18%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.15%
[Alpha=0.50] Top-5 Accuracy: 97.21%
Result: Top-1: 84.15%, Top-5: 97.21%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.20%
[Alpha=0.50] Top-5 Accuracy: 97.24%
Result: Top-1: 84.20%, Top-5: 97.24%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.26%
[Alpha=0.50] Top-5 Accuracy: 97.26%
Result: Top-1: 84.26%, Top-5: 97.26%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 84.21%
[Alpha=0.50] Top-5 Accuracy: 97.22%
Result: Top-1: 84.21%, Top-5: 97.22%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=125
============================================================
slurmstepd-jnfat06: error: *** JOB 1675189 ON jnfat06 CANCELLED AT 2025-09-15T12:09:03 ***
