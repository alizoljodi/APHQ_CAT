Starting Swin-Base W2A2 QDROP experiment at Thu Sep 11 10:44:59 AM CEST 2025
2025-09-11 10:45:09,624 - INFO - Starting multi-seed experiment
2025-09-11 10:45:09,624 - INFO - Architecture: swin_base
2025-09-11 10:45:09,624 - INFO - Weight bits: 2
2025-09-11 10:45:09,624 - INFO - Activation bits: 2
2025-09-11 10:45:09,624 - INFO - Seeds: [1001, 1002, 1003]
2025-09-11 10:45:09,624 - INFO - Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
2025-09-11 10:45:09,624 - INFO - Cluster numbers: [8, 16, 32, 64, 128, 256]
2025-09-11 10:45:09,625 - INFO - PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]
2025-09-11 10:45:09,625 - INFO - Output directory: ./experiment_results/swin_base_w2_a2_20250911_104509
2025-09-11 10:45:09,625 - INFO - Checking basic requirements...
2025-09-11 10:45:09,625 - INFO - Basic checks passed
2025-09-11 10:45:09,625 - INFO - 
Starting experiments for 3 seeds...
2025-09-11 10:45:09,625 - INFO - Total parameter combinations: 600
2025-09-11 10:45:09,625 - INFO - Total experiments: 1800
2025-09-11 10:45:09,625 - INFO - 
============================================================
2025-09-11 10:45:09,625 - INFO - Running experiment 1/3 for seed 1001
2025-09-11 10:45:09,625 - INFO - ============================================================
2025-09-11 10:45:09,625 - INFO - Running experiment for seed 1001
2025-09-11 10:45:09,626 - INFO - Command: /home/alz07xz/project/PD-Quant/pd_quant/bin/python ../test_quant.py --model swin_base --w_bit 2 --a_bit 2 --seed 1001 --config ../configs/4bit/qdrop_baseline.py --dataset /home/alz07xz/imagenet --calib-size 1000 --calib-batch-size 32 --val-batch-size 500 --num-workers 8 --device cuda --alpha-list 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 --num-clusters-list 8 16 32 64 128 256 --pca-dim-list 1 25 50 75 100 125 150 175 200 220 --calibrate --optimize
2025-09-11 10:45:09,626 - INFO - Working directory: /home/alz07xz/project/APHQ_CAT/qdrop
2025-09-11 10:53:07 - start the process.
Namespace(model='swin_base', config='../configs/4bit/qdrop_baseline.py', dataset='/home/alz07xz/imagenet', val_batch_size=500, num_workers=8, device='cuda', reconstruct_mlp=False, load_reconstruct_checkpoint=False, test_reconstruct_checkpoint=False, calibrate=True, load_calibrate_checkpoint=None, test_calibrate_checkpoint=False, optimize=True, load_optimize_checkpoint='', test_optimize_checkpoint=False, print_freq=10, seed=1001, alpha_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], num_clusters_list=[8, 16, 32, 64, 128, 256], pca_dim_list=[1, 25, 50, 75, 100, 125, 150, 175, 200, 220], w_bit=2, a_bit=2, calib_size=1000, calib_batch_size=32)
deefe /home/alz07xz/project/APHQ_CAT/configs/4bit
Successfully imported Config class!
optim_size: 1024
calib_size: 1000
optim_batch_size: 32
calib_batch_size: 32
w_bit: 2
a_bit: 2
qconv_a_bit: 8
qhead_a_bit: 4
calib_metric: mse
matmul_head_channel_wise: True
token_channel_wise: False
eq_n: 128
search_round: 3
keep_gpu: True
optim_metric: mse
use_mean_hessian: False
temp: 20
recon_metric: hessian_perturb
pct: 0.99
optim_mode: qdrop
drop_prob: 0.5
reconstruct_mlp: False
Building model ...
/home/alz07xz/project/PD-Quant/pd_quant/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
No checkpoint found at './checkpoint/vit_raw/swin_base_patch4_window7_224.bin'
Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)
[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Building validation dataloader ...
Validating on test set on fp ...
Test: [0/100]	Time 16.171 (16.171)	Loss 0.4076 (0.4076)	Prec@1 91.800 (91.800)	Prec@5 98.400 (98.400)
Test: [10/100]	Time 1.052 (2.559)	Loss 0.4707 (0.5107)	Prec@1 91.600 (88.745)	Prec@5 98.800 (98.491)
Test: [20/100]	Time 1.055 (1.843)	Loss 0.5991 (0.5373)	Prec@1 86.000 (88.381)	Prec@5 98.000 (98.171)
Test: [30/100]	Time 1.059 (1.590)	Loss 0.4928 (0.5636)	Prec@1 88.200 (87.555)	Prec@5 99.400 (98.129)
Test: [40/100]	Time 4.320 (1.543)	Loss 0.7451 (0.5610)	Prec@1 82.400 (87.663)	Prec@5 97.000 (98.185)
Test: [50/100]	Time 1.064 (1.506)	Loss 0.9181 (0.6040)	Prec@1 77.800 (86.451)	Prec@5 94.800 (97.808)
Test: [60/100]	Time 1.068 (1.449)	Loss 0.5948 (0.6094)	Prec@1 87.200 (86.338)	Prec@5 96.600 (97.764)
Test: [70/100]	Time 1.071 (1.421)	Loss 0.6936 (0.6248)	Prec@1 84.200 (85.859)	Prec@5 97.800 (97.668)
Test: [80/100]	Time 1.061 (1.377)	Loss 0.4770 (0.6272)	Prec@1 88.400 (85.780)	Prec@5 99.200 (97.602)
Test: [90/100]	Time 1.069 (1.343)	Loss 0.9203 (0.6428)	Prec@1 77.000 (85.305)	Prec@5 95.400 (97.525)
 * Prec@1 85.274 Prec@5 97.568 Loss 0.641 Time 134.370
Wraping quantiztion modules (reparam: False, recon: False) ...
2025-09-11 10:56:04 - start mse guided calibration
  0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   1%|          | 1/149 [00:14<35:55, 14.57s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|          | 1/149 [00:14<35:55, 14.57s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|▏         | 2/149 [01:33<2:07:53, 52.20s/it]calibrating layers.0.blocks.0.attn.proj:   1%|▏         | 2/149 [01:33<2:07:53, 52.20s/it]calibrating layers.0.blocks.0.attn.proj:   2%|▏         | 3/149 [02:12<1:53:09, 46.50s/it]calibrating layers.0.blocks.0.attn.matmul1:   2%|▏         | 3/149 [02:12<1:53:09, 46.50s/it]calibrating layers.0.blocks.0.attn.matmul1:   3%|▎         | 4/149 [07:29<6:10:15, 153.21s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 4/149 [07:29<6:10:15, 153.21s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 5/149 [12:48<8:31:24, 213.09s/it]calibrating layers.0.blocks.0.mlp.fc1:   3%|▎         | 5/149 [12:48<8:31:24, 213.09s/it]     calibrating layers.0.blocks.0.mlp.fc1:   4%|▍         | 6/149 [15:47<8:00:13, 201.50s/it]calibrating layers.0.blocks.0.mlp.fc2:   4%|▍         | 6/149 [15:47<8:00:13, 201.50s/it]calibrating layers.0.blocks.0.mlp.fc2:   5%|▍         | 7/149 [18:54<7:45:39, 196.76s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▍         | 7/149 [18:54<7:45:39, 196.76s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▌         | 8/149 [20:13<6:13:52, 159.10s/it]calibrating layers.0.blocks.1.attn.proj:   5%|▌         | 8/149 [20:13<6:13:52, 159.10s/it]calibrating layers.0.blocks.1.attn.proj:   6%|▌         | 9/149 [20:53<4:44:14, 121.82s/it]calibrating layers.0.blocks.1.attn.matmul1:   6%|▌         | 9/149 [20:53<4:44:14, 121.82s/it]calibrating layers.0.blocks.1.attn.matmul1:   7%|▋         | 10/149 [26:13<7:03:51, 182.96s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 10/149 [26:13<7:03:51, 182.96s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 11/149 [31:34<8:38:02, 225.24s/it]calibrating layers.0.blocks.1.mlp.fc1:   7%|▋         | 11/149 [31:34<8:38:02, 225.24s/it]     calibrating layers.0.blocks.1.mlp.fc1:   8%|▊         | 12/149 [34:33<8:02:36, 211.36s/it]calibrating layers.0.blocks.1.mlp.fc2:   8%|▊         | 12/149 [34:33<8:02:36, 211.36s/it]calibrating layers.0.blocks.1.mlp.fc2:   9%|▊         | 13/149 [37:41<7:42:37, 204.10s/it]calibrating layers.1.downsample.reduction:   9%|▊         | 13/149 [37:41<7:42:37, 204.10s/it]calibrating layers.1.downsample.reduction:   9%|▉         | 14/149 [38:22<5:48:49, 155.03s/it]calibrating layers.1.blocks.0.attn.qkv:   9%|▉         | 14/149 [38:22<5:48:49, 155.03s/it]   calibrating layers.1.blocks.0.attn.qkv:  10%|█         | 15/149 [39:08<4:32:23, 121.97s/it]calibrating layers.1.blocks.0.attn.proj:  10%|█         | 15/149 [39:08<4:32:23, 121.97s/it]calibrating layers.1.blocks.0.attn.proj:  11%|█         | 16/149 [39:31<3:24:15, 92.15s/it] calibrating layers.1.blocks.0.attn.matmul1:  11%|█         | 16/149 [39:31<3:24:15, 92.15s/it]calibrating layers.1.blocks.0.attn.matmul1:  11%|█▏        | 17/149 [40:08<2:46:15, 75.57s/it]calibrating layers.1.blocks.0.attn.matmul2:  11%|█▏        | 17/149 [40:08<2:46:15, 75.57s/it]calibrating layers.1.blocks.0.attn.matmul2:  12%|█▏        | 18/149 [41:00<2:29:45, 68.59s/it]calibrating layers.1.blocks.0.mlp.fc1:  12%|█▏        | 18/149 [41:00<2:29:45, 68.59s/it]     calibrating layers.1.blocks.0.mlp.fc1:  13%|█▎        | 19/149 [42:29<2:42:00, 74.77s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 19/149 [42:29<2:42:00, 74.77s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 20/149 [44:02<2:52:13, 80.11s/it]calibrating layers.1.blocks.1.attn.qkv:  13%|█▎        | 20/149 [44:02<2:52:13, 80.11s/it]calibrating layers.1.blocks.1.attn.qkv:  14%|█▍        | 21/149 [44:49<2:29:54, 70.27s/it]calibrating layers.1.blocks.1.attn.proj:  14%|█▍        | 21/149 [44:49<2:29:54, 70.27s/it]calibrating layers.1.blocks.1.attn.proj:  15%|█▍        | 22/149 [45:13<1:59:21, 56.39s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▍        | 22/149 [45:13<1:59:21, 56.39s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▌        | 23/149 [45:51<1:46:44, 50.83s/it]calibrating layers.1.blocks.1.attn.matmul2:  15%|█▌        | 23/149 [45:51<1:46:44, 50.83s/it]calibrating layers.1.blocks.1.attn.matmul2:  16%|█▌        | 24/149 [46:43<1:46:51, 51.29s/it]calibrating layers.1.blocks.1.mlp.fc1:  16%|█▌        | 24/149 [46:43<1:46:51, 51.29s/it]     calibrating layers.1.blocks.1.mlp.fc1:  17%|█▋        | 25/149 [48:12<2:09:20, 62.59s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 25/149 [48:12<2:09:20, 62.59s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 26/149 [49:44<2:26:29, 71.46s/it]calibrating layers.2.downsample.reduction:  17%|█▋        | 26/149 [49:44<2:26:29, 71.46s/it]calibrating layers.2.downsample.reduction:  18%|█▊        | 27/149 [50:10<1:57:17, 57.68s/it]calibrating layers.2.blocks.0.attn.qkv:  18%|█▊        | 27/149 [50:10<1:57:17, 57.68s/it]   calibrating layers.2.blocks.0.attn.qkv:  19%|█▉        | 28/149 [50:43<1:41:12, 50.18s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 28/149 [50:43<1:41:12, 50.18s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 29/149 [50:57<1:19:05, 39.54s/it]calibrating layers.2.blocks.0.attn.matmul1:  19%|█▉        | 29/149 [50:57<1:19:05, 39.54s/it]calibrating layers.2.blocks.0.attn.matmul1:  20%|██        | 30/149 [51:15<1:05:11, 32.87s/it]calibrating layers.2.blocks.0.attn.matmul2:  20%|██        | 30/149 [51:15<1:05:11, 32.87s/it]calibrating layers.2.blocks.0.attn.matmul2:  21%|██        | 31/149 [51:32<55:37, 28.28s/it]  calibrating layers.2.blocks.0.mlp.fc1:  21%|██        | 31/149 [51:32<55:37, 28.28s/it]     calibrating layers.2.blocks.0.mlp.fc1:  21%|██▏       | 32/149 [52:20<1:06:43, 34.22s/it]calibrating layers.2.blocks.0.mlp.fc2:  21%|██▏       | 32/149 [52:20<1:06:43, 34.22s/it]calibrating layers.2.blocks.0.mlp.fc2:  22%|██▏       | 33/149 [53:09<1:14:53, 38.74s/it]calibrating layers.2.blocks.1.attn.qkv:  22%|██▏       | 33/149 [53:09<1:14:53, 38.74s/it]calibrating layers.2.blocks.1.attn.qkv:  23%|██▎       | 34/149 [53:42<1:10:25, 36.74s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 34/149 [53:42<1:10:25, 36.74s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 35/149 [53:56<57:12, 30.11s/it]  calibrating layers.2.blocks.1.attn.matmul1:  23%|██▎       | 35/149 [53:56<57:12, 30.11s/it]calibrating layers.2.blocks.1.attn.matmul1:  24%|██▍       | 36/149 [54:14<49:41, 26.38s/it]calibrating layers.2.blocks.1.attn.matmul2:  24%|██▍       | 36/149 [54:14<49:41, 26.38s/it]calibrating layers.2.blocks.1.attn.matmul2:  25%|██▍       | 37/149 [54:32<44:30, 23.85s/it]calibrating layers.2.blocks.1.mlp.fc1:  25%|██▍       | 37/149 [54:32<44:30, 23.85s/it]     calibrating layers.2.blocks.1.mlp.fc1:  26%|██▌       | 38/149 [55:20<57:43, 31.20s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 38/149 [55:20<57:43, 31.20s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 39/149 [56:10<1:07:10, 36.64s/it]calibrating layers.2.blocks.2.attn.qkv:  26%|██▌       | 39/149 [56:10<1:07:10, 36.64s/it]calibrating layers.2.blocks.2.attn.qkv:  27%|██▋       | 40/149 [56:42<1:04:12, 35.35s/it]calibrating layers.2.blocks.2.attn.proj:  27%|██▋       | 40/149 [56:42<1:04:12, 35.35s/it]calibrating layers.2.blocks.2.attn.proj:  28%|██▊       | 41/149 [56:57<52:31, 29.18s/it]  calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 41/149 [56:57<52:31, 29.18s/it]calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 42/149 [57:14<45:51, 25.72s/it]calibrating layers.2.blocks.2.attn.matmul2:  28%|██▊       | 42/149 [57:14<45:51, 25.72s/it]calibrating layers.2.blocks.2.attn.matmul2:  29%|██▉       | 43/149 [57:32<41:16, 23.36s/it]calibrating layers.2.blocks.2.mlp.fc1:  29%|██▉       | 43/149 [57:32<41:16, 23.36s/it]     calibrating layers.2.blocks.2.mlp.fc1:  30%|██▉       | 44/149 [58:21<54:01, 30.87s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|██▉       | 44/149 [58:21<54:01, 30.87s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|███       | 45/149 [59:10<1:03:17, 36.51s/it]calibrating layers.2.blocks.3.attn.qkv:  30%|███       | 45/149 [59:10<1:03:17, 36.51s/it]calibrating layers.2.blocks.3.attn.qkv:  31%|███       | 46/149 [59:43<1:00:33, 35.28s/it]calibrating layers.2.blocks.3.attn.proj:  31%|███       | 46/149 [59:43<1:00:33, 35.28s/it]calibrating layers.2.blocks.3.attn.proj:  32%|███▏      | 47/149 [59:57<49:30, 29.12s/it]  calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 47/149 [59:57<49:30, 29.12s/it]calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 48/149 [1:00:15<43:14, 25.69s/it]calibrating layers.2.blocks.3.attn.matmul2:  32%|███▏      | 48/149 [1:00:15<43:14, 25.69s/it]calibrating layers.2.blocks.3.attn.matmul2:  33%|███▎      | 49/149 [1:00:33<38:56, 23.37s/it]calibrating layers.2.blocks.3.mlp.fc1:  33%|███▎      | 49/149 [1:00:33<38:56, 23.37s/it]     calibrating layers.2.blocks.3.mlp.fc1:  34%|███▎      | 50/149 [1:01:22<51:02, 30.93s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▎      | 50/149 [1:01:22<51:02, 30.93s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▍      | 51/149 [1:02:11<59:44, 36.58s/it]calibrating layers.2.blocks.4.attn.qkv:  34%|███▍      | 51/149 [1:02:11<59:44, 36.58s/it]calibrating layers.2.blocks.4.attn.qkv:  35%|███▍      | 52/149 [1:02:44<57:06, 35.33s/it]calibrating layers.2.blocks.4.attn.proj:  35%|███▍      | 52/149 [1:02:44<57:06, 35.33s/it]calibrating layers.2.blocks.4.attn.proj:  36%|███▌      | 53/149 [1:02:58<46:37, 29.14s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 53/149 [1:02:58<46:37, 29.14s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 54/149 [1:03:16<40:42, 25.71s/it]calibrating layers.2.blocks.4.attn.matmul2:  36%|███▌      | 54/149 [1:03:16<40:42, 25.71s/it]calibrating layers.2.blocks.4.attn.matmul2:  37%|███▋      | 55/149 [1:03:34<36:34, 23.35s/it]calibrating layers.2.blocks.4.mlp.fc1:  37%|███▋      | 55/149 [1:03:34<36:34, 23.35s/it]     calibrating layers.2.blocks.4.mlp.fc1:  38%|███▊      | 56/149 [1:04:22<47:50, 30.87s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 56/149 [1:04:22<47:50, 30.87s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 57/149 [1:05:12<55:51, 36.43s/it]calibrating layers.2.blocks.5.attn.qkv:  38%|███▊      | 57/149 [1:05:12<55:51, 36.43s/it]calibrating layers.2.blocks.5.attn.qkv:  39%|███▉      | 58/149 [1:05:44<53:23, 35.20s/it]calibrating layers.2.blocks.5.attn.proj:  39%|███▉      | 58/149 [1:05:44<53:23, 35.20s/it]calibrating layers.2.blocks.5.attn.proj:  40%|███▉      | 59/149 [1:05:59<43:38, 29.10s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|███▉      | 59/149 [1:05:59<43:38, 29.10s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|████      | 60/149 [1:06:17<38:07, 25.71s/it]calibrating layers.2.blocks.5.attn.matmul2:  40%|████      | 60/149 [1:06:17<38:07, 25.71s/it]calibrating layers.2.blocks.5.attn.matmul2:  41%|████      | 61/149 [1:06:35<34:25, 23.47s/it]calibrating layers.2.blocks.5.mlp.fc1:  41%|████      | 61/149 [1:06:35<34:25, 23.47s/it]     calibrating layers.2.blocks.5.mlp.fc1:  42%|████▏     | 62/149 [1:07:23<44:53, 30.96s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 62/149 [1:07:23<44:53, 30.96s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 63/149 [1:08:13<52:23, 36.55s/it]calibrating layers.2.blocks.6.attn.qkv:  42%|████▏     | 63/149 [1:08:13<52:23, 36.55s/it]calibrating layers.2.blocks.6.attn.qkv:  43%|████▎     | 64/149 [1:08:46<50:02, 35.32s/it]calibrating layers.2.blocks.6.attn.proj:  43%|████▎     | 64/149 [1:08:46<50:02, 35.32s/it]calibrating layers.2.blocks.6.attn.proj:  44%|████▎     | 65/149 [1:09:00<40:51, 29.18s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▎     | 65/149 [1:09:00<40:51, 29.18s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▍     | 66/149 [1:09:18<35:39, 25.78s/it]calibrating layers.2.blocks.6.attn.matmul2:  44%|████▍     | 66/149 [1:09:18<35:39, 25.78s/it]calibrating layers.2.blocks.6.attn.matmul2:  45%|████▍     | 67/149 [1:09:36<31:51, 23.31s/it]calibrating layers.2.blocks.6.mlp.fc1:  45%|████▍     | 67/149 [1:09:36<31:51, 23.31s/it]     calibrating layers.2.blocks.6.mlp.fc1:  46%|████▌     | 68/149 [1:10:24<41:41, 30.89s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▌     | 68/149 [1:10:24<41:41, 30.89s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▋     | 69/149 [1:11:14<48:44, 36.55s/it]calibrating layers.2.blocks.7.attn.qkv:  46%|████▋     | 69/149 [1:11:14<48:44, 36.55s/it]calibrating layers.2.blocks.7.attn.qkv:  47%|████▋     | 70/149 [1:11:47<46:31, 35.34s/it]calibrating layers.2.blocks.7.attn.proj:  47%|████▋     | 70/149 [1:11:47<46:31, 35.34s/it]calibrating layers.2.blocks.7.attn.proj:  48%|████▊     | 71/149 [1:12:01<37:55, 29.18s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 71/149 [1:12:01<37:55, 29.18s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 72/149 [1:12:19<33:00, 25.72s/it]calibrating layers.2.blocks.7.attn.matmul2:  48%|████▊     | 72/149 [1:12:19<33:00, 25.72s/it]calibrating layers.2.blocks.7.attn.matmul2:  49%|████▉     | 73/149 [1:12:37<29:40, 23.42s/it]calibrating layers.2.blocks.7.mlp.fc1:  49%|████▉     | 73/149 [1:12:37<29:40, 23.42s/it]     calibrating layers.2.blocks.7.mlp.fc1:  50%|████▉     | 74/149 [1:13:26<38:38, 30.91s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|████▉     | 74/149 [1:13:26<38:38, 30.91s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|█████     | 75/149 [1:14:15<45:01, 36.51s/it]calibrating layers.2.blocks.8.attn.qkv:  50%|█████     | 75/149 [1:14:15<45:01, 36.51s/it]calibrating layers.2.blocks.8.attn.qkv:  51%|█████     | 76/149 [1:14:47<42:46, 35.15s/it]calibrating layers.2.blocks.8.attn.proj:  51%|█████     | 76/149 [1:14:47<42:46, 35.15s/it]calibrating layers.2.blocks.8.attn.proj:  52%|█████▏    | 77/149 [1:15:02<34:48, 29.01s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 77/149 [1:15:02<34:48, 29.01s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 78/149 [1:15:19<30:13, 25.54s/it]calibrating layers.2.blocks.8.attn.matmul2:  52%|█████▏    | 78/149 [1:15:19<30:13, 25.54s/it]calibrating layers.2.blocks.8.attn.matmul2:  53%|█████▎    | 79/149 [1:15:37<27:04, 23.20s/it]calibrating layers.2.blocks.8.mlp.fc1:  53%|█████▎    | 79/149 [1:15:37<27:04, 23.20s/it]     calibrating layers.2.blocks.8.mlp.fc1:  54%|█████▎    | 80/149 [1:16:26<35:25, 30.81s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▎    | 80/149 [1:16:26<35:25, 30.81s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▍    | 81/149 [1:17:15<41:25, 36.55s/it]calibrating layers.2.blocks.9.attn.qkv:  54%|█████▍    | 81/149 [1:17:15<41:25, 36.55s/it]calibrating layers.2.blocks.9.attn.qkv:  55%|█████▌    | 82/149 [1:17:47<39:16, 35.16s/it]calibrating layers.2.blocks.9.attn.proj:  55%|█████▌    | 82/149 [1:17:47<39:16, 35.16s/it]calibrating layers.2.blocks.9.attn.proj:  56%|█████▌    | 83/149 [1:18:02<31:52, 28.98s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▌    | 83/149 [1:18:02<31:52, 28.98s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▋    | 84/149 [1:18:19<27:37, 25.49s/it]calibrating layers.2.blocks.9.attn.matmul2:  56%|█████▋    | 84/149 [1:18:19<27:37, 25.49s/it]calibrating layers.2.blocks.9.attn.matmul2:  57%|█████▋    | 85/149 [1:18:37<24:45, 23.22s/it]calibrating layers.2.blocks.9.mlp.fc1:  57%|█████▋    | 85/149 [1:18:37<24:45, 23.22s/it]     calibrating layers.2.blocks.9.mlp.fc1:  58%|█████▊    | 86/149 [1:19:26<32:17, 30.76s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 86/149 [1:19:26<32:17, 30.76s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 87/149 [1:20:15<37:41, 36.48s/it]calibrating layers.2.blocks.10.attn.qkv:  58%|█████▊    | 87/149 [1:20:15<37:41, 36.48s/it]calibrating layers.2.blocks.10.attn.qkv:  59%|█████▉    | 88/149 [1:20:48<35:52, 35.28s/it]calibrating layers.2.blocks.10.attn.proj:  59%|█████▉    | 88/149 [1:20:48<35:52, 35.28s/it]calibrating layers.2.blocks.10.attn.proj:  60%|█████▉    | 89/149 [1:21:03<29:08, 29.13s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|█████▉    | 89/149 [1:21:03<29:08, 29.13s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|██████    | 90/149 [1:21:20<25:17, 25.71s/it]calibrating layers.2.blocks.10.attn.matmul2:  60%|██████    | 90/149 [1:21:20<25:17, 25.71s/it]calibrating layers.2.blocks.10.attn.matmul2:  61%|██████    | 91/149 [1:21:38<22:33, 23.33s/it]calibrating layers.2.blocks.10.mlp.fc1:  61%|██████    | 91/149 [1:21:38<22:33, 23.33s/it]     calibrating layers.2.blocks.10.mlp.fc1:  62%|██████▏   | 92/149 [1:22:27<29:19, 30.87s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 92/149 [1:22:27<29:19, 30.87s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 93/149 [1:23:16<34:03, 36.49s/it]calibrating layers.2.blocks.11.attn.qkv:  62%|██████▏   | 93/149 [1:23:16<34:03, 36.49s/it]calibrating layers.2.blocks.11.attn.qkv:  63%|██████▎   | 94/149 [1:23:48<32:13, 35.15s/it]calibrating layers.2.blocks.11.attn.proj:  63%|██████▎   | 94/149 [1:23:48<32:13, 35.15s/it]calibrating layers.2.blocks.11.attn.proj:  64%|██████▍   | 95/149 [1:24:03<26:05, 28.99s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 95/149 [1:24:03<26:05, 28.99s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 96/149 [1:24:20<22:34, 25.56s/it]calibrating layers.2.blocks.11.attn.matmul2:  64%|██████▍   | 96/149 [1:24:20<22:34, 25.56s/it]calibrating layers.2.blocks.11.attn.matmul2:  65%|██████▌   | 97/149 [1:24:38<20:06, 23.20s/it]calibrating layers.2.blocks.11.mlp.fc1:  65%|██████▌   | 97/149 [1:24:38<20:06, 23.20s/it]     calibrating layers.2.blocks.11.mlp.fc1:  66%|██████▌   | 98/149 [1:25:27<26:09, 30.77s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▌   | 98/149 [1:25:27<26:09, 30.77s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▋   | 99/149 [1:26:16<30:21, 36.43s/it]calibrating layers.2.blocks.12.attn.qkv:  66%|██████▋   | 99/149 [1:26:16<30:21, 36.43s/it]calibrating layers.2.blocks.12.attn.qkv:  67%|██████▋   | 100/149 [1:26:48<28:41, 35.13s/it]calibrating layers.2.blocks.12.attn.proj:  67%|██████▋   | 100/149 [1:26:48<28:41, 35.13s/it]calibrating layers.2.blocks.12.attn.proj:  68%|██████▊   | 101/149 [1:27:03<23:12, 29.01s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 101/149 [1:27:03<23:12, 29.01s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 102/149 [1:27:21<20:02, 25.59s/it]calibrating layers.2.blocks.12.attn.matmul2:  68%|██████▊   | 102/149 [1:27:21<20:02, 25.59s/it]calibrating layers.2.blocks.12.attn.matmul2:  69%|██████▉   | 103/149 [1:27:38<17:45, 23.16s/it]calibrating layers.2.blocks.12.mlp.fc1:  69%|██████▉   | 103/149 [1:27:38<17:45, 23.16s/it]     calibrating layers.2.blocks.12.mlp.fc1:  70%|██████▉   | 104/149 [1:28:26<23:00, 30.67s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|██████▉   | 104/149 [1:28:26<23:00, 30.67s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|███████   | 105/149 [1:29:16<26:40, 36.37s/it]calibrating layers.2.blocks.13.attn.qkv:  70%|███████   | 105/149 [1:29:16<26:40, 36.37s/it]calibrating layers.2.blocks.13.attn.qkv:  71%|███████   | 106/149 [1:29:48<25:11, 35.16s/it]calibrating layers.2.blocks.13.attn.proj:  71%|███████   | 106/149 [1:29:48<25:11, 35.16s/it]calibrating layers.2.blocks.13.attn.proj:  72%|███████▏  | 107/149 [1:30:03<20:19, 29.03s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 107/149 [1:30:03<20:19, 29.03s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 108/149 [1:30:20<17:25, 25.51s/it]calibrating layers.2.blocks.13.attn.matmul2:  72%|███████▏  | 108/149 [1:30:20<17:25, 25.51s/it]calibrating layers.2.blocks.13.attn.matmul2:  73%|███████▎  | 109/149 [1:30:38<15:27, 23.19s/it]calibrating layers.2.blocks.13.mlp.fc1:  73%|███████▎  | 109/149 [1:30:38<15:27, 23.19s/it]     calibrating layers.2.blocks.13.mlp.fc1:  74%|███████▍  | 110/149 [1:31:27<20:00, 30.77s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 110/149 [1:31:27<20:00, 30.77s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 111/149 [1:32:16<23:06, 36.49s/it]calibrating layers.2.blocks.14.attn.qkv:  74%|███████▍  | 111/149 [1:32:16<23:06, 36.49s/it]calibrating layers.2.blocks.14.attn.qkv:  75%|███████▌  | 112/149 [1:32:49<21:44, 35.25s/it]calibrating layers.2.blocks.14.attn.proj:  75%|███████▌  | 112/149 [1:32:49<21:44, 35.25s/it]calibrating layers.2.blocks.14.attn.proj:  76%|███████▌  | 113/149 [1:33:03<17:27, 29.09s/it]calibrating layers.2.blocks.14.attn.matmul1:  76%|███████▌  | 113/149 [1:33:03<17:27, 29.09s/it]calibrating layers.2.blocks.14.attn.matmul1:  77%|███████▋  | 114/149 [1:33:21<14:59, 25.69s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 114/149 [1:33:21<14:59, 25.69s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 115/149 [1:33:39<13:11, 23.29s/it]calibrating layers.2.blocks.14.mlp.fc1:  77%|███████▋  | 115/149 [1:33:39<13:11, 23.29s/it]     calibrating layers.2.blocks.14.mlp.fc1:  78%|███████▊  | 116/149 [1:34:27<16:58, 30.85s/it]calibrating layers.2.blocks.14.mlp.fc2:  78%|███████▊  | 116/149 [1:34:27<16:58, 30.85s/it]calibrating layers.2.blocks.14.mlp.fc2:  79%|███████▊  | 117/149 [1:35:17<19:28, 36.51s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▊  | 117/149 [1:35:17<19:28, 36.51s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▉  | 118/149 [1:35:49<18:07, 35.08s/it]calibrating layers.2.blocks.15.attn.proj:  79%|███████▉  | 118/149 [1:35:49<18:07, 35.08s/it]calibrating layers.2.blocks.15.attn.proj:  80%|███████▉  | 119/149 [1:36:03<14:25, 28.86s/it]calibrating layers.2.blocks.15.attn.matmul1:  80%|███████▉  | 119/149 [1:36:03<14:25, 28.86s/it]calibrating layers.2.blocks.15.attn.matmul1:  81%|████████  | 120/149 [1:36:20<12:15, 25.37s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 120/149 [1:36:20<12:15, 25.37s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 121/149 [1:36:38<10:45, 23.04s/it]calibrating layers.2.blocks.15.mlp.fc1:  81%|████████  | 121/149 [1:36:38<10:45, 23.04s/it]     calibrating layers.2.blocks.15.mlp.fc1:  82%|████████▏ | 122/149 [1:37:26<13:42, 30.48s/it]calibrating layers.2.blocks.15.mlp.fc2:  82%|████████▏ | 122/149 [1:37:26<13:42, 30.48s/it]calibrating layers.2.blocks.15.mlp.fc2:  83%|████████▎ | 123/149 [1:38:15<15:39, 36.14s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 123/149 [1:38:15<15:39, 36.14s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 124/149 [1:38:47<14:31, 34.88s/it]calibrating layers.2.blocks.16.attn.proj:  83%|████████▎ | 124/149 [1:38:47<14:31, 34.88s/it]calibrating layers.2.blocks.16.attn.proj:  84%|████████▍ | 125/149 [1:39:02<11:31, 28.81s/it]calibrating layers.2.blocks.16.attn.matmul1:  84%|████████▍ | 125/149 [1:39:02<11:31, 28.81s/it]calibrating layers.2.blocks.16.attn.matmul1:  85%|████████▍ | 126/149 [1:39:19<09:43, 25.37s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▍ | 126/149 [1:39:19<09:43, 25.37s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▌ | 127/149 [1:39:37<08:27, 23.06s/it]calibrating layers.2.blocks.16.mlp.fc1:  85%|████████▌ | 127/149 [1:39:37<08:27, 23.06s/it]     calibrating layers.2.blocks.16.mlp.fc1:  86%|████████▌ | 128/149 [1:40:25<10:42, 30.62s/it]calibrating layers.2.blocks.16.mlp.fc2:  86%|████████▌ | 128/149 [1:40:25<10:42, 30.62s/it]calibrating layers.2.blocks.16.mlp.fc2:  87%|████████▋ | 129/149 [1:41:15<12:07, 36.39s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 129/149 [1:41:15<12:07, 36.39s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 130/149 [1:41:47<11:09, 35.22s/it]calibrating layers.2.blocks.17.attn.proj:  87%|████████▋ | 130/149 [1:41:47<11:09, 35.22s/it]calibrating layers.2.blocks.17.attn.proj:  88%|████████▊ | 131/149 [1:42:02<08:42, 29.05s/it]calibrating layers.2.blocks.17.attn.matmul1:  88%|████████▊ | 131/149 [1:42:02<08:42, 29.05s/it]calibrating layers.2.blocks.17.attn.matmul1:  89%|████████▊ | 132/149 [1:42:20<07:16, 25.67s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▊ | 132/149 [1:42:20<07:16, 25.67s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▉ | 133/149 [1:42:38<06:14, 23.42s/it]calibrating layers.2.blocks.17.mlp.fc1:  89%|████████▉ | 133/149 [1:42:38<06:14, 23.42s/it]     calibrating layers.2.blocks.17.mlp.fc1:  90%|████████▉ | 134/149 [1:43:26<07:43, 30.92s/it]calibrating layers.2.blocks.17.mlp.fc2:  90%|████████▉ | 134/149 [1:43:26<07:43, 30.92s/it]calibrating layers.2.blocks.17.mlp.fc2:  91%|█████████ | 135/149 [1:44:16<08:31, 36.56s/it]calibrating layers.3.downsample.reduction:  91%|█████████ | 135/149 [1:44:16<08:31, 36.56s/it]calibrating layers.3.downsample.reduction:  91%|█████████▏| 136/149 [1:44:33<06:39, 30.75s/it]calibrating layers.3.blocks.0.attn.qkv:  91%|█████████▏| 136/149 [1:44:33<06:39, 30.75s/it]   calibrating layers.3.blocks.0.attn.qkv:  92%|█████████▏| 137/149 [1:44:59<05:49, 29.12s/it]calibrating layers.3.blocks.0.attn.proj:  92%|█████████▏| 137/149 [1:44:59<05:49, 29.12s/it]calibrating layers.3.blocks.0.attn.proj:  93%|█████████▎| 138/149 [1:45:10<04:21, 23.77s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 138/149 [1:45:10<04:21, 23.77s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 139/149 [1:45:27<03:36, 21.65s/it]calibrating layers.3.blocks.0.attn.matmul2:  93%|█████████▎| 139/149 [1:45:27<03:36, 21.65s/it]calibrating layers.3.blocks.0.attn.matmul2:  94%|█████████▍| 140/149 [1:45:42<02:58, 19.86s/it]calibrating layers.3.blocks.0.mlp.fc1:  94%|█████████▍| 140/149 [1:45:42<02:58, 19.86s/it]     calibrating layers.3.blocks.0.mlp.fc1:  95%|█████████▍| 141/149 [1:46:14<03:07, 23.40s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▍| 141/149 [1:46:14<03:07, 23.40s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▌| 142/149 [1:46:46<03:02, 26.09s/it]calibrating layers.3.blocks.1.attn.qkv:  95%|█████████▌| 142/149 [1:46:46<03:02, 26.09s/it]calibrating layers.3.blocks.1.attn.qkv:  96%|█████████▌| 143/149 [1:47:12<02:35, 25.84s/it]calibrating layers.3.blocks.1.attn.proj:  96%|█████████▌| 143/149 [1:47:12<02:35, 25.84s/it]calibrating layers.3.blocks.1.attn.proj:  97%|█████████▋| 144/149 [1:47:23<01:47, 21.45s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 144/149 [1:47:23<01:47, 21.45s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 145/149 [1:47:40<01:20, 20.06s/it]calibrating layers.3.blocks.1.attn.matmul2:  97%|█████████▋| 145/149 [1:47:40<01:20, 20.06s/it]calibrating layers.3.blocks.1.attn.matmul2:  98%|█████████▊| 146/149 [1:47:56<00:56, 18.84s/it]calibrating layers.3.blocks.1.mlp.fc1:  98%|█████████▊| 146/149 [1:47:56<00:56, 18.84s/it]     calibrating layers.3.blocks.1.mlp.fc1:  99%|█████████▊| 147/149 [1:48:28<00:45, 22.75s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▊| 147/149 [1:48:28<00:45, 22.75s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▉| 148/149 [1:49:00<00:25, 25.69s/it]calibrating head.fc:  99%|█████████▉| 148/149 [1:49:00<00:25, 25.69s/it]                  calibrating head.fc: 100%|██████████| 149/149 [1:49:04<00:00, 19.20s/it]calibrating head.fc: 100%|██████████| 149/149 [1:49:04<00:00, 43.92s/it]
2025-09-11 12:45:42 - mse guided calibration finished.
Saving checkpoint to ./checkpoint/quant_result/20250911_1053/swin_base_w2_a2_calibsize_1000_mse.pth
Validating after calibration ...
Test: [0/100]	Time 5.512 (5.512)	Loss 7.1781 (7.1781)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.000)
Test: [10/100]	Time 2.359 (2.649)	Loss 6.8563 (7.2043)	Prec@1 0.200 (0.018)	Prec@5 1.000 (0.091)
Test: [20/100]	Time 2.361 (2.512)	Loss 7.2404 (7.1923)	Prec@1 0.000 (0.010)	Prec@5 0.000 (0.048)
Test: [30/100]	Time 2.360 (2.463)	Loss 7.6057 (7.1736)	Prec@1 0.000 (0.006)	Prec@5 0.000 (0.039)
Test: [40/100]	Time 2.359 (2.437)	Loss 6.9512 (7.1611)	Prec@1 0.000 (0.005)	Prec@5 0.000 (0.044)
Test: [50/100]	Time 2.358 (2.422)	Loss 6.5840 (7.0806)	Prec@1 0.200 (0.090)	Prec@5 0.200 (0.439)
Test: [60/100]	Time 2.355 (2.412)	Loss 6.9620 (7.0532)	Prec@1 0.000 (0.075)	Prec@5 0.000 (0.390)
Test: [70/100]	Time 2.356 (2.404)	Loss 6.7031 (7.0170)	Prec@1 0.000 (0.093)	Prec@5 0.400 (0.575)
Test: [80/100]	Time 2.358 (2.398)	Loss 6.9666 (7.0000)	Prec@1 0.000 (0.151)	Prec@5 0.000 (0.652)
Test: [90/100]	Time 2.360 (2.394)	Loss 6.9679 (6.9812)	Prec@1 0.000 (0.143)	Prec@5 0.400 (0.677)
 * Prec@1 0.132 Prec@5 0.638 Loss 6.991 Time 239.270
Building calibrator ...
2025-09-11 12:49:46 - start mse guided block reconstruction
reconstructing patch_embed ...
initializing raw input and raw output ...
adaround training for patch_embed ...
wraping quantizers in patch_embed ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.050 (rec:0.050, round:0.000)	b=0.00	count=500
Total loss:	0.032 (rec:0.032, round:0.000)	b=0.00	count=1000
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=1500
Total loss:	0.018 (rec:0.018, round:0.000)	b=0.00	count=2000
Total loss:	0.013 (rec:0.013, round:0.000)	b=0.00	count=2500
Total loss:	0.011 (rec:0.011, round:0.000)	b=0.00	count=3000
Total loss:	0.011 (rec:0.011, round:0.000)	b=0.00	count=3500
Total loss:	57.608 (rec:0.006, round:57.602)	b=20.00	count=4000
Total loss:	38.038 (rec:0.014, round:38.024)	b=19.44	count=4500
Total loss:	35.351 (rec:0.010, round:35.341)	b=18.88	count=5000
Total loss:	33.879 (rec:0.009, round:33.871)	b=18.31	count=5500
Total loss:	32.530 (rec:0.012, round:32.518)	b=17.75	count=6000
Total loss:	31.268 (rec:0.009, round:31.259)	b=17.19	count=6500
Total loss:	29.979 (rec:0.010, round:29.969)	b=16.62	count=7000
Total loss:	28.547 (rec:0.011, round:28.536)	b=16.06	count=7500
Total loss:	27.016 (rec:0.009, round:27.007)	b=15.50	count=8000
Total loss:	25.404 (rec:0.011, round:25.393)	b=14.94	count=8500
Total loss:	23.994 (rec:0.016, round:23.978)	b=14.38	count=9000
Total loss:	22.243 (rec:0.017, round:22.226)	b=13.81	count=9500
Total loss:	20.391 (rec:0.017, round:20.374)	b=13.25	count=10000
Total loss:	18.454 (rec:0.021, round:18.434)	b=12.69	count=10500
Total loss:	16.604 (rec:0.017, round:16.587)	b=12.12	count=11000
Total loss:	14.903 (rec:0.029, round:14.874)	b=11.56	count=11500
Total loss:	12.808 (rec:0.021, round:12.786)	b=11.00	count=12000
Total loss:	10.977 (rec:0.030, round:10.947)	b=10.44	count=12500
Total loss:	9.276 (rec:0.032, round:9.244)	b=9.88	count=13000
Total loss:	7.808 (rec:0.042, round:7.766)	b=9.31	count=13500
Total loss:	6.130 (rec:0.057, round:6.073)	b=8.75	count=14000
Total loss:	4.939 (rec:0.057, round:4.883)	b=8.19	count=14500
Total loss:	3.932 (rec:0.065, round:3.867)	b=7.62	count=15000
Total loss:	3.063 (rec:0.091, round:2.973)	b=7.06	count=15500
Total loss:	2.435 (rec:0.096, round:2.339)	b=6.50	count=16000
Total loss:	1.862 (rec:0.112, round:1.750)	b=5.94	count=16500
Total loss:	1.442 (rec:0.146, round:1.295)	b=5.38	count=17000
Total loss:	1.083 (rec:0.156, round:0.927)	b=4.81	count=17500
Total loss:	0.822 (rec:0.176, round:0.646)	b=4.25	count=18000
Total loss:	0.609 (rec:0.202, round:0.407)	b=3.69	count=18500
Total loss:	0.418 (rec:0.189, round:0.229)	b=3.12	count=19000
Total loss:	0.272 (rec:0.209, round:0.063)	b=2.56	count=19500
Total loss:	0.258 (rec:0.235, round:0.022)	b=2.00	count=20000
finished reconstructing patch_embed.
reconstructing layers.0.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.0.blocks.0 ...
wraping quantizers in layers.0.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.077 (rec:1.077, round:0.000)	b=0.00	count=500
Total loss:	0.919 (rec:0.919, round:0.000)	b=0.00	count=1000
Total loss:	0.878 (rec:0.878, round:0.000)	b=0.00	count=1500
Total loss:	0.801 (rec:0.801, round:0.000)	b=0.00	count=2000
Total loss:	0.796 (rec:0.796, round:0.000)	b=0.00	count=2500
Total loss:	0.711 (rec:0.711, round:0.000)	b=0.00	count=3000
Total loss:	0.693 (rec:0.693, round:0.000)	b=0.00	count=3500
Total loss:	1562.016 (rec:0.694, round:1561.322)	b=20.00	count=4000
Total loss:	724.128 (rec:0.719, round:723.409)	b=19.44	count=4500
Total loss:	646.366 (rec:0.708, round:645.658)	b=18.88	count=5000
Total loss:	586.599 (rec:0.678, round:585.921)	b=18.31	count=5500
Total loss:	535.907 (rec:0.645, round:535.262)	b=17.75	count=6000
Total loss:	490.737 (rec:0.733, round:490.004)	b=17.19	count=6500
Total loss:	450.623 (rec:0.619, round:450.004)	b=16.62	count=7000
Total loss:	415.609 (rec:0.667, round:414.942)	b=16.06	count=7500
Total loss:	383.277 (rec:0.703, round:382.574)	b=15.50	count=8000
Total loss:	353.032 (rec:0.700, round:352.332)	b=14.94	count=8500
Total loss:	326.078 (rec:0.726, round:325.352)	b=14.38	count=9000
Total loss:	301.658 (rec:0.680, round:300.978)	b=13.81	count=9500
Total loss:	277.847 (rec:0.649, round:277.198)	b=13.25	count=10000
Total loss:	255.598 (rec:0.696, round:254.902)	b=12.69	count=10500
Total loss:	233.764 (rec:0.645, round:233.118)	b=12.12	count=11000
Total loss:	213.175 (rec:0.679, round:212.497)	b=11.56	count=11500
Total loss:	192.856 (rec:0.681, round:192.175)	b=11.00	count=12000
Total loss:	172.373 (rec:0.660, round:171.713)	b=10.44	count=12500
Total loss:	152.457 (rec:0.658, round:151.799)	b=9.88	count=13000
Total loss:	132.112 (rec:0.729, round:131.383)	b=9.31	count=13500
Total loss:	111.754 (rec:0.660, round:111.094)	b=8.75	count=14000
Total loss:	92.848 (rec:0.702, round:92.146)	b=8.19	count=14500
Total loss:	74.543 (rec:0.713, round:73.830)	b=7.62	count=15000
Total loss:	56.961 (rec:0.666, round:56.295)	b=7.06	count=15500
Total loss:	41.508 (rec:0.713, round:40.795)	b=6.50	count=16000
Total loss:	27.702 (rec:0.718, round:26.984)	b=5.94	count=16500
Total loss:	16.798 (rec:0.714, round:16.084)	b=5.38	count=17000
Total loss:	9.483 (rec:0.762, round:8.721)	b=4.81	count=17500
Total loss:	4.349 (rec:0.723, round:3.627)	b=4.25	count=18000
Total loss:	2.151 (rec:0.832, round:1.319)	b=3.69	count=18500
Total loss:	1.125 (rec:0.797, round:0.328)	b=3.12	count=19000
Total loss:	0.784 (rec:0.745, round:0.040)	b=2.56	count=19500
Total loss:	0.651 (rec:0.642, round:0.009)	b=2.00	count=20000
finished reconstructing layers.0.blocks.0.
reconstructing layers.0.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.0.blocks.1 ...
wraping quantizers in layers.0.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.327 (rec:1.327, round:0.000)	b=0.00	count=500
Total loss:	1.161 (rec:1.161, round:0.000)	b=0.00	count=1000
Total loss:	1.163 (rec:1.163, round:0.000)	b=0.00	count=1500
Total loss:	1.004 (rec:1.004, round:0.000)	b=0.00	count=2000
Total loss:	1.052 (rec:1.052, round:0.000)	b=0.00	count=2500
Total loss:	1.022 (rec:1.022, round:0.000)	b=0.00	count=3000
Total loss:	0.993 (rec:0.993, round:0.000)	b=0.00	count=3500
Total loss:	1617.971 (rec:1.040, round:1616.931)	b=20.00	count=4000
Total loss:	815.021 (rec:1.002, round:814.019)	b=19.44	count=4500
Total loss:	737.488 (rec:0.979, round:736.509)	b=18.88	count=5000
Total loss:	680.899 (rec:1.027, round:679.871)	b=18.31	count=5500
Total loss:	632.134 (rec:1.033, round:631.101)	b=17.75	count=6000
Total loss:	588.537 (rec:0.917, round:587.619)	b=17.19	count=6500
Total loss:	548.895 (rec:0.896, round:547.999)	b=16.62	count=7000
Total loss:	512.525 (rec:1.096, round:511.429)	b=16.06	count=7500
Total loss:	478.574 (rec:1.004, round:477.571)	b=15.50	count=8000
Total loss:	446.555 (rec:0.987, round:445.567)	b=14.94	count=8500
Total loss:	416.117 (rec:1.027, round:415.090)	b=14.38	count=9000
Total loss:	387.515 (rec:0.982, round:386.533)	b=13.81	count=9500
Total loss:	359.682 (rec:0.981, round:358.701)	b=13.25	count=10000
Total loss:	333.120 (rec:0.943, round:332.177)	b=12.69	count=10500
Total loss:	307.968 (rec:0.978, round:306.990)	b=12.12	count=11000
Total loss:	282.758 (rec:0.970, round:281.788)	b=11.56	count=11500
Total loss:	258.111 (rec:0.943, round:257.168)	b=11.00	count=12000
Total loss:	232.908 (rec:0.979, round:231.930)	b=10.44	count=12500
Total loss:	209.016 (rec:0.992, round:208.024)	b=9.88	count=13000
Total loss:	184.619 (rec:0.990, round:183.629)	b=9.31	count=13500
Total loss:	160.256 (rec:1.068, round:159.188)	b=8.75	count=14000
Total loss:	135.905 (rec:1.051, round:134.854)	b=8.19	count=14500
Total loss:	111.086 (rec:0.985, round:110.101)	b=7.62	count=15000
Total loss:	87.534 (rec:1.017, round:86.517)	b=7.06	count=15500
Total loss:	64.298 (rec:1.017, round:63.281)	b=6.50	count=16000
Total loss:	43.899 (rec:1.023, round:42.876)	b=5.94	count=16500
Total loss:	26.524 (rec:1.034, round:25.490)	b=5.38	count=17000
Total loss:	14.209 (rec:1.086, round:13.123)	b=4.81	count=17500
Total loss:	6.370 (rec:1.030, round:5.341)	b=4.25	count=18000
Total loss:	2.666 (rec:1.084, round:1.582)	b=3.69	count=18500
Total loss:	1.381 (rec:1.028, round:0.354)	b=3.12	count=19000
Total loss:	1.114 (rec:1.070, round:0.045)	b=2.56	count=19500
Total loss:	1.090 (rec:1.088, round:0.002)	b=2.00	count=20000
finished reconstructing layers.0.blocks.1.
reconstructing layers.1.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.downsample ...
wraping quantizers in layers.1.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.764 (rec:1.764, round:0.000)	b=0.00	count=500
Total loss:	1.602 (rec:1.602, round:0.000)	b=0.00	count=1000
Total loss:	1.526 (rec:1.526, round:0.000)	b=0.00	count=1500
Total loss:	1.507 (rec:1.507, round:0.000)	b=0.00	count=2000
Total loss:	1.521 (rec:1.521, round:0.000)	b=0.00	count=2500
Total loss:	1.411 (rec:1.411, round:0.000)	b=0.00	count=3000
Total loss:	1.427 (rec:1.427, round:0.000)	b=0.00	count=3500
Total loss:	1054.977 (rec:1.407, round:1053.570)	b=20.00	count=4000
Total loss:	552.741 (rec:1.455, round:551.287)	b=19.44	count=4500
Total loss:	507.297 (rec:1.381, round:505.916)	b=18.88	count=5000
Total loss:	478.923 (rec:1.382, round:477.542)	b=18.31	count=5500
Total loss:	456.493 (rec:1.400, round:455.093)	b=17.75	count=6000
Total loss:	436.614 (rec:1.399, round:435.215)	b=17.19	count=6500
Total loss:	418.649 (rec:1.428, round:417.221)	b=16.62	count=7000
Total loss:	402.053 (rec:1.464, round:400.589)	b=16.06	count=7500
Total loss:	384.965 (rec:1.458, round:383.507)	b=15.50	count=8000
Total loss:	369.083 (rec:1.448, round:367.635)	b=14.94	count=8500
Total loss:	353.102 (rec:1.421, round:351.681)	b=14.38	count=9000
Total loss:	337.170 (rec:1.488, round:335.682)	b=13.81	count=9500
Total loss:	321.052 (rec:1.486, round:319.566)	b=13.25	count=10000
Total loss:	304.374 (rec:1.432, round:302.942)	b=12.69	count=10500
Total loss:	288.073 (rec:1.468, round:286.605)	b=12.12	count=11000
Total loss:	270.256 (rec:1.462, round:268.795)	b=11.56	count=11500
Total loss:	251.979 (rec:1.495, round:250.484)	b=11.00	count=12000
Total loss:	232.412 (rec:1.476, round:230.936)	b=10.44	count=12500
Total loss:	212.149 (rec:1.549, round:210.600)	b=9.88	count=13000
Total loss:	190.632 (rec:1.523, round:189.109)	b=9.31	count=13500
Total loss:	168.899 (rec:1.563, round:167.337)	b=8.75	count=14000
Total loss:	145.692 (rec:1.547, round:144.145)	b=8.19	count=14500
Total loss:	121.961 (rec:1.476, round:120.485)	b=7.62	count=15000
Total loss:	98.829 (rec:1.548, round:97.281)	b=7.06	count=15500
Total loss:	74.736 (rec:1.538, round:73.199)	b=6.50	count=16000
Total loss:	52.346 (rec:1.539, round:50.806)	b=5.94	count=16500
Total loss:	32.399 (rec:1.463, round:30.936)	b=5.38	count=17000
Total loss:	18.183 (rec:1.538, round:16.645)	b=4.81	count=17500
Total loss:	9.437 (rec:1.576, round:7.861)	b=4.25	count=18000
Total loss:	4.772 (rec:1.584, round:3.188)	b=3.69	count=18500
Total loss:	2.583 (rec:1.611, round:0.972)	b=3.12	count=19000
Total loss:	1.768 (rec:1.577, round:0.192)	b=2.56	count=19500
Total loss:	1.657 (rec:1.641, round:0.016)	b=2.00	count=20000
finished reconstructing layers.1.downsample.
reconstructing layers.1.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.blocks.0 ...
wraping quantizers in layers.1.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.536 (rec:1.536, round:0.000)	b=0.00	count=500
Total loss:	1.387 (rec:1.387, round:0.000)	b=0.00	count=1000
Total loss:	1.341 (rec:1.341, round:0.000)	b=0.00	count=1500
Total loss:	1.257 (rec:1.257, round:0.000)	b=0.00	count=2000
Total loss:	1.290 (rec:1.290, round:0.000)	b=0.00	count=2500
Total loss:	1.317 (rec:1.317, round:0.000)	b=0.00	count=3000
Total loss:	1.239 (rec:1.239, round:0.000)	b=0.00	count=3500
Total loss:	6560.223 (rec:1.296, round:6558.927)	b=20.00	count=4000
Total loss:	3126.422 (rec:1.262, round:3125.161)	b=19.44	count=4500
Total loss:	2845.831 (rec:1.256, round:2844.575)	b=18.88	count=5000
Total loss:	2644.455 (rec:1.240, round:2643.214)	b=18.31	count=5500
Total loss:	2471.943 (rec:1.266, round:2470.677)	b=17.75	count=6000
Total loss:	2314.679 (rec:1.261, round:2313.418)	b=17.19	count=6500
Total loss:	2167.412 (rec:1.251, round:2166.162)	b=16.62	count=7000
Total loss:	2030.741 (rec:1.252, round:2029.490)	b=16.06	count=7500
Total loss:	1901.538 (rec:1.284, round:1900.254)	b=15.50	count=8000
Total loss:	1780.170 (rec:1.264, round:1778.906)	b=14.94	count=8500
Total loss:	1661.548 (rec:1.265, round:1660.283)	b=14.38	count=9000
Total loss:	1550.026 (rec:1.273, round:1548.753)	b=13.81	count=9500
Total loss:	1442.300 (rec:1.291, round:1441.009)	b=13.25	count=10000
Total loss:	1335.324 (rec:1.297, round:1334.027)	b=12.69	count=10500
Total loss:	1229.690 (rec:1.273, round:1228.417)	b=12.12	count=11000
Total loss:	1127.702 (rec:1.309, round:1126.393)	b=11.56	count=11500
Total loss:	1024.853 (rec:1.274, round:1023.579)	b=11.00	count=12000
Total loss:	922.330 (rec:1.247, round:921.082)	b=10.44	count=12500
Total loss:	818.019 (rec:1.229, round:816.790)	b=9.88	count=13000
Total loss:	714.729 (rec:1.333, round:713.396)	b=9.31	count=13500
Total loss:	610.958 (rec:1.326, round:609.632)	b=8.75	count=14000
Total loss:	507.739 (rec:1.345, round:506.393)	b=8.19	count=14500
Total loss:	406.954 (rec:1.348, round:405.606)	b=7.62	count=15000
Total loss:	308.871 (rec:1.361, round:307.510)	b=7.06	count=15500
Total loss:	216.773 (rec:1.285, round:215.488)	b=6.50	count=16000
Total loss:	137.629 (rec:1.328, round:136.301)	b=5.94	count=16500
Total loss:	76.223 (rec:1.334, round:74.889)	b=5.38	count=17000
Total loss:	36.064 (rec:1.359, round:34.705)	b=4.81	count=17500
Total loss:	14.241 (rec:1.309, round:12.931)	b=4.25	count=18000
Total loss:	4.973 (rec:1.321, round:3.652)	b=3.69	count=18500
Total loss:	1.929 (rec:1.317, round:0.612)	b=3.12	count=19000
Total loss:	1.381 (rec:1.333, round:0.048)	b=2.56	count=19500
Total loss:	1.363 (rec:1.363, round:0.000)	b=2.00	count=20000
finished reconstructing layers.1.blocks.0.
reconstructing layers.1.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.blocks.1 ...
wraping quantizers in layers.1.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.681 (rec:1.681, round:0.000)	b=0.00	count=500
Total loss:	1.554 (rec:1.554, round:0.000)	b=0.00	count=1000
Total loss:	1.561 (rec:1.561, round:0.000)	b=0.00	count=1500
Total loss:	1.559 (rec:1.559, round:0.000)	b=0.00	count=2000
Total loss:	1.516 (rec:1.516, round:0.000)	b=0.00	count=2500
Total loss:	1.417 (rec:1.417, round:0.000)	b=0.00	count=3000
Total loss:	1.500 (rec:1.500, round:0.000)	b=0.00	count=3500
Total loss:	6831.556 (rec:1.470, round:6830.085)	b=20.00	count=4000
Total loss:	3425.273 (rec:1.444, round:3423.829)	b=19.44	count=4500
Total loss:	3146.373 (rec:1.477, round:3144.896)	b=18.88	count=5000
Total loss:	2953.251 (rec:1.431, round:2951.820)	b=18.31	count=5500
Total loss:	2786.082 (rec:1.471, round:2784.611)	b=17.75	count=6000
Total loss:	2632.733 (rec:1.479, round:2631.254)	b=17.19	count=6500
Total loss:	2489.337 (rec:1.431, round:2487.907)	b=16.62	count=7000
Total loss:	2352.827 (rec:1.487, round:2351.340)	b=16.06	count=7500
Total loss:	2220.288 (rec:1.440, round:2218.847)	b=15.50	count=8000
Total loss:	2091.113 (rec:1.482, round:2089.631)	b=14.94	count=8500
Total loss:	1966.851 (rec:1.477, round:1965.374)	b=14.38	count=9000
Total loss:	1845.544 (rec:1.473, round:1844.072)	b=13.81	count=9500
Total loss:	1726.283 (rec:1.497, round:1724.786)	b=13.25	count=10000
Total loss:	1608.390 (rec:1.513, round:1606.877)	b=12.69	count=10500
Total loss:	1491.035 (rec:1.450, round:1489.584)	b=12.12	count=11000
Total loss:	1375.764 (rec:1.466, round:1374.298)	b=11.56	count=11500
Total loss:	1260.473 (rec:1.533, round:1258.940)	b=11.00	count=12000
Total loss:	1143.771 (rec:1.460, round:1142.311)	b=10.44	count=12500
Total loss:	1029.109 (rec:1.456, round:1027.653)	b=9.88	count=13000
Total loss:	913.625 (rec:1.523, round:912.102)	b=9.31	count=13500
Total loss:	798.685 (rec:1.501, round:797.184)	b=8.75	count=14000
Total loss:	684.251 (rec:1.503, round:682.748)	b=8.19	count=14500
Total loss:	570.010 (rec:1.551, round:568.459)	b=7.62	count=15000
Total loss:	459.472 (rec:1.548, round:457.923)	b=7.06	count=15500
Total loss:	350.449 (rec:1.516, round:348.933)	b=6.50	count=16000
Total loss:	246.467 (rec:1.515, round:244.952)	b=5.94	count=16500
Total loss:	149.362 (rec:1.510, round:147.851)	b=5.38	count=17000
Total loss:	66.784 (rec:1.489, round:65.295)	b=4.81	count=17500
Total loss:	19.794 (rec:1.524, round:18.270)	b=4.25	count=18000
Total loss:	5.208 (rec:1.480, round:3.728)	b=3.69	count=18500
Total loss:	2.099 (rec:1.560, round:0.539)	b=3.12	count=19000
Total loss:	1.481 (rec:1.454, round:0.026)	b=2.56	count=19500
Total loss:	1.495 (rec:1.495, round:0.000)	b=2.00	count=20000
finished reconstructing layers.1.blocks.1.
reconstructing layers.2.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.downsample ...
wraping quantizers in layers.2.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.760 (rec:1.760, round:0.000)	b=0.00	count=500
Total loss:	1.519 (rec:1.519, round:0.000)	b=0.00	count=1000
Total loss:	1.495 (rec:1.495, round:0.000)	b=0.00	count=1500
Total loss:	1.448 (rec:1.448, round:0.000)	b=0.00	count=2000
Total loss:	1.441 (rec:1.441, round:0.000)	b=0.00	count=2500
Total loss:	1.355 (rec:1.355, round:0.000)	b=0.00	count=3000
Total loss:	1.342 (rec:1.342, round:0.000)	b=0.00	count=3500
Total loss:	4267.111 (rec:1.354, round:4265.756)	b=20.00	count=4000
Total loss:	2074.819 (rec:1.302, round:2073.517)	b=19.44	count=4500
Total loss:	1900.747 (rec:1.368, round:1899.380)	b=18.88	count=5000
Total loss:	1784.123 (rec:1.401, round:1782.722)	b=18.31	count=5500
Total loss:	1687.944 (rec:1.353, round:1686.592)	b=17.75	count=6000
Total loss:	1604.915 (rec:1.379, round:1603.535)	b=17.19	count=6500
Total loss:	1527.259 (rec:1.392, round:1525.867)	b=16.62	count=7000
Total loss:	1454.223 (rec:1.384, round:1452.839)	b=16.06	count=7500
Total loss:	1385.915 (rec:1.388, round:1384.527)	b=15.50	count=8000
Total loss:	1321.553 (rec:1.395, round:1320.158)	b=14.94	count=8500
Total loss:	1256.833 (rec:1.345, round:1255.488)	b=14.38	count=9000
Total loss:	1193.012 (rec:1.392, round:1191.620)	b=13.81	count=9500
Total loss:	1129.512 (rec:1.347, round:1128.165)	b=13.25	count=10000
Total loss:	1065.623 (rec:1.360, round:1064.263)	b=12.69	count=10500
Total loss:	1000.597 (rec:1.340, round:999.257)	b=12.12	count=11000
Total loss:	935.048 (rec:1.403, round:933.645)	b=11.56	count=11500
Total loss:	867.264 (rec:1.397, round:865.867)	b=11.00	count=12000
Total loss:	796.361 (rec:1.378, round:794.983)	b=10.44	count=12500
Total loss:	724.127 (rec:1.423, round:722.704)	b=9.88	count=13000
Total loss:	650.867 (rec:1.376, round:649.491)	b=9.31	count=13500
Total loss:	573.390 (rec:1.429, round:571.961)	b=8.75	count=14000
Total loss:	492.700 (rec:1.397, round:491.304)	b=8.19	count=14500
Total loss:	411.187 (rec:1.440, round:409.747)	b=7.62	count=15000
Total loss:	328.013 (rec:1.427, round:326.586)	b=7.06	count=15500
Total loss:	245.217 (rec:1.429, round:243.788)	b=6.50	count=16000
Total loss:	167.339 (rec:1.440, round:165.899)	b=5.94	count=16500
Total loss:	101.236 (rec:1.501, round:99.735)	b=5.38	count=17000
Total loss:	51.912 (rec:1.457, round:50.454)	b=4.81	count=17500
Total loss:	22.401 (rec:1.474, round:20.927)	b=4.25	count=18000
Total loss:	7.741 (rec:1.498, round:6.243)	b=3.69	count=18500
Total loss:	2.727 (rec:1.502, round:1.224)	b=3.12	count=19000
Total loss:	1.597 (rec:1.467, round:0.130)	b=2.56	count=19500
Total loss:	1.488 (rec:1.480, round:0.009)	b=2.00	count=20000
finished reconstructing layers.2.downsample.
reconstructing layers.2.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.0 ...
wraping quantizers in layers.2.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.466 (rec:1.466, round:0.000)	b=0.00	count=500
Total loss:	1.520 (rec:1.520, round:0.000)	b=0.00	count=1000
Total loss:	1.536 (rec:1.536, round:0.000)	b=0.00	count=1500
Total loss:	1.397 (rec:1.397, round:0.000)	b=0.00	count=2000
Total loss:	1.423 (rec:1.423, round:0.000)	b=0.00	count=2500
Total loss:	1.417 (rec:1.417, round:0.000)	b=0.00	count=3000
Total loss:	1.388 (rec:1.388, round:0.000)	b=0.00	count=3500
Total loss:	28068.822 (rec:1.359, round:28067.463)	b=20.00	count=4000
Total loss:	13060.288 (rec:1.355, round:13058.933)	b=19.44	count=4500
Total loss:	11983.154 (rec:1.349, round:11981.805)	b=18.88	count=5000
Total loss:	11236.075 (rec:1.357, round:11234.719)	b=18.31	count=5500
Total loss:	10573.117 (rec:1.323, round:10571.794)	b=17.75	count=6000
Total loss:	9955.574 (rec:1.313, round:9954.261)	b=17.19	count=6500
Total loss:	9365.425 (rec:1.308, round:9364.117)	b=16.62	count=7000
Total loss:	8788.183 (rec:1.323, round:8786.859)	b=16.06	count=7500
Total loss:	8230.574 (rec:1.360, round:8229.214)	b=15.50	count=8000
Total loss:	7688.044 (rec:1.325, round:7686.719)	b=14.94	count=8500
Total loss:	7160.689 (rec:1.325, round:7159.364)	b=14.38	count=9000
Total loss:	6649.032 (rec:1.335, round:6647.697)	b=13.81	count=9500
Total loss:	6146.380 (rec:1.301, round:6145.079)	b=13.25	count=10000
Total loss:	5659.665 (rec:1.364, round:5658.301)	b=12.69	count=10500
Total loss:	5189.309 (rec:1.308, round:5188.001)	b=12.12	count=11000
Total loss:	4730.124 (rec:1.431, round:4728.692)	b=11.56	count=11500
Total loss:	4278.172 (rec:1.328, round:4276.845)	b=11.00	count=12000
Total loss:	3841.715 (rec:1.326, round:3840.389)	b=10.44	count=12500
Total loss:	3410.965 (rec:1.334, round:3409.631)	b=9.88	count=13000
Total loss:	2991.272 (rec:1.335, round:2989.937)	b=9.31	count=13500
Total loss:	2582.334 (rec:1.315, round:2581.019)	b=8.75	count=14000
Total loss:	2185.437 (rec:1.395, round:2184.042)	b=8.19	count=14500
Total loss:	1799.061 (rec:1.284, round:1797.777)	b=7.62	count=15000
Total loss:	1429.845 (rec:1.345, round:1428.501)	b=7.06	count=15500
Total loss:	1077.367 (rec:1.388, round:1075.979)	b=6.50	count=16000
Total loss:	741.897 (rec:1.367, round:740.530)	b=5.94	count=16500
Total loss:	416.579 (rec:1.304, round:415.275)	b=5.38	count=17000
Total loss:	146.917 (rec:1.374, round:145.543)	b=4.81	count=17500
Total loss:	36.331 (rec:1.344, round:34.987)	b=4.25	count=18000
Total loss:	9.351 (rec:1.362, round:7.989)	b=3.69	count=18500
Total loss:	2.670 (rec:1.370, round:1.300)	b=3.12	count=19000
Total loss:	1.456 (rec:1.368, round:0.088)	b=2.56	count=19500
Total loss:	1.336 (rec:1.336, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.0.
reconstructing layers.2.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.1 ...
wraping quantizers in layers.2.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.559 (rec:1.559, round:0.000)	b=0.00	count=500
Total loss:	1.457 (rec:1.457, round:0.000)	b=0.00	count=1000
Total loss:	1.459 (rec:1.459, round:0.000)	b=0.00	count=1500
Total loss:	1.472 (rec:1.472, round:0.000)	b=0.00	count=2000
Total loss:	1.420 (rec:1.420, round:0.000)	b=0.00	count=2500
Total loss:	1.437 (rec:1.437, round:0.000)	b=0.00	count=3000
Total loss:	1.394 (rec:1.394, round:0.000)	b=0.00	count=3500
Total loss:	28248.051 (rec:1.423, round:28246.629)	b=20.00	count=4000
Total loss:	13420.627 (rec:1.392, round:13419.234)	b=19.44	count=4500
Total loss:	12341.278 (rec:1.419, round:12339.859)	b=18.88	count=5000
Total loss:	11602.668 (rec:1.407, round:11601.262)	b=18.31	count=5500
Total loss:	10959.866 (rec:1.401, round:10958.465)	b=17.75	count=6000
Total loss:	10355.588 (rec:1.385, round:10354.203)	b=17.19	count=6500
Total loss:	9779.475 (rec:1.390, round:9778.085)	b=16.62	count=7000
Total loss:	9217.723 (rec:1.387, round:9216.336)	b=16.06	count=7500
Total loss:	8669.292 (rec:1.391, round:8667.900)	b=15.50	count=8000
Total loss:	8133.188 (rec:1.378, round:8131.811)	b=14.94	count=8500
Total loss:	7612.467 (rec:1.393, round:7611.074)	b=14.38	count=9000
Total loss:	7102.672 (rec:1.414, round:7101.258)	b=13.81	count=9500
Total loss:	6598.574 (rec:1.356, round:6597.218)	b=13.25	count=10000
Total loss:	6107.131 (rec:1.454, round:6105.678)	b=12.69	count=10500
Total loss:	5623.464 (rec:1.400, round:5622.064)	b=12.12	count=11000
Total loss:	5144.517 (rec:1.396, round:5143.121)	b=11.56	count=11500
Total loss:	4671.415 (rec:1.415, round:4670.000)	b=11.00	count=12000
Total loss:	4207.726 (rec:1.375, round:4206.351)	b=10.44	count=12500
Total loss:	3748.209 (rec:1.377, round:3746.832)	b=9.88	count=13000
Total loss:	3297.277 (rec:1.416, round:3295.861)	b=9.31	count=13500
Total loss:	2854.805 (rec:1.391, round:2853.414)	b=8.75	count=14000
Total loss:	2422.787 (rec:1.417, round:2421.370)	b=8.19	count=14500
Total loss:	2003.332 (rec:1.414, round:2001.917)	b=7.62	count=15000
Total loss:	1596.921 (rec:1.391, round:1595.530)	b=7.06	count=15500
Total loss:	1208.020 (rec:1.385, round:1206.634)	b=6.50	count=16000
Total loss:	842.720 (rec:1.402, round:841.318)	b=5.94	count=16500
Total loss:	492.393 (rec:1.403, round:490.989)	b=5.38	count=17000
Total loss:	186.401 (rec:1.425, round:184.976)	b=4.81	count=17500
Total loss:	46.830 (rec:1.439, round:45.391)	b=4.25	count=18000
Total loss:	11.307 (rec:1.386, round:9.921)	b=3.69	count=18500
Total loss:	2.838 (rec:1.418, round:1.420)	b=3.12	count=19000
Total loss:	1.531 (rec:1.442, round:0.089)	b=2.56	count=19500
Total loss:	1.409 (rec:1.409, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.1.
reconstructing layers.2.blocks.2 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.2 ...
wraping quantizers in layers.2.blocks.2 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.568 (rec:1.568, round:0.000)	b=0.00	count=500
Total loss:	1.488 (rec:1.488, round:0.000)	b=0.00	count=1000
Total loss:	1.515 (rec:1.515, round:0.000)	b=0.00	count=1500
Total loss:	1.468 (rec:1.468, round:0.000)	b=0.00	count=2000
Total loss:	1.451 (rec:1.451, round:0.000)	b=0.00	count=2500
Total loss:	1.443 (rec:1.443, round:0.000)	b=0.00	count=3000
Total loss:	1.450 (rec:1.450, round:0.000)	b=0.00	count=3500
Total loss:	28283.131 (rec:1.413, round:28281.717)	b=20.00	count=4000
Total loss:	13584.945 (rec:1.405, round:13583.540)	b=19.44	count=4500
Total loss:	12507.820 (rec:1.392, round:12506.428)	b=18.88	count=5000
Total loss:	11771.822 (rec:1.386, round:11770.437)	b=18.31	count=5500
Total loss:	11127.084 (rec:1.389, round:11125.695)	b=17.75	count=6000
Total loss:	10523.041 (rec:1.366, round:10521.675)	b=17.19	count=6500
Total loss:	9946.021 (rec:1.413, round:9944.607)	b=16.62	count=7000
Total loss:	9385.259 (rec:1.389, round:9383.869)	b=16.06	count=7500
Total loss:	8838.265 (rec:1.364, round:8836.900)	b=15.50	count=8000
Total loss:	8305.646 (rec:1.428, round:8304.219)	b=14.94	count=8500
Total loss:	7779.288 (rec:1.353, round:7777.935)	b=14.38	count=9000
Total loss:	7256.208 (rec:1.366, round:7254.842)	b=13.81	count=9500
Total loss:	6746.725 (rec:1.384, round:6745.340)	b=13.25	count=10000
Total loss:	6243.537 (rec:1.369, round:6242.168)	b=12.69	count=10500
Total loss:	5750.999 (rec:1.370, round:5749.629)	b=12.12	count=11000
Total loss:	5260.550 (rec:1.413, round:5259.137)	b=11.56	count=11500
Total loss:	4780.652 (rec:1.354, round:4779.298)	b=11.00	count=12000
Total loss:	4307.292 (rec:1.365, round:4305.927)	b=10.44	count=12500
Total loss:	3839.550 (rec:1.394, round:3838.156)	b=9.88	count=13000
Total loss:	3377.704 (rec:1.402, round:3376.302)	b=9.31	count=13500
Total loss:	2924.817 (rec:1.372, round:2923.445)	b=8.75	count=14000
Total loss:	2486.100 (rec:1.396, round:2484.704)	b=8.19	count=14500
Total loss:	2059.107 (rec:1.377, round:2057.731)	b=7.62	count=15000
Total loss:	1644.884 (rec:1.412, round:1643.473)	b=7.06	count=15500
Total loss:	1247.433 (rec:1.366, round:1246.067)	b=6.50	count=16000
Total loss:	874.288 (rec:1.387, round:872.901)	b=5.94	count=16500
Total loss:	515.407 (rec:1.404, round:514.003)	b=5.38	count=17000
Total loss:	197.435 (rec:1.399, round:196.036)	b=4.81	count=17500
Total loss:	52.883 (rec:1.402, round:51.480)	b=4.25	count=18000
Total loss:	12.975 (rec:1.411, round:11.564)	b=3.69	count=18500
Total loss:	3.094 (rec:1.414, round:1.680)	b=3.12	count=19000
Total loss:	1.550 (rec:1.444, round:0.106)	b=2.56	count=19500
Total loss:	1.408 (rec:1.407, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.2.
reconstructing layers.2.blocks.3 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.3 ...
wraping quantizers in layers.2.blocks.3 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.655 (rec:1.655, round:0.000)	b=0.00	count=500
Total loss:	1.572 (rec:1.572, round:0.000)	b=0.00	count=1000
Total loss:	1.645 (rec:1.645, round:0.000)	b=0.00	count=1500
Total loss:	1.622 (rec:1.622, round:0.000)	b=0.00	count=2000
Total loss:	1.584 (rec:1.584, round:0.000)	b=0.00	count=2500
Total loss:	1.550 (rec:1.550, round:0.000)	b=0.00	count=3000
Total loss:	1.517 (rec:1.517, round:0.000)	b=0.00	count=3500
Total loss:	28370.330 (rec:1.535, round:28368.795)	b=20.00	count=4000
Total loss:	13687.960 (rec:1.488, round:13686.473)	b=19.44	count=4500
Total loss:	12615.619 (rec:1.526, round:12614.094)	b=18.88	count=5000
Total loss:	11886.943 (rec:1.497, round:11885.446)	b=18.31	count=5500
Total loss:	11250.007 (rec:1.477, round:11248.529)	b=17.75	count=6000
Total loss:	10656.007 (rec:1.490, round:10654.518)	b=17.19	count=6500
Total loss:	10085.563 (rec:1.479, round:10084.085)	b=16.62	count=7000
Total loss:	9528.262 (rec:1.519, round:9526.742)	b=16.06	count=7500
Total loss:	8985.234 (rec:1.506, round:8983.729)	b=15.50	count=8000
Total loss:	8455.212 (rec:1.496, round:8453.716)	b=14.94	count=8500
Total loss:	7929.433 (rec:1.509, round:7927.924)	b=14.38	count=9000
Total loss:	7412.944 (rec:1.447, round:7411.497)	b=13.81	count=9500
Total loss:	6901.083 (rec:1.499, round:6899.584)	b=13.25	count=10000
Total loss:	6398.180 (rec:1.484, round:6396.695)	b=12.69	count=10500
Total loss:	5903.311 (rec:1.473, round:5901.838)	b=12.12	count=11000
Total loss:	5416.719 (rec:1.491, round:5415.228)	b=11.56	count=11500
Total loss:	4928.669 (rec:1.509, round:4927.160)	b=11.00	count=12000
Total loss:	4452.310 (rec:1.501, round:4450.809)	b=10.44	count=12500
Total loss:	3977.400 (rec:1.497, round:3975.903)	b=9.88	count=13000
Total loss:	3510.771 (rec:1.506, round:3509.265)	b=9.31	count=13500
Total loss:	3051.148 (rec:1.516, round:3049.632)	b=8.75	count=14000
Total loss:	2599.755 (rec:1.528, round:2598.227)	b=8.19	count=14500
Total loss:	2158.778 (rec:1.496, round:2157.282)	b=7.62	count=15000
Total loss:	1730.690 (rec:1.461, round:1729.229)	b=7.06	count=15500
Total loss:	1320.417 (rec:1.473, round:1318.944)	b=6.50	count=16000
Total loss:	933.013 (rec:1.510, round:931.503)	b=5.94	count=16500
Total loss:	552.239 (rec:1.498, round:550.742)	b=5.38	count=17000
Total loss:	214.182 (rec:1.513, round:212.670)	b=4.81	count=17500
Total loss:	58.481 (rec:1.515, round:56.966)	b=4.25	count=18000
Total loss:	13.969 (rec:1.501, round:12.468)	b=3.69	count=18500
Total loss:	3.237 (rec:1.526, round:1.711)	b=3.12	count=19000
Total loss:	1.641 (rec:1.526, round:0.115)	b=2.56	count=19500
Total loss:	1.495 (rec:1.494, round:0.002)	b=2.00	count=20000
finished reconstructing layers.2.blocks.3.
reconstructing layers.2.blocks.4 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.4 ...
wraping quantizers in layers.2.blocks.4 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.631 (rec:1.631, round:0.000)	b=0.00	count=500
Total loss:	1.592 (rec:1.592, round:0.000)	b=0.00	count=1000
Total loss:	1.584 (rec:1.584, round:0.000)	b=0.00	count=1500
Total loss:	1.514 (rec:1.514, round:0.000)	b=0.00	count=2000
Total loss:	1.520 (rec:1.520, round:0.000)	b=0.00	count=2500
Total loss:	1.481 (rec:1.481, round:0.000)	b=0.00	count=3000
Total loss:	1.508 (rec:1.508, round:0.000)	b=0.00	count=3500
Total loss:	28529.467 (rec:1.473, round:28527.994)	b=20.00	count=4000
Total loss:	13894.329 (rec:1.466, round:13892.863)	b=19.44	count=4500
Total loss:	12817.121 (rec:1.438, round:12815.684)	b=18.88	count=5000
Total loss:	12093.916 (rec:1.451, round:12092.465)	b=18.31	count=5500
Total loss:	11464.861 (rec:1.450, round:11463.411)	b=17.75	count=6000
Total loss:	10869.976 (rec:1.496, round:10868.479)	b=17.19	count=6500
Total loss:	10304.618 (rec:1.473, round:10303.146)	b=16.62	count=7000
Total loss:	9754.229 (rec:1.460, round:9752.770)	b=16.06	count=7500
Total loss:	9213.091 (rec:1.442, round:9211.648)	b=15.50	count=8000
Total loss:	8679.074 (rec:1.463, round:8677.611)	b=14.94	count=8500
Total loss:	8151.254 (rec:1.441, round:8149.813)	b=14.38	count=9000
Total loss:	7629.635 (rec:1.452, round:7628.183)	b=13.81	count=9500
Total loss:	7107.648 (rec:1.455, round:7106.192)	b=13.25	count=10000
Total loss:	6594.273 (rec:1.412, round:6592.861)	b=12.69	count=10500
Total loss:	6083.902 (rec:1.467, round:6082.435)	b=12.12	count=11000
Total loss:	5582.205 (rec:1.461, round:5580.745)	b=11.56	count=11500
Total loss:	5091.137 (rec:1.455, round:5089.682)	b=11.00	count=12000
Total loss:	4600.854 (rec:1.436, round:4599.418)	b=10.44	count=12500
Total loss:	4115.841 (rec:1.415, round:4114.425)	b=9.88	count=13000
Total loss:	3633.071 (rec:1.417, round:3631.654)	b=9.31	count=13500
Total loss:	3161.052 (rec:1.464, round:3159.589)	b=8.75	count=14000
Total loss:	2695.735 (rec:1.448, round:2694.287)	b=8.19	count=14500
Total loss:	2244.568 (rec:1.509, round:2243.060)	b=7.62	count=15000
Total loss:	1806.755 (rec:1.466, round:1805.289)	b=7.06	count=15500
Total loss:	1382.163 (rec:1.455, round:1380.708)	b=6.50	count=16000
Total loss:	976.273 (rec:1.458, round:974.815)	b=5.94	count=16500
Total loss:	580.943 (rec:1.490, round:579.453)	b=5.38	count=17000
Total loss:	235.285 (rec:1.476, round:233.809)	b=4.81	count=17500
Total loss:	68.574 (rec:1.472, round:67.101)	b=4.25	count=18000
Total loss:	16.392 (rec:1.448, round:14.944)	b=3.69	count=18500
Total loss:	3.503 (rec:1.486, round:2.017)	b=3.12	count=19000
Total loss:	1.602 (rec:1.470, round:0.131)	b=2.56	count=19500
Total loss:	1.450 (rec:1.447, round:0.003)	b=2.00	count=20000
finished reconstructing layers.2.blocks.4.
reconstructing layers.2.blocks.5 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.5 ...
wraping quantizers in layers.2.blocks.5 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.746 (rec:1.746, round:0.000)	b=0.00	count=500
Total loss:	1.739 (rec:1.739, round:0.000)	b=0.00	count=1000
Total loss:	1.654 (rec:1.654, round:0.000)	b=0.00	count=1500
Total loss:	1.636 (rec:1.636, round:0.000)	b=0.00	count=2000
Total loss:	1.641 (rec:1.641, round:0.000)	b=0.00	count=2500
Total loss:	1.629 (rec:1.629, round:0.000)	b=0.00	count=3000
Total loss:	1.615 (rec:1.615, round:0.000)	b=0.00	count=3500
Total loss:	28535.256 (rec:1.626, round:28533.631)	b=20.00	count=4000
Total loss:	13942.149 (rec:1.565, round:13940.584)	b=19.44	count=4500
Total loss:	12872.404 (rec:1.563, round:12870.841)	b=18.88	count=5000
Total loss:	12148.238 (rec:1.582, round:12146.656)	b=18.31	count=5500
Total loss:	11520.362 (rec:1.576, round:11518.787)	b=17.75	count=6000
Total loss:	10930.920 (rec:1.591, round:10929.329)	b=17.19	count=6500
Total loss:	10361.729 (rec:1.623, round:10360.106)	b=16.62	count=7000
Total loss:	9810.359 (rec:1.529, round:9808.830)	b=16.06	count=7500
Total loss:	9274.623 (rec:1.577, round:9273.046)	b=15.50	count=8000
Total loss:	8745.294 (rec:1.542, round:8743.752)	b=14.94	count=8500
Total loss:	8218.470 (rec:1.547, round:8216.923)	b=14.38	count=9000
Total loss:	7703.156 (rec:1.562, round:7701.594)	b=13.81	count=9500
Total loss:	7188.165 (rec:1.574, round:7186.591)	b=13.25	count=10000
Total loss:	6678.230 (rec:1.601, round:6676.630)	b=12.69	count=10500
Total loss:	6168.176 (rec:1.551, round:6166.625)	b=12.12	count=11000
Total loss:	5665.764 (rec:1.564, round:5664.200)	b=11.56	count=11500
Total loss:	5166.610 (rec:1.580, round:5165.031)	b=11.00	count=12000
Total loss:	4674.685 (rec:1.586, round:4673.099)	b=10.44	count=12500
Total loss:	4188.534 (rec:1.572, round:4186.962)	b=9.88	count=13000
Total loss:	3703.768 (rec:1.603, round:3702.166)	b=9.31	count=13500
Total loss:	3228.471 (rec:1.564, round:3226.908)	b=8.75	count=14000
Total loss:	2756.390 (rec:1.579, round:2754.812)	b=8.19	count=14500
Total loss:	2294.475 (rec:1.577, round:2292.898)	b=7.62	count=15000
Total loss:	1847.205 (rec:1.590, round:1845.615)	b=7.06	count=15500
Total loss:	1415.877 (rec:1.580, round:1414.297)	b=6.50	count=16000
Total loss:	1004.551 (rec:1.572, round:1002.978)	b=5.94	count=16500
Total loss:	602.687 (rec:1.536, round:601.151)	b=5.38	count=17000
Total loss:	254.476 (rec:1.607, round:252.869)	b=4.81	count=17500
Total loss:	80.259 (rec:1.619, round:78.640)	b=4.25	count=18000
Total loss:	19.222 (rec:1.622, round:17.600)	b=3.69	count=18500
Total loss:	3.933 (rec:1.618, round:2.316)	b=3.12	count=19000
Total loss:	1.756 (rec:1.588, round:0.168)	b=2.56	count=19500
Total loss:	1.567 (rec:1.562, round:0.006)	b=2.00	count=20000
finished reconstructing layers.2.blocks.5.
reconstructing layers.2.blocks.6 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.6 ...
wraping quantizers in layers.2.blocks.6 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.619 (rec:1.619, round:0.000)	b=0.00	count=500
Total loss:	1.577 (rec:1.577, round:0.000)	b=0.00	count=1000
Total loss:	1.552 (rec:1.552, round:0.000)	b=0.00	count=1500
Total loss:	1.547 (rec:1.547, round:0.000)	b=0.00	count=2000
Total loss:	1.482 (rec:1.482, round:0.000)	b=0.00	count=2500
Total loss:	1.542 (rec:1.542, round:0.000)	b=0.00	count=3000
Total loss:	1.503 (rec:1.503, round:0.000)	b=0.00	count=3500
Total loss:	28513.150 (rec:1.491, round:28511.658)	b=20.00	count=4000
Total loss:	13922.693 (rec:1.496, round:13921.197)	b=19.44	count=4500
Total loss:	12848.975 (rec:1.485, round:12847.490)	b=18.88	count=5000
Total loss:	12125.158 (rec:1.485, round:12123.673)	b=18.31	count=5500
Total loss:	11490.510 (rec:1.472, round:11489.038)	b=17.75	count=6000
Total loss:	10900.569 (rec:1.481, round:10899.089)	b=17.19	count=6500
Total loss:	10333.693 (rec:1.413, round:10332.281)	b=16.62	count=7000
Total loss:	9781.508 (rec:1.456, round:9780.053)	b=16.06	count=7500
Total loss:	9241.261 (rec:1.473, round:9239.787)	b=15.50	count=8000
Total loss:	8707.157 (rec:1.459, round:8705.698)	b=14.94	count=8500
Total loss:	8181.623 (rec:1.454, round:8180.169)	b=14.38	count=9000
Total loss:	7660.103 (rec:1.476, round:7658.626)	b=13.81	count=9500
Total loss:	7145.590 (rec:1.456, round:7144.135)	b=13.25	count=10000
Total loss:	6635.056 (rec:1.448, round:6633.608)	b=12.69	count=10500
Total loss:	6132.880 (rec:1.435, round:6131.445)	b=12.12	count=11000
Total loss:	5631.068 (rec:1.429, round:5629.639)	b=11.56	count=11500
Total loss:	5130.979 (rec:1.454, round:5129.525)	b=11.00	count=12000
Total loss:	4637.119 (rec:1.492, round:4635.627)	b=10.44	count=12500
Total loss:	4145.102 (rec:1.453, round:4143.648)	b=9.88	count=13000
Total loss:	3664.122 (rec:1.475, round:3662.647)	b=9.31	count=13500
Total loss:	3192.818 (rec:1.452, round:3191.366)	b=8.75	count=14000
Total loss:	2724.258 (rec:1.467, round:2722.790)	b=8.19	count=14500
Total loss:	2266.524 (rec:1.458, round:2265.066)	b=7.62	count=15000
Total loss:	1820.603 (rec:1.429, round:1819.173)	b=7.06	count=15500
Total loss:	1389.693 (rec:1.455, round:1388.239)	b=6.50	count=16000
Total loss:	977.239 (rec:1.487, round:975.752)	b=5.94	count=16500
Total loss:	584.599 (rec:1.511, round:583.088)	b=5.38	count=17000
Total loss:	257.762 (rec:1.464, round:256.298)	b=4.81	count=17500
Total loss:	85.021 (rec:1.467, round:83.554)	b=4.25	count=18000
Total loss:	20.290 (rec:1.496, round:18.794)	b=3.69	count=18500
Total loss:	3.892 (rec:1.488, round:2.404)	b=3.12	count=19000
Total loss:	1.617 (rec:1.504, round:0.113)	b=2.56	count=19500
Total loss:	1.465 (rec:1.464, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.6.
reconstructing layers.2.blocks.7 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.7 ...
wraping quantizers in layers.2.blocks.7 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.731 (rec:1.731, round:0.000)	b=0.00	count=500
Total loss:	1.731 (rec:1.731, round:0.000)	b=0.00	count=1000
Total loss:	1.671 (rec:1.671, round:0.000)	b=0.00	count=1500
Total loss:	1.614 (rec:1.614, round:0.000)	b=0.00	count=2000
Total loss:	1.624 (rec:1.624, round:0.000)	b=0.00	count=2500
Total loss:	1.582 (rec:1.582, round:0.000)	b=0.00	count=3000
Total loss:	1.528 (rec:1.528, round:0.000)	b=0.00	count=3500
Total loss:	28212.611 (rec:1.498, round:28211.113)	b=20.00	count=4000
Total loss:	13882.751 (rec:1.542, round:13881.209)	b=19.44	count=4500
Total loss:	12811.110 (rec:1.549, round:12809.562)	b=18.88	count=5000
Total loss:	12096.016 (rec:1.561, round:12094.455)	b=18.31	count=5500
Total loss:	11476.031 (rec:1.534, round:11474.497)	b=17.75	count=6000
Total loss:	10900.870 (rec:1.519, round:10899.352)	b=17.19	count=6500
Total loss:	10347.341 (rec:1.526, round:10345.814)	b=16.62	count=7000
Total loss:	9809.294 (rec:1.525, round:9807.770)	b=16.06	count=7500
Total loss:	9283.086 (rec:1.550, round:9281.536)	b=15.50	count=8000
Total loss:	8766.669 (rec:1.519, round:8765.150)	b=14.94	count=8500
Total loss:	8258.007 (rec:1.518, round:8256.488)	b=14.38	count=9000
Total loss:	7751.792 (rec:1.558, round:7750.235)	b=13.81	count=9500
Total loss:	7251.257 (rec:1.527, round:7249.729)	b=13.25	count=10000
Total loss:	6756.234 (rec:1.524, round:6754.710)	b=12.69	count=10500
Total loss:	6259.049 (rec:1.520, round:6257.529)	b=12.12	count=11000
Total loss:	5767.472 (rec:1.522, round:5765.950)	b=11.56	count=11500
Total loss:	5277.697 (rec:1.511, round:5276.186)	b=11.00	count=12000
Total loss:	4790.130 (rec:1.526, round:4788.604)	b=10.44	count=12500
Total loss:	4305.396 (rec:1.522, round:4303.874)	b=9.88	count=13000
Total loss:	3824.429 (rec:1.563, round:3822.865)	b=9.31	count=13500
Total loss:	3343.815 (rec:1.549, round:3342.266)	b=8.75	count=14000
Total loss:	2866.100 (rec:1.558, round:2864.542)	b=8.19	count=14500
Total loss:	2392.891 (rec:1.551, round:2391.341)	b=7.62	count=15000
Total loss:	1934.703 (rec:1.560, round:1933.143)	b=7.06	count=15500
Total loss:	1492.223 (rec:1.578, round:1490.645)	b=6.50	count=16000
Total loss:	1072.184 (rec:1.560, round:1070.625)	b=5.94	count=16500
Total loss:	687.675 (rec:1.536, round:686.138)	b=5.38	count=17000
Total loss:	373.329 (rec:1.548, round:371.781)	b=4.81	count=17500
Total loss:	166.230 (rec:1.563, round:164.668)	b=4.25	count=18000
Total loss:	50.013 (rec:1.569, round:48.443)	b=3.69	count=18500
Total loss:	7.298 (rec:1.557, round:5.741)	b=3.12	count=19000
Total loss:	1.772 (rec:1.560, round:0.212)	b=2.56	count=19500
Total loss:	1.561 (rec:1.561, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.7.
reconstructing layers.2.blocks.8 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.8 ...
wraping quantizers in layers.2.blocks.8 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.577 (rec:1.577, round:0.000)	b=0.00	count=500
Total loss:	1.497 (rec:1.497, round:0.000)	b=0.00	count=1000
Total loss:	1.508 (rec:1.508, round:0.000)	b=0.00	count=1500
Total loss:	1.407 (rec:1.407, round:0.000)	b=0.00	count=2000
Total loss:	1.336 (rec:1.336, round:0.000)	b=0.00	count=2500
Total loss:	1.443 (rec:1.443, round:0.000)	b=0.00	count=3000
Total loss:	1.365 (rec:1.365, round:0.000)	b=0.00	count=3500
Total loss:	28228.180 (rec:1.313, round:28226.867)	b=20.00	count=4000
Total loss:	13922.630 (rec:1.336, round:13921.294)	b=19.44	count=4500
Total loss:	12836.115 (rec:1.230, round:12834.886)	b=18.88	count=5000
Total loss:	12098.295 (rec:1.319, round:12096.976)	b=18.31	count=5500
Total loss:	11454.653 (rec:1.293, round:11453.360)	b=17.75	count=6000
Total loss:	10856.072 (rec:1.330, round:10854.742)	b=17.19	count=6500
Total loss:	10285.725 (rec:1.308, round:10284.417)	b=16.62	count=7000
Total loss:	9736.461 (rec:1.371, round:9735.090)	b=16.06	count=7500
Total loss:	9200.225 (rec:1.309, round:9198.916)	b=15.50	count=8000
Total loss:	8680.810 (rec:1.269, round:8679.540)	b=14.94	count=8500
Total loss:	8171.874 (rec:1.295, round:8170.579)	b=14.38	count=9000
Total loss:	7672.644 (rec:1.286, round:7671.358)	b=13.81	count=9500
Total loss:	7182.167 (rec:1.221, round:7180.945)	b=13.25	count=10000
Total loss:	6702.041 (rec:1.224, round:6700.817)	b=12.69	count=10500
Total loss:	6230.820 (rec:1.252, round:6229.568)	b=12.12	count=11000
Total loss:	5761.514 (rec:1.253, round:5760.261)	b=11.56	count=11500
Total loss:	5293.589 (rec:1.262, round:5292.327)	b=11.00	count=12000
Total loss:	4831.378 (rec:1.307, round:4830.071)	b=10.44	count=12500
Total loss:	4371.679 (rec:1.184, round:4370.495)	b=9.88	count=13000
Total loss:	3917.760 (rec:1.287, round:3916.473)	b=9.31	count=13500
Total loss:	3460.888 (rec:1.241, round:3459.647)	b=8.75	count=14000
Total loss:	3004.698 (rec:1.202, round:3003.496)	b=8.19	count=14500
Total loss:	2556.343 (rec:1.274, round:2555.069)	b=7.62	count=15000
Total loss:	2110.351 (rec:1.302, round:2109.048)	b=7.06	count=15500
Total loss:	1675.755 (rec:1.242, round:1674.514)	b=6.50	count=16000
Total loss:	1264.179 (rec:1.254, round:1262.925)	b=5.94	count=16500
Total loss:	884.311 (rec:1.252, round:883.059)	b=5.38	count=17000
Total loss:	569.470 (rec:1.266, round:568.203)	b=4.81	count=17500
Total loss:	329.999 (rec:1.234, round:328.765)	b=4.25	count=18000
Total loss:	154.402 (rec:1.220, round:153.182)	b=3.69	count=18500
Total loss:	35.273 (rec:1.246, round:34.028)	b=3.12	count=19000
Total loss:	3.035 (rec:1.259, round:1.777)	b=2.56	count=19500
Total loss:	1.289 (rec:1.263, round:0.026)	b=2.00	count=20000
finished reconstructing layers.2.blocks.8.
reconstructing layers.2.blocks.9 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.9 ...
wraping quantizers in layers.2.blocks.9 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.715 (rec:1.715, round:0.000)	b=0.00	count=500
Total loss:	1.544 (rec:1.544, round:0.000)	b=0.00	count=1000
Total loss:	1.468 (rec:1.468, round:0.000)	b=0.00	count=1500
Total loss:	1.365 (rec:1.365, round:0.000)	b=0.00	count=2000
Total loss:	1.430 (rec:1.430, round:0.000)	b=0.00	count=2500
Total loss:	1.392 (rec:1.392, round:0.000)	b=0.00	count=3000
Total loss:	1.520 (rec:1.520, round:0.000)	b=0.00	count=3500
Total loss:	27993.264 (rec:1.395, round:27991.869)	b=20.00	count=4000
Total loss:	13511.712 (rec:1.370, round:13510.342)	b=19.44	count=4500
Total loss:	12412.608 (rec:1.269, round:12411.340)	b=18.88	count=5000
Total loss:	11650.913 (rec:1.237, round:11649.677)	b=18.31	count=5500
Total loss:	10985.468 (rec:1.243, round:10984.225)	b=17.75	count=6000
Total loss:	10366.043 (rec:1.225, round:10364.818)	b=17.19	count=6500
Total loss:	9775.840 (rec:1.241, round:9774.599)	b=16.62	count=7000
Total loss:	9204.027 (rec:1.274, round:9202.754)	b=16.06	count=7500
Total loss:	8654.920 (rec:1.113, round:8653.807)	b=15.50	count=8000
Total loss:	8125.043 (rec:1.311, round:8123.732)	b=14.94	count=8500
Total loss:	7614.414 (rec:1.221, round:7613.193)	b=14.38	count=9000
Total loss:	7121.640 (rec:1.156, round:7120.484)	b=13.81	count=9500
Total loss:	6641.805 (rec:1.123, round:6640.682)	b=13.25	count=10000
Total loss:	6174.370 (rec:1.315, round:6173.055)	b=12.69	count=10500
Total loss:	5720.675 (rec:1.202, round:5719.473)	b=12.12	count=11000
Total loss:	5275.608 (rec:1.249, round:5274.359)	b=11.56	count=11500
Total loss:	4837.993 (rec:1.165, round:4836.829)	b=11.00	count=12000
Total loss:	4404.989 (rec:1.132, round:4403.857)	b=10.44	count=12500
Total loss:	3975.317 (rec:1.244, round:3974.074)	b=9.88	count=13000
Total loss:	3555.287 (rec:1.269, round:3554.018)	b=9.31	count=13500
Total loss:	3137.745 (rec:1.181, round:3136.564)	b=8.75	count=14000
Total loss:	2726.657 (rec:1.182, round:2725.475)	b=8.19	count=14500
Total loss:	2323.003 (rec:1.151, round:2321.853)	b=7.62	count=15000
Total loss:	1925.129 (rec:1.114, round:1924.015)	b=7.06	count=15500
Total loss:	1540.793 (rec:1.128, round:1539.664)	b=6.50	count=16000
Total loss:	1177.907 (rec:1.102, round:1176.804)	b=5.94	count=16500
Total loss:	852.378 (rec:1.177, round:851.201)	b=5.38	count=17000
Total loss:	575.130 (rec:1.092, round:574.039)	b=4.81	count=17500
Total loss:	355.081 (rec:1.144, round:353.937)	b=4.25	count=18000
Total loss:	183.094 (rec:1.170, round:181.924)	b=3.69	count=18500
Total loss:	61.618 (rec:1.116, round:60.502)	b=3.12	count=19000
Total loss:	8.404 (rec:1.148, round:7.256)	b=2.56	count=19500
Total loss:	1.339 (rec:1.186, round:0.153)	b=2.00	count=20000
finished reconstructing layers.2.blocks.9.
reconstructing layers.2.blocks.10 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.10 ...
wraping quantizers in layers.2.blocks.10 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.747 (rec:1.747, round:0.000)	b=0.00	count=500
Total loss:	1.659 (rec:1.659, round:0.000)	b=0.00	count=1000
Total loss:	1.444 (rec:1.444, round:0.000)	b=0.00	count=1500
Total loss:	1.403 (rec:1.403, round:0.000)	b=0.00	count=2000
Total loss:	1.372 (rec:1.372, round:0.000)	b=0.00	count=2500
Total loss:	1.393 (rec:1.393, round:0.000)	b=0.00	count=3000
Total loss:	1.377 (rec:1.377, round:0.000)	b=0.00	count=3500
Total loss:	27691.955 (rec:1.319, round:27690.637)	b=20.00	count=4000
Total loss:	13005.906 (rec:1.161, round:13004.745)	b=19.44	count=4500
Total loss:	11886.221 (rec:1.284, round:11884.937)	b=18.88	count=5000
Total loss:	11094.123 (rec:1.268, round:11092.855)	b=18.31	count=5500
Total loss:	10392.726 (rec:1.292, round:10391.434)	b=17.75	count=6000
Total loss:	9731.110 (rec:1.179, round:9729.932)	b=17.19	count=6500
Total loss:	9103.829 (rec:1.152, round:9102.678)	b=16.62	count=7000
Total loss:	8505.824 (rec:1.282, round:8504.543)	b=16.06	count=7500
Total loss:	7933.822 (rec:1.115, round:7932.707)	b=15.50	count=8000
Total loss:	7388.175 (rec:1.285, round:7386.891)	b=14.94	count=8500
Total loss:	6867.024 (rec:1.201, round:6865.823)	b=14.38	count=9000
Total loss:	6365.103 (rec:1.187, round:6363.916)	b=13.81	count=9500
Total loss:	5883.233 (rec:1.123, round:5882.109)	b=13.25	count=10000
Total loss:	5423.823 (rec:1.119, round:5422.704)	b=12.69	count=10500
Total loss:	4981.562 (rec:1.097, round:4980.465)	b=12.12	count=11000
Total loss:	4552.186 (rec:1.071, round:4551.115)	b=11.56	count=11500
Total loss:	4132.209 (rec:1.066, round:4131.143)	b=11.00	count=12000
Total loss:	3730.013 (rec:1.102, round:3728.911)	b=10.44	count=12500
Total loss:	3338.520 (rec:1.139, round:3337.381)	b=9.88	count=13000
Total loss:	2956.973 (rec:1.085, round:2955.889)	b=9.31	count=13500
Total loss:	2586.917 (rec:1.172, round:2585.746)	b=8.75	count=14000
Total loss:	2221.212 (rec:1.105, round:2220.107)	b=8.19	count=14500
Total loss:	1873.005 (rec:1.132, round:1871.873)	b=7.62	count=15000
Total loss:	1533.528 (rec:1.156, round:1532.372)	b=7.06	count=15500
Total loss:	1208.164 (rec:1.150, round:1207.014)	b=6.50	count=16000
Total loss:	906.184 (rec:1.043, round:905.141)	b=5.94	count=16500
Total loss:	644.132 (rec:1.128, round:643.004)	b=5.38	count=17000
Total loss:	427.847 (rec:1.093, round:426.754)	b=4.81	count=17500
Total loss:	253.910 (rec:1.025, round:252.885)	b=4.25	count=18000
Total loss:	120.409 (rec:1.118, round:119.291)	b=3.69	count=18500
Total loss:	33.697 (rec:1.091, round:32.606)	b=3.12	count=19000
Total loss:	4.332 (rec:1.067, round:3.265)	b=2.56	count=19500
Total loss:	1.332 (rec:1.212, round:0.120)	b=2.00	count=20000
finished reconstructing layers.2.blocks.10.
reconstructing layers.2.blocks.11 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.11 ...
wraping quantizers in layers.2.blocks.11 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.698 (rec:1.698, round:0.000)	b=0.00	count=500
Total loss:	1.193 (rec:1.193, round:0.000)	b=0.00	count=1000
Total loss:	1.230 (rec:1.230, round:0.000)	b=0.00	count=1500
Total loss:	1.281 (rec:1.281, round:0.000)	b=0.00	count=2000
Total loss:	1.249 (rec:1.249, round:0.000)	b=0.00	count=2500
Total loss:	1.077 (rec:1.077, round:0.000)	b=0.00	count=3000
Total loss:	1.108 (rec:1.108, round:0.000)	b=0.00	count=3500
Total loss:	27329.025 (rec:1.035, round:27327.990)	b=20.00	count=4000
Total loss:	12636.207 (rec:1.039, round:12635.168)	b=19.44	count=4500
Total loss:	11529.528 (rec:0.869, round:11528.659)	b=18.88	count=5000
Total loss:	10742.225 (rec:0.986, round:10741.239)	b=18.31	count=5500
Total loss:	10040.294 (rec:1.013, round:10039.281)	b=17.75	count=6000
Total loss:	9386.088 (rec:0.982, round:9385.105)	b=17.19	count=6500
Total loss:	8766.111 (rec:0.998, round:8765.113)	b=16.62	count=7000
Total loss:	8171.785 (rec:0.944, round:8170.841)	b=16.06	count=7500
Total loss:	7611.285 (rec:0.990, round:7610.295)	b=15.50	count=8000
Total loss:	7074.497 (rec:0.974, round:7073.523)	b=14.94	count=8500
Total loss:	6562.134 (rec:0.878, round:6561.256)	b=14.38	count=9000
Total loss:	6076.279 (rec:0.907, round:6075.373)	b=13.81	count=9500
Total loss:	5608.558 (rec:0.857, round:5607.701)	b=13.25	count=10000
Total loss:	5162.056 (rec:0.911, round:5161.145)	b=12.69	count=10500
Total loss:	4727.014 (rec:0.913, round:4726.101)	b=12.12	count=11000
Total loss:	4309.178 (rec:0.904, round:4308.274)	b=11.56	count=11500
Total loss:	3904.120 (rec:0.919, round:3903.200)	b=11.00	count=12000
Total loss:	3511.037 (rec:0.912, round:3510.125)	b=10.44	count=12500
Total loss:	3130.129 (rec:0.963, round:3129.165)	b=9.88	count=13000
Total loss:	2763.124 (rec:0.905, round:2762.219)	b=9.31	count=13500
Total loss:	2405.619 (rec:0.892, round:2404.727)	b=8.75	count=14000
Total loss:	2059.345 (rec:0.888, round:2058.457)	b=8.19	count=14500
Total loss:	1720.071 (rec:0.953, round:1719.118)	b=7.62	count=15000
Total loss:	1395.602 (rec:0.872, round:1394.730)	b=7.06	count=15500
Total loss:	1083.955 (rec:0.886, round:1083.069)	b=6.50	count=16000
Total loss:	799.660 (rec:0.886, round:798.775)	b=5.94	count=16500
Total loss:	559.397 (rec:0.909, round:558.488)	b=5.38	count=17000
Total loss:	365.714 (rec:0.966, round:364.749)	b=4.81	count=17500
Total loss:	212.010 (rec:0.929, round:211.081)	b=4.25	count=18000
Total loss:	96.518 (rec:0.795, round:95.723)	b=3.69	count=18500
Total loss:	22.929 (rec:0.821, round:22.108)	b=3.12	count=19000
Total loss:	1.790 (rec:0.794, round:0.996)	b=2.56	count=19500
Total loss:	0.862 (rec:0.850, round:0.012)	b=2.00	count=20000
finished reconstructing layers.2.blocks.11.
reconstructing layers.2.blocks.12 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.12 ...
wraping quantizers in layers.2.blocks.12 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.676 (rec:1.676, round:0.000)	b=0.00	count=500
Total loss:	1.178 (rec:1.178, round:0.000)	b=0.00	count=1000
Total loss:	1.156 (rec:1.156, round:0.000)	b=0.00	count=1500
Total loss:	1.065 (rec:1.065, round:0.000)	b=0.00	count=2000
Total loss:	0.989 (rec:0.989, round:0.000)	b=0.00	count=2500
Total loss:	0.968 (rec:0.968, round:0.000)	b=0.00	count=3000
Total loss:	1.017 (rec:1.017, round:0.000)	b=0.00	count=3500
Total loss:	27332.131 (rec:0.904, round:27331.227)	b=20.00	count=4000
Total loss:	12507.439 (rec:0.951, round:12506.488)	b=19.44	count=4500
Total loss:	11433.231 (rec:0.950, round:11432.281)	b=18.88	count=5000
Total loss:	10670.215 (rec:0.901, round:10669.313)	b=18.31	count=5500
Total loss:	9997.566 (rec:0.892, round:9996.674)	b=17.75	count=6000
Total loss:	9374.942 (rec:0.897, round:9374.045)	b=17.19	count=6500
Total loss:	8782.674 (rec:0.871, round:8781.803)	b=16.62	count=7000
Total loss:	8215.604 (rec:0.856, round:8214.748)	b=16.06	count=7500
Total loss:	7674.175 (rec:0.797, round:7673.378)	b=15.50	count=8000
Total loss:	7157.266 (rec:0.867, round:7156.398)	b=14.94	count=8500
Total loss:	6656.588 (rec:0.872, round:6655.716)	b=14.38	count=9000
Total loss:	6174.125 (rec:0.805, round:6173.320)	b=13.81	count=9500
Total loss:	5707.506 (rec:0.882, round:5706.624)	b=13.25	count=10000
Total loss:	5254.968 (rec:0.771, round:5254.198)	b=12.69	count=10500
Total loss:	4819.784 (rec:0.826, round:4818.958)	b=12.12	count=11000
Total loss:	4397.076 (rec:0.818, round:4396.259)	b=11.56	count=11500
Total loss:	3989.597 (rec:0.820, round:3988.778)	b=11.00	count=12000
Total loss:	3587.029 (rec:0.790, round:3586.239)	b=10.44	count=12500
Total loss:	3195.160 (rec:0.807, round:3194.354)	b=9.88	count=13000
Total loss:	2816.457 (rec:0.832, round:2815.625)	b=9.31	count=13500
Total loss:	2447.681 (rec:0.790, round:2446.891)	b=8.75	count=14000
Total loss:	2090.481 (rec:0.758, round:2089.723)	b=8.19	count=14500
Total loss:	1742.249 (rec:0.870, round:1741.379)	b=7.62	count=15000
Total loss:	1404.750 (rec:0.730, round:1404.020)	b=7.06	count=15500
Total loss:	1085.914 (rec:0.866, round:1085.048)	b=6.50	count=16000
Total loss:	799.466 (rec:0.781, round:798.685)	b=5.94	count=16500
Total loss:	557.001 (rec:0.774, round:556.228)	b=5.38	count=17000
Total loss:	363.364 (rec:0.841, round:362.522)	b=4.81	count=17500
Total loss:	209.917 (rec:0.813, round:209.105)	b=4.25	count=18000
Total loss:	94.875 (rec:0.779, round:94.096)	b=3.69	count=18500
Total loss:	24.219 (rec:0.818, round:23.402)	b=3.12	count=19000
Total loss:	2.510 (rec:0.811, round:1.699)	b=2.56	count=19500
Total loss:	0.903 (rec:0.867, round:0.037)	b=2.00	count=20000
finished reconstructing layers.2.blocks.12.
reconstructing layers.2.blocks.13 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.13 ...
wraping quantizers in layers.2.blocks.13 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.328 (rec:1.328, round:0.000)	b=0.00	count=500
Total loss:	0.867 (rec:0.867, round:0.000)	b=0.00	count=1000
Total loss:	0.911 (rec:0.911, round:0.000)	b=0.00	count=1500
Total loss:	0.874 (rec:0.874, round:0.000)	b=0.00	count=2000
Total loss:	0.773 (rec:0.773, round:0.000)	b=0.00	count=2500
Total loss:	0.809 (rec:0.809, round:0.000)	b=0.00	count=3000
Total loss:	0.824 (rec:0.824, round:0.000)	b=0.00	count=3500
Total loss:	27530.641 (rec:0.730, round:27529.910)	b=20.00	count=4000
Total loss:	12541.035 (rec:0.780, round:12540.255)	b=19.44	count=4500
Total loss:	11460.893 (rec:0.716, round:11460.176)	b=18.88	count=5000
Total loss:	10690.914 (rec:0.633, round:10690.281)	b=18.31	count=5500
Total loss:	10000.731 (rec:0.620, round:10000.111)	b=17.75	count=6000
Total loss:	9341.399 (rec:0.723, round:9340.677)	b=17.19	count=6500
Total loss:	8714.897 (rec:0.726, round:8714.171)	b=16.62	count=7000
Total loss:	8109.195 (rec:0.658, round:8108.537)	b=16.06	count=7500
Total loss:	7525.437 (rec:0.696, round:7524.741)	b=15.50	count=8000
Total loss:	6962.096 (rec:0.698, round:6961.397)	b=14.94	count=8500
Total loss:	6421.075 (rec:0.743, round:6420.332)	b=14.38	count=9000
Total loss:	5903.022 (rec:0.701, round:5902.322)	b=13.81	count=9500
Total loss:	5415.224 (rec:0.631, round:5414.593)	b=13.25	count=10000
Total loss:	4947.291 (rec:0.656, round:4946.635)	b=12.69	count=10500
Total loss:	4501.472 (rec:0.693, round:4500.779)	b=12.12	count=11000
Total loss:	4071.470 (rec:0.649, round:4070.821)	b=11.56	count=11500
Total loss:	3658.665 (rec:0.678, round:3657.987)	b=11.00	count=12000
Total loss:	3261.800 (rec:0.640, round:3261.160)	b=10.44	count=12500
Total loss:	2882.180 (rec:0.640, round:2881.540)	b=9.88	count=13000
Total loss:	2517.415 (rec:0.706, round:2516.708)	b=9.31	count=13500
Total loss:	2168.474 (rec:0.612, round:2167.862)	b=8.75	count=14000
Total loss:	1827.438 (rec:0.614, round:1826.823)	b=8.19	count=14500
Total loss:	1507.278 (rec:0.649, round:1506.630)	b=7.62	count=15000
Total loss:	1198.590 (rec:0.631, round:1197.960)	b=7.06	count=15500
Total loss:	909.115 (rec:0.576, round:908.539)	b=6.50	count=16000
Total loss:	655.772 (rec:0.669, round:655.103)	b=5.94	count=16500
Total loss:	442.346 (rec:0.649, round:441.697)	b=5.38	count=17000
Total loss:	274.636 (rec:0.599, round:274.037)	b=4.81	count=17500
Total loss:	149.957 (rec:0.639, round:149.319)	b=4.25	count=18000
Total loss:	58.578 (rec:0.636, round:57.942)	b=3.69	count=18500
Total loss:	10.658 (rec:0.631, round:10.027)	b=3.12	count=19000
Total loss:	1.158 (rec:0.671, round:0.487)	b=2.56	count=19500
Total loss:	0.612 (rec:0.606, round:0.006)	b=2.00	count=20000
finished reconstructing layers.2.blocks.13.
reconstructing layers.2.blocks.14 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.14 ...
wraping quantizers in layers.2.blocks.14 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.244 (rec:1.244, round:0.000)	b=0.00	count=500
Total loss:	0.998 (rec:0.998, round:0.000)	b=0.00	count=1000
Total loss:	0.816 (rec:0.816, round:0.000)	b=0.00	count=1500
Total loss:	0.805 (rec:0.805, round:0.000)	b=0.00	count=2000
Total loss:	0.765 (rec:0.765, round:0.000)	b=0.00	count=2500
Total loss:	0.757 (rec:0.757, round:0.000)	b=0.00	count=3000
Total loss:	0.677 (rec:0.677, round:0.000)	b=0.00	count=3500
Total loss:	28141.154 (rec:0.671, round:28140.484)	b=20.00	count=4000
Total loss:	12986.720 (rec:0.631, round:12986.088)	b=19.44	count=4500
Total loss:	11894.730 (rec:0.696, round:11894.034)	b=18.88	count=5000
Total loss:	11120.635 (rec:0.630, round:11120.005)	b=18.31	count=5500
Total loss:	10421.870 (rec:0.656, round:10421.215)	b=17.75	count=6000
Total loss:	9761.327 (rec:0.646, round:9760.682)	b=17.19	count=6500
Total loss:	9124.011 (rec:0.617, round:9123.394)	b=16.62	count=7000
Total loss:	8512.923 (rec:0.639, round:8512.284)	b=16.06	count=7500
Total loss:	7916.481 (rec:0.620, round:7915.862)	b=15.50	count=8000
Total loss:	7345.080 (rec:0.600, round:7344.480)	b=14.94	count=8500
Total loss:	6796.862 (rec:0.612, round:6796.250)	b=14.38	count=9000
Total loss:	6263.301 (rec:0.606, round:6262.695)	b=13.81	count=9500
Total loss:	5752.999 (rec:0.607, round:5752.391)	b=13.25	count=10000
Total loss:	5262.001 (rec:0.629, round:5261.373)	b=12.69	count=10500
Total loss:	4789.588 (rec:0.573, round:4789.015)	b=12.12	count=11000
Total loss:	4339.054 (rec:0.645, round:4338.408)	b=11.56	count=11500
Total loss:	3906.610 (rec:0.609, round:3906.001)	b=11.00	count=12000
Total loss:	3489.827 (rec:0.623, round:3489.204)	b=10.44	count=12500
Total loss:	3086.364 (rec:0.586, round:3085.778)	b=9.88	count=13000
Total loss:	2693.516 (rec:0.603, round:2692.914)	b=9.31	count=13500
Total loss:	2318.348 (rec:0.568, round:2317.780)	b=8.75	count=14000
Total loss:	1956.521 (rec:0.602, round:1955.919)	b=8.19	count=14500
Total loss:	1608.830 (rec:0.594, round:1608.236)	b=7.62	count=15000
Total loss:	1276.646 (rec:0.606, round:1276.041)	b=7.06	count=15500
Total loss:	966.214 (rec:0.654, round:965.560)	b=6.50	count=16000
Total loss:	685.129 (rec:0.619, round:684.510)	b=5.94	count=16500
Total loss:	447.718 (rec:0.603, round:447.115)	b=5.38	count=17000
Total loss:	259.986 (rec:0.582, round:259.404)	b=4.81	count=17500
Total loss:	123.945 (rec:0.621, round:123.325)	b=4.25	count=18000
Total loss:	35.973 (rec:0.631, round:35.342)	b=3.69	count=18500
Total loss:	5.649 (rec:0.597, round:5.052)	b=3.12	count=19000
Total loss:	0.990 (rec:0.599, round:0.391)	b=2.56	count=19500
Total loss:	0.632 (rec:0.616, round:0.016)	b=2.00	count=20000
finished reconstructing layers.2.blocks.14.
reconstructing layers.2.blocks.15 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.15 ...
wraping quantizers in layers.2.blocks.15 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.968 (rec:0.968, round:0.000)	b=0.00	count=500
Total loss:	0.777 (rec:0.777, round:0.000)	b=0.00	count=1000
Total loss:	0.716 (rec:0.716, round:0.000)	b=0.00	count=1500
Total loss:	0.646 (rec:0.646, round:0.000)	b=0.00	count=2000
Total loss:	0.659 (rec:0.659, round:0.000)	b=0.00	count=2500
Total loss:	0.614 (rec:0.614, round:0.000)	b=0.00	count=3000
Total loss:	0.547 (rec:0.547, round:0.000)	b=0.00	count=3500
Total loss:	28465.064 (rec:0.560, round:28464.504)	b=20.00	count=4000
Total loss:	13348.814 (rec:0.537, round:13348.277)	b=19.44	count=4500
Total loss:	12247.057 (rec:0.506, round:12246.551)	b=18.88	count=5000
Total loss:	11460.408 (rec:0.518, round:11459.891)	b=18.31	count=5500
Total loss:	10743.249 (rec:0.538, round:10742.711)	b=17.75	count=6000
Total loss:	10050.069 (rec:0.502, round:10049.567)	b=17.19	count=6500
Total loss:	9376.151 (rec:0.509, round:9375.642)	b=16.62	count=7000
Total loss:	8720.722 (rec:0.512, round:8720.209)	b=16.06	count=7500
Total loss:	8068.538 (rec:0.462, round:8068.076)	b=15.50	count=8000
Total loss:	7439.065 (rec:0.518, round:7438.548)	b=14.94	count=8500
Total loss:	6829.131 (rec:0.513, round:6828.618)	b=14.38	count=9000
Total loss:	6248.166 (rec:0.481, round:6247.685)	b=13.81	count=9500
Total loss:	5694.155 (rec:0.463, round:5693.692)	b=13.25	count=10000
Total loss:	5168.820 (rec:0.469, round:5168.351)	b=12.69	count=10500
Total loss:	4673.562 (rec:0.501, round:4673.061)	b=12.12	count=11000
Total loss:	4201.608 (rec:0.442, round:4201.166)	b=11.56	count=11500
Total loss:	3754.962 (rec:0.498, round:3754.463)	b=11.00	count=12000
Total loss:	3329.935 (rec:0.470, round:3329.465)	b=10.44	count=12500
Total loss:	2925.672 (rec:0.458, round:2925.214)	b=9.88	count=13000
Total loss:	2538.323 (rec:0.486, round:2537.837)	b=9.31	count=13500
Total loss:	2172.299 (rec:0.477, round:2171.823)	b=8.75	count=14000
Total loss:	1821.157 (rec:0.504, round:1820.652)	b=8.19	count=14500
Total loss:	1489.872 (rec:0.485, round:1489.386)	b=7.62	count=15000
Total loss:	1176.071 (rec:0.491, round:1175.581)	b=7.06	count=15500
Total loss:	884.545 (rec:0.495, round:884.049)	b=6.50	count=16000
Total loss:	624.108 (rec:0.508, round:623.600)	b=5.94	count=16500
Total loss:	406.359 (rec:0.499, round:405.860)	b=5.38	count=17000
Total loss:	233.242 (rec:0.506, round:232.736)	b=4.81	count=17500
Total loss:	107.829 (rec:0.479, round:107.350)	b=4.25	count=18000
Total loss:	30.334 (rec:0.504, round:29.830)	b=3.69	count=18500
Total loss:	4.188 (rec:0.501, round:3.687)	b=3.12	count=19000
Total loss:	0.692 (rec:0.512, round:0.180)	b=2.56	count=19500
Total loss:	0.489 (rec:0.486, round:0.003)	b=2.00	count=20000
finished reconstructing layers.2.blocks.15.
reconstructing layers.2.blocks.16 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.16 ...
wraping quantizers in layers.2.blocks.16 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.055 (rec:1.055, round:0.000)	b=0.00	count=500
Total loss:	1.027 (rec:1.027, round:0.000)	b=0.00	count=1000
Total loss:	0.853 (rec:0.853, round:0.000)	b=0.00	count=1500
Total loss:	0.878 (rec:0.878, round:0.000)	b=0.00	count=2000
Total loss:	0.779 (rec:0.779, round:0.000)	b=0.00	count=2500
Total loss:	0.761 (rec:0.761, round:0.000)	b=0.00	count=3000
Total loss:	0.794 (rec:0.794, round:0.000)	b=0.00	count=3500
Total loss:	28489.297 (rec:0.735, round:28488.562)	b=20.00	count=4000
Total loss:	13602.601 (rec:0.755, round:13601.846)	b=19.44	count=4500
Total loss:	12507.268 (rec:0.745, round:12506.522)	b=18.88	count=5000
Total loss:	11734.621 (rec:0.699, round:11733.922)	b=18.31	count=5500
Total loss:	11045.377 (rec:0.650, round:11044.727)	b=17.75	count=6000
Total loss:	10388.319 (rec:0.698, round:10387.621)	b=17.19	count=6500
Total loss:	9755.619 (rec:0.674, round:9754.945)	b=16.62	count=7000
Total loss:	9140.462 (rec:0.708, round:9139.754)	b=16.06	count=7500
Total loss:	8539.597 (rec:0.689, round:8538.908)	b=15.50	count=8000
Total loss:	7956.145 (rec:0.660, round:7955.484)	b=14.94	count=8500
Total loss:	7395.746 (rec:0.673, round:7395.073)	b=14.38	count=9000
Total loss:	6853.885 (rec:0.673, round:6853.212)	b=13.81	count=9500
Total loss:	6326.938 (rec:0.676, round:6326.263)	b=13.25	count=10000
Total loss:	5818.118 (rec:0.643, round:5817.475)	b=12.69	count=10500
Total loss:	5323.553 (rec:0.696, round:5322.857)	b=12.12	count=11000
Total loss:	4847.781 (rec:0.664, round:4847.117)	b=11.56	count=11500
Total loss:	4388.373 (rec:0.659, round:4387.714)	b=11.00	count=12000
Total loss:	3944.330 (rec:0.666, round:3943.664)	b=10.44	count=12500
Total loss:	3514.228 (rec:0.666, round:3513.562)	b=9.88	count=13000
Total loss:	3095.430 (rec:0.654, round:3094.776)	b=9.31	count=13500
Total loss:	2690.939 (rec:0.702, round:2690.237)	b=8.75	count=14000
Total loss:	2297.099 (rec:0.649, round:2296.449)	b=8.19	count=14500
Total loss:	1915.152 (rec:0.679, round:1914.473)	b=7.62	count=15000
Total loss:	1553.204 (rec:0.633, round:1552.571)	b=7.06	count=15500
Total loss:	1210.149 (rec:0.643, round:1209.506)	b=6.50	count=16000
Total loss:	895.501 (rec:0.684, round:894.817)	b=5.94	count=16500
Total loss:	615.391 (rec:0.644, round:614.747)	b=5.38	count=17000
Total loss:	378.697 (rec:0.657, round:378.040)	b=4.81	count=17500
Total loss:	197.509 (rec:0.676, round:196.833)	b=4.25	count=18000
Total loss:	74.267 (rec:0.663, round:73.604)	b=3.69	count=18500
Total loss:	14.994 (rec:0.647, round:14.346)	b=3.12	count=19000
Total loss:	1.715 (rec:0.684, round:1.031)	b=2.56	count=19500
Total loss:	0.721 (rec:0.689, round:0.032)	b=2.00	count=20000
finished reconstructing layers.2.blocks.16.
reconstructing layers.2.blocks.17 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.17 ...
wraping quantizers in layers.2.blocks.17 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.250 (rec:1.250, round:0.000)	b=0.00	count=500
Total loss:	1.188 (rec:1.188, round:0.000)	b=0.00	count=1000
Total loss:	1.030 (rec:1.030, round:0.000)	b=0.00	count=1500
Total loss:	1.024 (rec:1.024, round:0.000)	b=0.00	count=2000
Total loss:	1.038 (rec:1.038, round:0.000)	b=0.00	count=2500
Total loss:	0.996 (rec:0.996, round:0.000)	b=0.00	count=3000
Total loss:	0.945 (rec:0.945, round:0.000)	b=0.00	count=3500
Total loss:	28354.510 (rec:0.976, round:28353.533)	b=20.00	count=4000
Total loss:	13562.502 (rec:0.922, round:13561.580)	b=19.44	count=4500
Total loss:	12486.384 (rec:0.916, round:12485.468)	b=18.88	count=5000
Total loss:	11745.682 (rec:0.961, round:11744.721)	b=18.31	count=5500
Total loss:	11098.626 (rec:0.840, round:11097.786)	b=17.75	count=6000
Total loss:	10494.050 (rec:0.819, round:10493.230)	b=17.19	count=6500
Total loss:	9907.609 (rec:0.879, round:9906.730)	b=16.62	count=7000
Total loss:	9332.298 (rec:0.847, round:9331.450)	b=16.06	count=7500
Total loss:	8770.455 (rec:0.870, round:8769.585)	b=15.50	count=8000
Total loss:	8224.977 (rec:0.860, round:8224.116)	b=14.94	count=8500
Total loss:	7694.256 (rec:0.814, round:7693.442)	b=14.38	count=9000
Total loss:	7182.296 (rec:0.973, round:7181.323)	b=13.81	count=9500
Total loss:	6684.011 (rec:0.816, round:6683.194)	b=13.25	count=10000
Total loss:	6197.361 (rec:0.828, round:6196.533)	b=12.69	count=10500
Total loss:	5715.806 (rec:0.925, round:5714.882)	b=12.12	count=11000
Total loss:	5248.568 (rec:0.862, round:5247.707)	b=11.56	count=11500
Total loss:	4794.648 (rec:0.797, round:4793.851)	b=11.00	count=12000
Total loss:	4349.885 (rec:0.821, round:4349.064)	b=10.44	count=12500
Total loss:	3911.523 (rec:0.854, round:3910.669)	b=9.88	count=13000
Total loss:	3479.268 (rec:0.799, round:3478.469)	b=9.31	count=13500
Total loss:	3054.997 (rec:0.855, round:3054.142)	b=8.75	count=14000
Total loss:	2639.271 (rec:0.809, round:2638.462)	b=8.19	count=14500
Total loss:	2234.149 (rec:0.850, round:2233.300)	b=7.62	count=15000
Total loss:	1840.698 (rec:0.777, round:1839.922)	b=7.06	count=15500
Total loss:	1467.234 (rec:0.810, round:1466.424)	b=6.50	count=16000
Total loss:	1112.471 (rec:0.878, round:1111.593)	b=5.94	count=16500
Total loss:	789.055 (rec:0.769, round:788.286)	b=5.38	count=17000
Total loss:	508.216 (rec:0.784, round:507.432)	b=4.81	count=17500
Total loss:	282.964 (rec:0.805, round:282.159)	b=4.25	count=18000
Total loss:	120.708 (rec:0.806, round:119.901)	b=3.69	count=18500
Total loss:	28.152 (rec:0.853, round:27.299)	b=3.12	count=19000
Total loss:	2.550 (rec:0.860, round:1.690)	b=2.56	count=19500
Total loss:	0.890 (rec:0.844, round:0.046)	b=2.00	count=20000
finished reconstructing layers.2.blocks.17.
reconstructing layers.3.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.downsample ...
wraping quantizers in layers.3.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.543 (rec:1.543, round:0.000)	b=0.00	count=500
Total loss:	1.250 (rec:1.250, round:0.000)	b=0.00	count=1000
Total loss:	1.263 (rec:1.263, round:0.000)	b=0.00	count=1500
Total loss:	1.234 (rec:1.234, round:0.000)	b=0.00	count=2000
Total loss:	1.185 (rec:1.185, round:0.000)	b=0.00	count=2500
Total loss:	1.101 (rec:1.101, round:0.000)	b=0.00	count=3000
Total loss:	1.147 (rec:1.147, round:0.000)	b=0.00	count=3500
Total loss:	19008.539 (rec:1.264, round:19007.275)	b=20.00	count=4000
Total loss:	9071.187 (rec:1.091, round:9070.096)	b=19.44	count=4500
Total loss:	8394.112 (rec:1.174, round:8392.938)	b=18.88	count=5000
Total loss:	7958.776 (rec:0.972, round:7957.804)	b=18.31	count=5500
Total loss:	7589.917 (rec:1.043, round:7588.875)	b=17.75	count=6000
Total loss:	7249.098 (rec:1.084, round:7248.014)	b=17.19	count=6500
Total loss:	6929.235 (rec:1.136, round:6928.100)	b=16.62	count=7000
Total loss:	6620.856 (rec:1.036, round:6619.820)	b=16.06	count=7500
Total loss:	6321.244 (rec:1.067, round:6320.177)	b=15.50	count=8000
Total loss:	6022.045 (rec:1.010, round:6021.036)	b=14.94	count=8500
Total loss:	5729.532 (rec:0.973, round:5728.559)	b=14.38	count=9000
Total loss:	5437.214 (rec:1.085, round:5436.129)	b=13.81	count=9500
Total loss:	5146.181 (rec:1.210, round:5144.971)	b=13.25	count=10000
Total loss:	4849.191 (rec:1.049, round:4848.143)	b=12.69	count=10500
Total loss:	4551.404 (rec:1.015, round:4550.390)	b=12.12	count=11000
Total loss:	4249.048 (rec:1.136, round:4247.913)	b=11.56	count=11500
Total loss:	3944.978 (rec:1.122, round:3943.855)	b=11.00	count=12000
Total loss:	3629.994 (rec:1.055, round:3628.939)	b=10.44	count=12500
Total loss:	3310.269 (rec:1.167, round:3309.103)	b=9.88	count=13000
Total loss:	2984.259 (rec:1.138, round:2983.121)	b=9.31	count=13500
Total loss:	2653.004 (rec:1.082, round:2651.921)	b=8.75	count=14000
Total loss:	2313.368 (rec:1.122, round:2312.246)	b=8.19	count=14500
Total loss:	1971.121 (rec:1.151, round:1969.970)	b=7.62	count=15000
Total loss:	1620.965 (rec:1.026, round:1619.939)	b=7.06	count=15500
Total loss:	1269.438 (rec:1.077, round:1268.362)	b=6.50	count=16000
Total loss:	925.180 (rec:1.169, round:924.011)	b=5.94	count=16500
Total loss:	588.379 (rec:1.064, round:587.315)	b=5.38	count=17000
Total loss:	292.661 (rec:1.053, round:291.608)	b=4.81	count=17500
Total loss:	100.700 (rec:1.033, round:99.668)	b=4.25	count=18000
Total loss:	22.344 (rec:1.058, round:21.286)	b=3.69	count=18500
Total loss:	3.528 (rec:1.034, round:2.494)	b=3.12	count=19000
Total loss:	1.171 (rec:0.984, round:0.187)	b=2.56	count=19500
Total loss:	1.138 (rec:1.130, round:0.008)	b=2.00	count=20000
finished reconstructing layers.3.downsample.
reconstructing layers.3.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.blocks.0 ...
wraping quantizers in layers.3.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.279 (rec:1.279, round:0.000)	b=0.00	count=500
Total loss:	1.123 (rec:1.123, round:0.000)	b=0.00	count=1000
Total loss:	1.055 (rec:1.055, round:0.000)	b=0.00	count=1500
Total loss:	0.908 (rec:0.908, round:0.000)	b=0.00	count=2000
Total loss:	0.890 (rec:0.890, round:0.000)	b=0.00	count=2500
Total loss:	0.832 (rec:0.832, round:0.000)	b=0.00	count=3000
Total loss:	0.844 (rec:0.844, round:0.000)	b=0.00	count=3500
Total loss:	117509.781 (rec:0.802, round:117508.977)	b=20.00	count=4000
Total loss:	57009.715 (rec:0.790, round:57008.926)	b=19.44	count=4500
Total loss:	52847.719 (rec:0.756, round:52846.961)	b=18.88	count=5000
Total loss:	50165.262 (rec:0.742, round:50164.520)	b=18.31	count=5500
Total loss:	47866.062 (rec:0.748, round:47865.316)	b=17.75	count=6000
Total loss:	45725.004 (rec:0.745, round:45724.258)	b=17.19	count=6500
Total loss:	43652.535 (rec:0.702, round:43651.832)	b=16.62	count=7000
Total loss:	41600.555 (rec:0.700, round:41599.855)	b=16.06	count=7500
Total loss:	39574.379 (rec:0.705, round:39573.672)	b=15.50	count=8000
Total loss:	37548.570 (rec:0.660, round:37547.910)	b=14.94	count=8500
Total loss:	35526.086 (rec:0.626, round:35525.461)	b=14.38	count=9000
Total loss:	33496.180 (rec:0.684, round:33495.496)	b=13.81	count=9500
Total loss:	31467.795 (rec:0.642, round:31467.152)	b=13.25	count=10000
Total loss:	29432.994 (rec:0.679, round:29432.314)	b=12.69	count=10500
Total loss:	27388.840 (rec:0.655, round:27388.186)	b=12.12	count=11000
Total loss:	25342.617 (rec:0.640, round:25341.977)	b=11.56	count=11500
Total loss:	23288.418 (rec:0.662, round:23287.756)	b=11.00	count=12000
Total loss:	21236.916 (rec:0.645, round:21236.271)	b=10.44	count=12500
Total loss:	19180.918 (rec:0.626, round:19180.293)	b=9.88	count=13000
Total loss:	17130.225 (rec:0.650, round:17129.574)	b=9.31	count=13500
Total loss:	15087.936 (rec:0.598, round:15087.337)	b=8.75	count=14000
Total loss:	13061.371 (rec:0.627, round:13060.744)	b=8.19	count=14500
Total loss:	11063.773 (rec:0.645, round:11063.128)	b=7.62	count=15000
Total loss:	9104.424 (rec:0.650, round:9103.774)	b=7.06	count=15500
Total loss:	7215.027 (rec:0.629, round:7214.397)	b=6.50	count=16000
Total loss:	5422.124 (rec:0.626, round:5421.498)	b=5.94	count=16500
Total loss:	3760.030 (rec:0.608, round:3759.422)	b=5.38	count=17000
Total loss:	2302.157 (rec:0.612, round:2301.545)	b=4.81	count=17500
Total loss:	1108.238 (rec:0.623, round:1107.615)	b=4.25	count=18000
Total loss:	304.181 (rec:0.607, round:303.574)	b=3.69	count=18500
Total loss:	31.968 (rec:0.654, round:31.315)	b=3.12	count=19000
Total loss:	1.906 (rec:0.605, round:1.301)	b=2.56	count=19500
Total loss:	0.666 (rec:0.648, round:0.017)	b=2.00	count=20000
finished reconstructing layers.3.blocks.0.
reconstructing layers.3.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.blocks.1 ...
wraping quantizers in layers.3.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.055 (rec:1.055, round:0.000)	b=0.00	count=500
Total loss:	0.958 (rec:0.958, round:0.000)	b=0.00	count=1000
Total loss:	0.821 (rec:0.821, round:0.000)	b=0.00	count=1500
Total loss:	0.779 (rec:0.779, round:0.000)	b=0.00	count=2000
Total loss:	0.715 (rec:0.715, round:0.000)	b=0.00	count=2500
Total loss:	0.731 (rec:0.731, round:0.000)	b=0.00	count=3000
Total loss:	0.687 (rec:0.687, round:0.000)	b=0.00	count=3500
Total loss:	116073.250 (rec:0.614, round:116072.633)	b=20.00	count=4000
Total loss:	54850.070 (rec:0.550, round:54849.520)	b=19.44	count=4500
Total loss:	50700.191 (rec:0.625, round:50699.566)	b=18.88	count=5000
Total loss:	47971.035 (rec:0.591, round:47970.445)	b=18.31	count=5500
Total loss:	45597.848 (rec:0.620, round:45597.227)	b=17.75	count=6000
Total loss:	43363.375 (rec:0.623, round:43362.750)	b=17.19	count=6500
Total loss:	41190.984 (rec:0.587, round:41190.398)	b=16.62	count=7000
Total loss:	39056.586 (rec:0.556, round:39056.031)	b=16.06	count=7500
Total loss:	36945.930 (rec:0.585, round:36945.344)	b=15.50	count=8000
Total loss:	34847.863 (rec:0.543, round:34847.320)	b=14.94	count=8500
Total loss:	32756.412 (rec:0.571, round:32755.840)	b=14.38	count=9000
Total loss:	30669.637 (rec:0.512, round:30669.125)	b=13.81	count=9500
Total loss:	28590.570 (rec:0.513, round:28590.057)	b=13.25	count=10000
Total loss:	26522.291 (rec:0.534, round:26521.758)	b=12.69	count=10500
Total loss:	24468.789 (rec:0.496, round:24468.293)	b=12.12	count=11000
Total loss:	22432.066 (rec:0.534, round:22431.533)	b=11.56	count=11500
Total loss:	20417.586 (rec:0.524, round:20417.062)	b=11.00	count=12000
Total loss:	18427.539 (rec:0.502, round:18427.037)	b=10.44	count=12500
Total loss:	16457.219 (rec:0.487, round:16456.732)	b=9.88	count=13000
Total loss:	14526.903 (rec:0.475, round:14526.428)	b=9.31	count=13500
Total loss:	12628.493 (rec:0.534, round:12627.959)	b=8.75	count=14000
Total loss:	10769.530 (rec:0.454, round:10769.076)	b=8.19	count=14500
Total loss:	8967.370 (rec:0.501, round:8966.869)	b=7.62	count=15000
Total loss:	7225.587 (rec:0.524, round:7225.064)	b=7.06	count=15500
Total loss:	5559.161 (rec:0.552, round:5558.608)	b=6.50	count=16000
Total loss:	4006.521 (rec:0.499, round:4006.022)	b=5.94	count=16500
Total loss:	2611.010 (rec:0.509, round:2610.501)	b=5.38	count=17000
Total loss:	1466.125 (rec:0.510, round:1465.615)	b=4.81	count=17500
Total loss:	622.849 (rec:0.498, round:622.351)	b=4.25	count=18000
Total loss:	137.596 (rec:0.507, round:137.089)	b=3.69	count=18500
Total loss:	12.977 (rec:0.515, round:12.462)	b=3.12	count=19000
Total loss:	1.120 (rec:0.537, round:0.583)	b=2.56	count=19500
Total loss:	0.524 (rec:0.505, round:0.019)	b=2.00	count=20000
finished reconstructing layers.3.blocks.1.
reconstructing head ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for head ...
wraping quantizers in head ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.557 (rec:1.557, round:0.000)	b=0.00	count=500
Total loss:	1.700 (rec:1.700, round:0.000)	b=0.00	count=1000
Total loss:	1.425 (rec:1.425, round:0.000)	b=0.00	count=1500
Total loss:	1.266 (rec:1.266, round:0.000)	b=0.00	count=2000
Total loss:	0.962 (rec:0.962, round:0.000)	b=0.00	count=2500
Total loss:	0.980 (rec:0.980, round:0.000)	b=0.00	count=3000
Total loss:	0.899 (rec:0.899, round:0.000)	b=0.00	count=3500
Total loss:	8824.433 (rec:0.666, round:8823.767)	b=20.00	count=4000
Total loss:	4415.192 (rec:0.734, round:4414.458)	b=19.44	count=4500
Total loss:	4037.043 (rec:0.646, round:4036.398)	b=18.88	count=5000
Total loss:	3769.588 (rec:0.634, round:3768.955)	b=18.31	count=5500
Total loss:	3529.705 (rec:0.540, round:3529.166)	b=17.75	count=6000
Total loss:	3303.234 (rec:0.503, round:3302.730)	b=17.19	count=6500
Total loss:	3083.541 (rec:0.559, round:3082.982)	b=16.62	count=7000
Total loss:	2865.990 (rec:0.422, round:2865.568)	b=16.06	count=7500
Total loss:	2655.528 (rec:0.361, round:2655.167)	b=15.50	count=8000
Total loss:	2446.769 (rec:0.304, round:2446.466)	b=14.94	count=8500
Total loss:	2245.113 (rec:0.281, round:2244.832)	b=14.38	count=9000
Total loss:	2050.633 (rec:0.393, round:2050.239)	b=13.81	count=9500
Total loss:	1862.272 (rec:0.236, round:1862.036)	b=13.25	count=10000
Total loss:	1679.469 (rec:0.249, round:1679.220)	b=12.69	count=10500
Total loss:	1508.706 (rec:0.233, round:1508.473)	b=12.12	count=11000
Total loss:	1344.100 (rec:0.270, round:1343.830)	b=11.56	count=11500
Total loss:	1191.912 (rec:0.232, round:1191.680)	b=11.00	count=12000
Total loss:	1048.946 (rec:0.305, round:1048.640)	b=10.44	count=12500
Total loss:	914.616 (rec:0.226, round:914.390)	b=9.88	count=13000
Total loss:	788.504 (rec:0.196, round:788.308)	b=9.31	count=13500
Total loss:	671.838 (rec:0.178, round:671.660)	b=8.75	count=14000
Total loss:	563.062 (rec:0.217, round:562.845)	b=8.19	count=14500
Total loss:	462.937 (rec:0.191, round:462.747)	b=7.62	count=15000
Total loss:	372.061 (rec:0.194, round:371.867)	b=7.06	count=15500
Total loss:	290.385 (rec:0.147, round:290.237)	b=6.50	count=16000
Total loss:	217.226 (rec:0.180, round:217.045)	b=5.94	count=16500
Total loss:	152.559 (rec:0.213, round:152.346)	b=5.38	count=17000
Total loss:	97.558 (rec:0.167, round:97.390)	b=4.81	count=17500
Total loss:	54.567 (rec:0.126, round:54.441)	b=4.25	count=18000
Total loss:	23.658 (rec:0.237, round:23.421)	b=3.69	count=18500
Total loss:	6.149 (rec:0.167, round:5.982)	b=3.12	count=19000
Total loss:	0.762 (rec:0.218, round:0.545)	b=2.56	count=19500
Total loss:	0.205 (rec:0.194, round:0.011)	b=2.00	count=20000
finished reconstructing head.
2025-09-11 15:43:25 - mse guided block reconstruction finished.
Saving checkpoint to ./checkpoint/quant_result/20250911_1053/swin_base_w2_a2_optimsize_1024_mse_qdrop.pth
Validating on calibration set after block reconstruction ...
Test: [0/32]	Time 0.633 (0.633)	Loss 3.5608 (3.5608)	Prec@1 81.250 (81.250)	Prec@5 93.750 (93.750)
Test: [10/32]	Time 0.097 (0.146)	Loss 3.2429 (3.3726)	Prec@1 78.125 (79.545)	Prec@5 93.750 (92.614)
Test: [20/32]	Time 0.097 (0.122)	Loss 3.5578 (3.4566)	Prec@1 81.250 (79.464)	Prec@5 93.750 (93.155)
Test: [30/32]	Time 0.097 (0.114)	Loss 3.8694 (3.4699)	Prec@1 62.500 (79.940)	Prec@5 90.625 (93.145)
 * Prec@1 80.078 Prec@5 93.066 Loss 3.459 Time 3.769
Validating on test set after block reconstruction ...
Test: [0/100]	Time 5.665 (5.665)	Loss 5.7543 (5.7543)	Prec@1 16.200 (16.200)	Prec@5 36.600 (36.600)
Test: [10/100]	Time 2.351 (2.652)	Loss 5.5020 (5.5690)	Prec@1 24.200 (17.691)	Prec@5 39.800 (39.036)
Test: [20/100]	Time 2.365 (2.511)	Loss 4.4581 (5.2947)	Prec@1 18.600 (18.952)	Prec@5 55.600 (42.705)
Test: [30/100]	Time 2.360 (2.462)	Loss 4.9862 (5.0897)	Prec@1 23.000 (20.787)	Prec@5 50.400 (46.271)
Test: [40/100]	Time 2.360 (2.436)	Loss 5.2276 (5.2494)	Prec@1 16.800 (19.590)	Prec@5 39.000 (42.746)
Test: [50/100]	Time 2.355 (2.421)	Loss 5.6985 (5.3299)	Prec@1 14.000 (18.373)	Prec@5 30.000 (40.275)
Test: [60/100]	Time 2.356 (2.411)	Loss 5.5976 (5.3875)	Prec@1 16.400 (17.580)	Prec@5 29.600 (38.308)
Test: [70/100]	Time 2.362 (2.404)	Loss 5.6175 (5.4285)	Prec@1 6.200 (16.814)	Prec@5 26.400 (36.817)
Test: [80/100]	Time 2.363 (2.398)	Loss 6.0222 (5.4718)	Prec@1 4.800 (16.168)	Prec@5 14.200 (35.526)
Test: [90/100]	Time 2.361 (2.394)	Loss 5.8083 (5.4981)	Prec@1 10.800 (15.743)	Prec@5 18.800 (34.613)
 * Prec@1 15.696 Prec@5 34.488 Loss 5.518 Time 239.356
2025-09-11 15:47:28 - finished the process.
Extracting logits from quantized and full-precision models...
Testing combinations:
  Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
  Cluster numbers: [8, 16, 32, 64, 128, 256]
  PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.70%
[Alpha=0.10] Top-5 Accuracy: 34.55%
Result: Top-1: 15.70%, Top-5: 34.55%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.58%
Result: Top-1: 15.74%, Top-5: 34.58%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.75%
[Alpha=0.10] Top-5 Accuracy: 34.56%
Result: Top-1: 15.75%, Top-5: 34.56%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.55%
Result: Top-1: 15.74%, Top-5: 34.55%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.57%
Result: Top-1: 15.74%, Top-5: 34.57%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.59%
Result: Top-1: 15.74%, Top-5: 34.59%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.56%
Result: Top-1: 15.74%, Top-5: 34.56%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.53%
Result: Top-1: 15.74%, Top-5: 34.53%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.76%
[Alpha=0.10] Top-5 Accuracy: 34.58%
Result: Top-1: 15.76%, Top-5: 34.58%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.76%
[Alpha=0.10] Top-5 Accuracy: 34.58%
Result: Top-1: 15.76%, Top-5: 34.58%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.73%
[Alpha=0.10] Top-5 Accuracy: 34.54%
Result: Top-1: 15.73%, Top-5: 34.54%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.76%
[Alpha=0.10] Top-5 Accuracy: 34.58%
Result: Top-1: 15.76%, Top-5: 34.58%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.78%
[Alpha=0.10] Top-5 Accuracy: 34.62%
Result: Top-1: 15.78%, Top-5: 34.62%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.78%
[Alpha=0.10] Top-5 Accuracy: 34.61%
Result: Top-1: 15.78%, Top-5: 34.61%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.62%
Result: Top-1: 15.74%, Top-5: 34.62%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.76%
[Alpha=0.10] Top-5 Accuracy: 34.60%
Result: Top-1: 15.76%, Top-5: 34.60%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.61%
Result: Top-1: 15.74%, Top-5: 34.61%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.77%
[Alpha=0.10] Top-5 Accuracy: 34.59%
Result: Top-1: 15.77%, Top-5: 34.59%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.75%
[Alpha=0.10] Top-5 Accuracy: 34.64%
Result: Top-1: 15.75%, Top-5: 34.64%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.76%
[Alpha=0.10] Top-5 Accuracy: 34.59%
Result: Top-1: 15.76%, Top-5: 34.59%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.71%
[Alpha=0.10] Top-5 Accuracy: 34.56%
Result: Top-1: 15.71%, Top-5: 34.56%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.77%
[Alpha=0.10] Top-5 Accuracy: 34.60%
Result: Top-1: 15.77%, Top-5: 34.60%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.76%
[Alpha=0.10] Top-5 Accuracy: 34.55%
Result: Top-1: 15.76%, Top-5: 34.55%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.80%
[Alpha=0.10] Top-5 Accuracy: 34.59%
Result: Top-1: 15.80%, Top-5: 34.59%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.61%
Result: Top-1: 15.74%, Top-5: 34.61%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.72%
[Alpha=0.10] Top-5 Accuracy: 34.58%
Result: Top-1: 15.72%, Top-5: 34.58%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.77%
[Alpha=0.10] Top-5 Accuracy: 34.54%
Result: Top-1: 15.77%, Top-5: 34.54%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.81%
[Alpha=0.10] Top-5 Accuracy: 34.60%
Result: Top-1: 15.81%, Top-5: 34.60%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.76%
[Alpha=0.10] Top-5 Accuracy: 34.58%
Result: Top-1: 15.76%, Top-5: 34.58%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.78%
[Alpha=0.10] Top-5 Accuracy: 34.54%
Result: Top-1: 15.78%, Top-5: 34.54%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.66%
[Alpha=0.10] Top-5 Accuracy: 34.57%
Result: Top-1: 15.66%, Top-5: 34.57%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.69%
[Alpha=0.10] Top-5 Accuracy: 34.54%
Result: Top-1: 15.69%, Top-5: 34.54%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.70%
[Alpha=0.10] Top-5 Accuracy: 34.49%
Result: Top-1: 15.70%, Top-5: 34.49%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.75%
[Alpha=0.10] Top-5 Accuracy: 34.53%
Result: Top-1: 15.75%, Top-5: 34.53%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.53%
Result: Top-1: 15.74%, Top-5: 34.53%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.73%
[Alpha=0.10] Top-5 Accuracy: 34.58%
Result: Top-1: 15.73%, Top-5: 34.58%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.73%
[Alpha=0.10] Top-5 Accuracy: 34.50%
Result: Top-1: 15.73%, Top-5: 34.50%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.75%
[Alpha=0.10] Top-5 Accuracy: 34.62%
Result: Top-1: 15.75%, Top-5: 34.62%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.74%
[Alpha=0.10] Top-5 Accuracy: 34.60%
Result: Top-1: 15.74%, Top-5: 34.60%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.72%
[Alpha=0.10] Top-5 Accuracy: 34.55%
Result: Top-1: 15.72%, Top-5: 34.55%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.72%
[Alpha=0.10] Top-5 Accuracy: 34.51%
Result: Top-1: 15.72%, Top-5: 34.51%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.69%
[Alpha=0.10] Top-5 Accuracy: 34.54%
Result: Top-1: 15.69%, Top-5: 34.54%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.69%
[Alpha=0.10] Top-5 Accuracy: 34.53%
Result: Top-1: 15.69%, Top-5: 34.53%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.76%
[Alpha=0.10] Top-5 Accuracy: 34.54%
Result: Top-1: 15.76%, Top-5: 34.54%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.65%
[Alpha=0.10] Top-5 Accuracy: 34.53%
Result: Top-1: 15.65%, Top-5: 34.53%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.69%
[Alpha=0.10] Top-5 Accuracy: 34.55%
Result: Top-1: 15.69%, Top-5: 34.55%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.72%
[Alpha=0.10] Top-5 Accuracy: 34.46%
Result: Top-1: 15.72%, Top-5: 34.46%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.65%
[Alpha=0.10] Top-5 Accuracy: 34.52%
Result: Top-1: 15.65%, Top-5: 34.52%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.45%
[Alpha=0.10] Top-5 Accuracy: 34.35%
Result: Top-1: 15.45%, Top-5: 34.35%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.70%
[Alpha=0.10] Top-5 Accuracy: 34.50%
Result: Top-1: 15.70%, Top-5: 34.50%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.71%
[Alpha=0.10] Top-5 Accuracy: 34.48%
Result: Top-1: 15.71%, Top-5: 34.48%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.48%
[Alpha=0.10] Top-5 Accuracy: 34.24%
Result: Top-1: 15.48%, Top-5: 34.24%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 14.95%
[Alpha=0.10] Top-5 Accuracy: 33.29%
Result: Top-1: 14.95%, Top-5: 33.29%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.08%
[Alpha=0.10] Top-5 Accuracy: 33.38%
Result: Top-1: 15.08%, Top-5: 33.38%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.11%
[Alpha=0.10] Top-5 Accuracy: 33.30%
Result: Top-1: 15.11%, Top-5: 33.30%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.31%
[Alpha=0.10] Top-5 Accuracy: 33.83%
Result: Top-1: 15.31%, Top-5: 33.83%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.58%
[Alpha=0.10] Top-5 Accuracy: 34.19%
Result: Top-1: 15.58%, Top-5: 34.19%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.67%
[Alpha=0.10] Top-5 Accuracy: 34.33%
Result: Top-1: 15.67%, Top-5: 34.33%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.47%
[Alpha=0.10] Top-5 Accuracy: 34.22%
Result: Top-1: 15.47%, Top-5: 34.22%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 15.48%
[Alpha=0.10] Top-5 Accuracy: 34.13%
Result: Top-1: 15.48%, Top-5: 34.13%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.60%
[Alpha=0.20] Top-5 Accuracy: 34.59%
Result: Top-1: 15.60%, Top-5: 34.59%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.60%
[Alpha=0.20] Top-5 Accuracy: 34.59%
Result: Top-1: 15.60%, Top-5: 34.59%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.54%
[Alpha=0.20] Top-5 Accuracy: 34.54%
Result: Top-1: 15.54%, Top-5: 34.54%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.55%
[Alpha=0.20] Top-5 Accuracy: 34.55%
Result: Top-1: 15.55%, Top-5: 34.55%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.52%
[Alpha=0.20] Top-5 Accuracy: 34.54%
Result: Top-1: 15.52%, Top-5: 34.54%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.50%
[Alpha=0.20] Top-5 Accuracy: 34.54%
Result: Top-1: 15.50%, Top-5: 34.54%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.55%
[Alpha=0.20] Top-5 Accuracy: 34.54%
Result: Top-1: 15.55%, Top-5: 34.54%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.53%
[Alpha=0.20] Top-5 Accuracy: 34.54%
Result: Top-1: 15.53%, Top-5: 34.54%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.60%
[Alpha=0.20] Top-5 Accuracy: 34.47%
Result: Top-1: 15.60%, Top-5: 34.47%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.60%
[Alpha=0.20] Top-5 Accuracy: 34.54%
Result: Top-1: 15.60%, Top-5: 34.54%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.62%
[Alpha=0.20] Top-5 Accuracy: 34.58%
Result: Top-1: 15.62%, Top-5: 34.58%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.61%
[Alpha=0.20] Top-5 Accuracy: 34.55%
Result: Top-1: 15.61%, Top-5: 34.55%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.61%
[Alpha=0.20] Top-5 Accuracy: 34.60%
Result: Top-1: 15.61%, Top-5: 34.60%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.51%
[Alpha=0.20] Top-5 Accuracy: 34.56%
Result: Top-1: 15.51%, Top-5: 34.56%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.60%
[Alpha=0.20] Top-5 Accuracy: 34.60%
Result: Top-1: 15.60%, Top-5: 34.60%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.57%
[Alpha=0.20] Top-5 Accuracy: 34.57%
Result: Top-1: 15.57%, Top-5: 34.57%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.62%
[Alpha=0.20] Top-5 Accuracy: 34.61%
Result: Top-1: 15.62%, Top-5: 34.61%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.60%
[Alpha=0.20] Top-5 Accuracy: 34.57%
Result: Top-1: 15.60%, Top-5: 34.57%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.61%
[Alpha=0.20] Top-5 Accuracy: 34.59%
Result: Top-1: 15.61%, Top-5: 34.59%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.58%
[Alpha=0.20] Top-5 Accuracy: 34.54%
Result: Top-1: 15.58%, Top-5: 34.54%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.62%
[Alpha=0.20] Top-5 Accuracy: 34.55%
Result: Top-1: 15.62%, Top-5: 34.55%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.55%
[Alpha=0.20] Top-5 Accuracy: 34.54%
Result: Top-1: 15.55%, Top-5: 34.54%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.58%
[Alpha=0.20] Top-5 Accuracy: 34.40%
Result: Top-1: 15.58%, Top-5: 34.40%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.69%
[Alpha=0.20] Top-5 Accuracy: 34.46%
Result: Top-1: 15.69%, Top-5: 34.46%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.56%
[Alpha=0.20] Top-5 Accuracy: 34.39%
Result: Top-1: 15.56%, Top-5: 34.39%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.53%
[Alpha=0.20] Top-5 Accuracy: 34.39%
Result: Top-1: 15.53%, Top-5: 34.39%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.52%
[Alpha=0.20] Top-5 Accuracy: 34.41%
Result: Top-1: 15.52%, Top-5: 34.41%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.59%
[Alpha=0.20] Top-5 Accuracy: 34.45%
Result: Top-1: 15.59%, Top-5: 34.45%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.58%
[Alpha=0.20] Top-5 Accuracy: 34.44%
Result: Top-1: 15.58%, Top-5: 34.44%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.62%
[Alpha=0.20] Top-5 Accuracy: 34.46%
Result: Top-1: 15.62%, Top-5: 34.46%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.42%
[Alpha=0.20] Top-5 Accuracy: 34.53%
Result: Top-1: 15.42%, Top-5: 34.53%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.43%
[Alpha=0.20] Top-5 Accuracy: 34.26%
Result: Top-1: 15.43%, Top-5: 34.26%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.40%
[Alpha=0.20] Top-5 Accuracy: 34.34%
Result: Top-1: 15.40%, Top-5: 34.34%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.43%
[Alpha=0.20] Top-5 Accuracy: 34.30%
Result: Top-1: 15.43%, Top-5: 34.30%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.46%
[Alpha=0.20] Top-5 Accuracy: 34.30%
Result: Top-1: 15.46%, Top-5: 34.30%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.42%
[Alpha=0.20] Top-5 Accuracy: 34.30%
Result: Top-1: 15.42%, Top-5: 34.30%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.45%
[Alpha=0.20] Top-5 Accuracy: 34.37%
Result: Top-1: 15.45%, Top-5: 34.37%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.39%
[Alpha=0.20] Top-5 Accuracy: 34.35%
Result: Top-1: 15.39%, Top-5: 34.35%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.39%
[Alpha=0.20] Top-5 Accuracy: 34.32%
Result: Top-1: 15.39%, Top-5: 34.32%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.51%
[Alpha=0.20] Top-5 Accuracy: 34.42%
Result: Top-1: 15.51%, Top-5: 34.42%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.46%
[Alpha=0.20] Top-5 Accuracy: 34.38%
Result: Top-1: 15.46%, Top-5: 34.38%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.32%
[Alpha=0.20] Top-5 Accuracy: 34.14%
Result: Top-1: 15.32%, Top-5: 34.14%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.26%
[Alpha=0.20] Top-5 Accuracy: 34.23%
Result: Top-1: 15.26%, Top-5: 34.23%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.35%
[Alpha=0.20] Top-5 Accuracy: 34.24%
Result: Top-1: 15.35%, Top-5: 34.24%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.33%
[Alpha=0.20] Top-5 Accuracy: 34.26%
Result: Top-1: 15.33%, Top-5: 34.26%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.23%
[Alpha=0.20] Top-5 Accuracy: 34.32%
Result: Top-1: 15.23%, Top-5: 34.32%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.30%
[Alpha=0.20] Top-5 Accuracy: 34.12%
Result: Top-1: 15.30%, Top-5: 34.12%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.25%
[Alpha=0.20] Top-5 Accuracy: 34.15%
Result: Top-1: 15.25%, Top-5: 34.15%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.02%
[Alpha=0.20] Top-5 Accuracy: 33.78%
Result: Top-1: 15.02%, Top-5: 33.78%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.37%
[Alpha=0.20] Top-5 Accuracy: 34.22%
Result: Top-1: 15.37%, Top-5: 34.22%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 15.14%
[Alpha=0.20] Top-5 Accuracy: 34.22%
Result: Top-1: 15.14%, Top-5: 34.22%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 14.58%
[Alpha=0.20] Top-5 Accuracy: 33.46%
Result: Top-1: 14.58%, Top-5: 33.46%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 14.27%
[Alpha=0.20] Top-5 Accuracy: 32.31%
Result: Top-1: 14.27%, Top-5: 32.31%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 14.28%
[Alpha=0.20] Top-5 Accuracy: 32.31%
Result: Top-1: 14.28%, Top-5: 32.31%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 14.45%
[Alpha=0.20] Top-5 Accuracy: 32.19%
Result: Top-1: 14.45%, Top-5: 32.19%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 14.58%
[Alpha=0.20] Top-5 Accuracy: 32.90%
Result: Top-1: 14.58%, Top-5: 32.90%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 14.97%
[Alpha=0.20] Top-5 Accuracy: 33.55%
Result: Top-1: 14.97%, Top-5: 33.55%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 14.95%
[Alpha=0.20] Top-5 Accuracy: 33.60%
Result: Top-1: 14.95%, Top-5: 33.60%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 14.63%
[Alpha=0.20] Top-5 Accuracy: 33.33%
Result: Top-1: 14.63%, Top-5: 33.33%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 14.87%
[Alpha=0.20] Top-5 Accuracy: 33.41%
Result: Top-1: 14.87%, Top-5: 33.41%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 15.12%
[Alpha=0.30] Top-5 Accuracy: 34.01%
Result: Top-1: 15.12%, Top-5: 34.01%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.82%
[Alpha=0.30] Top-5 Accuracy: 33.85%
Result: Top-1: 14.82%, Top-5: 33.85%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.73%
[Alpha=0.30] Top-5 Accuracy: 33.88%
Result: Top-1: 14.73%, Top-5: 33.88%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.80%
[Alpha=0.30] Top-5 Accuracy: 33.91%
Result: Top-1: 14.80%, Top-5: 33.91%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.78%
[Alpha=0.30] Top-5 Accuracy: 33.90%
Result: Top-1: 14.78%, Top-5: 33.90%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.78%
[Alpha=0.30] Top-5 Accuracy: 33.91%
Result: Top-1: 14.78%, Top-5: 33.91%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.77%
[Alpha=0.30] Top-5 Accuracy: 33.83%
Result: Top-1: 14.77%, Top-5: 33.83%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.70%
[Alpha=0.30] Top-5 Accuracy: 33.84%
Result: Top-1: 14.70%, Top-5: 33.84%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.83%
[Alpha=0.30] Top-5 Accuracy: 33.76%
Result: Top-1: 14.83%, Top-5: 33.76%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.83%
[Alpha=0.30] Top-5 Accuracy: 33.83%
Result: Top-1: 14.83%, Top-5: 33.83%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 15.06%
[Alpha=0.30] Top-5 Accuracy: 34.04%
Result: Top-1: 15.06%, Top-5: 34.04%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.61%
[Alpha=0.30] Top-5 Accuracy: 33.76%
Result: Top-1: 14.61%, Top-5: 33.76%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.67%
[Alpha=0.30] Top-5 Accuracy: 33.68%
Result: Top-1: 14.67%, Top-5: 33.68%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.66%
[Alpha=0.30] Top-5 Accuracy: 33.79%
Result: Top-1: 14.66%, Top-5: 33.79%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.72%
[Alpha=0.30] Top-5 Accuracy: 33.86%
Result: Top-1: 14.72%, Top-5: 33.86%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.66%
[Alpha=0.30] Top-5 Accuracy: 33.74%
Result: Top-1: 14.66%, Top-5: 33.74%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.64%
[Alpha=0.30] Top-5 Accuracy: 33.78%
Result: Top-1: 14.64%, Top-5: 33.78%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.64%
[Alpha=0.30] Top-5 Accuracy: 33.69%
Result: Top-1: 14.64%, Top-5: 33.69%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.74%
[Alpha=0.30] Top-5 Accuracy: 33.89%
Result: Top-1: 14.74%, Top-5: 33.89%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.68%
[Alpha=0.30] Top-5 Accuracy: 33.73%
Result: Top-1: 14.68%, Top-5: 33.73%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.97%
[Alpha=0.30] Top-5 Accuracy: 33.95%
Result: Top-1: 14.97%, Top-5: 33.95%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.22%
[Alpha=0.30] Top-5 Accuracy: 33.40%
Result: Top-1: 14.22%, Top-5: 33.40%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.31%
[Alpha=0.30] Top-5 Accuracy: 33.43%
Result: Top-1: 14.31%, Top-5: 33.43%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.34%
[Alpha=0.30] Top-5 Accuracy: 33.46%
Result: Top-1: 14.34%, Top-5: 33.46%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.26%
[Alpha=0.30] Top-5 Accuracy: 33.44%
Result: Top-1: 14.26%, Top-5: 33.44%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.28%
[Alpha=0.30] Top-5 Accuracy: 33.55%
Result: Top-1: 14.28%, Top-5: 33.55%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.25%
[Alpha=0.30] Top-5 Accuracy: 33.44%
Result: Top-1: 14.25%, Top-5: 33.44%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.11%
[Alpha=0.30] Top-5 Accuracy: 33.41%
Result: Top-1: 14.11%, Top-5: 33.41%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.31%
[Alpha=0.30] Top-5 Accuracy: 33.56%
Result: Top-1: 14.31%, Top-5: 33.56%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.35%
[Alpha=0.30] Top-5 Accuracy: 33.42%
Result: Top-1: 14.35%, Top-5: 33.42%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.83%
[Alpha=0.30] Top-5 Accuracy: 33.80%
Result: Top-1: 14.83%, Top-5: 33.80%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.81%
[Alpha=0.30] Top-5 Accuracy: 33.08%
Result: Top-1: 13.81%, Top-5: 33.08%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.87%
[Alpha=0.30] Top-5 Accuracy: 33.16%
Result: Top-1: 13.87%, Top-5: 33.16%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.64%
[Alpha=0.30] Top-5 Accuracy: 33.00%
Result: Top-1: 13.64%, Top-5: 33.00%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.90%
[Alpha=0.30] Top-5 Accuracy: 33.12%
Result: Top-1: 13.90%, Top-5: 33.12%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.01%
[Alpha=0.30] Top-5 Accuracy: 33.04%
Result: Top-1: 14.01%, Top-5: 33.04%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.03%
[Alpha=0.30] Top-5 Accuracy: 33.28%
Result: Top-1: 14.03%, Top-5: 33.28%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.83%
[Alpha=0.30] Top-5 Accuracy: 33.16%
Result: Top-1: 13.83%, Top-5: 33.16%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.87%
[Alpha=0.30] Top-5 Accuracy: 33.05%
Result: Top-1: 13.87%, Top-5: 33.05%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.81%
[Alpha=0.30] Top-5 Accuracy: 33.31%
Result: Top-1: 13.81%, Top-5: 33.31%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.60%
[Alpha=0.30] Top-5 Accuracy: 33.54%
Result: Top-1: 14.60%, Top-5: 33.54%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.39%
[Alpha=0.30] Top-5 Accuracy: 32.69%
Result: Top-1: 13.39%, Top-5: 32.69%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.38%
[Alpha=0.30] Top-5 Accuracy: 32.75%
Result: Top-1: 13.38%, Top-5: 32.75%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.45%
[Alpha=0.30] Top-5 Accuracy: 32.76%
Result: Top-1: 13.45%, Top-5: 32.76%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.44%
[Alpha=0.30] Top-5 Accuracy: 32.72%
Result: Top-1: 13.44%, Top-5: 32.72%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.35%
[Alpha=0.30] Top-5 Accuracy: 32.79%
Result: Top-1: 13.35%, Top-5: 32.79%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.40%
[Alpha=0.30] Top-5 Accuracy: 32.66%
Result: Top-1: 13.40%, Top-5: 32.66%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.58%
[Alpha=0.30] Top-5 Accuracy: 32.93%
Result: Top-1: 13.58%, Top-5: 32.93%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.03%
[Alpha=0.30] Top-5 Accuracy: 32.31%
Result: Top-1: 13.03%, Top-5: 32.31%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 13.66%
[Alpha=0.30] Top-5 Accuracy: 32.80%
Result: Top-1: 13.66%, Top-5: 32.80%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 14.08%
[Alpha=0.30] Top-5 Accuracy: 32.88%
Result: Top-1: 14.08%, Top-5: 32.88%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 12.24%
[Alpha=0.30] Top-5 Accuracy: 31.32%
Result: Top-1: 12.24%, Top-5: 31.32%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 11.97%
[Alpha=0.30] Top-5 Accuracy: 30.23%
Result: Top-1: 11.97%, Top-5: 30.23%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 11.79%
[Alpha=0.30] Top-5 Accuracy: 30.01%
Result: Top-1: 11.79%, Top-5: 30.01%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 11.93%
[Alpha=0.30] Top-5 Accuracy: 29.92%
Result: Top-1: 11.93%, Top-5: 29.92%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 12.05%
[Alpha=0.30] Top-5 Accuracy: 30.96%
Result: Top-1: 12.05%, Top-5: 30.96%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 12.72%
[Alpha=0.30] Top-5 Accuracy: 31.59%
Result: Top-1: 12.72%, Top-5: 31.59%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 12.44%
[Alpha=0.30] Top-5 Accuracy: 31.60%
Result: Top-1: 12.44%, Top-5: 31.60%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 12.08%
[Alpha=0.30] Top-5 Accuracy: 31.17%
Result: Top-1: 12.08%, Top-5: 31.17%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 12.85%
[Alpha=0.30] Top-5 Accuracy: 31.71%
Result: Top-1: 12.85%, Top-5: 31.71%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 13.16%
[Alpha=0.40] Top-5 Accuracy: 32.52%
Result: Top-1: 13.16%, Top-5: 32.52%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.49%
[Alpha=0.40] Top-5 Accuracy: 32.22%
Result: Top-1: 12.49%, Top-5: 32.22%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.32%
[Alpha=0.40] Top-5 Accuracy: 32.01%
Result: Top-1: 12.32%, Top-5: 32.01%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.48%
[Alpha=0.40] Top-5 Accuracy: 32.15%
Result: Top-1: 12.48%, Top-5: 32.15%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.45%
[Alpha=0.40] Top-5 Accuracy: 32.14%
Result: Top-1: 12.45%, Top-5: 32.14%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.46%
[Alpha=0.40] Top-5 Accuracy: 32.17%
Result: Top-1: 12.46%, Top-5: 32.17%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.04%
[Alpha=0.40] Top-5 Accuracy: 31.92%
Result: Top-1: 12.04%, Top-5: 31.92%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.25%
[Alpha=0.40] Top-5 Accuracy: 32.04%
Result: Top-1: 12.25%, Top-5: 32.04%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.37%
[Alpha=0.40] Top-5 Accuracy: 32.11%
Result: Top-1: 12.37%, Top-5: 32.11%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.63%
[Alpha=0.40] Top-5 Accuracy: 32.22%
Result: Top-1: 12.63%, Top-5: 32.22%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 13.08%
[Alpha=0.40] Top-5 Accuracy: 32.46%
Result: Top-1: 13.08%, Top-5: 32.46%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.83%
[Alpha=0.40] Top-5 Accuracy: 32.06%
Result: Top-1: 11.83%, Top-5: 32.06%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.82%
[Alpha=0.40] Top-5 Accuracy: 31.89%
Result: Top-1: 11.82%, Top-5: 31.89%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.19%
[Alpha=0.40] Top-5 Accuracy: 32.00%
Result: Top-1: 12.19%, Top-5: 32.00%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.25%
[Alpha=0.40] Top-5 Accuracy: 32.25%
Result: Top-1: 12.25%, Top-5: 32.25%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.82%
[Alpha=0.40] Top-5 Accuracy: 32.03%
Result: Top-1: 11.82%, Top-5: 32.03%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.94%
[Alpha=0.40] Top-5 Accuracy: 32.10%
Result: Top-1: 11.94%, Top-5: 32.10%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.73%
[Alpha=0.40] Top-5 Accuracy: 31.79%
Result: Top-1: 11.73%, Top-5: 31.79%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.15%
[Alpha=0.40] Top-5 Accuracy: 32.24%
Result: Top-1: 12.15%, Top-5: 32.24%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.00%
[Alpha=0.40] Top-5 Accuracy: 31.91%
Result: Top-1: 12.00%, Top-5: 31.91%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.88%
[Alpha=0.40] Top-5 Accuracy: 32.24%
Result: Top-1: 12.88%, Top-5: 32.24%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.11%
[Alpha=0.40] Top-5 Accuracy: 31.43%
Result: Top-1: 11.11%, Top-5: 31.43%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.55%
[Alpha=0.40] Top-5 Accuracy: 31.72%
Result: Top-1: 11.55%, Top-5: 31.72%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.31%
[Alpha=0.40] Top-5 Accuracy: 31.67%
Result: Top-1: 11.31%, Top-5: 31.67%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.38%
[Alpha=0.40] Top-5 Accuracy: 31.60%
Result: Top-1: 11.38%, Top-5: 31.60%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.46%
[Alpha=0.40] Top-5 Accuracy: 31.81%
Result: Top-1: 11.46%, Top-5: 31.81%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.39%
[Alpha=0.40] Top-5 Accuracy: 31.61%
Result: Top-1: 11.39%, Top-5: 31.61%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.18%
[Alpha=0.40] Top-5 Accuracy: 31.59%
Result: Top-1: 11.18%, Top-5: 31.59%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.45%
[Alpha=0.40] Top-5 Accuracy: 31.79%
Result: Top-1: 11.45%, Top-5: 31.79%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 11.42%
[Alpha=0.40] Top-5 Accuracy: 31.69%
Result: Top-1: 11.42%, Top-5: 31.69%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.65%
[Alpha=0.40] Top-5 Accuracy: 31.98%
Result: Top-1: 12.65%, Top-5: 31.98%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 10.50%
[Alpha=0.40] Top-5 Accuracy: 30.92%
Result: Top-1: 10.50%, Top-5: 30.92%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 10.56%
[Alpha=0.40] Top-5 Accuracy: 31.17%
Result: Top-1: 10.56%, Top-5: 31.17%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 10.21%
[Alpha=0.40] Top-5 Accuracy: 30.76%
Result: Top-1: 10.21%, Top-5: 30.76%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 10.79%
[Alpha=0.40] Top-5 Accuracy: 31.18%
Result: Top-1: 10.79%, Top-5: 31.18%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 10.86%
[Alpha=0.40] Top-5 Accuracy: 31.11%
Result: Top-1: 10.86%, Top-5: 31.11%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 10.86%
[Alpha=0.40] Top-5 Accuracy: 31.23%
Result: Top-1: 10.86%, Top-5: 31.23%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 10.65%
[Alpha=0.40] Top-5 Accuracy: 30.94%
Result: Top-1: 10.65%, Top-5: 30.94%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 10.42%
[Alpha=0.40] Top-5 Accuracy: 30.81%
Result: Top-1: 10.42%, Top-5: 30.81%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 10.49%
[Alpha=0.40] Top-5 Accuracy: 31.20%
Result: Top-1: 10.49%, Top-5: 31.20%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 12.33%
[Alpha=0.40] Top-5 Accuracy: 31.56%
Result: Top-1: 12.33%, Top-5: 31.56%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 10.18%
[Alpha=0.40] Top-5 Accuracy: 30.47%
Result: Top-1: 10.18%, Top-5: 30.47%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 9.95%
[Alpha=0.40] Top-5 Accuracy: 30.37%
Result: Top-1: 9.95%, Top-5: 30.37%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 9.88%
[Alpha=0.40] Top-5 Accuracy: 30.55%
Result: Top-1: 9.88%, Top-5: 30.55%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 9.68%
[Alpha=0.40] Top-5 Accuracy: 30.31%
Result: Top-1: 9.68%, Top-5: 30.31%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 9.85%
[Alpha=0.40] Top-5 Accuracy: 30.47%
Result: Top-1: 9.85%, Top-5: 30.47%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 9.86%
[Alpha=0.40] Top-5 Accuracy: 30.49%
Result: Top-1: 9.86%, Top-5: 30.49%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=175
============================================================
slurmstepd-jnfat06: error: *** JOB 1659762 ON jnfat06 CANCELLED AT 2025-09-12T13:43:18 ***
