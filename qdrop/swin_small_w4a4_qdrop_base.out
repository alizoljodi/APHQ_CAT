Starting Swin-Small W4A4 QDROP experiment at Thu Sep 11 10:45:05 AM CEST 2025
2025-09-11 10:45:09,624 - INFO - Starting multi-seed experiment
2025-09-11 10:45:09,624 - INFO - Architecture: swin_small
2025-09-11 10:45:09,624 - INFO - Weight bits: 4
2025-09-11 10:45:09,624 - INFO - Activation bits: 4
2025-09-11 10:45:09,624 - INFO - Seeds: [1001, 1002, 1003]
2025-09-11 10:45:09,624 - INFO - Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
2025-09-11 10:45:09,624 - INFO - Cluster numbers: [8, 16, 32, 64, 128, 256]
2025-09-11 10:45:09,624 - INFO - PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]
2025-09-11 10:45:09,624 - INFO - Output directory: ./experiment_results/swin_small_w4_a4_20250911_104509
2025-09-11 10:45:09,624 - INFO - Checking basic requirements...
2025-09-11 10:45:09,625 - INFO - Basic checks passed
2025-09-11 10:45:09,625 - INFO - 
Starting experiments for 3 seeds...
2025-09-11 10:45:09,625 - INFO - Total parameter combinations: 600
2025-09-11 10:45:09,625 - INFO - Total experiments: 1800
2025-09-11 10:45:09,625 - INFO - 
============================================================
2025-09-11 10:45:09,625 - INFO - Running experiment 1/3 for seed 1001
2025-09-11 10:45:09,625 - INFO - ============================================================
2025-09-11 10:45:09,625 - INFO - Running experiment for seed 1001
2025-09-11 10:45:09,625 - INFO - Command: /home/alz07xz/project/PD-Quant/pd_quant/bin/python ../test_quant.py --model swin_small --w_bit 4 --a_bit 4 --seed 1001 --config ../configs/4bit/qdrop_baseline.py --dataset /home/alz07xz/imagenet --calib-size 1000 --calib-batch-size 32 --val-batch-size 500 --num-workers 8 --device cuda --alpha-list 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 --num-clusters-list 8 16 32 64 128 256 --pca-dim-list 1 25 50 75 100 125 150 175 200 220 --calibrate --optimize
2025-09-11 10:45:09,626 - INFO - Working directory: /home/alz07xz/project/APHQ_CAT/qdrop
2025-09-11 10:55:07 - start the process.
Namespace(model='swin_small', config='../configs/4bit/qdrop_baseline.py', dataset='/home/alz07xz/imagenet', val_batch_size=500, num_workers=8, device='cuda', reconstruct_mlp=False, load_reconstruct_checkpoint=False, test_reconstruct_checkpoint=False, calibrate=True, load_calibrate_checkpoint=None, test_calibrate_checkpoint=False, optimize=True, load_optimize_checkpoint='', test_optimize_checkpoint=False, print_freq=10, seed=1001, alpha_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], num_clusters_list=[8, 16, 32, 64, 128, 256], pca_dim_list=[1, 25, 50, 75, 100, 125, 150, 175, 200, 220], w_bit=4, a_bit=4, calib_size=1000, calib_batch_size=32)
deefe /home/alz07xz/project/APHQ_CAT/configs/4bit
Successfully imported Config class!
optim_size: 1024
calib_size: 1000
optim_batch_size: 32
calib_batch_size: 32
w_bit: 4
a_bit: 4
qconv_a_bit: 8
qhead_a_bit: 4
calib_metric: mse
matmul_head_channel_wise: True
token_channel_wise: False
eq_n: 128
search_round: 3
keep_gpu: True
optim_metric: mse
use_mean_hessian: False
temp: 20
recon_metric: hessian_perturb
pct: 0.99
optim_mode: qdrop
drop_prob: 0.5
reconstruct_mlp: False
Building model ...
/home/alz07xz/project/PD-Quant/pd_quant/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
No checkpoint found at './checkpoint/vit_raw/swin_small_patch4_window7_224.bin'
Loading pretrained weights from Hugging Face hub (timm/swin_small_patch4_window7_224.ms_in22k_ft_in1k)
[timm/swin_small_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Building validation dataloader ...
Validating on test set on fp ...
Test: [0/100]	Time 18.001 (18.001)	Loss 0.4363 (0.4363)	Prec@1 91.600 (91.600)	Prec@5 98.200 (98.200)
Test: [10/100]	Time 0.752 (2.637)	Loss 0.4835 (0.5209)	Prec@1 90.200 (87.800)	Prec@5 98.400 (98.073)
Test: [20/100]	Time 0.760 (1.951)	Loss 0.6876 (0.5743)	Prec@1 82.000 (86.400)	Prec@5 97.200 (97.743)
Test: [30/100]	Time 0.759 (1.711)	Loss 0.5090 (0.6038)	Prec@1 88.000 (85.516)	Prec@5 99.200 (97.677)
Test: [40/100]	Time 9.873 (1.814)	Loss 0.7823 (0.5960)	Prec@1 79.400 (85.780)	Prec@5 96.800 (97.741)
Test: [50/100]	Time 3.298 (1.765)	Loss 0.9962 (0.6402)	Prec@1 76.000 (84.569)	Prec@5 93.200 (97.353)
Test: [60/100]	Time 0.757 (1.668)	Loss 0.6332 (0.6450)	Prec@1 86.000 (84.538)	Prec@5 96.200 (97.285)
Test: [70/100]	Time 0.764 (1.599)	Loss 0.7378 (0.6617)	Prec@1 82.200 (83.859)	Prec@5 97.600 (97.141)
Test: [80/100]	Time 8.867 (1.603)	Loss 0.5519 (0.6669)	Prec@1 87.000 (83.805)	Prec@5 97.800 (97.030)
Test: [90/100]	Time 3.706 (1.616)	Loss 1.0055 (0.6846)	Prec@1 74.800 (83.189)	Prec@5 94.400 (96.912)
 * Prec@1 83.316 Prec@5 96.976 Loss 0.680 Time 157.987
Wraping quantiztion modules (reparam: False, recon: False) ...
2025-09-11 10:58:28 - start mse guided calibration
  0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   1%|          | 1/149 [00:11<27:13, 11.03s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|          | 1/149 [00:11<27:13, 11.03s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|▏         | 2/149 [01:06<1:31:35, 37.38s/it]calibrating layers.0.blocks.0.attn.proj:   1%|▏         | 2/149 [01:06<1:31:35, 37.38s/it]calibrating layers.0.blocks.0.attn.proj:   2%|▏         | 3/149 [01:31<1:16:52, 31.59s/it]calibrating layers.0.blocks.0.attn.matmul1:   2%|▏         | 3/149 [01:31<1:16:52, 31.59s/it]calibrating layers.0.blocks.0.attn.matmul1:   3%|▎         | 4/149 [04:36<3:43:07, 92.33s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 4/149 [04:36<3:43:07, 92.33s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 5/149 [09:53<6:55:17, 173.04s/it]calibrating layers.0.blocks.0.mlp.fc1:   3%|▎         | 5/149 [09:53<6:55:17, 173.04s/it]     calibrating layers.0.blocks.0.mlp.fc1:   4%|▍         | 6/149 [11:38<5:57:43, 150.09s/it]calibrating layers.0.blocks.0.mlp.fc2:   4%|▍         | 6/149 [11:38<5:57:43, 150.09s/it]calibrating layers.0.blocks.0.mlp.fc2:   5%|▍         | 7/149 [13:41<5:34:07, 141.18s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▍         | 7/149 [13:41<5:34:07, 141.18s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▌         | 8/149 [14:37<4:28:09, 114.11s/it]calibrating layers.0.blocks.1.attn.proj:   5%|▌         | 8/149 [14:37<4:28:09, 114.11s/it]calibrating layers.0.blocks.1.attn.proj:   6%|▌         | 9/149 [15:02<3:20:59, 86.14s/it] calibrating layers.0.blocks.1.attn.matmul1:   6%|▌         | 9/149 [15:02<3:20:59, 86.14s/it]calibrating layers.0.blocks.1.attn.matmul1:   7%|▋         | 10/149 [18:07<4:30:11, 116.63s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 10/149 [18:07<4:30:11, 116.63s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 11/149 [23:23<6:48:48, 177.74s/it]calibrating layers.0.blocks.1.mlp.fc1:   7%|▋         | 11/149 [23:23<6:48:48, 177.74s/it]     calibrating layers.0.blocks.1.mlp.fc1:   8%|▊         | 12/149 [25:09<5:55:46, 155.82s/it]calibrating layers.0.blocks.1.mlp.fc2:   8%|▊         | 12/149 [25:09<5:55:46, 155.82s/it]calibrating layers.0.blocks.1.mlp.fc2:   9%|▊         | 13/149 [27:12<5:30:40, 145.88s/it]calibrating layers.1.downsample.reduction:   9%|▊         | 13/149 [27:12<5:30:40, 145.88s/it]calibrating layers.1.downsample.reduction:   9%|▉         | 14/149 [27:42<4:09:27, 110.87s/it]calibrating layers.1.blocks.0.attn.qkv:   9%|▉         | 14/149 [27:42<4:09:27, 110.87s/it]   calibrating layers.1.blocks.0.attn.qkv:  10%|█         | 15/149 [28:14<3:14:51, 87.25s/it] calibrating layers.1.blocks.0.attn.proj:  10%|█         | 15/149 [28:14<3:14:51, 87.25s/it]calibrating layers.1.blocks.0.attn.proj:  11%|█         | 16/149 [28:31<2:26:18, 66.00s/it]calibrating layers.1.blocks.0.attn.matmul1:  11%|█         | 16/149 [28:31<2:26:18, 66.00s/it]calibrating layers.1.blocks.0.attn.matmul1:  11%|█▏        | 17/149 [28:59<2:00:10, 54.62s/it]calibrating layers.1.blocks.0.attn.matmul2:  11%|█▏        | 17/149 [28:59<2:00:10, 54.62s/it]calibrating layers.1.blocks.0.attn.matmul2:  12%|█▏        | 18/149 [29:35<1:47:14, 49.12s/it]calibrating layers.1.blocks.0.mlp.fc1:  12%|█▏        | 18/149 [29:35<1:47:14, 49.12s/it]     calibrating layers.1.blocks.0.mlp.fc1:  13%|█▎        | 19/149 [30:30<1:50:11, 50.85s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 19/149 [30:30<1:50:11, 50.85s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 20/149 [31:32<1:56:38, 54.25s/it]calibrating layers.1.blocks.1.attn.qkv:  13%|█▎        | 20/149 [31:32<1:56:38, 54.25s/it]calibrating layers.1.blocks.1.attn.qkv:  14%|█▍        | 21/149 [32:07<1:42:58, 48.27s/it]calibrating layers.1.blocks.1.attn.proj:  14%|█▍        | 21/149 [32:07<1:42:58, 48.27s/it]calibrating layers.1.blocks.1.attn.proj:  15%|█▍        | 22/149 [32:24<1:22:47, 39.11s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▍        | 22/149 [32:24<1:22:47, 39.11s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▌        | 23/149 [32:53<1:15:44, 36.06s/it]calibrating layers.1.blocks.1.attn.matmul2:  15%|█▌        | 23/149 [32:53<1:15:44, 36.06s/it]calibrating layers.1.blocks.1.attn.matmul2:  16%|█▌        | 24/149 [33:30<1:15:30, 36.24s/it]calibrating layers.1.blocks.1.mlp.fc1:  16%|█▌        | 24/149 [33:30<1:15:30, 36.24s/it]     calibrating layers.1.blocks.1.mlp.fc1:  17%|█▋        | 25/149 [34:25<1:26:29, 41.85s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 25/149 [34:25<1:26:29, 41.85s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 26/149 [35:27<1:38:16, 47.94s/it]calibrating layers.2.downsample.reduction:  17%|█▋        | 26/149 [35:27<1:38:16, 47.94s/it]calibrating layers.2.downsample.reduction:  18%|█▊        | 27/149 [35:44<1:18:21, 38.54s/it]calibrating layers.2.blocks.0.attn.qkv:  18%|█▊        | 27/149 [35:44<1:18:21, 38.54s/it]   calibrating layers.2.blocks.0.attn.qkv:  19%|█▉        | 28/149 [36:04<1:06:55, 33.18s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 28/149 [36:04<1:06:55, 33.18s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 29/149 [36:15<52:54, 26.46s/it]  calibrating layers.2.blocks.0.attn.matmul1:  19%|█▉        | 29/149 [36:15<52:54, 26.46s/it]calibrating layers.2.blocks.0.attn.matmul1:  20%|██        | 30/149 [36:27<43:54, 22.14s/it]calibrating layers.2.blocks.0.attn.matmul2:  20%|██        | 30/149 [36:27<43:54, 22.14s/it]calibrating layers.2.blocks.0.attn.matmul2:  21%|██        | 31/149 [36:40<38:16, 19.46s/it]calibrating layers.2.blocks.0.mlp.fc1:  21%|██        | 31/149 [36:40<38:16, 19.46s/it]     calibrating layers.2.blocks.0.mlp.fc1:  21%|██▏       | 32/149 [37:11<44:39, 22.90s/it]calibrating layers.2.blocks.0.mlp.fc2:  21%|██▏       | 32/149 [37:11<44:39, 22.90s/it]calibrating layers.2.blocks.0.mlp.fc2:  22%|██▏       | 33/149 [37:43<49:06, 25.40s/it]calibrating layers.2.blocks.1.attn.qkv:  22%|██▏       | 33/149 [37:43<49:06, 25.40s/it]calibrating layers.2.blocks.1.attn.qkv:  23%|██▎       | 34/149 [38:03<45:50, 23.91s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 34/149 [38:03<45:50, 23.91s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 35/149 [38:14<37:49, 19.90s/it]calibrating layers.2.blocks.1.attn.matmul1:  23%|██▎       | 35/149 [38:14<37:49, 19.90s/it]calibrating layers.2.blocks.1.attn.matmul1:  24%|██▍       | 36/149 [38:27<33:36, 17.84s/it]calibrating layers.2.blocks.1.attn.matmul2:  24%|██▍       | 36/149 [38:27<33:36, 17.84s/it]calibrating layers.2.blocks.1.attn.matmul2:  25%|██▍       | 37/149 [38:40<30:43, 16.46s/it]calibrating layers.2.blocks.1.mlp.fc1:  25%|██▍       | 37/149 [38:40<30:43, 16.46s/it]     calibrating layers.2.blocks.1.mlp.fc1:  26%|██▌       | 38/149 [39:11<38:19, 20.72s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 38/149 [39:11<38:19, 20.72s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 39/149 [39:42<43:46, 23.88s/it]calibrating layers.2.blocks.2.attn.qkv:  26%|██▌       | 39/149 [39:42<43:46, 23.88s/it]calibrating layers.2.blocks.2.attn.qkv:  27%|██▋       | 40/149 [40:02<41:32, 22.87s/it]calibrating layers.2.blocks.2.attn.proj:  27%|██▋       | 40/149 [40:02<41:32, 22.87s/it]calibrating layers.2.blocks.2.attn.proj:  28%|██▊       | 41/149 [40:13<34:31, 19.18s/it]calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 41/149 [40:13<34:31, 19.18s/it]calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 42/149 [40:26<30:51, 17.30s/it]calibrating layers.2.blocks.2.attn.matmul2:  28%|██▊       | 42/149 [40:26<30:51, 17.30s/it]calibrating layers.2.blocks.2.attn.matmul2:  29%|██▉       | 43/149 [40:39<28:22, 16.06s/it]calibrating layers.2.blocks.2.mlp.fc1:  29%|██▉       | 43/149 [40:39<28:22, 16.06s/it]     calibrating layers.2.blocks.2.mlp.fc1:  30%|██▉       | 44/149 [41:10<35:51, 20.49s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|██▉       | 44/149 [41:10<35:51, 20.49s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|███       | 45/149 [41:41<41:10, 23.76s/it]calibrating layers.2.blocks.3.attn.qkv:  30%|███       | 45/149 [41:41<41:10, 23.76s/it]calibrating layers.2.blocks.3.attn.qkv:  31%|███       | 46/149 [42:02<39:10, 22.82s/it]calibrating layers.2.blocks.3.attn.proj:  31%|███       | 46/149 [42:02<39:10, 22.82s/it]calibrating layers.2.blocks.3.attn.proj:  32%|███▏      | 47/149 [42:12<32:33, 19.15s/it]calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 47/149 [42:12<32:33, 19.15s/it]calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 48/149 [42:26<29:13, 17.36s/it]calibrating layers.2.blocks.3.attn.matmul2:  32%|███▏      | 48/149 [42:26<29:13, 17.36s/it]calibrating layers.2.blocks.3.attn.matmul2:  33%|███▎      | 49/149 [42:39<26:58, 16.18s/it]calibrating layers.2.blocks.3.mlp.fc1:  33%|███▎      | 49/149 [42:39<26:58, 16.18s/it]     calibrating layers.2.blocks.3.mlp.fc1:  34%|███▎      | 50/149 [43:10<33:54, 20.55s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▎      | 50/149 [43:10<33:54, 20.55s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▍      | 51/149 [43:41<38:42, 23.70s/it]calibrating layers.2.blocks.4.attn.qkv:  34%|███▍      | 51/149 [43:41<38:42, 23.70s/it]calibrating layers.2.blocks.4.attn.qkv:  35%|███▍      | 52/149 [44:01<36:38, 22.66s/it]calibrating layers.2.blocks.4.attn.proj:  35%|███▍      | 52/149 [44:01<36:38, 22.66s/it]calibrating layers.2.blocks.4.attn.proj:  36%|███▌      | 53/149 [44:12<30:25, 19.01s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 53/149 [44:12<30:25, 19.01s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 54/149 [44:25<27:28, 17.35s/it]calibrating layers.2.blocks.4.attn.matmul2:  36%|███▌      | 54/149 [44:25<27:28, 17.35s/it]calibrating layers.2.blocks.4.attn.matmul2:  37%|███▋      | 55/149 [44:38<25:18, 16.15s/it]calibrating layers.2.blocks.4.mlp.fc1:  37%|███▋      | 55/149 [44:38<25:18, 16.15s/it]     calibrating layers.2.blocks.4.mlp.fc1:  38%|███▊      | 56/149 [45:09<31:54, 20.59s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 56/149 [45:09<31:54, 20.59s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 57/149 [45:41<36:35, 23.86s/it]calibrating layers.2.blocks.5.attn.qkv:  38%|███▊      | 57/149 [45:41<36:35, 23.86s/it]calibrating layers.2.blocks.5.attn.qkv:  39%|███▉      | 58/149 [46:01<34:42, 22.89s/it]calibrating layers.2.blocks.5.attn.proj:  39%|███▉      | 58/149 [46:01<34:42, 22.89s/it]calibrating layers.2.blocks.5.attn.proj:  40%|███▉      | 59/149 [46:12<28:45, 19.18s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|███▉      | 59/149 [46:12<28:45, 19.18s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|████      | 60/149 [46:25<25:41, 17.32s/it]calibrating layers.2.blocks.5.attn.matmul2:  40%|████      | 60/149 [46:25<25:41, 17.32s/it]calibrating layers.2.blocks.5.attn.matmul2:  41%|████      | 61/149 [46:38<23:31, 16.04s/it]calibrating layers.2.blocks.5.mlp.fc1:  41%|████      | 61/149 [46:38<23:31, 16.04s/it]     calibrating layers.2.blocks.5.mlp.fc1:  42%|████▏     | 62/149 [47:09<29:37, 20.44s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 62/149 [47:09<29:37, 20.44s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 63/149 [47:40<33:52, 23.63s/it]calibrating layers.2.blocks.6.attn.qkv:  42%|████▏     | 63/149 [47:40<33:52, 23.63s/it]calibrating layers.2.blocks.6.attn.qkv:  43%|████▎     | 64/149 [48:00<32:04, 22.64s/it]calibrating layers.2.blocks.6.attn.proj:  43%|████▎     | 64/149 [48:00<32:04, 22.64s/it]calibrating layers.2.blocks.6.attn.proj:  44%|████▎     | 65/149 [48:11<26:34, 18.99s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▎     | 65/149 [48:11<26:34, 18.99s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▍     | 66/149 [48:23<23:24, 16.92s/it]calibrating layers.2.blocks.6.attn.matmul2:  44%|████▍     | 66/149 [48:23<23:24, 16.92s/it]calibrating layers.2.blocks.6.attn.matmul2:  45%|████▍     | 67/149 [48:36<21:36, 15.81s/it]calibrating layers.2.blocks.6.mlp.fc1:  45%|████▍     | 67/149 [48:36<21:36, 15.81s/it]     calibrating layers.2.blocks.6.mlp.fc1:  46%|████▌     | 68/149 [49:07<27:28, 20.35s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▌     | 68/149 [49:07<27:28, 20.35s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▋     | 69/149 [49:38<31:33, 23.66s/it]calibrating layers.2.blocks.7.attn.qkv:  46%|████▋     | 69/149 [49:38<31:33, 23.66s/it]calibrating layers.2.blocks.7.attn.qkv:  47%|████▋     | 70/149 [49:59<29:50, 22.66s/it]calibrating layers.2.blocks.7.attn.proj:  47%|████▋     | 70/149 [49:59<29:50, 22.66s/it]calibrating layers.2.blocks.7.attn.proj:  48%|████▊     | 71/149 [50:09<24:42, 19.00s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 71/149 [50:09<24:42, 19.00s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 72/149 [50:22<22:02, 17.18s/it]calibrating layers.2.blocks.7.attn.matmul2:  48%|████▊     | 72/149 [50:22<22:02, 17.18s/it]calibrating layers.2.blocks.7.attn.matmul2:  49%|████▉     | 73/149 [50:35<20:13, 15.96s/it]calibrating layers.2.blocks.7.mlp.fc1:  49%|████▉     | 73/149 [50:35<20:13, 15.96s/it]     calibrating layers.2.blocks.7.mlp.fc1:  50%|████▉     | 74/149 [51:06<25:32, 20.43s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|████▉     | 74/149 [51:06<25:32, 20.43s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|█████     | 75/149 [51:37<29:05, 23.59s/it]calibrating layers.2.blocks.8.attn.qkv:  50%|█████     | 75/149 [51:37<29:05, 23.59s/it]calibrating layers.2.blocks.8.attn.qkv:  51%|█████     | 76/149 [51:57<27:22, 22.50s/it]calibrating layers.2.blocks.8.attn.proj:  51%|█████     | 76/149 [51:57<27:22, 22.50s/it]calibrating layers.2.blocks.8.attn.proj:  52%|█████▏    | 77/149 [52:07<22:37, 18.86s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 77/149 [52:07<22:37, 18.86s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 78/149 [52:20<20:11, 17.07s/it]calibrating layers.2.blocks.8.attn.matmul2:  52%|█████▏    | 78/149 [52:20<20:11, 17.07s/it]calibrating layers.2.blocks.8.attn.matmul2:  53%|█████▎    | 79/149 [52:33<18:30, 15.87s/it]calibrating layers.2.blocks.8.mlp.fc1:  53%|█████▎    | 79/149 [52:33<18:30, 15.87s/it]     calibrating layers.2.blocks.8.mlp.fc1:  54%|█████▎    | 80/149 [53:04<23:27, 20.40s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▎    | 80/149 [53:04<23:27, 20.40s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▍    | 81/149 [53:35<26:48, 23.66s/it]calibrating layers.2.blocks.9.attn.qkv:  54%|█████▍    | 81/149 [53:35<26:48, 23.66s/it]calibrating layers.2.blocks.9.attn.qkv:  55%|█████▌    | 82/149 [53:56<25:23, 22.74s/it]calibrating layers.2.blocks.9.attn.proj:  55%|█████▌    | 82/149 [53:56<25:23, 22.74s/it]calibrating layers.2.blocks.9.attn.proj:  56%|█████▌    | 83/149 [54:07<20:58, 19.07s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▌    | 83/149 [54:07<20:58, 19.07s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▋    | 84/149 [54:19<18:40, 17.23s/it]calibrating layers.2.blocks.9.attn.matmul2:  56%|█████▋    | 84/149 [54:19<18:40, 17.23s/it]calibrating layers.2.blocks.9.attn.matmul2:  57%|█████▋    | 85/149 [54:33<17:06, 16.04s/it]calibrating layers.2.blocks.9.mlp.fc1:  57%|█████▋    | 85/149 [54:33<17:06, 16.04s/it]     calibrating layers.2.blocks.9.mlp.fc1:  58%|█████▊    | 86/149 [55:04<21:32, 20.52s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 86/149 [55:04<21:32, 20.52s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 87/149 [55:35<24:33, 23.76s/it]calibrating layers.2.blocks.10.attn.qkv:  58%|█████▊    | 87/149 [55:35<24:33, 23.76s/it]calibrating layers.2.blocks.10.attn.qkv:  59%|█████▉    | 88/149 [55:56<23:11, 22.82s/it]calibrating layers.2.blocks.10.attn.proj:  59%|█████▉    | 88/149 [55:56<23:11, 22.82s/it]calibrating layers.2.blocks.10.attn.proj:  60%|█████▉    | 89/149 [56:06<19:07, 19.13s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|█████▉    | 89/149 [56:06<19:07, 19.13s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|██████    | 90/149 [56:19<17:03, 17.35s/it]calibrating layers.2.blocks.10.attn.matmul2:  60%|██████    | 90/149 [56:19<17:03, 17.35s/it]calibrating layers.2.blocks.10.attn.matmul2:  61%|██████    | 91/149 [56:33<15:35, 16.13s/it]calibrating layers.2.blocks.10.mlp.fc1:  61%|██████    | 91/149 [56:33<15:35, 16.13s/it]     calibrating layers.2.blocks.10.mlp.fc1:  62%|██████▏   | 92/149 [57:03<19:31, 20.55s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 92/149 [57:03<19:31, 20.55s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 93/149 [57:34<22:05, 23.68s/it]calibrating layers.2.blocks.11.attn.qkv:  62%|██████▏   | 93/149 [57:34<22:05, 23.68s/it]calibrating layers.2.blocks.11.attn.qkv:  63%|██████▎   | 94/149 [57:54<20:42, 22.59s/it]calibrating layers.2.blocks.11.attn.proj:  63%|██████▎   | 94/149 [57:54<20:42, 22.59s/it]calibrating layers.2.blocks.11.attn.proj:  64%|██████▍   | 95/149 [58:05<17:02, 18.94s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 95/149 [58:05<17:02, 18.94s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 96/149 [58:17<14:54, 16.87s/it]calibrating layers.2.blocks.11.attn.matmul2:  64%|██████▍   | 96/149 [58:17<14:54, 16.87s/it]calibrating layers.2.blocks.11.attn.matmul2:  65%|██████▌   | 97/149 [58:30<13:39, 15.76s/it]calibrating layers.2.blocks.11.mlp.fc1:  65%|██████▌   | 97/149 [58:30<13:39, 15.76s/it]     calibrating layers.2.blocks.11.mlp.fc1:  66%|██████▌   | 98/149 [59:01<17:14, 20.29s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▌   | 98/149 [59:01<17:14, 20.29s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▋   | 99/149 [59:32<19:41, 23.62s/it]calibrating layers.2.blocks.12.attn.qkv:  66%|██████▋   | 99/149 [59:32<19:41, 23.62s/it]calibrating layers.2.blocks.12.attn.qkv:  67%|██████▋   | 100/149 [59:53<18:31, 22.69s/it]calibrating layers.2.blocks.12.attn.proj:  67%|██████▋   | 100/149 [59:53<18:31, 22.69s/it]calibrating layers.2.blocks.12.attn.proj:  68%|██████▊   | 101/149 [1:00:03<15:14, 19.04s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 101/149 [1:00:03<15:14, 19.04s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 102/149 [1:00:16<13:20, 17.03s/it]calibrating layers.2.blocks.12.attn.matmul2:  68%|██████▊   | 102/149 [1:00:16<13:20, 17.03s/it]calibrating layers.2.blocks.12.attn.matmul2:  69%|██████▉   | 103/149 [1:00:29<12:10, 15.88s/it]calibrating layers.2.blocks.12.mlp.fc1:  69%|██████▉   | 103/149 [1:00:29<12:10, 15.88s/it]     calibrating layers.2.blocks.12.mlp.fc1:  70%|██████▉   | 104/149 [1:01:00<15:17, 20.39s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|██████▉   | 104/149 [1:01:00<15:17, 20.39s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|███████   | 105/149 [1:01:31<17:20, 23.64s/it]calibrating layers.2.blocks.13.attn.qkv:  70%|███████   | 105/149 [1:01:31<17:20, 23.64s/it]calibrating layers.2.blocks.13.attn.qkv:  71%|███████   | 106/149 [1:01:52<16:16, 22.72s/it]calibrating layers.2.blocks.13.attn.proj:  71%|███████   | 106/149 [1:01:52<16:16, 22.72s/it]calibrating layers.2.blocks.13.attn.proj:  72%|███████▏  | 107/149 [1:02:02<13:21, 19.07s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 107/149 [1:02:02<13:21, 19.07s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 108/149 [1:02:16<11:52, 17.38s/it]calibrating layers.2.blocks.13.attn.matmul2:  72%|███████▏  | 108/149 [1:02:16<11:52, 17.38s/it]calibrating layers.2.blocks.13.attn.matmul2:  73%|███████▎  | 109/149 [1:02:29<10:49, 16.23s/it]calibrating layers.2.blocks.13.mlp.fc1:  73%|███████▎  | 109/149 [1:02:29<10:49, 16.23s/it]     calibrating layers.2.blocks.13.mlp.fc1:  74%|███████▍  | 110/149 [1:03:00<13:25, 20.65s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 110/149 [1:03:00<13:25, 20.65s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 111/149 [1:03:32<15:09, 23.94s/it]calibrating layers.2.blocks.14.attn.qkv:  74%|███████▍  | 111/149 [1:03:32<15:09, 23.94s/it]calibrating layers.2.blocks.14.attn.qkv:  75%|███████▌  | 112/149 [1:03:52<14:08, 22.93s/it]calibrating layers.2.blocks.14.attn.proj:  75%|███████▌  | 112/149 [1:03:52<14:08, 22.93s/it]calibrating layers.2.blocks.14.attn.proj:  76%|███████▌  | 113/149 [1:04:03<11:32, 19.23s/it]calibrating layers.2.blocks.14.attn.matmul1:  76%|███████▌  | 113/149 [1:04:03<11:32, 19.23s/it]calibrating layers.2.blocks.14.attn.matmul1:  77%|███████▋  | 114/149 [1:04:16<10:03, 17.23s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 114/149 [1:04:16<10:03, 17.23s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 115/149 [1:04:29<09:07, 16.09s/it]calibrating layers.2.blocks.14.mlp.fc1:  77%|███████▋  | 115/149 [1:04:29<09:07, 16.09s/it]     calibrating layers.2.blocks.14.mlp.fc1:  78%|███████▊  | 116/149 [1:05:00<11:19, 20.59s/it]calibrating layers.2.blocks.14.mlp.fc2:  78%|███████▊  | 116/149 [1:05:00<11:19, 20.59s/it]calibrating layers.2.blocks.14.mlp.fc2:  79%|███████▊  | 117/149 [1:05:32<12:43, 23.87s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▊  | 117/149 [1:05:32<12:43, 23.87s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▉  | 118/149 [1:05:52<11:50, 22.91s/it]calibrating layers.2.blocks.15.attn.proj:  79%|███████▉  | 118/149 [1:05:52<11:50, 22.91s/it]calibrating layers.2.blocks.15.attn.proj:  80%|███████▉  | 119/149 [1:06:03<09:35, 19.20s/it]calibrating layers.2.blocks.15.attn.matmul1:  80%|███████▉  | 119/149 [1:06:03<09:35, 19.20s/it]calibrating layers.2.blocks.15.attn.matmul1:  81%|████████  | 120/149 [1:06:16<08:26, 17.45s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 120/149 [1:06:16<08:26, 17.45s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 121/149 [1:06:30<07:36, 16.29s/it]calibrating layers.2.blocks.15.mlp.fc1:  81%|████████  | 121/149 [1:06:30<07:36, 16.29s/it]     calibrating layers.2.blocks.15.mlp.fc1:  82%|████████▏ | 122/149 [1:07:01<09:20, 20.75s/it]calibrating layers.2.blocks.15.mlp.fc2:  82%|████████▏ | 122/149 [1:07:01<09:20, 20.75s/it]calibrating layers.2.blocks.15.mlp.fc2:  83%|████████▎ | 123/149 [1:07:33<10:24, 24.02s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 123/149 [1:07:33<10:24, 24.02s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 124/149 [1:07:53<09:36, 23.05s/it]calibrating layers.2.blocks.16.attn.proj:  83%|████████▎ | 124/149 [1:07:53<09:36, 23.05s/it]calibrating layers.2.blocks.16.attn.proj:  84%|████████▍ | 125/149 [1:08:04<07:43, 19.31s/it]calibrating layers.2.blocks.16.attn.matmul1:  84%|████████▍ | 125/149 [1:08:04<07:43, 19.31s/it]calibrating layers.2.blocks.16.attn.matmul1:  85%|████████▍ | 126/149 [1:08:17<06:43, 17.55s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▍ | 126/149 [1:08:17<06:43, 17.55s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▌ | 127/149 [1:08:31<05:58, 16.30s/it]calibrating layers.2.blocks.16.mlp.fc1:  85%|████████▌ | 127/149 [1:08:31<05:58, 16.30s/it]     calibrating layers.2.blocks.16.mlp.fc1:  86%|████████▌ | 128/149 [1:09:02<07:16, 20.76s/it]calibrating layers.2.blocks.16.mlp.fc2:  86%|████████▌ | 128/149 [1:09:02<07:16, 20.76s/it]calibrating layers.2.blocks.16.mlp.fc2:  87%|████████▋ | 129/149 [1:09:33<07:59, 23.99s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 129/149 [1:09:33<07:59, 23.99s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 130/149 [1:09:54<07:17, 23.02s/it]calibrating layers.2.blocks.17.attn.proj:  87%|████████▋ | 130/149 [1:09:54<07:17, 23.02s/it]calibrating layers.2.blocks.17.attn.proj:  88%|████████▊ | 131/149 [1:10:05<05:47, 19.29s/it]calibrating layers.2.blocks.17.attn.matmul1:  88%|████████▊ | 131/149 [1:10:05<05:47, 19.29s/it]calibrating layers.2.blocks.17.attn.matmul1:  89%|████████▊ | 132/149 [1:10:17<04:52, 17.18s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▊ | 132/149 [1:10:17<04:52, 17.18s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▉ | 133/149 [1:10:30<04:15, 15.98s/it]calibrating layers.2.blocks.17.mlp.fc1:  89%|████████▉ | 133/149 [1:10:30<04:15, 15.98s/it]     calibrating layers.2.blocks.17.mlp.fc1:  90%|████████▉ | 134/149 [1:11:01<05:07, 20.47s/it]calibrating layers.2.blocks.17.mlp.fc2:  90%|████████▉ | 134/149 [1:11:01<05:07, 20.47s/it]calibrating layers.2.blocks.17.mlp.fc2:  91%|█████████ | 135/149 [1:11:33<05:33, 23.79s/it]calibrating layers.3.downsample.reduction:  91%|█████████ | 135/149 [1:11:33<05:33, 23.79s/it]calibrating layers.3.downsample.reduction:  91%|█████████▏| 136/149 [1:11:44<04:22, 20.17s/it]calibrating layers.3.blocks.0.attn.qkv:  91%|█████████▏| 136/149 [1:11:44<04:22, 20.17s/it]   calibrating layers.3.blocks.0.attn.qkv:  92%|█████████▏| 137/149 [1:12:00<03:44, 18.68s/it]calibrating layers.3.blocks.0.attn.proj:  92%|█████████▏| 137/149 [1:12:00<03:44, 18.68s/it]calibrating layers.3.blocks.0.attn.proj:  93%|█████████▎| 138/149 [1:12:07<02:47, 15.20s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 138/149 [1:12:07<02:47, 15.20s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 139/149 [1:12:19<02:23, 14.34s/it]calibrating layers.3.blocks.0.attn.matmul2:  93%|█████████▎| 139/149 [1:12:19<02:23, 14.34s/it]calibrating layers.3.blocks.0.attn.matmul2:  94%|█████████▍| 140/149 [1:12:31<02:02, 13.66s/it]calibrating layers.3.blocks.0.mlp.fc1:  94%|█████████▍| 140/149 [1:12:31<02:02, 13.66s/it]     calibrating layers.3.blocks.0.mlp.fc1:  95%|█████████▍| 141/149 [1:12:52<02:06, 15.76s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▍| 141/149 [1:12:52<02:06, 15.76s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▌| 142/149 [1:13:13<02:01, 17.39s/it]calibrating layers.3.blocks.1.attn.qkv:  95%|█████████▌| 142/149 [1:13:13<02:01, 17.39s/it]calibrating layers.3.blocks.1.attn.qkv:  96%|█████████▌| 143/149 [1:13:28<01:40, 16.76s/it]calibrating layers.3.blocks.1.attn.proj:  96%|█████████▌| 143/149 [1:13:28<01:40, 16.76s/it]calibrating layers.3.blocks.1.attn.proj:  97%|█████████▋| 144/149 [1:13:35<01:09, 13.85s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 144/149 [1:13:35<01:09, 13.85s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 145/149 [1:13:48<00:53, 13.36s/it]calibrating layers.3.blocks.1.attn.matmul2:  97%|█████████▋| 145/149 [1:13:48<00:53, 13.36s/it]calibrating layers.3.blocks.1.attn.matmul2:  98%|█████████▊| 146/149 [1:14:00<00:38, 12.97s/it]calibrating layers.3.blocks.1.mlp.fc1:  98%|█████████▊| 146/149 [1:14:00<00:38, 12.97s/it]     calibrating layers.3.blocks.1.mlp.fc1:  99%|█████████▊| 147/149 [1:14:20<00:30, 15.33s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▊| 147/149 [1:14:20<00:30, 15.33s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▉| 148/149 [1:14:42<00:17, 17.18s/it]calibrating head.fc:  99%|█████████▉| 148/149 [1:14:42<00:17, 17.18s/it]                  calibrating head.fc: 100%|██████████| 149/149 [1:14:45<00:00, 12.96s/it]calibrating head.fc: 100%|██████████| 149/149 [1:14:45<00:00, 30.10s/it]
2025-09-11 12:13:22 - mse guided calibration finished.
Saving checkpoint to ./checkpoint/quant_result/20250911_1055/swin_small_w4_a4_calibsize_1000_mse.pth
Validating after calibration ...
Test: [0/100]	Time 17.018 (17.018)	Loss 0.6645 (0.6645)	Prec@1 87.400 (87.400)	Prec@5 97.600 (97.600)
Test: [10/100]	Time 1.761 (3.148)	Loss 0.7315 (0.7835)	Prec@1 87.000 (84.655)	Prec@5 97.600 (97.255)
Test: [20/100]	Time 1.754 (2.485)	Loss 1.0253 (0.8670)	Prec@1 76.000 (82.324)	Prec@5 97.000 (96.724)
Test: [30/100]	Time 1.759 (2.252)	Loss 0.8665 (0.8873)	Prec@1 83.200 (81.394)	Prec@5 98.200 (96.606)
Test: [40/100]	Time 1.761 (2.132)	Loss 1.1658 (0.8762)	Prec@1 73.200 (81.820)	Prec@5 94.600 (96.649)
Test: [50/100]	Time 1.759 (2.059)	Loss 1.4022 (0.9376)	Prec@1 69.400 (80.431)	Prec@5 90.800 (96.039)
Test: [60/100]	Time 1.762 (2.010)	Loss 0.9882 (0.9562)	Prec@1 83.000 (80.272)	Prec@5 94.600 (95.803)
Test: [70/100]	Time 1.760 (1.975)	Loss 1.2208 (0.9907)	Prec@1 75.600 (79.468)	Prec@5 94.000 (95.490)
Test: [80/100]	Time 1.759 (1.948)	Loss 0.9821 (1.0055)	Prec@1 80.600 (79.281)	Prec@5 95.400 (95.277)
Test: [90/100]	Time 1.756 (1.927)	Loss 1.3844 (1.0288)	Prec@1 70.600 (78.633)	Prec@5 92.000 (95.086)
 * Prec@1 78.862 Prec@5 95.224 Loss 1.018 Time 191.420
Building calibrator ...
2025-09-11 12:16:53 - start mse guided block reconstruction
reconstructing patch_embed ...
initializing raw input and raw output ...
adaround training for patch_embed ...
wraping quantizers in patch_embed ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.098 (rec:0.098, round:0.000)	b=0.00	count=500
Total loss:	0.059 (rec:0.059, round:0.000)	b=0.00	count=1000
Total loss:	0.046 (rec:0.046, round:0.000)	b=0.00	count=1500
Total loss:	0.039 (rec:0.039, round:0.000)	b=0.00	count=2000
Total loss:	0.023 (rec:0.023, round:0.000)	b=0.00	count=2500
Total loss:	0.028 (rec:0.028, round:0.000)	b=0.00	count=3000
Total loss:	0.016 (rec:0.016, round:0.000)	b=0.00	count=3500
Total loss:	43.246 (rec:0.015, round:43.230)	b=20.00	count=4000
Total loss:	29.177 (rec:0.027, round:29.151)	b=19.44	count=4500
Total loss:	27.169 (rec:0.026, round:27.143)	b=18.88	count=5000
Total loss:	25.773 (rec:0.023, round:25.749)	b=18.31	count=5500
Total loss:	24.758 (rec:0.024, round:24.734)	b=17.75	count=6000
Total loss:	23.588 (rec:0.014, round:23.574)	b=17.19	count=6500
Total loss:	22.495 (rec:0.025, round:22.471)	b=16.62	count=7000
Total loss:	21.534 (rec:0.013, round:21.520)	b=16.06	count=7500
Total loss:	20.528 (rec:0.014, round:20.513)	b=15.50	count=8000
Total loss:	19.579 (rec:0.026, round:19.552)	b=14.94	count=8500
Total loss:	18.752 (rec:0.022, round:18.730)	b=14.38	count=9000
Total loss:	18.025 (rec:0.034, round:17.990)	b=13.81	count=9500
Total loss:	16.971 (rec:0.028, round:16.943)	b=13.25	count=10000
Total loss:	15.896 (rec:0.031, round:15.865)	b=12.69	count=10500
Total loss:	14.782 (rec:0.026, round:14.756)	b=12.12	count=11000
Total loss:	13.702 (rec:0.036, round:13.666)	b=11.56	count=11500
Total loss:	12.508 (rec:0.049, round:12.459)	b=11.00	count=12000
Total loss:	11.328 (rec:0.043, round:11.284)	b=10.44	count=12500
Total loss:	10.051 (rec:0.060, round:9.991)	b=9.88	count=13000
Total loss:	9.134 (rec:0.075, round:9.059)	b=9.31	count=13500
Total loss:	8.117 (rec:0.071, round:8.046)	b=8.75	count=14000
Total loss:	6.937 (rec:0.061, round:6.876)	b=8.19	count=14500
Total loss:	5.849 (rec:0.111, round:5.738)	b=7.62	count=15000
Total loss:	4.679 (rec:0.109, round:4.570)	b=7.06	count=15500
Total loss:	3.779 (rec:0.112, round:3.667)	b=6.50	count=16000
Total loss:	2.966 (rec:0.152, round:2.815)	b=5.94	count=16500
Total loss:	2.198 (rec:0.169, round:2.029)	b=5.38	count=17000
Total loss:	1.527 (rec:0.188, round:1.340)	b=4.81	count=17500
Total loss:	1.065 (rec:0.233, round:0.832)	b=4.25	count=18000
Total loss:	0.879 (rec:0.279, round:0.599)	b=3.69	count=18500
Total loss:	0.893 (rec:0.468, round:0.425)	b=3.12	count=19000
Total loss:	0.696 (rec:0.402, round:0.294)	b=2.56	count=19500
Total loss:	0.623 (rec:0.390, round:0.232)	b=2.00	count=20000
finished reconstructing patch_embed.
reconstructing layers.0.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.0.blocks.0 ...
wraping quantizers in layers.0.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.265 (rec:1.265, round:0.000)	b=0.00	count=500
Total loss:	1.188 (rec:1.188, round:0.000)	b=0.00	count=1000
Total loss:	1.053 (rec:1.053, round:0.000)	b=0.00	count=1500
Total loss:	1.046 (rec:1.046, round:0.000)	b=0.00	count=2000
Total loss:	0.978 (rec:0.978, round:0.000)	b=0.00	count=2500
Total loss:	0.967 (rec:0.967, round:0.000)	b=0.00	count=3000
Total loss:	0.992 (rec:0.992, round:0.000)	b=0.00	count=3500
Total loss:	878.023 (rec:1.021, round:877.002)	b=20.00	count=4000
Total loss:	415.755 (rec:0.955, round:414.800)	b=19.44	count=4500
Total loss:	369.173 (rec:1.025, round:368.148)	b=18.88	count=5000
Total loss:	335.792 (rec:0.942, round:334.851)	b=18.31	count=5500
Total loss:	307.694 (rec:0.960, round:306.735)	b=17.75	count=6000
Total loss:	283.895 (rec:0.992, round:282.904)	b=17.19	count=6500
Total loss:	261.670 (rec:0.972, round:260.698)	b=16.62	count=7000
Total loss:	242.267 (rec:1.000, round:241.267)	b=16.06	count=7500
Total loss:	224.116 (rec:0.977, round:223.139)	b=15.50	count=8000
Total loss:	207.762 (rec:0.966, round:206.796)	b=14.94	count=8500
Total loss:	192.889 (rec:0.912, round:191.977)	b=14.38	count=9000
Total loss:	179.223 (rec:1.007, round:178.216)	b=13.81	count=9500
Total loss:	165.950 (rec:0.910, round:165.041)	b=13.25	count=10000
Total loss:	153.043 (rec:0.942, round:152.102)	b=12.69	count=10500
Total loss:	140.545 (rec:0.946, round:139.599)	b=12.12	count=11000
Total loss:	128.330 (rec:1.025, round:127.304)	b=11.56	count=11500
Total loss:	116.233 (rec:0.953, round:115.280)	b=11.00	count=12000
Total loss:	104.637 (rec:0.912, round:103.725)	b=10.44	count=12500
Total loss:	92.822 (rec:0.970, round:91.853)	b=9.88	count=13000
Total loss:	80.846 (rec:0.982, round:79.864)	b=9.31	count=13500
Total loss:	68.955 (rec:0.973, round:67.982)	b=8.75	count=14000
Total loss:	56.840 (rec:0.970, round:55.869)	b=8.19	count=14500
Total loss:	45.621 (rec:0.919, round:44.701)	b=7.62	count=15000
Total loss:	35.781 (rec:1.068, round:34.713)	b=7.06	count=15500
Total loss:	26.060 (rec:0.991, round:25.070)	b=6.50	count=16000
Total loss:	17.380 (rec:1.069, round:16.311)	b=5.94	count=16500
Total loss:	10.972 (rec:0.957, round:10.015)	b=5.38	count=17000
Total loss:	6.489 (rec:1.072, round:5.417)	b=4.81	count=17500
Total loss:	3.688 (rec:1.028, round:2.661)	b=4.25	count=18000
Total loss:	2.094 (rec:1.051, round:1.042)	b=3.69	count=18500
Total loss:	1.356 (rec:1.071, round:0.285)	b=3.12	count=19000
Total loss:	1.164 (rec:1.106, round:0.058)	b=2.56	count=19500
Total loss:	1.068 (rec:1.059, round:0.009)	b=2.00	count=20000
finished reconstructing layers.0.blocks.0.
reconstructing layers.0.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.0.blocks.1 ...
wraping quantizers in layers.0.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.741 (rec:1.741, round:0.000)	b=0.00	count=500
Total loss:	1.538 (rec:1.538, round:0.000)	b=0.00	count=1000
Total loss:	1.525 (rec:1.525, round:0.000)	b=0.00	count=1500
Total loss:	1.548 (rec:1.548, round:0.000)	b=0.00	count=2000
Total loss:	1.507 (rec:1.507, round:0.000)	b=0.00	count=2500
Total loss:	1.638 (rec:1.638, round:0.000)	b=0.00	count=3000
Total loss:	1.435 (rec:1.435, round:0.000)	b=0.00	count=3500
Total loss:	881.399 (rec:1.414, round:879.985)	b=20.00	count=4000
Total loss:	457.339 (rec:1.464, round:455.875)	b=19.44	count=4500
Total loss:	411.854 (rec:1.495, round:410.359)	b=18.88	count=5000
Total loss:	379.970 (rec:1.410, round:378.560)	b=18.31	count=5500
Total loss:	352.374 (rec:1.464, round:350.910)	b=17.75	count=6000
Total loss:	327.736 (rec:1.400, round:326.336)	b=17.19	count=6500
Total loss:	305.666 (rec:1.539, round:304.127)	b=16.62	count=7000
Total loss:	286.169 (rec:1.468, round:284.701)	b=16.06	count=7500
Total loss:	267.359 (rec:1.503, round:265.855)	b=15.50	count=8000
Total loss:	250.607 (rec:1.397, round:249.210)	b=14.94	count=8500
Total loss:	234.800 (rec:1.407, round:233.393)	b=14.38	count=9000
Total loss:	219.610 (rec:1.526, round:218.084)	b=13.81	count=9500
Total loss:	204.836 (rec:1.420, round:203.416)	b=13.25	count=10000
Total loss:	190.564 (rec:1.524, round:189.040)	b=12.69	count=10500
Total loss:	176.725 (rec:1.463, round:175.262)	b=12.12	count=11000
Total loss:	163.043 (rec:1.506, round:161.537)	b=11.56	count=11500
Total loss:	149.114 (rec:1.551, round:147.563)	b=11.00	count=12000
Total loss:	134.829 (rec:1.474, round:133.355)	b=10.44	count=12500
Total loss:	120.708 (rec:1.499, round:119.210)	b=9.88	count=13000
Total loss:	106.908 (rec:1.519, round:105.390)	b=9.31	count=13500
Total loss:	92.300 (rec:1.497, round:90.803)	b=8.75	count=14000
Total loss:	78.135 (rec:1.438, round:76.697)	b=8.19	count=14500
Total loss:	64.354 (rec:1.426, round:62.927)	b=7.62	count=15000
Total loss:	50.707 (rec:1.458, round:49.249)	b=7.06	count=15500
Total loss:	37.308 (rec:1.516, round:35.792)	b=6.50	count=16000
Total loss:	25.543 (rec:1.548, round:23.995)	b=5.94	count=16500
Total loss:	16.027 (rec:1.521, round:14.506)	b=5.38	count=17000
Total loss:	8.615 (rec:1.461, round:7.154)	b=4.81	count=17500
Total loss:	4.492 (rec:1.567, round:2.925)	b=4.25	count=18000
Total loss:	2.472 (rec:1.530, round:0.942)	b=3.69	count=18500
Total loss:	1.686 (rec:1.522, round:0.164)	b=3.12	count=19000
Total loss:	1.607 (rec:1.591, round:0.015)	b=2.56	count=19500
Total loss:	1.569 (rec:1.568, round:0.001)	b=2.00	count=20000
finished reconstructing layers.0.blocks.1.
reconstructing layers.1.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.downsample ...
wraping quantizers in layers.1.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.931 (rec:1.931, round:0.000)	b=0.00	count=500
Total loss:	1.700 (rec:1.700, round:0.000)	b=0.00	count=1000
Total loss:	1.910 (rec:1.910, round:0.000)	b=0.00	count=1500
Total loss:	1.868 (rec:1.868, round:0.000)	b=0.00	count=2000
Total loss:	1.768 (rec:1.768, round:0.000)	b=0.00	count=2500
Total loss:	1.754 (rec:1.754, round:0.000)	b=0.00	count=3000
Total loss:	2.035 (rec:2.035, round:0.000)	b=0.00	count=3500
Total loss:	584.599 (rec:1.999, round:582.600)	b=20.00	count=4000
Total loss:	322.438 (rec:1.926, round:320.512)	b=19.44	count=4500
Total loss:	295.952 (rec:1.738, round:294.213)	b=18.88	count=5000
Total loss:	277.795 (rec:1.831, round:275.964)	b=18.31	count=5500
Total loss:	261.803 (rec:1.926, round:259.877)	b=17.75	count=6000
Total loss:	248.651 (rec:1.829, round:246.821)	b=17.19	count=6500
Total loss:	235.912 (rec:1.751, round:234.160)	b=16.62	count=7000
Total loss:	224.342 (rec:1.779, round:222.563)	b=16.06	count=7500
Total loss:	214.144 (rec:1.847, round:212.296)	b=15.50	count=8000
Total loss:	204.209 (rec:1.869, round:202.340)	b=14.94	count=8500
Total loss:	193.835 (rec:1.817, round:192.017)	b=14.38	count=9000
Total loss:	184.239 (rec:1.955, round:182.284)	b=13.81	count=9500
Total loss:	174.319 (rec:1.813, round:172.507)	b=13.25	count=10000
Total loss:	164.227 (rec:1.729, round:162.498)	b=12.69	count=10500
Total loss:	154.013 (rec:1.782, round:152.231)	b=12.12	count=11000
Total loss:	143.400 (rec:1.722, round:141.678)	b=11.56	count=11500
Total loss:	132.771 (rec:1.989, round:130.781)	b=11.00	count=12000
Total loss:	120.977 (rec:1.899, round:119.078)	b=10.44	count=12500
Total loss:	109.676 (rec:2.001, round:107.674)	b=9.88	count=13000
Total loss:	97.814 (rec:1.865, round:95.949)	b=9.31	count=13500
Total loss:	85.546 (rec:1.777, round:83.768)	b=8.75	count=14000
Total loss:	73.243 (rec:1.931, round:71.313)	b=8.19	count=14500
Total loss:	60.780 (rec:1.876, round:58.904)	b=7.62	count=15000
Total loss:	47.912 (rec:1.892, round:46.020)	b=7.06	count=15500
Total loss:	36.055 (rec:2.040, round:34.015)	b=6.50	count=16000
Total loss:	24.873 (rec:1.880, round:22.993)	b=5.94	count=16500
Total loss:	16.116 (rec:1.846, round:14.270)	b=5.38	count=17000
Total loss:	9.941 (rec:1.911, round:8.030)	b=4.81	count=17500
Total loss:	5.949 (rec:2.051, round:3.898)	b=4.25	count=18000
Total loss:	3.329 (rec:1.771, round:1.558)	b=3.69	count=18500
Total loss:	2.378 (rec:1.923, round:0.455)	b=3.12	count=19000
Total loss:	2.065 (rec:1.988, round:0.077)	b=2.56	count=19500
Total loss:	1.865 (rec:1.857, round:0.008)	b=2.00	count=20000
finished reconstructing layers.1.downsample.
reconstructing layers.1.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.blocks.0 ...
wraping quantizers in layers.1.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.919 (rec:1.919, round:0.000)	b=0.00	count=500
Total loss:	1.823 (rec:1.823, round:0.000)	b=0.00	count=1000
Total loss:	1.966 (rec:1.966, round:0.000)	b=0.00	count=1500
Total loss:	1.860 (rec:1.860, round:0.000)	b=0.00	count=2000
Total loss:	1.839 (rec:1.839, round:0.000)	b=0.00	count=2500
Total loss:	1.922 (rec:1.922, round:0.000)	b=0.00	count=3000
Total loss:	1.708 (rec:1.708, round:0.000)	b=0.00	count=3500
Total loss:	3775.077 (rec:1.877, round:3773.200)	b=20.00	count=4000
Total loss:	1927.008 (rec:1.893, round:1925.115)	b=19.44	count=4500
Total loss:	1761.229 (rec:1.867, round:1759.362)	b=18.88	count=5000
Total loss:	1645.665 (rec:1.836, round:1643.829)	b=18.31	count=5500
Total loss:	1545.523 (rec:1.721, round:1543.802)	b=17.75	count=6000
Total loss:	1455.013 (rec:1.909, round:1453.104)	b=17.19	count=6500
Total loss:	1369.586 (rec:1.915, round:1367.671)	b=16.62	count=7000
Total loss:	1290.308 (rec:1.758, round:1288.549)	b=16.06	count=7500
Total loss:	1215.010 (rec:1.846, round:1213.163)	b=15.50	count=8000
Total loss:	1141.745 (rec:1.783, round:1139.962)	b=14.94	count=8500
Total loss:	1071.864 (rec:1.897, round:1069.967)	b=14.38	count=9000
Total loss:	1003.041 (rec:1.821, round:1001.220)	b=13.81	count=9500
Total loss:	935.949 (rec:1.771, round:934.178)	b=13.25	count=10000
Total loss:	869.606 (rec:1.869, round:867.737)	b=12.69	count=10500
Total loss:	803.028 (rec:1.861, round:801.167)	b=12.12	count=11000
Total loss:	737.447 (rec:1.842, round:735.605)	b=11.56	count=11500
Total loss:	672.708 (rec:1.743, round:670.966)	b=11.00	count=12000
Total loss:	607.751 (rec:1.876, round:605.875)	b=10.44	count=12500
Total loss:	541.942 (rec:1.909, round:540.033)	b=9.88	count=13000
Total loss:	475.413 (rec:1.777, round:473.636)	b=9.31	count=13500
Total loss:	411.426 (rec:1.868, round:409.558)	b=8.75	count=14000
Total loss:	344.976 (rec:1.827, round:343.148)	b=8.19	count=14500
Total loss:	279.892 (rec:1.886, round:278.006)	b=7.62	count=15000
Total loss:	217.607 (rec:1.805, round:215.803)	b=7.06	count=15500
Total loss:	158.195 (rec:1.828, round:156.367)	b=6.50	count=16000
Total loss:	104.387 (rec:1.898, round:102.488)	b=5.94	count=16500
Total loss:	60.828 (rec:1.880, round:58.948)	b=5.38	count=17000
Total loss:	29.171 (rec:1.789, round:27.382)	b=4.81	count=17500
Total loss:	11.730 (rec:1.844, round:9.886)	b=4.25	count=18000
Total loss:	4.430 (rec:2.017, round:2.413)	b=3.69	count=18500
Total loss:	2.145 (rec:1.805, round:0.340)	b=3.12	count=19000
Total loss:	1.846 (rec:1.825, round:0.021)	b=2.56	count=19500
Total loss:	1.861 (rec:1.860, round:0.001)	b=2.00	count=20000
finished reconstructing layers.1.blocks.0.
reconstructing layers.1.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.blocks.1 ...
wraping quantizers in layers.1.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.874 (rec:1.874, round:0.000)	b=0.00	count=500
Total loss:	1.999 (rec:1.999, round:0.000)	b=0.00	count=1000
Total loss:	1.971 (rec:1.971, round:0.000)	b=0.00	count=1500
Total loss:	1.772 (rec:1.772, round:0.000)	b=0.00	count=2000
Total loss:	1.847 (rec:1.847, round:0.000)	b=0.00	count=2500
Total loss:	1.737 (rec:1.737, round:0.000)	b=0.00	count=3000
Total loss:	1.920 (rec:1.920, round:0.000)	b=0.00	count=3500
Total loss:	3845.331 (rec:1.816, round:3843.515)	b=20.00	count=4000
Total loss:	2008.557 (rec:1.882, round:2006.675)	b=19.44	count=4500
Total loss:	1845.976 (rec:1.909, round:1844.067)	b=18.88	count=5000
Total loss:	1732.345 (rec:1.918, round:1730.427)	b=18.31	count=5500
Total loss:	1635.418 (rec:1.976, round:1633.442)	b=17.75	count=6000
Total loss:	1547.940 (rec:1.914, round:1546.026)	b=17.19	count=6500
Total loss:	1465.982 (rec:1.913, round:1464.069)	b=16.62	count=7000
Total loss:	1387.200 (rec:1.889, round:1385.311)	b=16.06	count=7500
Total loss:	1311.775 (rec:1.950, round:1309.825)	b=15.50	count=8000
Total loss:	1238.143 (rec:2.030, round:1236.113)	b=14.94	count=8500
Total loss:	1167.137 (rec:1.988, round:1165.150)	b=14.38	count=9000
Total loss:	1098.187 (rec:1.936, round:1096.251)	b=13.81	count=9500
Total loss:	1029.157 (rec:1.722, round:1027.435)	b=13.25	count=10000
Total loss:	961.031 (rec:1.921, round:959.111)	b=12.69	count=10500
Total loss:	893.141 (rec:1.869, round:891.272)	b=12.12	count=11000
Total loss:	824.841 (rec:1.900, round:822.941)	b=11.56	count=11500
Total loss:	756.375 (rec:1.948, round:754.428)	b=11.00	count=12000
Total loss:	686.661 (rec:1.906, round:684.755)	b=10.44	count=12500
Total loss:	616.195 (rec:1.822, round:614.374)	b=9.88	count=13000
Total loss:	547.264 (rec:1.920, round:545.344)	b=9.31	count=13500
Total loss:	476.971 (rec:1.823, round:475.148)	b=8.75	count=14000
Total loss:	406.164 (rec:1.911, round:404.252)	b=8.19	count=14500
Total loss:	336.733 (rec:1.957, round:334.776)	b=7.62	count=15000
Total loss:	267.442 (rec:1.901, round:265.541)	b=7.06	count=15500
Total loss:	199.957 (rec:1.871, round:198.085)	b=6.50	count=16000
Total loss:	136.450 (rec:1.907, round:134.543)	b=5.94	count=16500
Total loss:	82.295 (rec:1.920, round:80.375)	b=5.38	count=17000
Total loss:	41.105 (rec:1.888, round:39.217)	b=4.81	count=17500
Total loss:	16.034 (rec:1.991, round:14.043)	b=4.25	count=18000
Total loss:	5.309 (rec:1.865, round:3.444)	b=3.69	count=18500
Total loss:	2.410 (rec:1.920, round:0.490)	b=3.12	count=19000
Total loss:	1.931 (rec:1.908, round:0.022)	b=2.56	count=19500
Total loss:	1.871 (rec:1.871, round:0.001)	b=2.00	count=20000
finished reconstructing layers.1.blocks.1.
reconstructing layers.2.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.downsample ...
wraping quantizers in layers.2.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.932 (rec:1.932, round:0.000)	b=0.00	count=500
Total loss:	1.951 (rec:1.951, round:0.000)	b=0.00	count=1000
Total loss:	1.898 (rec:1.898, round:0.000)	b=0.00	count=1500
Total loss:	1.947 (rec:1.947, round:0.000)	b=0.00	count=2000
Total loss:	1.908 (rec:1.908, round:0.000)	b=0.00	count=2500
Total loss:	1.883 (rec:1.883, round:0.000)	b=0.00	count=3000
Total loss:	1.746 (rec:1.746, round:0.000)	b=0.00	count=3500
Total loss:	2463.509 (rec:1.870, round:2461.639)	b=20.00	count=4000
Total loss:	1250.880 (rec:1.896, round:1248.985)	b=19.44	count=4500
Total loss:	1141.769 (rec:1.928, round:1139.842)	b=18.88	count=5000
Total loss:	1064.782 (rec:1.986, round:1062.796)	b=18.31	count=5500
Total loss:	998.455 (rec:1.820, round:996.635)	b=17.75	count=6000
Total loss:	938.171 (rec:2.065, round:936.106)	b=17.19	count=6500
Total loss:	882.675 (rec:1.831, round:880.845)	b=16.62	count=7000
Total loss:	831.765 (rec:1.802, round:829.963)	b=16.06	count=7500
Total loss:	783.039 (rec:1.939, round:781.100)	b=15.50	count=8000
Total loss:	735.689 (rec:1.796, round:733.892)	b=14.94	count=8500
Total loss:	690.640 (rec:1.814, round:688.826)	b=14.38	count=9000
Total loss:	646.002 (rec:1.821, round:644.181)	b=13.81	count=9500
Total loss:	602.357 (rec:1.930, round:600.427)	b=13.25	count=10000
Total loss:	560.969 (rec:1.798, round:559.171)	b=12.69	count=10500
Total loss:	519.823 (rec:1.919, round:517.903)	b=12.12	count=11000
Total loss:	479.228 (rec:1.839, round:477.389)	b=11.56	count=11500
Total loss:	438.909 (rec:1.834, round:437.075)	b=11.00	count=12000
Total loss:	397.139 (rec:1.757, round:395.382)	b=10.44	count=12500
Total loss:	356.681 (rec:1.819, round:354.862)	b=9.88	count=13000
Total loss:	315.435 (rec:1.912, round:313.523)	b=9.31	count=13500
Total loss:	273.847 (rec:1.930, round:271.917)	b=8.75	count=14000
Total loss:	232.186 (rec:1.837, round:230.349)	b=8.19	count=14500
Total loss:	190.341 (rec:1.788, round:188.553)	b=7.62	count=15000
Total loss:	149.138 (rec:1.913, round:147.225)	b=7.06	count=15500
Total loss:	111.005 (rec:1.946, round:109.059)	b=6.50	count=16000
Total loss:	76.411 (rec:1.967, round:74.444)	b=5.94	count=16500
Total loss:	48.085 (rec:1.944, round:46.141)	b=5.38	count=17000
Total loss:	26.528 (rec:2.067, round:24.461)	b=4.81	count=17500
Total loss:	12.702 (rec:1.798, round:10.904)	b=4.25	count=18000
Total loss:	5.652 (rec:1.908, round:3.744)	b=3.69	count=18500
Total loss:	2.779 (rec:1.907, round:0.872)	b=3.12	count=19000
Total loss:	2.029 (rec:1.911, round:0.117)	b=2.56	count=19500
Total loss:	1.849 (rec:1.844, round:0.005)	b=2.00	count=20000
finished reconstructing layers.2.downsample.
reconstructing layers.2.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.0 ...
wraping quantizers in layers.2.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.881 (rec:1.881, round:0.000)	b=0.00	count=500
Total loss:	1.923 (rec:1.923, round:0.000)	b=0.00	count=1000
Total loss:	1.794 (rec:1.794, round:0.000)	b=0.00	count=1500
Total loss:	1.802 (rec:1.802, round:0.000)	b=0.00	count=2000
Total loss:	1.808 (rec:1.808, round:0.000)	b=0.00	count=2500
Total loss:	1.892 (rec:1.892, round:0.000)	b=0.00	count=3000
Total loss:	1.859 (rec:1.859, round:0.000)	b=0.00	count=3500
Total loss:	16001.257 (rec:1.734, round:15999.523)	b=20.00	count=4000
Total loss:	7491.894 (rec:1.905, round:7489.989)	b=19.44	count=4500
Total loss:	6890.105 (rec:1.703, round:6888.402)	b=18.88	count=5000
Total loss:	6483.112 (rec:1.719, round:6481.393)	b=18.31	count=5500
Total loss:	6130.760 (rec:1.665, round:6129.095)	b=17.75	count=6000
Total loss:	5797.124 (rec:1.857, round:5795.268)	b=17.19	count=6500
Total loss:	5478.660 (rec:1.970, round:5476.690)	b=16.62	count=7000
Total loss:	5170.601 (rec:1.705, round:5168.895)	b=16.06	count=7500
Total loss:	4868.369 (rec:1.836, round:4866.533)	b=15.50	count=8000
Total loss:	4569.680 (rec:1.699, round:4567.981)	b=14.94	count=8500
Total loss:	4276.858 (rec:1.719, round:4275.139)	b=14.38	count=9000
Total loss:	3987.825 (rec:1.750, round:3986.075)	b=13.81	count=9500
Total loss:	3704.076 (rec:1.732, round:3702.344)	b=13.25	count=10000
Total loss:	3426.164 (rec:1.740, round:3424.424)	b=12.69	count=10500
Total loss:	3151.395 (rec:1.820, round:3149.574)	b=12.12	count=11000
Total loss:	2880.993 (rec:1.851, round:2879.142)	b=11.56	count=11500
Total loss:	2615.505 (rec:1.828, round:2613.677)	b=11.00	count=12000
Total loss:	2354.196 (rec:1.747, round:2352.448)	b=10.44	count=12500
Total loss:	2094.157 (rec:1.691, round:2092.466)	b=9.88	count=13000
Total loss:	1840.872 (rec:1.759, round:1839.112)	b=9.31	count=13500
Total loss:	1593.718 (rec:1.736, round:1591.983)	b=8.75	count=14000
Total loss:	1351.646 (rec:1.792, round:1349.854)	b=8.19	count=14500
Total loss:	1115.431 (rec:1.846, round:1113.585)	b=7.62	count=15000
Total loss:	888.552 (rec:2.129, round:886.422)	b=7.06	count=15500
Total loss:	668.915 (rec:1.704, round:667.211)	b=6.50	count=16000
Total loss:	458.232 (rec:1.930, round:456.303)	b=5.94	count=16500
Total loss:	259.480 (rec:1.919, round:257.561)	b=5.38	count=17000
Total loss:	101.391 (rec:1.876, round:99.516)	b=4.81	count=17500
Total loss:	30.623 (rec:1.846, round:28.777)	b=4.25	count=18000
Total loss:	8.225 (rec:2.020, round:6.205)	b=3.69	count=18500
Total loss:	2.723 (rec:1.863, round:0.860)	b=3.12	count=19000
Total loss:	1.915 (rec:1.879, round:0.036)	b=2.56	count=19500
Total loss:	1.883 (rec:1.883, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.0.
reconstructing layers.2.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.1 ...
wraping quantizers in layers.2.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	2.053 (rec:2.053, round:0.000)	b=0.00	count=500
Total loss:	2.098 (rec:2.098, round:0.000)	b=0.00	count=1000
Total loss:	1.873 (rec:1.873, round:0.000)	b=0.00	count=1500
Total loss:	1.954 (rec:1.954, round:0.000)	b=0.00	count=2000
Total loss:	2.071 (rec:2.071, round:0.000)	b=0.00	count=2500
Total loss:	2.119 (rec:2.119, round:0.000)	b=0.00	count=3000
Total loss:	1.939 (rec:1.939, round:0.000)	b=0.00	count=3500
Total loss:	16067.968 (rec:1.994, round:16065.975)	b=20.00	count=4000
Total loss:	7768.312 (rec:1.927, round:7766.385)	b=19.44	count=4500
Total loss:	7161.492 (rec:1.942, round:7159.550)	b=18.88	count=5000
Total loss:	6753.863 (rec:1.917, round:6751.946)	b=18.31	count=5500
Total loss:	6395.636 (rec:1.778, round:6393.858)	b=17.75	count=6000
Total loss:	6063.216 (rec:1.999, round:6061.218)	b=17.19	count=6500
Total loss:	5746.770 (rec:1.871, round:5744.898)	b=16.62	count=7000
Total loss:	5437.594 (rec:1.901, round:5435.693)	b=16.06	count=7500
Total loss:	5134.261 (rec:1.778, round:5132.483)	b=15.50	count=8000
Total loss:	4835.623 (rec:2.170, round:4833.453)	b=14.94	count=8500
Total loss:	4539.628 (rec:1.809, round:4537.820)	b=14.38	count=9000
Total loss:	4248.024 (rec:1.933, round:4246.091)	b=13.81	count=9500
Total loss:	3961.067 (rec:2.096, round:3958.971)	b=13.25	count=10000
Total loss:	3674.224 (rec:1.993, round:3672.231)	b=12.69	count=10500
Total loss:	3390.112 (rec:1.881, round:3388.231)	b=12.12	count=11000
Total loss:	3111.009 (rec:1.996, round:3109.013)	b=11.56	count=11500
Total loss:	2832.972 (rec:1.996, round:2830.976)	b=11.00	count=12000
Total loss:	2559.987 (rec:1.767, round:2558.219)	b=10.44	count=12500
Total loss:	2290.468 (rec:1.946, round:2288.521)	b=9.88	count=13000
Total loss:	2022.073 (rec:1.984, round:2020.089)	b=9.31	count=13500
Total loss:	1756.957 (rec:1.974, round:1754.983)	b=8.75	count=14000
Total loss:	1497.380 (rec:1.930, round:1495.451)	b=8.19	count=14500
Total loss:	1242.414 (rec:1.957, round:1240.457)	b=7.62	count=15000
Total loss:	995.308 (rec:1.845, round:993.464)	b=7.06	count=15500
Total loss:	758.656 (rec:1.838, round:756.819)	b=6.50	count=16000
Total loss:	531.577 (rec:1.874, round:529.703)	b=5.94	count=16500
Total loss:	314.580 (rec:2.007, round:312.573)	b=5.38	count=17000
Total loss:	140.291 (rec:1.700, round:138.591)	b=4.81	count=17500
Total loss:	47.270 (rec:1.972, round:45.298)	b=4.25	count=18000
Total loss:	11.833 (rec:2.001, round:9.832)	b=3.69	count=18500
Total loss:	2.867 (rec:1.826, round:1.041)	b=3.12	count=19000
Total loss:	2.020 (rec:1.975, round:0.045)	b=2.56	count=19500
Total loss:	1.888 (rec:1.887, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.1.
reconstructing layers.2.blocks.2 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.2 ...
wraping quantizers in layers.2.blocks.2 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.927 (rec:1.927, round:0.000)	b=0.00	count=500
Total loss:	1.820 (rec:1.820, round:0.000)	b=0.00	count=1000
Total loss:	1.835 (rec:1.835, round:0.000)	b=0.00	count=1500
Total loss:	1.826 (rec:1.826, round:0.000)	b=0.00	count=2000
Total loss:	1.867 (rec:1.867, round:0.000)	b=0.00	count=2500
Total loss:	1.889 (rec:1.889, round:0.000)	b=0.00	count=3000
Total loss:	1.873 (rec:1.873, round:0.000)	b=0.00	count=3500
Total loss:	16043.239 (rec:1.807, round:16041.432)	b=20.00	count=4000
Total loss:	7857.729 (rec:1.889, round:7855.840)	b=19.44	count=4500
Total loss:	7246.223 (rec:1.859, round:7244.365)	b=18.88	count=5000
Total loss:	6834.962 (rec:1.738, round:6833.225)	b=18.31	count=5500
Total loss:	6471.138 (rec:1.753, round:6469.385)	b=17.75	count=6000
Total loss:	6135.477 (rec:2.016, round:6133.460)	b=17.19	count=6500
Total loss:	5811.093 (rec:1.731, round:5809.362)	b=16.62	count=7000
Total loss:	5491.316 (rec:1.766, round:5489.550)	b=16.06	count=7500
Total loss:	5180.558 (rec:1.828, round:5178.730)	b=15.50	count=8000
Total loss:	4873.359 (rec:1.854, round:4871.504)	b=14.94	count=8500
Total loss:	4574.998 (rec:1.725, round:4573.273)	b=14.38	count=9000
Total loss:	4275.524 (rec:1.816, round:4273.708)	b=13.81	count=9500
Total loss:	3982.371 (rec:1.771, round:3980.600)	b=13.25	count=10000
Total loss:	3695.014 (rec:1.847, round:3693.168)	b=12.69	count=10500
Total loss:	3408.831 (rec:1.821, round:3407.009)	b=12.12	count=11000
Total loss:	3126.421 (rec:1.991, round:3124.430)	b=11.56	count=11500
Total loss:	2845.827 (rec:1.801, round:2844.026)	b=11.00	count=12000
Total loss:	2567.661 (rec:1.727, round:2565.934)	b=10.44	count=12500
Total loss:	2292.692 (rec:1.762, round:2290.930)	b=9.88	count=13000
Total loss:	2020.928 (rec:1.795, round:2019.132)	b=9.31	count=13500
Total loss:	1754.911 (rec:1.903, round:1753.008)	b=8.75	count=14000
Total loss:	1494.316 (rec:1.787, round:1492.528)	b=8.19	count=14500
Total loss:	1236.688 (rec:1.734, round:1234.955)	b=7.62	count=15000
Total loss:	987.849 (rec:1.859, round:985.990)	b=7.06	count=15500
Total loss:	752.342 (rec:1.857, round:750.485)	b=6.50	count=16000
Total loss:	524.733 (rec:1.788, round:522.944)	b=5.94	count=16500
Total loss:	311.500 (rec:1.807, round:309.693)	b=5.38	count=17000
Total loss:	142.950 (rec:1.918, round:141.032)	b=4.81	count=17500
Total loss:	48.940 (rec:1.787, round:47.153)	b=4.25	count=18000
Total loss:	12.164 (rec:1.965, round:10.199)	b=3.69	count=18500
Total loss:	2.842 (rec:1.839, round:1.003)	b=3.12	count=19000
Total loss:	1.867 (rec:1.830, round:0.037)	b=2.56	count=19500
Total loss:	1.815 (rec:1.813, round:0.002)	b=2.00	count=20000
finished reconstructing layers.2.blocks.2.
reconstructing layers.2.blocks.3 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.3 ...
wraping quantizers in layers.2.blocks.3 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.764 (rec:1.764, round:0.000)	b=0.00	count=500
Total loss:	1.664 (rec:1.664, round:0.000)	b=0.00	count=1000
Total loss:	1.684 (rec:1.684, round:0.000)	b=0.00	count=1500
Total loss:	1.643 (rec:1.643, round:0.000)	b=0.00	count=2000
Total loss:	1.689 (rec:1.689, round:0.000)	b=0.00	count=2500
Total loss:	1.723 (rec:1.723, round:0.000)	b=0.00	count=3000
Total loss:	1.704 (rec:1.704, round:0.000)	b=0.00	count=3500
Total loss:	16083.749 (rec:1.653, round:16082.096)	b=20.00	count=4000
Total loss:	7770.724 (rec:1.651, round:7769.073)	b=19.44	count=4500
Total loss:	7162.090 (rec:1.655, round:7160.435)	b=18.88	count=5000
Total loss:	6751.728 (rec:1.659, round:6750.068)	b=18.31	count=5500
Total loss:	6389.667 (rec:1.630, round:6388.037)	b=17.75	count=6000
Total loss:	6049.333 (rec:1.711, round:6047.622)	b=17.19	count=6500
Total loss:	5724.306 (rec:1.763, round:5722.542)	b=16.62	count=7000
Total loss:	5409.172 (rec:1.541, round:5407.631)	b=16.06	count=7500
Total loss:	5096.515 (rec:1.699, round:5094.816)	b=15.50	count=8000
Total loss:	4790.228 (rec:1.725, round:4788.503)	b=14.94	count=8500
Total loss:	4489.185 (rec:1.661, round:4487.523)	b=14.38	count=9000
Total loss:	4191.092 (rec:1.642, round:4189.450)	b=13.81	count=9500
Total loss:	3896.167 (rec:1.622, round:3894.545)	b=13.25	count=10000
Total loss:	3602.523 (rec:1.723, round:3600.800)	b=12.69	count=10500
Total loss:	3315.137 (rec:1.716, round:3313.421)	b=12.12	count=11000
Total loss:	3031.774 (rec:1.764, round:3030.010)	b=11.56	count=11500
Total loss:	2754.003 (rec:1.619, round:2752.384)	b=11.00	count=12000
Total loss:	2481.477 (rec:1.670, round:2479.807)	b=10.44	count=12500
Total loss:	2211.865 (rec:1.677, round:2210.187)	b=9.88	count=13000
Total loss:	1945.550 (rec:1.760, round:1943.790)	b=9.31	count=13500
Total loss:	1686.484 (rec:1.627, round:1684.856)	b=8.75	count=14000
Total loss:	1433.768 (rec:1.662, round:1432.106)	b=8.19	count=14500
Total loss:	1187.001 (rec:1.640, round:1185.361)	b=7.62	count=15000
Total loss:	949.279 (rec:1.519, round:947.760)	b=7.06	count=15500
Total loss:	719.871 (rec:1.668, round:718.204)	b=6.50	count=16000
Total loss:	500.813 (rec:1.662, round:499.151)	b=5.94	count=16500
Total loss:	299.129 (rec:1.813, round:297.315)	b=5.38	count=17000
Total loss:	141.580 (rec:1.680, round:139.901)	b=4.81	count=17500
Total loss:	50.207 (rec:1.698, round:48.509)	b=4.25	count=18000
Total loss:	12.239 (rec:1.734, round:10.505)	b=3.69	count=18500
Total loss:	2.674 (rec:1.620, round:1.054)	b=3.12	count=19000
Total loss:	1.760 (rec:1.727, round:0.033)	b=2.56	count=19500
Total loss:	1.730 (rec:1.730, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.3.
reconstructing layers.2.blocks.4 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.4 ...
wraping quantizers in layers.2.blocks.4 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.849 (rec:1.849, round:0.000)	b=0.00	count=500
Total loss:	1.934 (rec:1.934, round:0.000)	b=0.00	count=1000
Total loss:	2.001 (rec:2.001, round:0.000)	b=0.00	count=1500
Total loss:	1.829 (rec:1.829, round:0.000)	b=0.00	count=2000
Total loss:	1.868 (rec:1.868, round:0.000)	b=0.00	count=2500
Total loss:	1.752 (rec:1.752, round:0.000)	b=0.00	count=3000
Total loss:	1.908 (rec:1.908, round:0.000)	b=0.00	count=3500
Total loss:	16088.104 (rec:1.831, round:16086.273)	b=20.00	count=4000
Total loss:	7944.710 (rec:1.982, round:7942.729)	b=19.44	count=4500
Total loss:	7330.991 (rec:1.944, round:7329.046)	b=18.88	count=5000
Total loss:	6920.579 (rec:1.910, round:6918.669)	b=18.31	count=5500
Total loss:	6559.173 (rec:1.896, round:6557.276)	b=17.75	count=6000
Total loss:	6224.675 (rec:1.819, round:6222.856)	b=17.19	count=6500
Total loss:	5903.557 (rec:1.963, round:5901.594)	b=16.62	count=7000
Total loss:	5585.939 (rec:1.922, round:5584.018)	b=16.06	count=7500
Total loss:	5275.763 (rec:2.023, round:5273.739)	b=15.50	count=8000
Total loss:	4968.712 (rec:1.866, round:4966.847)	b=14.94	count=8500
Total loss:	4669.073 (rec:1.833, round:4667.240)	b=14.38	count=9000
Total loss:	4371.205 (rec:1.876, round:4369.329)	b=13.81	count=9500
Total loss:	4075.764 (rec:1.946, round:4073.818)	b=13.25	count=10000
Total loss:	3780.258 (rec:1.827, round:3778.431)	b=12.69	count=10500
Total loss:	3490.540 (rec:1.816, round:3488.723)	b=12.12	count=11000
Total loss:	3203.015 (rec:1.839, round:3201.177)	b=11.56	count=11500
Total loss:	2915.996 (rec:1.874, round:2914.122)	b=11.00	count=12000
Total loss:	2635.557 (rec:1.821, round:2633.735)	b=10.44	count=12500
Total loss:	2359.218 (rec:1.889, round:2357.329)	b=9.88	count=13000
Total loss:	2085.842 (rec:1.875, round:2083.967)	b=9.31	count=13500
Total loss:	1815.927 (rec:1.860, round:1814.066)	b=8.75	count=14000
Total loss:	1549.882 (rec:1.869, round:1548.013)	b=8.19	count=14500
Total loss:	1289.630 (rec:1.889, round:1287.741)	b=7.62	count=15000
Total loss:	1035.627 (rec:1.918, round:1033.709)	b=7.06	count=15500
Total loss:	792.241 (rec:1.850, round:790.391)	b=6.50	count=16000
Total loss:	559.260 (rec:1.874, round:557.386)	b=5.94	count=16500
Total loss:	345.282 (rec:1.871, round:343.411)	b=5.38	count=17000
Total loss:	172.439 (rec:1.805, round:170.634)	b=4.81	count=17500
Total loss:	64.401 (rec:1.874, round:62.527)	b=4.25	count=18000
Total loss:	16.003 (rec:1.895, round:14.108)	b=3.69	count=18500
Total loss:	3.153 (rec:1.816, round:1.337)	b=3.12	count=19000
Total loss:	1.924 (rec:1.892, round:0.032)	b=2.56	count=19500
Total loss:	1.974 (rec:1.973, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.4.
reconstructing layers.2.blocks.5 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.5 ...
wraping quantizers in layers.2.blocks.5 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.973 (rec:1.973, round:0.000)	b=0.00	count=500
Total loss:	1.926 (rec:1.926, round:0.000)	b=0.00	count=1000
Total loss:	1.860 (rec:1.860, round:0.000)	b=0.00	count=1500
Total loss:	2.043 (rec:2.043, round:0.000)	b=0.00	count=2000
Total loss:	1.799 (rec:1.799, round:0.000)	b=0.00	count=2500
Total loss:	2.012 (rec:2.012, round:0.000)	b=0.00	count=3000
Total loss:	1.894 (rec:1.894, round:0.000)	b=0.00	count=3500
Total loss:	16088.016 (rec:2.027, round:16085.988)	b=20.00	count=4000
Total loss:	7973.854 (rec:1.981, round:7971.872)	b=19.44	count=4500
Total loss:	7360.033 (rec:1.975, round:7358.059)	b=18.88	count=5000
Total loss:	6945.081 (rec:1.974, round:6943.107)	b=18.31	count=5500
Total loss:	6584.873 (rec:1.832, round:6583.040)	b=17.75	count=6000
Total loss:	6248.316 (rec:2.010, round:6246.307)	b=17.19	count=6500
Total loss:	5923.436 (rec:1.965, round:5921.471)	b=16.62	count=7000
Total loss:	5606.498 (rec:2.073, round:5604.425)	b=16.06	count=7500
Total loss:	5296.705 (rec:1.929, round:5294.775)	b=15.50	count=8000
Total loss:	4993.771 (rec:1.970, round:4991.802)	b=14.94	count=8500
Total loss:	4689.512 (rec:1.815, round:4687.696)	b=14.38	count=9000
Total loss:	4392.755 (rec:1.975, round:4390.780)	b=13.81	count=9500
Total loss:	4095.737 (rec:2.027, round:4093.709)	b=13.25	count=10000
Total loss:	3800.293 (rec:1.997, round:3798.296)	b=12.69	count=10500
Total loss:	3510.631 (rec:1.937, round:3508.694)	b=12.12	count=11000
Total loss:	3222.392 (rec:1.907, round:3220.485)	b=11.56	count=11500
Total loss:	2938.273 (rec:1.947, round:2936.325)	b=11.00	count=12000
Total loss:	2654.581 (rec:1.928, round:2652.653)	b=10.44	count=12500
Total loss:	2375.404 (rec:1.863, round:2373.541)	b=9.88	count=13000
Total loss:	2097.140 (rec:1.994, round:2095.146)	b=9.31	count=13500
Total loss:	1825.394 (rec:1.815, round:1823.579)	b=8.75	count=14000
Total loss:	1558.152 (rec:1.943, round:1556.209)	b=8.19	count=14500
Total loss:	1296.167 (rec:1.941, round:1294.226)	b=7.62	count=15000
Total loss:	1040.814 (rec:1.920, round:1038.894)	b=7.06	count=15500
Total loss:	795.122 (rec:2.030, round:793.092)	b=6.50	count=16000
Total loss:	563.355 (rec:2.104, round:561.250)	b=5.94	count=16500
Total loss:	355.532 (rec:2.144, round:353.388)	b=5.38	count=17000
Total loss:	183.806 (rec:1.900, round:181.906)	b=4.81	count=17500
Total loss:	70.974 (rec:1.977, round:68.997)	b=4.25	count=18000
Total loss:	18.670 (rec:1.932, round:16.738)	b=3.69	count=18500
Total loss:	3.865 (rec:1.901, round:1.964)	b=3.12	count=19000
Total loss:	1.935 (rec:1.873, round:0.062)	b=2.56	count=19500
Total loss:	2.007 (rec:2.007, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.5.
reconstructing layers.2.blocks.6 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.6 ...
wraping quantizers in layers.2.blocks.6 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.984 (rec:1.984, round:0.000)	b=0.00	count=500
Total loss:	1.876 (rec:1.876, round:0.000)	b=0.00	count=1000
Total loss:	1.871 (rec:1.871, round:0.000)	b=0.00	count=1500
Total loss:	1.998 (rec:1.998, round:0.000)	b=0.00	count=2000
Total loss:	1.868 (rec:1.868, round:0.000)	b=0.00	count=2500
Total loss:	1.836 (rec:1.836, round:0.000)	b=0.00	count=3000
Total loss:	2.013 (rec:2.013, round:0.000)	b=0.00	count=3500
Total loss:	16063.983 (rec:1.808, round:16062.176)	b=20.00	count=4000
Total loss:	7971.723 (rec:1.828, round:7969.896)	b=19.44	count=4500
Total loss:	7355.104 (rec:1.943, round:7353.162)	b=18.88	count=5000
Total loss:	6936.636 (rec:1.763, round:6934.873)	b=18.31	count=5500
Total loss:	6574.893 (rec:1.825, round:6573.068)	b=17.75	count=6000
Total loss:	6237.238 (rec:1.878, round:6235.360)	b=17.19	count=6500
Total loss:	5915.473 (rec:1.908, round:5913.564)	b=16.62	count=7000
Total loss:	5597.578 (rec:1.849, round:5595.729)	b=16.06	count=7500
Total loss:	5285.971 (rec:1.751, round:5284.220)	b=15.50	count=8000
Total loss:	4979.875 (rec:1.842, round:4978.033)	b=14.94	count=8500
Total loss:	4679.608 (rec:1.831, round:4677.777)	b=14.38	count=9000
Total loss:	4378.817 (rec:1.951, round:4376.865)	b=13.81	count=9500
Total loss:	4081.068 (rec:1.888, round:4079.180)	b=13.25	count=10000
Total loss:	3786.361 (rec:1.836, round:3784.525)	b=12.69	count=10500
Total loss:	3494.180 (rec:1.811, round:3492.368)	b=12.12	count=11000
Total loss:	3206.297 (rec:1.924, round:3204.374)	b=11.56	count=11500
Total loss:	2921.696 (rec:1.898, round:2919.798)	b=11.00	count=12000
Total loss:	2636.453 (rec:1.795, round:2634.658)	b=10.44	count=12500
Total loss:	2359.491 (rec:1.854, round:2357.637)	b=9.88	count=13000
Total loss:	2084.480 (rec:1.805, round:2082.675)	b=9.31	count=13500
Total loss:	1812.183 (rec:1.744, round:1810.439)	b=8.75	count=14000
Total loss:	1546.085 (rec:1.758, round:1544.327)	b=8.19	count=14500
Total loss:	1285.803 (rec:1.813, round:1283.990)	b=7.62	count=15000
Total loss:	1030.942 (rec:1.861, round:1029.081)	b=7.06	count=15500
Total loss:	785.900 (rec:1.877, round:784.024)	b=6.50	count=16000
Total loss:	554.607 (rec:1.771, round:552.837)	b=5.94	count=16500
Total loss:	348.633 (rec:1.827, round:346.806)	b=5.38	count=17000
Total loss:	184.125 (rec:1.827, round:182.298)	b=4.81	count=17500
Total loss:	73.921 (rec:1.821, round:72.100)	b=4.25	count=18000
Total loss:	19.462 (rec:1.842, round:17.620)	b=3.69	count=18500
Total loss:	3.737 (rec:1.799, round:1.937)	b=3.12	count=19000
Total loss:	1.995 (rec:1.930, round:0.065)	b=2.56	count=19500
Total loss:	1.899 (rec:1.899, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.6.
reconstructing layers.2.blocks.7 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.7 ...
wraping quantizers in layers.2.blocks.7 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.878 (rec:1.878, round:0.000)	b=0.00	count=500
Total loss:	2.037 (rec:2.037, round:0.000)	b=0.00	count=1000
Total loss:	1.841 (rec:1.841, round:0.000)	b=0.00	count=1500
Total loss:	1.752 (rec:1.752, round:0.000)	b=0.00	count=2000
Total loss:	1.744 (rec:1.744, round:0.000)	b=0.00	count=2500
Total loss:	1.714 (rec:1.714, round:0.000)	b=0.00	count=3000
Total loss:	1.822 (rec:1.822, round:0.000)	b=0.00	count=3500
Total loss:	15911.912 (rec:1.795, round:15910.117)	b=20.00	count=4000
Total loss:	7799.831 (rec:1.824, round:7798.006)	b=19.44	count=4500
Total loss:	7184.275 (rec:1.742, round:7182.533)	b=18.88	count=5000
Total loss:	6760.986 (rec:1.916, round:6759.071)	b=18.31	count=5500
Total loss:	6389.992 (rec:1.745, round:6388.247)	b=17.75	count=6000
Total loss:	6044.364 (rec:1.823, round:6042.541)	b=17.19	count=6500
Total loss:	5711.699 (rec:1.647, round:5710.052)	b=16.62	count=7000
Total loss:	5391.640 (rec:1.855, round:5389.784)	b=16.06	count=7500
Total loss:	5079.758 (rec:1.756, round:5078.002)	b=15.50	count=8000
Total loss:	4773.057 (rec:1.798, round:4771.258)	b=14.94	count=8500
Total loss:	4469.815 (rec:1.646, round:4468.170)	b=14.38	count=9000
Total loss:	4171.657 (rec:1.700, round:4169.957)	b=13.81	count=9500
Total loss:	3876.411 (rec:1.709, round:3874.702)	b=13.25	count=10000
Total loss:	3584.900 (rec:1.756, round:3583.144)	b=12.69	count=10500
Total loss:	3300.582 (rec:1.749, round:3298.833)	b=12.12	count=11000
Total loss:	3019.970 (rec:1.766, round:3018.204)	b=11.56	count=11500
Total loss:	2743.185 (rec:1.793, round:2741.392)	b=11.00	count=12000
Total loss:	2469.103 (rec:1.721, round:2467.382)	b=10.44	count=12500
Total loss:	2201.891 (rec:1.683, round:2200.207)	b=9.88	count=13000
Total loss:	1939.265 (rec:1.715, round:1937.549)	b=9.31	count=13500
Total loss:	1683.424 (rec:1.691, round:1681.734)	b=8.75	count=14000
Total loss:	1436.242 (rec:1.710, round:1434.532)	b=8.19	count=14500
Total loss:	1193.203 (rec:1.690, round:1191.512)	b=7.62	count=15000
Total loss:	957.011 (rec:1.745, round:955.266)	b=7.06	count=15500
Total loss:	729.518 (rec:1.654, round:727.865)	b=6.50	count=16000
Total loss:	519.075 (rec:1.657, round:517.418)	b=5.94	count=16500
Total loss:	330.811 (rec:1.844, round:328.967)	b=5.38	count=17000
Total loss:	178.828 (rec:1.837, round:176.991)	b=4.81	count=17500
Total loss:	73.475 (rec:1.711, round:71.764)	b=4.25	count=18000
Total loss:	19.160 (rec:1.836, round:17.325)	b=3.69	count=18500
Total loss:	3.682 (rec:1.809, round:1.872)	b=3.12	count=19000
Total loss:	1.874 (rec:1.805, round:0.069)	b=2.56	count=19500
Total loss:	1.806 (rec:1.805, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.7.
reconstructing layers.2.blocks.8 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.8 ...
wraping quantizers in layers.2.blocks.8 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.866 (rec:1.866, round:0.000)	b=0.00	count=500
Total loss:	1.868 (rec:1.868, round:0.000)	b=0.00	count=1000
Total loss:	1.700 (rec:1.700, round:0.000)	b=0.00	count=1500
Total loss:	1.649 (rec:1.649, round:0.000)	b=0.00	count=2000
Total loss:	1.695 (rec:1.695, round:0.000)	b=0.00	count=2500
Total loss:	1.698 (rec:1.698, round:0.000)	b=0.00	count=3000
Total loss:	1.710 (rec:1.710, round:0.000)	b=0.00	count=3500
Total loss:	15610.700 (rec:1.686, round:15609.014)	b=20.00	count=4000
Total loss:	7400.990 (rec:1.639, round:7399.351)	b=19.44	count=4500
Total loss:	6773.510 (rec:1.588, round:6771.922)	b=18.88	count=5000
Total loss:	6326.445 (rec:1.630, round:6324.815)	b=18.31	count=5500
Total loss:	5929.472 (rec:1.556, round:5927.916)	b=17.75	count=6000
Total loss:	5557.176 (rec:1.463, round:5555.713)	b=17.19	count=6500
Total loss:	5204.260 (rec:1.551, round:5202.709)	b=16.62	count=7000
Total loss:	4863.409 (rec:1.541, round:4861.868)	b=16.06	count=7500
Total loss:	4536.428 (rec:1.623, round:4534.804)	b=15.50	count=8000
Total loss:	4220.879 (rec:1.799, round:4219.080)	b=14.94	count=8500
Total loss:	3917.026 (rec:1.683, round:3915.344)	b=14.38	count=9000
Total loss:	3622.319 (rec:1.476, round:3620.843)	b=13.81	count=9500
Total loss:	3335.926 (rec:1.454, round:3334.472)	b=13.25	count=10000
Total loss:	3057.044 (rec:1.506, round:3055.539)	b=12.69	count=10500
Total loss:	2789.683 (rec:1.710, round:2787.973)	b=12.12	count=11000
Total loss:	2530.418 (rec:1.613, round:2528.805)	b=11.56	count=11500
Total loss:	2285.116 (rec:1.756, round:2283.361)	b=11.00	count=12000
Total loss:	2046.174 (rec:1.585, round:2044.589)	b=10.44	count=12500
Total loss:	1815.856 (rec:1.649, round:1814.207)	b=9.88	count=13000
Total loss:	1594.682 (rec:1.474, round:1593.207)	b=9.31	count=13500
Total loss:	1381.097 (rec:1.602, round:1379.496)	b=8.75	count=14000
Total loss:	1176.515 (rec:1.631, round:1174.884)	b=8.19	count=14500
Total loss:	982.252 (rec:1.561, round:980.691)	b=7.62	count=15000
Total loss:	796.470 (rec:1.495, round:794.975)	b=7.06	count=15500
Total loss:	625.681 (rec:1.556, round:624.125)	b=6.50	count=16000
Total loss:	467.242 (rec:1.497, round:465.745)	b=5.94	count=16500
Total loss:	326.563 (rec:1.539, round:325.024)	b=5.38	count=17000
Total loss:	209.364 (rec:1.602, round:207.763)	b=4.81	count=17500
Total loss:	116.961 (rec:1.504, round:115.456)	b=4.25	count=18000
Total loss:	48.526 (rec:1.544, round:46.982)	b=3.69	count=18500
Total loss:	10.874 (rec:1.607, round:9.267)	b=3.12	count=19000
Total loss:	2.474 (rec:1.681, round:0.793)	b=2.56	count=19500
Total loss:	1.605 (rec:1.591, round:0.014)	b=2.00	count=20000
finished reconstructing layers.2.blocks.8.
reconstructing layers.2.blocks.9 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.9 ...
wraping quantizers in layers.2.blocks.9 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	2.216 (rec:2.216, round:0.000)	b=0.00	count=500
Total loss:	1.797 (rec:1.797, round:0.000)	b=0.00	count=1000
Total loss:	2.029 (rec:2.029, round:0.000)	b=0.00	count=1500
Total loss:	1.952 (rec:1.952, round:0.000)	b=0.00	count=2000
Total loss:	1.736 (rec:1.736, round:0.000)	b=0.00	count=2500
Total loss:	1.844 (rec:1.844, round:0.000)	b=0.00	count=3000
Total loss:	1.622 (rec:1.622, round:0.000)	b=0.00	count=3500
Total loss:	15352.121 (rec:1.722, round:15350.399)	b=20.00	count=4000
Total loss:	6837.960 (rec:1.746, round:6836.214)	b=19.44	count=4500
Total loss:	6188.124 (rec:1.762, round:6186.362)	b=18.88	count=5000
Total loss:	5716.554 (rec:2.056, round:5714.497)	b=18.31	count=5500
Total loss:	5295.030 (rec:1.804, round:5293.226)	b=17.75	count=6000
Total loss:	4903.622 (rec:1.774, round:4901.848)	b=17.19	count=6500
Total loss:	4534.096 (rec:1.581, round:4532.515)	b=16.62	count=7000
Total loss:	4188.940 (rec:1.708, round:4187.232)	b=16.06	count=7500
Total loss:	3864.037 (rec:1.810, round:3862.227)	b=15.50	count=8000
Total loss:	3556.681 (rec:1.803, round:3554.878)	b=14.94	count=8500
Total loss:	3272.376 (rec:1.531, round:3270.844)	b=14.38	count=9000
Total loss:	3000.144 (rec:1.817, round:2998.327)	b=13.81	count=9500
Total loss:	2741.858 (rec:1.810, round:2740.048)	b=13.25	count=10000
Total loss:	2494.441 (rec:1.690, round:2492.750)	b=12.69	count=10500
Total loss:	2261.071 (rec:1.615, round:2259.456)	b=12.12	count=11000
Total loss:	2037.382 (rec:1.773, round:2035.610)	b=11.56	count=11500
Total loss:	1826.933 (rec:1.692, round:1825.241)	b=11.00	count=12000
Total loss:	1627.033 (rec:1.630, round:1625.403)	b=10.44	count=12500
Total loss:	1431.520 (rec:1.552, round:1429.968)	b=9.88	count=13000
Total loss:	1247.812 (rec:1.525, round:1246.287)	b=9.31	count=13500
Total loss:	1074.792 (rec:1.917, round:1072.875)	b=8.75	count=14000
Total loss:	912.102 (rec:1.868, round:910.234)	b=8.19	count=14500
Total loss:	757.900 (rec:1.704, round:756.195)	b=7.62	count=15000
Total loss:	613.826 (rec:1.576, round:612.251)	b=7.06	count=15500
Total loss:	478.288 (rec:1.718, round:476.570)	b=6.50	count=16000
Total loss:	353.016 (rec:1.690, round:351.326)	b=5.94	count=16500
Total loss:	245.524 (rec:1.881, round:243.643)	b=5.38	count=17000
Total loss:	153.402 (rec:1.605, round:151.797)	b=4.81	count=17500
Total loss:	84.515 (rec:1.611, round:82.904)	b=4.25	count=18000
Total loss:	36.611 (rec:1.727, round:34.883)	b=3.69	count=18500
Total loss:	10.524 (rec:1.877, round:8.648)	b=3.12	count=19000
Total loss:	2.355 (rec:1.487, round:0.868)	b=2.56	count=19500
Total loss:	1.749 (rec:1.728, round:0.021)	b=2.00	count=20000
finished reconstructing layers.2.blocks.9.
reconstructing layers.2.blocks.10 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.10 ...
wraping quantizers in layers.2.blocks.10 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	2.022 (rec:2.022, round:0.000)	b=0.00	count=500
Total loss:	1.884 (rec:1.884, round:0.000)	b=0.00	count=1000
Total loss:	2.107 (rec:2.107, round:0.000)	b=0.00	count=1500
Total loss:	1.720 (rec:1.720, round:0.000)	b=0.00	count=2000
Total loss:	1.758 (rec:1.758, round:0.000)	b=0.00	count=2500
Total loss:	1.965 (rec:1.965, round:0.000)	b=0.00	count=3000
Total loss:	1.920 (rec:1.920, round:0.000)	b=0.00	count=3500
Total loss:	14830.687 (rec:1.624, round:14829.062)	b=20.00	count=4000
Total loss:	6247.118 (rec:1.634, round:6245.484)	b=19.44	count=4500
Total loss:	5581.613 (rec:1.574, round:5580.039)	b=18.88	count=5000
Total loss:	5078.840 (rec:1.831, round:5077.009)	b=18.31	count=5500
Total loss:	4633.637 (rec:1.794, round:4631.843)	b=17.75	count=6000
Total loss:	4231.151 (rec:1.796, round:4229.355)	b=17.19	count=6500
Total loss:	3861.437 (rec:1.757, round:3859.680)	b=16.62	count=7000
Total loss:	3515.370 (rec:1.904, round:3513.466)	b=16.06	count=7500
Total loss:	3198.951 (rec:1.630, round:3197.321)	b=15.50	count=8000
Total loss:	2909.005 (rec:1.495, round:2907.509)	b=14.94	count=8500
Total loss:	2641.794 (rec:1.656, round:2640.137)	b=14.38	count=9000
Total loss:	2398.424 (rec:1.533, round:2396.891)	b=13.81	count=9500
Total loss:	2171.563 (rec:1.588, round:2169.975)	b=13.25	count=10000
Total loss:	1960.791 (rec:1.633, round:1959.158)	b=12.69	count=10500
Total loss:	1762.391 (rec:1.731, round:1760.660)	b=12.12	count=11000
Total loss:	1576.415 (rec:1.613, round:1574.802)	b=11.56	count=11500
Total loss:	1401.481 (rec:1.733, round:1399.748)	b=11.00	count=12000
Total loss:	1240.278 (rec:1.742, round:1238.537)	b=10.44	count=12500
Total loss:	1088.081 (rec:1.582, round:1086.499)	b=9.88	count=13000
Total loss:	944.204 (rec:1.768, round:942.436)	b=9.31	count=13500
Total loss:	808.782 (rec:1.790, round:806.992)	b=8.75	count=14000
Total loss:	683.188 (rec:1.862, round:681.327)	b=8.19	count=14500
Total loss:	563.322 (rec:1.796, round:561.526)	b=7.62	count=15000
Total loss:	452.654 (rec:1.823, round:450.831)	b=7.06	count=15500
Total loss:	351.520 (rec:1.968, round:349.552)	b=6.50	count=16000
Total loss:	259.425 (rec:1.692, round:257.733)	b=5.94	count=16500
Total loss:	179.778 (rec:1.692, round:178.086)	b=5.38	count=17000
Total loss:	113.049 (rec:1.513, round:111.536)	b=4.81	count=17500
Total loss:	60.862 (rec:1.781, round:59.081)	b=4.25	count=18000
Total loss:	26.329 (rec:1.767, round:24.562)	b=3.69	count=18500
Total loss:	7.793 (rec:1.630, round:6.163)	b=3.12	count=19000
Total loss:	2.488 (rec:1.734, round:0.754)	b=2.56	count=19500
Total loss:	1.654 (rec:1.621, round:0.033)	b=2.00	count=20000
finished reconstructing layers.2.blocks.10.
reconstructing layers.2.blocks.11 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.11 ...
wraping quantizers in layers.2.blocks.11 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	2.114 (rec:2.114, round:0.000)	b=0.00	count=500
Total loss:	2.166 (rec:2.166, round:0.000)	b=0.00	count=1000
Total loss:	2.076 (rec:2.076, round:0.000)	b=0.00	count=1500
Total loss:	1.850 (rec:1.850, round:0.000)	b=0.00	count=2000
Total loss:	1.804 (rec:1.804, round:0.000)	b=0.00	count=2500
Total loss:	1.826 (rec:1.826, round:0.000)	b=0.00	count=3000
Total loss:	1.766 (rec:1.766, round:0.000)	b=0.00	count=3500
Total loss:	14138.216 (rec:1.824, round:14136.393)	b=20.00	count=4000
Total loss:	5547.226 (rec:1.570, round:5545.656)	b=19.44	count=4500
Total loss:	4818.956 (rec:1.705, round:4817.250)	b=18.88	count=5000
Total loss:	4238.771 (rec:1.657, round:4237.115)	b=18.31	count=5500
Total loss:	3735.761 (rec:1.701, round:3734.060)	b=17.75	count=6000
Total loss:	3288.846 (rec:1.734, round:3287.112)	b=17.19	count=6500
Total loss:	2896.437 (rec:1.493, round:2894.944)	b=16.62	count=7000
Total loss:	2557.827 (rec:1.647, round:2556.180)	b=16.06	count=7500
Total loss:	2264.679 (rec:1.717, round:2262.962)	b=15.50	count=8000
Total loss:	2010.074 (rec:1.569, round:2008.505)	b=14.94	count=8500
Total loss:	1787.288 (rec:1.787, round:1785.501)	b=14.38	count=9000
Total loss:	1585.807 (rec:1.687, round:1584.120)	b=13.81	count=9500
Total loss:	1410.388 (rec:1.642, round:1408.746)	b=13.25	count=10000
Total loss:	1253.351 (rec:1.547, round:1251.803)	b=12.69	count=10500
Total loss:	1112.229 (rec:1.706, round:1110.523)	b=12.12	count=11000
Total loss:	983.409 (rec:1.559, round:981.850)	b=11.56	count=11500
Total loss:	864.342 (rec:1.702, round:862.640)	b=11.00	count=12000
Total loss:	754.918 (rec:1.803, round:753.115)	b=10.44	count=12500
Total loss:	655.015 (rec:1.562, round:653.453)	b=9.88	count=13000
Total loss:	563.224 (rec:1.680, round:561.544)	b=9.31	count=13500
Total loss:	477.185 (rec:1.803, round:475.382)	b=8.75	count=14000
Total loss:	397.612 (rec:1.855, round:395.757)	b=8.19	count=14500
Total loss:	324.050 (rec:1.645, round:322.405)	b=7.62	count=15000
Total loss:	257.980 (rec:1.654, round:256.326)	b=7.06	count=15500
Total loss:	196.497 (rec:1.758, round:194.739)	b=6.50	count=16000
Total loss:	142.554 (rec:1.829, round:140.725)	b=5.94	count=16500
Total loss:	94.392 (rec:1.582, round:92.810)	b=5.38	count=17000
Total loss:	56.233 (rec:1.748, round:54.485)	b=4.81	count=17500
Total loss:	27.585 (rec:1.798, round:25.788)	b=4.25	count=18000
Total loss:	10.261 (rec:1.530, round:8.731)	b=3.69	count=18500
Total loss:	3.384 (rec:1.647, round:1.737)	b=3.12	count=19000
Total loss:	1.709 (rec:1.577, round:0.133)	b=2.56	count=19500
Total loss:	1.595 (rec:1.589, round:0.006)	b=2.00	count=20000
finished reconstructing layers.2.blocks.11.
reconstructing layers.2.blocks.12 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.12 ...
wraping quantizers in layers.2.blocks.12 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.665 (rec:1.665, round:0.000)	b=0.00	count=500
Total loss:	1.514 (rec:1.514, round:0.000)	b=0.00	count=1000
Total loss:	1.707 (rec:1.707, round:0.000)	b=0.00	count=1500
Total loss:	1.442 (rec:1.442, round:0.000)	b=0.00	count=2000
Total loss:	1.493 (rec:1.493, round:0.000)	b=0.00	count=2500
Total loss:	1.411 (rec:1.411, round:0.000)	b=0.00	count=3000
Total loss:	1.330 (rec:1.330, round:0.000)	b=0.00	count=3500
Total loss:	13225.660 (rec:1.298, round:13224.362)	b=20.00	count=4000
Total loss:	4859.932 (rec:1.330, round:4858.602)	b=19.44	count=4500
Total loss:	4242.859 (rec:1.286, round:4241.573)	b=18.88	count=5000
Total loss:	3769.197 (rec:1.221, round:3767.976)	b=18.31	count=5500
Total loss:	3350.579 (rec:1.271, round:3349.308)	b=17.75	count=6000
Total loss:	2977.374 (rec:1.271, round:2976.103)	b=17.19	count=6500
Total loss:	2643.567 (rec:1.328, round:2642.239)	b=16.62	count=7000
Total loss:	2345.778 (rec:1.393, round:2344.385)	b=16.06	count=7500
Total loss:	2081.856 (rec:1.477, round:2080.379)	b=15.50	count=8000
Total loss:	1853.848 (rec:1.278, round:1852.571)	b=14.94	count=8500
Total loss:	1648.741 (rec:1.263, round:1647.478)	b=14.38	count=9000
Total loss:	1465.285 (rec:1.282, round:1464.003)	b=13.81	count=9500
Total loss:	1301.301 (rec:1.322, round:1299.980)	b=13.25	count=10000
Total loss:	1153.873 (rec:1.204, round:1152.669)	b=12.69	count=10500
Total loss:	1019.377 (rec:1.192, round:1018.185)	b=12.12	count=11000
Total loss:	895.405 (rec:1.211, round:894.194)	b=11.56	count=11500
Total loss:	783.860 (rec:1.351, round:782.509)	b=11.00	count=12000
Total loss:	681.370 (rec:1.205, round:680.165)	b=10.44	count=12500
Total loss:	588.422 (rec:1.396, round:587.026)	b=9.88	count=13000
Total loss:	500.680 (rec:1.353, round:499.327)	b=9.31	count=13500
Total loss:	419.991 (rec:1.329, round:418.661)	b=8.75	count=14000
Total loss:	346.873 (rec:1.377, round:345.496)	b=8.19	count=14500
Total loss:	279.123 (rec:1.316, round:277.807)	b=7.62	count=15000
Total loss:	216.705 (rec:1.252, round:215.453)	b=7.06	count=15500
Total loss:	160.351 (rec:1.300, round:159.051)	b=6.50	count=16000
Total loss:	111.066 (rec:1.248, round:109.818)	b=5.94	count=16500
Total loss:	69.771 (rec:1.326, round:68.445)	b=5.38	count=17000
Total loss:	36.885 (rec:1.193, round:35.692)	b=4.81	count=17500
Total loss:	15.126 (rec:1.220, round:13.907)	b=4.25	count=18000
Total loss:	4.129 (rec:1.229, round:2.900)	b=3.69	count=18500
Total loss:	1.427 (rec:1.209, round:0.218)	b=3.12	count=19000
Total loss:	1.318 (rec:1.311, round:0.007)	b=2.56	count=19500
Total loss:	1.400 (rec:1.400, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.12.
reconstructing layers.2.blocks.13 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.13 ...
wraping quantizers in layers.2.blocks.13 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	2.097 (rec:2.097, round:0.000)	b=0.00	count=500
Total loss:	1.751 (rec:1.751, round:0.000)	b=0.00	count=1000
Total loss:	1.838 (rec:1.838, round:0.000)	b=0.00	count=1500
Total loss:	1.753 (rec:1.753, round:0.000)	b=0.00	count=2000
Total loss:	1.816 (rec:1.816, round:0.000)	b=0.00	count=2500
Total loss:	1.849 (rec:1.849, round:0.000)	b=0.00	count=3000
Total loss:	1.714 (rec:1.714, round:0.000)	b=0.00	count=3500
Total loss:	13159.396 (rec:1.661, round:13157.735)	b=20.00	count=4000
Total loss:	4952.722 (rec:1.540, round:4951.182)	b=19.44	count=4500
Total loss:	4343.866 (rec:1.661, round:4342.205)	b=18.88	count=5000
Total loss:	3887.938 (rec:1.502, round:3886.436)	b=18.31	count=5500
Total loss:	3489.896 (rec:1.480, round:3488.417)	b=17.75	count=6000
Total loss:	3133.222 (rec:1.640, round:3131.582)	b=17.19	count=6500
Total loss:	2810.728 (rec:1.736, round:2808.991)	b=16.62	count=7000
Total loss:	2523.309 (rec:1.545, round:2521.764)	b=16.06	count=7500
Total loss:	2266.523 (rec:1.625, round:2264.897)	b=15.50	count=8000
Total loss:	2037.113 (rec:1.553, round:2035.560)	b=14.94	count=8500
Total loss:	1825.709 (rec:1.660, round:1824.049)	b=14.38	count=9000
Total loss:	1634.754 (rec:1.610, round:1633.144)	b=13.81	count=9500
Total loss:	1461.443 (rec:1.624, round:1459.818)	b=13.25	count=10000
Total loss:	1302.992 (rec:1.813, round:1301.179)	b=12.69	count=10500
Total loss:	1159.498 (rec:1.603, round:1157.895)	b=12.12	count=11000
Total loss:	1026.999 (rec:1.683, round:1025.315)	b=11.56	count=11500
Total loss:	902.915 (rec:1.605, round:901.310)	b=11.00	count=12000
Total loss:	787.210 (rec:1.697, round:785.513)	b=10.44	count=12500
Total loss:	679.412 (rec:1.443, round:677.969)	b=9.88	count=13000
Total loss:	581.716 (rec:1.692, round:580.023)	b=9.31	count=13500
Total loss:	490.250 (rec:1.519, round:488.732)	b=8.75	count=14000
Total loss:	404.516 (rec:1.637, round:402.879)	b=8.19	count=14500
Total loss:	326.735 (rec:1.772, round:324.964)	b=7.62	count=15000
Total loss:	255.607 (rec:1.591, round:254.016)	b=7.06	count=15500
Total loss:	189.528 (rec:1.490, round:188.038)	b=6.50	count=16000
Total loss:	132.290 (rec:1.617, round:130.673)	b=5.94	count=16500
Total loss:	82.838 (rec:1.467, round:81.371)	b=5.38	count=17000
Total loss:	43.976 (rec:1.723, round:42.253)	b=4.81	count=17500
Total loss:	17.709 (rec:1.633, round:16.076)	b=4.25	count=18000
Total loss:	5.083 (rec:1.678, round:3.405)	b=3.69	count=18500
Total loss:	1.871 (rec:1.546, round:0.326)	b=3.12	count=19000
Total loss:	1.640 (rec:1.621, round:0.019)	b=2.56	count=19500
Total loss:	1.623 (rec:1.622, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.13.
reconstructing layers.2.blocks.14 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.14 ...
wraping quantizers in layers.2.blocks.14 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.999 (rec:0.999, round:0.000)	b=0.00	count=500
Total loss:	0.665 (rec:0.665, round:0.000)	b=0.00	count=1000
Total loss:	0.671 (rec:0.671, round:0.000)	b=0.00	count=1500
Total loss:	0.606 (rec:0.606, round:0.000)	b=0.00	count=2000
Total loss:	0.613 (rec:0.613, round:0.000)	b=0.00	count=2500
Total loss:	0.599 (rec:0.599, round:0.000)	b=0.00	count=3000
Total loss:	0.517 (rec:0.517, round:0.000)	b=0.00	count=3500
Total loss:	15575.083 (rec:0.559, round:15574.523)	b=20.00	count=4000
Total loss:	7112.600 (rec:0.505, round:7112.094)	b=19.44	count=4500
Total loss:	6467.192 (rec:0.514, round:6466.679)	b=18.88	count=5000
Total loss:	5993.712 (rec:0.526, round:5993.186)	b=18.31	count=5500
Total loss:	5555.279 (rec:0.472, round:5554.807)	b=17.75	count=6000
Total loss:	5123.959 (rec:0.521, round:5123.438)	b=17.19	count=6500
Total loss:	4702.080 (rec:0.508, round:4701.572)	b=16.62	count=7000
Total loss:	4290.530 (rec:0.467, round:4290.063)	b=16.06	count=7500
Total loss:	3893.045 (rec:0.496, round:3892.549)	b=15.50	count=8000
Total loss:	3511.532 (rec:0.528, round:3511.004)	b=14.94	count=8500
Total loss:	3151.831 (rec:0.498, round:3151.333)	b=14.38	count=9000
Total loss:	2812.364 (rec:0.455, round:2811.909)	b=13.81	count=9500
Total loss:	2495.572 (rec:0.527, round:2495.045)	b=13.25	count=10000
Total loss:	2201.906 (rec:0.484, round:2201.422)	b=12.69	count=10500
Total loss:	1931.267 (rec:0.501, round:1930.766)	b=12.12	count=11000
Total loss:	1682.301 (rec:0.436, round:1681.865)	b=11.56	count=11500
Total loss:	1455.633 (rec:0.484, round:1455.149)	b=11.00	count=12000
Total loss:	1248.428 (rec:0.533, round:1247.895)	b=10.44	count=12500
Total loss:	1060.159 (rec:0.513, round:1059.646)	b=9.88	count=13000
Total loss:	889.173 (rec:0.515, round:888.657)	b=9.31	count=13500
Total loss:	734.316 (rec:0.481, round:733.835)	b=8.75	count=14000
Total loss:	595.525 (rec:0.481, round:595.044)	b=8.19	count=14500
Total loss:	469.028 (rec:0.499, round:468.529)	b=7.62	count=15000
Total loss:	358.076 (rec:0.537, round:357.539)	b=7.06	count=15500
Total loss:	262.162 (rec:0.463, round:261.699)	b=6.50	count=16000
Total loss:	179.397 (rec:0.442, round:178.955)	b=5.94	count=16500
Total loss:	110.846 (rec:0.504, round:110.342)	b=5.38	count=17000
Total loss:	58.283 (rec:0.491, round:57.791)	b=4.81	count=17500
Total loss:	22.906 (rec:0.535, round:22.371)	b=4.25	count=18000
Total loss:	5.237 (rec:0.485, round:4.751)	b=3.69	count=18500
Total loss:	0.866 (rec:0.474, round:0.392)	b=3.12	count=19000
Total loss:	0.548 (rec:0.530, round:0.018)	b=2.56	count=19500
Total loss:	0.518 (rec:0.518, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.14.
reconstructing layers.2.blocks.15 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.15 ...
wraping quantizers in layers.2.blocks.15 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.778 (rec:0.778, round:0.000)	b=0.00	count=500
Total loss:	0.459 (rec:0.459, round:0.000)	b=0.00	count=1000
Total loss:	0.414 (rec:0.414, round:0.000)	b=0.00	count=1500
Total loss:	0.413 (rec:0.413, round:0.000)	b=0.00	count=2000
Total loss:	0.384 (rec:0.384, round:0.000)	b=0.00	count=2500
Total loss:	0.378 (rec:0.378, round:0.000)	b=0.00	count=3000
Total loss:	0.361 (rec:0.361, round:0.000)	b=0.00	count=3500
Total loss:	15675.170 (rec:0.356, round:15674.813)	b=20.00	count=4000
Total loss:	7188.697 (rec:0.370, round:7188.326)	b=19.44	count=4500
Total loss:	6558.729 (rec:0.331, round:6558.397)	b=18.88	count=5000
Total loss:	6099.110 (rec:0.353, round:6098.757)	b=18.31	count=5500
Total loss:	5676.876 (rec:0.366, round:5676.510)	b=17.75	count=6000
Total loss:	5262.130 (rec:0.351, round:5261.779)	b=17.19	count=6500
Total loss:	4849.035 (rec:0.305, round:4848.730)	b=16.62	count=7000
Total loss:	4446.145 (rec:0.355, round:4445.790)	b=16.06	count=7500
Total loss:	4056.949 (rec:0.343, round:4056.606)	b=15.50	count=8000
Total loss:	3682.127 (rec:0.352, round:3681.774)	b=14.94	count=8500
Total loss:	3323.634 (rec:0.372, round:3323.262)	b=14.38	count=9000
Total loss:	2985.905 (rec:0.334, round:2985.571)	b=13.81	count=9500
Total loss:	2667.589 (rec:0.300, round:2667.289)	b=13.25	count=10000
Total loss:	2367.274 (rec:0.358, round:2366.917)	b=12.69	count=10500
Total loss:	2089.837 (rec:0.344, round:2089.493)	b=12.12	count=11000
Total loss:	1830.495 (rec:0.319, round:1830.177)	b=11.56	count=11500
Total loss:	1591.176 (rec:0.347, round:1590.829)	b=11.00	count=12000
Total loss:	1370.198 (rec:0.344, round:1369.855)	b=10.44	count=12500
Total loss:	1167.757 (rec:0.372, round:1167.385)	b=9.88	count=13000
Total loss:	983.835 (rec:0.354, round:983.482)	b=9.31	count=13500
Total loss:	813.600 (rec:0.327, round:813.273)	b=8.75	count=14000
Total loss:	660.169 (rec:0.334, round:659.835)	b=8.19	count=14500
Total loss:	522.139 (rec:0.330, round:521.809)	b=7.62	count=15000
Total loss:	396.937 (rec:0.304, round:396.633)	b=7.06	count=15500
Total loss:	287.802 (rec:0.347, round:287.455)	b=6.50	count=16000
Total loss:	194.925 (rec:0.312, round:194.613)	b=5.94	count=16500
Total loss:	119.742 (rec:0.324, round:119.419)	b=5.38	count=17000
Total loss:	62.420 (rec:0.333, round:62.087)	b=4.81	count=17500
Total loss:	23.776 (rec:0.319, round:23.457)	b=4.25	count=18000
Total loss:	4.935 (rec:0.306, round:4.629)	b=3.69	count=18500
Total loss:	0.678 (rec:0.345, round:0.334)	b=3.12	count=19000
Total loss:	0.326 (rec:0.315, round:0.011)	b=2.56	count=19500
Total loss:	0.347 (rec:0.347, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.15.
reconstructing layers.2.blocks.16 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.16 ...
wraping quantizers in layers.2.blocks.16 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.845 (rec:0.845, round:0.000)	b=0.00	count=500
Total loss:	0.595 (rec:0.595, round:0.000)	b=0.00	count=1000
Total loss:	0.499 (rec:0.499, round:0.000)	b=0.00	count=1500
Total loss:	0.496 (rec:0.496, round:0.000)	b=0.00	count=2000
Total loss:	0.479 (rec:0.479, round:0.000)	b=0.00	count=2500
Total loss:	0.468 (rec:0.468, round:0.000)	b=0.00	count=3000
Total loss:	0.464 (rec:0.464, round:0.000)	b=0.00	count=3500
Total loss:	15775.882 (rec:0.497, round:15775.385)	b=20.00	count=4000
Total loss:	7414.437 (rec:0.500, round:7413.937)	b=19.44	count=4500
Total loss:	6754.996 (rec:0.414, round:6754.583)	b=18.88	count=5000
Total loss:	6269.127 (rec:0.421, round:6268.706)	b=18.31	count=5500
Total loss:	5818.885 (rec:0.447, round:5818.438)	b=17.75	count=6000
Total loss:	5393.563 (rec:0.476, round:5393.087)	b=17.19	count=6500
Total loss:	4986.390 (rec:0.418, round:4985.972)	b=16.62	count=7000
Total loss:	4592.978 (rec:0.385, round:4592.593)	b=16.06	count=7500
Total loss:	4216.391 (rec:0.419, round:4215.973)	b=15.50	count=8000
Total loss:	3854.479 (rec:0.421, round:3854.058)	b=14.94	count=8500
Total loss:	3508.739 (rec:0.446, round:3508.293)	b=14.38	count=9000
Total loss:	3179.367 (rec:0.403, round:3178.963)	b=13.81	count=9500
Total loss:	2864.842 (rec:0.426, round:2864.415)	b=13.25	count=10000
Total loss:	2568.366 (rec:0.391, round:2567.975)	b=12.69	count=10500
Total loss:	2283.054 (rec:0.452, round:2282.602)	b=12.12	count=11000
Total loss:	2016.229 (rec:0.392, round:2015.837)	b=11.56	count=11500
Total loss:	1764.091 (rec:0.425, round:1763.666)	b=11.00	count=12000
Total loss:	1529.303 (rec:0.442, round:1528.861)	b=10.44	count=12500
Total loss:	1308.870 (rec:0.425, round:1308.445)	b=9.88	count=13000
Total loss:	1109.223 (rec:0.417, round:1108.806)	b=9.31	count=13500
Total loss:	923.770 (rec:0.401, round:923.369)	b=8.75	count=14000
Total loss:	754.659 (rec:0.417, round:754.242)	b=8.19	count=14500
Total loss:	600.438 (rec:0.407, round:600.031)	b=7.62	count=15000
Total loss:	460.757 (rec:0.422, round:460.336)	b=7.06	count=15500
Total loss:	337.000 (rec:0.391, round:336.608)	b=6.50	count=16000
Total loss:	232.752 (rec:0.437, round:232.315)	b=5.94	count=16500
Total loss:	145.317 (rec:0.399, round:144.918)	b=5.38	count=17000
Total loss:	77.637 (rec:0.385, round:77.251)	b=4.81	count=17500
Total loss:	29.915 (rec:0.396, round:29.519)	b=4.25	count=18000
Total loss:	6.321 (rec:0.405, round:5.916)	b=3.69	count=18500
Total loss:	0.898 (rec:0.381, round:0.517)	b=3.12	count=19000
Total loss:	0.417 (rec:0.391, round:0.026)	b=2.56	count=19500
Total loss:	0.427 (rec:0.426, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.16.
reconstructing layers.2.blocks.17 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.17 ...
wraping quantizers in layers.2.blocks.17 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.749 (rec:1.749, round:0.000)	b=0.00	count=500
Total loss:	1.395 (rec:1.395, round:0.000)	b=0.00	count=1000
Total loss:	1.320 (rec:1.320, round:0.000)	b=0.00	count=1500
Total loss:	1.261 (rec:1.261, round:0.000)	b=0.00	count=2000
Total loss:	1.414 (rec:1.414, round:0.000)	b=0.00	count=2500
Total loss:	1.329 (rec:1.329, round:0.000)	b=0.00	count=3000
Total loss:	1.285 (rec:1.285, round:0.000)	b=0.00	count=3500
Total loss:	15861.315 (rec:1.251, round:15860.064)	b=20.00	count=4000
Total loss:	7531.271 (rec:1.171, round:7530.100)	b=19.44	count=4500
Total loss:	6897.671 (rec:1.152, round:6896.520)	b=18.88	count=5000
Total loss:	6446.241 (rec:1.128, round:6445.113)	b=18.31	count=5500
Total loss:	6034.748 (rec:1.195, round:6033.553)	b=17.75	count=6000
Total loss:	5644.372 (rec:1.161, round:5643.211)	b=17.19	count=6500
Total loss:	5263.965 (rec:1.151, round:5262.814)	b=16.62	count=7000
Total loss:	4892.255 (rec:1.149, round:4891.106)	b=16.06	count=7500
Total loss:	4528.714 (rec:1.163, round:4527.551)	b=15.50	count=8000
Total loss:	4182.119 (rec:1.234, round:4180.885)	b=14.94	count=8500
Total loss:	3845.347 (rec:1.133, round:3844.214)	b=14.38	count=9000
Total loss:	3516.470 (rec:1.130, round:3515.341)	b=13.81	count=9500
Total loss:	3198.739 (rec:1.158, round:3197.580)	b=13.25	count=10000
Total loss:	2896.744 (rec:1.136, round:2895.608)	b=12.69	count=10500
Total loss:	2611.550 (rec:1.145, round:2610.405)	b=12.12	count=11000
Total loss:	2339.346 (rec:1.129, round:2338.217)	b=11.56	count=11500
Total loss:	2081.334 (rec:1.107, round:2080.227)	b=11.00	count=12000
Total loss:	1837.198 (rec:1.092, round:1836.107)	b=10.44	count=12500
Total loss:	1607.568 (rec:1.152, round:1606.416)	b=9.88	count=13000
Total loss:	1389.985 (rec:1.147, round:1388.838)	b=9.31	count=13500
Total loss:	1183.870 (rec:1.092, round:1182.778)	b=8.75	count=14000
Total loss:	991.395 (rec:1.164, round:990.230)	b=8.19	count=14500
Total loss:	812.197 (rec:1.113, round:811.085)	b=7.62	count=15000
Total loss:	645.045 (rec:1.118, round:643.927)	b=7.06	count=15500
Total loss:	491.833 (rec:1.124, round:490.709)	b=6.50	count=16000
Total loss:	353.718 (rec:1.099, round:352.619)	b=5.94	count=16500
Total loss:	233.041 (rec:1.188, round:231.853)	b=5.38	count=17000
Total loss:	132.621 (rec:1.215, round:131.405)	b=4.81	count=17500
Total loss:	58.677 (rec:1.166, round:57.511)	b=4.25	count=18000
Total loss:	16.611 (rec:1.113, round:15.497)	b=3.69	count=18500
Total loss:	3.034 (rec:1.151, round:1.883)	b=3.12	count=19000
Total loss:	1.212 (rec:1.108, round:0.104)	b=2.56	count=19500
Total loss:	1.122 (rec:1.119, round:0.003)	b=2.00	count=20000
finished reconstructing layers.2.blocks.17.
reconstructing layers.3.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.downsample ...
wraping quantizers in layers.3.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	2.045 (rec:2.045, round:0.000)	b=0.00	count=500
Total loss:	1.872 (rec:1.872, round:0.000)	b=0.00	count=1000
Total loss:	1.835 (rec:1.835, round:0.000)	b=0.00	count=1500
Total loss:	1.849 (rec:1.849, round:0.000)	b=0.00	count=2000
Total loss:	1.969 (rec:1.969, round:0.000)	b=0.00	count=2500
Total loss:	1.930 (rec:1.930, round:0.000)	b=0.00	count=3000
Total loss:	1.834 (rec:1.834, round:0.000)	b=0.00	count=3500
Total loss:	10474.730 (rec:1.936, round:10472.794)	b=20.00	count=4000
Total loss:	5187.254 (rec:1.606, round:5185.648)	b=19.44	count=4500
Total loss:	4775.673 (rec:1.794, round:4773.879)	b=18.88	count=5000
Total loss:	4489.107 (rec:1.776, round:4487.331)	b=18.31	count=5500
Total loss:	4239.760 (rec:1.798, round:4237.962)	b=17.75	count=6000
Total loss:	4008.988 (rec:1.736, round:4007.251)	b=17.19	count=6500
Total loss:	3789.469 (rec:1.696, round:3787.774)	b=16.62	count=7000
Total loss:	3581.200 (rec:1.634, round:3579.566)	b=16.06	count=7500
Total loss:	3377.323 (rec:1.729, round:3375.594)	b=15.50	count=8000
Total loss:	3180.782 (rec:1.929, round:3178.853)	b=14.94	count=8500
Total loss:	2989.842 (rec:1.683, round:2988.159)	b=14.38	count=9000
Total loss:	2803.489 (rec:1.567, round:2801.921)	b=13.81	count=9500
Total loss:	2619.342 (rec:1.779, round:2617.563)	b=13.25	count=10000
Total loss:	2437.655 (rec:1.629, round:2436.026)	b=12.69	count=10500
Total loss:	2257.990 (rec:1.549, round:2256.441)	b=12.12	count=11000
Total loss:	2082.443 (rec:1.714, round:2080.729)	b=11.56	count=11500
Total loss:	1909.636 (rec:1.807, round:1907.829)	b=11.00	count=12000
Total loss:	1734.969 (rec:1.667, round:1733.303)	b=10.44	count=12500
Total loss:	1562.914 (rec:1.571, round:1561.343)	b=9.88	count=13000
Total loss:	1392.391 (rec:2.013, round:1390.377)	b=9.31	count=13500
Total loss:	1221.387 (rec:1.670, round:1219.716)	b=8.75	count=14000
Total loss:	1052.397 (rec:1.817, round:1050.580)	b=8.19	count=14500
Total loss:	887.532 (rec:1.723, round:885.809)	b=7.62	count=15000
Total loss:	723.510 (rec:1.693, round:721.817)	b=7.06	count=15500
Total loss:	562.104 (rec:1.785, round:560.319)	b=6.50	count=16000
Total loss:	408.925 (rec:1.832, round:407.093)	b=5.94	count=16500
Total loss:	266.410 (rec:1.662, round:264.748)	b=5.38	count=17000
Total loss:	146.952 (rec:1.682, round:145.270)	b=4.81	count=17500
Total loss:	61.044 (rec:1.756, round:59.287)	b=4.25	count=18000
Total loss:	16.189 (rec:1.649, round:14.539)	b=3.69	count=18500
Total loss:	3.077 (rec:1.567, round:1.510)	b=3.12	count=19000
Total loss:	1.864 (rec:1.756, round:0.108)	b=2.56	count=19500
Total loss:	1.619 (rec:1.609, round:0.010)	b=2.00	count=20000
finished reconstructing layers.3.downsample.
reconstructing layers.3.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.blocks.0 ...
wraping quantizers in layers.3.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.636 (rec:1.636, round:0.000)	b=0.00	count=500
Total loss:	1.573 (rec:1.573, round:0.000)	b=0.00	count=1000
Total loss:	1.665 (rec:1.665, round:0.000)	b=0.00	count=1500
Total loss:	1.480 (rec:1.480, round:0.000)	b=0.00	count=2000
Total loss:	1.526 (rec:1.526, round:0.000)	b=0.00	count=2500
Total loss:	1.471 (rec:1.471, round:0.000)	b=0.00	count=3000
Total loss:	1.614 (rec:1.614, round:0.000)	b=0.00	count=3500
Total loss:	65900.094 (rec:1.503, round:65898.594)	b=20.00	count=4000
Total loss:	31957.217 (rec:1.388, round:31955.828)	b=19.44	count=4500
Total loss:	29599.566 (rec:1.414, round:29598.152)	b=18.88	count=5000
Total loss:	28057.688 (rec:1.446, round:28056.242)	b=18.31	count=5500
Total loss:	26726.096 (rec:1.361, round:26724.734)	b=17.75	count=6000
Total loss:	25466.324 (rec:1.294, round:25465.029)	b=17.19	count=6500
Total loss:	24239.412 (rec:1.410, round:24238.002)	b=16.62	count=7000
Total loss:	23036.271 (rec:1.333, round:23034.939)	b=16.06	count=7500
Total loss:	21842.982 (rec:1.302, round:21841.680)	b=15.50	count=8000
Total loss:	20650.260 (rec:1.302, round:20648.957)	b=14.94	count=8500
Total loss:	19459.365 (rec:1.321, round:19458.045)	b=14.38	count=9000
Total loss:	18271.949 (rec:1.342, round:18270.607)	b=13.81	count=9500
Total loss:	17080.367 (rec:1.383, round:17078.984)	b=13.25	count=10000
Total loss:	15883.921 (rec:1.236, round:15882.686)	b=12.69	count=10500
Total loss:	14690.359 (rec:1.349, round:14689.010)	b=12.12	count=11000
Total loss:	13495.120 (rec:1.342, round:13493.778)	b=11.56	count=11500
Total loss:	12302.448 (rec:1.495, round:12300.953)	b=11.00	count=12000
Total loss:	11118.358 (rec:1.306, round:11117.052)	b=10.44	count=12500
Total loss:	9944.372 (rec:1.330, round:9943.042)	b=9.88	count=13000
Total loss:	8798.216 (rec:1.427, round:8796.788)	b=9.31	count=13500
Total loss:	7672.197 (rec:1.237, round:7670.959)	b=8.75	count=14000
Total loss:	6568.484 (rec:1.267, round:6567.217)	b=8.19	count=14500
Total loss:	5499.902 (rec:1.302, round:5498.600)	b=7.62	count=15000
Total loss:	4474.019 (rec:1.270, round:4472.748)	b=7.06	count=15500
Total loss:	3499.246 (rec:1.331, round:3497.916)	b=6.50	count=16000
Total loss:	2589.311 (rec:1.387, round:2587.924)	b=5.94	count=16500
Total loss:	1760.747 (rec:1.379, round:1759.367)	b=5.38	count=17000
Total loss:	1047.125 (rec:1.329, round:1045.796)	b=4.81	count=17500
Total loss:	491.328 (rec:1.290, round:490.038)	b=4.25	count=18000
Total loss:	144.886 (rec:1.273, round:143.613)	b=3.69	count=18500
Total loss:	19.432 (rec:1.226, round:18.206)	b=3.12	count=19000
Total loss:	2.144 (rec:1.326, round:0.818)	b=2.56	count=19500
Total loss:	1.359 (rec:1.337, round:0.021)	b=2.00	count=20000
finished reconstructing layers.3.blocks.0.
reconstructing layers.3.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.blocks.1 ...
wraping quantizers in layers.3.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.762 (rec:1.762, round:0.000)	b=0.00	count=500
Total loss:	1.416 (rec:1.416, round:0.000)	b=0.00	count=1000
Total loss:	1.698 (rec:1.698, round:0.000)	b=0.00	count=1500
Total loss:	1.638 (rec:1.638, round:0.000)	b=0.00	count=2000
Total loss:	1.623 (rec:1.623, round:0.000)	b=0.00	count=2500
Total loss:	1.621 (rec:1.621, round:0.000)	b=0.00	count=3000
Total loss:	1.663 (rec:1.663, round:0.000)	b=0.00	count=3500
Total loss:	65291.973 (rec:1.620, round:65290.352)	b=20.00	count=4000
Total loss:	31589.836 (rec:1.492, round:31588.344)	b=19.44	count=4500
Total loss:	29189.408 (rec:1.387, round:29188.021)	b=18.88	count=5000
Total loss:	27583.316 (rec:1.495, round:27581.822)	b=18.31	count=5500
Total loss:	26161.580 (rec:1.428, round:26160.152)	b=17.75	count=6000
Total loss:	24814.387 (rec:1.520, round:24812.867)	b=17.19	count=6500
Total loss:	23506.871 (rec:1.489, round:23505.383)	b=16.62	count=7000
Total loss:	22211.492 (rec:1.410, round:22210.082)	b=16.06	count=7500
Total loss:	20935.627 (rec:1.452, round:20934.174)	b=15.50	count=8000
Total loss:	19669.949 (rec:1.326, round:19668.623)	b=14.94	count=8500
Total loss:	18414.021 (rec:1.204, round:18412.816)	b=14.38	count=9000
Total loss:	17163.041 (rec:1.409, round:17161.633)	b=13.81	count=9500
Total loss:	15911.866 (rec:1.244, round:15910.623)	b=13.25	count=10000
Total loss:	14677.189 (rec:1.288, round:14675.901)	b=12.69	count=10500
Total loss:	13457.647 (rec:1.283, round:13456.364)	b=12.12	count=11000
Total loss:	12253.766 (rec:1.379, round:12252.387)	b=11.56	count=11500
Total loss:	11082.348 (rec:1.434, round:11080.914)	b=11.00	count=12000
Total loss:	9933.213 (rec:1.325, round:9931.889)	b=10.44	count=12500
Total loss:	8815.898 (rec:1.352, round:8814.547)	b=9.88	count=13000
Total loss:	7727.440 (rec:1.371, round:7726.069)	b=9.31	count=13500
Total loss:	6675.760 (rec:1.496, round:6674.264)	b=8.75	count=14000
Total loss:	5662.195 (rec:1.253, round:5660.941)	b=8.19	count=14500
Total loss:	4696.516 (rec:1.320, round:4695.195)	b=7.62	count=15000
Total loss:	3781.467 (rec:1.413, round:3780.054)	b=7.06	count=15500
Total loss:	2922.004 (rec:1.264, round:2920.741)	b=6.50	count=16000
Total loss:	2131.389 (rec:1.413, round:2129.976)	b=5.94	count=16500
Total loss:	1430.766 (rec:1.456, round:1429.309)	b=5.38	count=17000
Total loss:	837.890 (rec:1.404, round:836.485)	b=4.81	count=17500
Total loss:	383.274 (rec:1.293, round:381.981)	b=4.25	count=18000
Total loss:	100.730 (rec:1.324, round:99.406)	b=3.69	count=18500
Total loss:	11.716 (rec:1.259, round:10.457)	b=3.12	count=19000
Total loss:	1.865 (rec:1.419, round:0.446)	b=2.56	count=19500
Total loss:	1.388 (rec:1.368, round:0.021)	b=2.00	count=20000
finished reconstructing layers.3.blocks.1.
reconstructing head ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for head ...
wraping quantizers in head ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.748 (rec:1.748, round:0.000)	b=0.00	count=500
Total loss:	1.070 (rec:1.070, round:0.000)	b=0.00	count=1000
Total loss:	0.905 (rec:0.905, round:0.000)	b=0.00	count=1500
Total loss:	0.749 (rec:0.749, round:0.000)	b=0.00	count=2000
Total loss:	1.050 (rec:1.050, round:0.000)	b=0.00	count=2500
Total loss:	1.060 (rec:1.060, round:0.000)	b=0.00	count=3000
Total loss:	0.546 (rec:0.546, round:0.000)	b=0.00	count=3500
Total loss:	7080.059 (rec:0.388, round:7079.671)	b=20.00	count=4000
Total loss:	4292.711 (rec:0.429, round:4292.282)	b=19.44	count=4500
Total loss:	4011.460 (rec:0.407, round:4011.053)	b=18.88	count=5000
Total loss:	3835.229 (rec:0.448, round:3834.781)	b=18.31	count=5500
Total loss:	3689.293 (rec:0.288, round:3689.005)	b=17.75	count=6000
Total loss:	3558.460 (rec:0.314, round:3558.146)	b=17.19	count=6500
Total loss:	3433.785 (rec:0.400, round:3433.385)	b=16.62	count=7000
Total loss:	3314.110 (rec:0.504, round:3313.606)	b=16.06	count=7500
Total loss:	3195.091 (rec:0.253, round:3194.838)	b=15.50	count=8000
Total loss:	3077.254 (rec:0.233, round:3077.021)	b=14.94	count=8500
Total loss:	2960.427 (rec:0.374, round:2960.053)	b=14.38	count=9000
Total loss:	2843.779 (rec:0.390, round:2843.389)	b=13.81	count=9500
Total loss:	2726.386 (rec:0.460, round:2725.926)	b=13.25	count=10000
Total loss:	2608.682 (rec:0.323, round:2608.359)	b=12.69	count=10500
Total loss:	2488.016 (rec:0.303, round:2487.713)	b=12.12	count=11000
Total loss:	2364.965 (rec:0.337, round:2364.627)	b=11.56	count=11500
Total loss:	2238.209 (rec:0.292, round:2237.917)	b=11.00	count=12000
Total loss:	2110.153 (rec:0.369, round:2109.784)	b=10.44	count=12500
Total loss:	1978.746 (rec:0.311, round:1978.435)	b=9.88	count=13000
Total loss:	1839.867 (rec:0.365, round:1839.503)	b=9.31	count=13500
Total loss:	1699.421 (rec:0.266, round:1699.155)	b=8.75	count=14000
Total loss:	1551.601 (rec:0.328, round:1551.273)	b=8.19	count=14500
Total loss:	1396.388 (rec:0.256, round:1396.132)	b=7.62	count=15000
Total loss:	1239.027 (rec:0.372, round:1238.655)	b=7.06	count=15500
Total loss:	1075.508 (rec:0.320, round:1075.189)	b=6.50	count=16000
Total loss:	909.260 (rec:0.267, round:908.993)	b=5.94	count=16500
Total loss:	738.462 (rec:0.298, round:738.163)	b=5.38	count=17000
Total loss:	566.661 (rec:0.269, round:566.392)	b=4.81	count=17500
Total loss:	397.721 (rec:0.483, round:397.237)	b=4.25	count=18000
Total loss:	240.114 (rec:0.416, round:239.699)	b=3.69	count=18500
Total loss:	107.813 (rec:0.271, round:107.542)	b=3.12	count=19000
Total loss:	26.451 (rec:0.416, round:26.036)	b=2.56	count=19500
Total loss:	2.903 (rec:0.358, round:2.545)	b=2.00	count=20000
finished reconstructing head.
2025-09-11 14:49:10 - mse guided block reconstruction finished.
Saving checkpoint to ./checkpoint/quant_result/20250911_1055/swin_small_w4_a4_optimsize_1024_mse_qdrop.pth
Validating on calibration set after block reconstruction ...
Test: [0/32]	Time 0.472 (0.472)	Loss 0.9975 (0.9975)	Prec@1 81.250 (81.250)	Prec@5 96.875 (96.875)
Test: [10/32]	Time 0.065 (0.102)	Loss 1.0525 (1.0277)	Prec@1 81.250 (79.261)	Prec@5 87.500 (93.182)
Test: [20/32]	Time 0.065 (0.085)	Loss 0.6320 (0.9275)	Prec@1 87.500 (80.506)	Prec@5 100.000 (95.387)
Test: [30/32]	Time 0.065 (0.079)	Loss 1.3091 (0.9554)	Prec@1 75.000 (80.544)	Prec@5 90.625 (94.657)
 * Prec@1 80.762 Prec@5 94.824 Loss 0.947 Time 2.620
Validating on test set after block reconstruction ...
Test: [0/100]	Time 5.216 (5.216)	Loss 0.5979 (0.5979)	Prec@1 89.200 (89.200)	Prec@5 96.800 (96.800)
Test: [10/100]	Time 1.749 (2.064)	Loss 0.6251 (0.7004)	Prec@1 89.800 (85.909)	Prec@5 97.400 (97.273)
Test: [20/100]	Time 1.751 (1.914)	Loss 0.8648 (0.7525)	Prec@1 78.600 (84.181)	Prec@5 96.400 (96.952)
Test: [30/100]	Time 1.753 (1.862)	Loss 0.6927 (0.7716)	Prec@1 85.200 (83.329)	Prec@5 98.800 (96.916)
Test: [40/100]	Time 1.753 (1.836)	Loss 0.9125 (0.7612)	Prec@1 76.400 (83.615)	Prec@5 95.600 (96.980)
Test: [50/100]	Time 1.752 (1.819)	Loss 1.2087 (0.8106)	Prec@1 72.000 (82.169)	Prec@5 92.600 (96.486)
Test: [60/100]	Time 1.753 (1.808)	Loss 0.8430 (0.8169)	Prec@1 83.800 (82.111)	Prec@5 94.800 (96.361)
Test: [70/100]	Time 1.753 (1.800)	Loss 0.9246 (0.8361)	Prec@1 79.800 (81.425)	Prec@5 96.400 (96.189)
Test: [80/100]	Time 1.755 (1.795)	Loss 0.7564 (0.8444)	Prec@1 83.200 (81.230)	Prec@5 96.600 (96.012)
Test: [90/100]	Time 1.752 (1.790)	Loss 1.0994 (0.8611)	Prec@1 72.800 (80.622)	Prec@5 94.600 (95.908)
 * Prec@1 80.792 Prec@5 96.008 Loss 0.854 Time 178.917
2025-09-11 14:52:11 - finished the process.
Extracting logits from quantized and full-precision models...
Testing combinations:
  Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
  Cluster numbers: [8, 16, 32, 64, 128, 256]
  PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.84%
[Alpha=0.10] Top-5 Accuracy: 96.04%
Result: Top-1: 80.84%, Top-5: 96.04%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.78%
[Alpha=0.10] Top-5 Accuracy: 96.03%
Result: Top-1: 80.78%, Top-5: 96.03%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.78%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.78%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.78%
[Alpha=0.10] Top-5 Accuracy: 96.01%
Result: Top-1: 80.78%, Top-5: 96.01%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.78%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.78%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.78%
[Alpha=0.10] Top-5 Accuracy: 96.01%
Result: Top-1: 80.78%, Top-5: 96.01%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.79%
[Alpha=0.10] Top-5 Accuracy: 96.03%
Result: Top-1: 80.79%, Top-5: 96.03%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.76%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.76%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.77%
[Alpha=0.10] Top-5 Accuracy: 96.00%
Result: Top-1: 80.77%, Top-5: 96.00%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.78%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.78%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.80%
[Alpha=0.10] Top-5 Accuracy: 96.05%
Result: Top-1: 80.80%, Top-5: 96.05%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.78%
[Alpha=0.10] Top-5 Accuracy: 96.03%
Result: Top-1: 80.78%, Top-5: 96.03%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.75%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.75%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.76%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.76%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.74%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.74%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.78%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.78%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.78%
[Alpha=0.10] Top-5 Accuracy: 96.00%
Result: Top-1: 80.78%, Top-5: 96.00%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.75%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.75%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.77%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.77%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.73%
[Alpha=0.10] Top-5 Accuracy: 96.00%
Result: Top-1: 80.73%, Top-5: 96.00%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.74%
[Alpha=0.10] Top-5 Accuracy: 96.03%
Result: Top-1: 80.74%, Top-5: 96.03%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.77%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.77%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.81%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.81%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.79%
[Alpha=0.10] Top-5 Accuracy: 96.00%
Result: Top-1: 80.79%, Top-5: 96.00%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.83%
[Alpha=0.10] Top-5 Accuracy: 96.03%
Result: Top-1: 80.83%, Top-5: 96.03%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.77%
[Alpha=0.10] Top-5 Accuracy: 96.04%
Result: Top-1: 80.77%, Top-5: 96.04%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.74%
[Alpha=0.10] Top-5 Accuracy: 96.03%
Result: Top-1: 80.74%, Top-5: 96.03%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.79%
[Alpha=0.10] Top-5 Accuracy: 96.03%
Result: Top-1: 80.79%, Top-5: 96.03%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.78%
[Alpha=0.10] Top-5 Accuracy: 96.01%
Result: Top-1: 80.78%, Top-5: 96.01%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.76%
[Alpha=0.10] Top-5 Accuracy: 96.00%
Result: Top-1: 80.76%, Top-5: 96.00%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.72%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.72%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.70%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.70%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.72%
[Alpha=0.10] Top-5 Accuracy: 95.97%
Result: Top-1: 80.72%, Top-5: 95.97%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.76%
[Alpha=0.10] Top-5 Accuracy: 96.01%
Result: Top-1: 80.76%, Top-5: 96.01%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.70%
[Alpha=0.10] Top-5 Accuracy: 96.01%
Result: Top-1: 80.70%, Top-5: 96.01%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.76%
[Alpha=0.10] Top-5 Accuracy: 96.01%
Result: Top-1: 80.76%, Top-5: 96.01%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.81%
[Alpha=0.10] Top-5 Accuracy: 96.04%
Result: Top-1: 80.81%, Top-5: 96.04%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.73%
[Alpha=0.10] Top-5 Accuracy: 95.99%
Result: Top-1: 80.73%, Top-5: 95.99%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.86%
[Alpha=0.10] Top-5 Accuracy: 96.05%
Result: Top-1: 80.86%, Top-5: 96.05%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.77%
[Alpha=0.10] Top-5 Accuracy: 96.01%
Result: Top-1: 80.77%, Top-5: 96.01%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.64%
[Alpha=0.10] Top-5 Accuracy: 96.01%
Result: Top-1: 80.64%, Top-5: 96.01%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.67%
[Alpha=0.10] Top-5 Accuracy: 95.91%
Result: Top-1: 80.67%, Top-5: 95.91%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.77%
[Alpha=0.10] Top-5 Accuracy: 96.00%
Result: Top-1: 80.77%, Top-5: 96.00%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.66%
[Alpha=0.10] Top-5 Accuracy: 96.00%
Result: Top-1: 80.66%, Top-5: 96.00%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.67%
[Alpha=0.10] Top-5 Accuracy: 95.98%
Result: Top-1: 80.67%, Top-5: 95.98%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.66%
[Alpha=0.10] Top-5 Accuracy: 95.95%
Result: Top-1: 80.66%, Top-5: 95.95%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.76%
[Alpha=0.10] Top-5 Accuracy: 96.04%
Result: Top-1: 80.76%, Top-5: 96.04%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.67%
[Alpha=0.10] Top-5 Accuracy: 96.01%
Result: Top-1: 80.67%, Top-5: 96.01%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.70%
[Alpha=0.10] Top-5 Accuracy: 95.91%
Result: Top-1: 80.70%, Top-5: 95.91%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.73%
[Alpha=0.10] Top-5 Accuracy: 96.02%
Result: Top-1: 80.73%, Top-5: 96.02%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 65.37%
[Alpha=0.10] Top-5 Accuracy: 93.42%
Result: Top-1: 65.37%, Top-5: 93.42%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 78.65%
[Alpha=0.10] Top-5 Accuracy: 95.10%
Result: Top-1: 78.65%, Top-5: 95.10%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 76.69%
[Alpha=0.10] Top-5 Accuracy: 95.00%
Result: Top-1: 76.69%, Top-5: 95.00%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 78.04%
[Alpha=0.10] Top-5 Accuracy: 95.48%
Result: Top-1: 78.04%, Top-5: 95.48%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.41%
[Alpha=0.10] Top-5 Accuracy: 95.87%
Result: Top-1: 80.41%, Top-5: 95.87%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 79.93%
[Alpha=0.10] Top-5 Accuracy: 95.86%
Result: Top-1: 79.93%, Top-5: 95.86%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 79.90%
[Alpha=0.10] Top-5 Accuracy: 95.69%
Result: Top-1: 79.90%, Top-5: 95.69%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.08%
[Alpha=0.10] Top-5 Accuracy: 95.83%
Result: Top-1: 80.08%, Top-5: 95.83%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 80.12%
[Alpha=0.10] Top-5 Accuracy: 95.78%
Result: Top-1: 80.12%, Top-5: 95.78%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 79.17%
[Alpha=0.10] Top-5 Accuracy: 95.57%
Result: Top-1: 79.17%, Top-5: 95.57%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.81%
[Alpha=0.20] Top-5 Accuracy: 96.04%
Result: Top-1: 80.81%, Top-5: 96.04%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.73%
[Alpha=0.20] Top-5 Accuracy: 96.03%
Result: Top-1: 80.73%, Top-5: 96.03%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.73%
[Alpha=0.20] Top-5 Accuracy: 96.02%
Result: Top-1: 80.73%, Top-5: 96.02%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.73%
[Alpha=0.20] Top-5 Accuracy: 96.01%
Result: Top-1: 80.73%, Top-5: 96.01%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.75%
[Alpha=0.20] Top-5 Accuracy: 96.02%
Result: Top-1: 80.75%, Top-5: 96.02%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.79%
[Alpha=0.20] Top-5 Accuracy: 96.01%
Result: Top-1: 80.79%, Top-5: 96.01%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.75%
[Alpha=0.20] Top-5 Accuracy: 96.03%
Result: Top-1: 80.75%, Top-5: 96.03%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.73%
[Alpha=0.20] Top-5 Accuracy: 96.02%
Result: Top-1: 80.73%, Top-5: 96.02%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.75%
[Alpha=0.20] Top-5 Accuracy: 96.01%
Result: Top-1: 80.75%, Top-5: 96.01%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.75%
[Alpha=0.20] Top-5 Accuracy: 96.02%
Result: Top-1: 80.75%, Top-5: 96.02%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.72%
[Alpha=0.20] Top-5 Accuracy: 96.03%
Result: Top-1: 80.72%, Top-5: 96.03%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.72%
[Alpha=0.20] Top-5 Accuracy: 96.06%
Result: Top-1: 80.72%, Top-5: 96.06%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.76%
[Alpha=0.20] Top-5 Accuracy: 96.05%
Result: Top-1: 80.76%, Top-5: 96.05%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.78%
[Alpha=0.20] Top-5 Accuracy: 96.02%
Result: Top-1: 80.78%, Top-5: 96.02%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.74%
[Alpha=0.20] Top-5 Accuracy: 96.02%
Result: Top-1: 80.74%, Top-5: 96.02%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.77%
[Alpha=0.20] Top-5 Accuracy: 96.02%
Result: Top-1: 80.77%, Top-5: 96.02%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.74%
[Alpha=0.20] Top-5 Accuracy: 96.00%
Result: Top-1: 80.74%, Top-5: 96.00%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.72%
[Alpha=0.20] Top-5 Accuracy: 96.03%
Result: Top-1: 80.72%, Top-5: 96.03%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.75%
[Alpha=0.20] Top-5 Accuracy: 96.01%
Result: Top-1: 80.75%, Top-5: 96.01%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.71%
[Alpha=0.20] Top-5 Accuracy: 96.01%
Result: Top-1: 80.71%, Top-5: 96.01%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.58%
[Alpha=0.20] Top-5 Accuracy: 95.99%
Result: Top-1: 80.58%, Top-5: 95.99%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.72%
[Alpha=0.20] Top-5 Accuracy: 96.00%
Result: Top-1: 80.72%, Top-5: 96.00%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.83%
[Alpha=0.20] Top-5 Accuracy: 96.03%
Result: Top-1: 80.83%, Top-5: 96.03%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.75%
[Alpha=0.20] Top-5 Accuracy: 95.99%
Result: Top-1: 80.75%, Top-5: 95.99%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.75%
[Alpha=0.20] Top-5 Accuracy: 96.03%
Result: Top-1: 80.75%, Top-5: 96.03%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.77%
[Alpha=0.20] Top-5 Accuracy: 96.04%
Result: Top-1: 80.77%, Top-5: 96.04%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.69%
[Alpha=0.20] Top-5 Accuracy: 96.03%
Result: Top-1: 80.69%, Top-5: 96.03%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.68%
[Alpha=0.20] Top-5 Accuracy: 96.01%
Result: Top-1: 80.68%, Top-5: 96.01%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.75%
[Alpha=0.20] Top-5 Accuracy: 95.97%
Result: Top-1: 80.75%, Top-5: 95.97%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.71%
[Alpha=0.20] Top-5 Accuracy: 95.98%
Result: Top-1: 80.71%, Top-5: 95.98%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.40%
[Alpha=0.20] Top-5 Accuracy: 95.97%
Result: Top-1: 80.40%, Top-5: 95.97%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.59%
[Alpha=0.20] Top-5 Accuracy: 95.98%
Result: Top-1: 80.59%, Top-5: 95.98%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.48%
[Alpha=0.20] Top-5 Accuracy: 95.96%
Result: Top-1: 80.48%, Top-5: 95.96%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.69%
[Alpha=0.20] Top-5 Accuracy: 95.98%
Result: Top-1: 80.69%, Top-5: 95.98%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.55%
[Alpha=0.20] Top-5 Accuracy: 95.98%
Result: Top-1: 80.55%, Top-5: 95.98%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.68%
[Alpha=0.20] Top-5 Accuracy: 95.97%
Result: Top-1: 80.68%, Top-5: 95.97%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.75%
[Alpha=0.20] Top-5 Accuracy: 96.04%
Result: Top-1: 80.75%, Top-5: 96.04%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.60%
[Alpha=0.20] Top-5 Accuracy: 95.99%
Result: Top-1: 80.60%, Top-5: 95.99%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.75%
[Alpha=0.20] Top-5 Accuracy: 96.02%
Result: Top-1: 80.75%, Top-5: 96.02%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.69%
[Alpha=0.20] Top-5 Accuracy: 95.99%
Result: Top-1: 80.69%, Top-5: 95.99%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 79.88%
[Alpha=0.20] Top-5 Accuracy: 95.91%
Result: Top-1: 79.88%, Top-5: 95.91%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.43%
[Alpha=0.20] Top-5 Accuracy: 95.76%
Result: Top-1: 80.43%, Top-5: 95.76%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.54%
[Alpha=0.20] Top-5 Accuracy: 95.93%
Result: Top-1: 80.54%, Top-5: 95.93%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.34%
[Alpha=0.20] Top-5 Accuracy: 95.92%
Result: Top-1: 80.34%, Top-5: 95.92%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.46%
[Alpha=0.20] Top-5 Accuracy: 95.93%
Result: Top-1: 80.46%, Top-5: 95.93%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.09%
[Alpha=0.20] Top-5 Accuracy: 95.86%
Result: Top-1: 80.09%, Top-5: 95.86%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.53%
[Alpha=0.20] Top-5 Accuracy: 96.00%
Result: Top-1: 80.53%, Top-5: 96.00%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.47%
[Alpha=0.20] Top-5 Accuracy: 95.95%
Result: Top-1: 80.47%, Top-5: 95.95%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.48%
[Alpha=0.20] Top-5 Accuracy: 95.82%
Result: Top-1: 80.48%, Top-5: 95.82%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 80.52%
[Alpha=0.20] Top-5 Accuracy: 95.98%
Result: Top-1: 80.52%, Top-5: 95.98%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 61.40%
[Alpha=0.20] Top-5 Accuracy: 87.86%
Result: Top-1: 61.40%, Top-5: 87.86%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 77.91%
[Alpha=0.20] Top-5 Accuracy: 93.89%
Result: Top-1: 77.91%, Top-5: 93.89%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 74.21%
[Alpha=0.20] Top-5 Accuracy: 93.25%
Result: Top-1: 74.21%, Top-5: 93.25%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 76.52%
[Alpha=0.20] Top-5 Accuracy: 94.43%
Result: Top-1: 76.52%, Top-5: 94.43%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 79.35%
[Alpha=0.20] Top-5 Accuracy: 95.58%
Result: Top-1: 79.35%, Top-5: 95.58%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 79.27%
[Alpha=0.20] Top-5 Accuracy: 95.48%
Result: Top-1: 79.27%, Top-5: 95.48%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 79.55%
[Alpha=0.20] Top-5 Accuracy: 95.22%
Result: Top-1: 79.55%, Top-5: 95.22%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 79.42%
[Alpha=0.20] Top-5 Accuracy: 95.46%
Result: Top-1: 79.42%, Top-5: 95.46%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 79.64%
[Alpha=0.20] Top-5 Accuracy: 95.33%
Result: Top-1: 79.64%, Top-5: 95.33%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 78.59%
[Alpha=0.20] Top-5 Accuracy: 94.79%
Result: Top-1: 78.59%, Top-5: 94.79%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.74%
[Alpha=0.30] Top-5 Accuracy: 96.05%
Result: Top-1: 80.74%, Top-5: 96.05%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.74%
[Alpha=0.30] Top-5 Accuracy: 96.05%
Result: Top-1: 80.74%, Top-5: 96.05%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.73%
[Alpha=0.30] Top-5 Accuracy: 96.04%
Result: Top-1: 80.73%, Top-5: 96.04%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.74%
[Alpha=0.30] Top-5 Accuracy: 96.02%
Result: Top-1: 80.74%, Top-5: 96.02%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.76%
[Alpha=0.30] Top-5 Accuracy: 96.04%
Result: Top-1: 80.76%, Top-5: 96.04%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.74%
[Alpha=0.30] Top-5 Accuracy: 96.02%
Result: Top-1: 80.74%, Top-5: 96.02%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.77%
[Alpha=0.30] Top-5 Accuracy: 96.05%
Result: Top-1: 80.77%, Top-5: 96.05%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.75%
[Alpha=0.30] Top-5 Accuracy: 96.04%
Result: Top-1: 80.75%, Top-5: 96.04%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.72%
[Alpha=0.30] Top-5 Accuracy: 96.03%
Result: Top-1: 80.72%, Top-5: 96.03%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.76%
[Alpha=0.30] Top-5 Accuracy: 96.04%
Result: Top-1: 80.76%, Top-5: 96.04%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.63%
[Alpha=0.30] Top-5 Accuracy: 96.05%
Result: Top-1: 80.63%, Top-5: 96.05%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.73%
[Alpha=0.30] Top-5 Accuracy: 96.05%
Result: Top-1: 80.73%, Top-5: 96.05%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.69%
[Alpha=0.30] Top-5 Accuracy: 96.04%
Result: Top-1: 80.69%, Top-5: 96.04%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.81%
[Alpha=0.30] Top-5 Accuracy: 96.00%
Result: Top-1: 80.81%, Top-5: 96.00%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.71%
[Alpha=0.30] Top-5 Accuracy: 95.99%
Result: Top-1: 80.71%, Top-5: 95.99%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.75%
[Alpha=0.30] Top-5 Accuracy: 96.02%
Result: Top-1: 80.75%, Top-5: 96.02%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.70%
[Alpha=0.30] Top-5 Accuracy: 95.99%
Result: Top-1: 80.70%, Top-5: 95.99%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.68%
[Alpha=0.30] Top-5 Accuracy: 96.01%
Result: Top-1: 80.68%, Top-5: 96.01%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.72%
[Alpha=0.30] Top-5 Accuracy: 96.01%
Result: Top-1: 80.72%, Top-5: 96.01%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.64%
[Alpha=0.30] Top-5 Accuracy: 96.01%
Result: Top-1: 80.64%, Top-5: 96.01%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.28%
[Alpha=0.30] Top-5 Accuracy: 95.97%
Result: Top-1: 80.28%, Top-5: 95.97%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.63%
[Alpha=0.30] Top-5 Accuracy: 95.99%
Result: Top-1: 80.63%, Top-5: 95.99%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.78%
[Alpha=0.30] Top-5 Accuracy: 95.99%
Result: Top-1: 80.78%, Top-5: 95.99%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.57%
[Alpha=0.30] Top-5 Accuracy: 95.99%
Result: Top-1: 80.57%, Top-5: 95.99%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.69%
[Alpha=0.30] Top-5 Accuracy: 96.03%
Result: Top-1: 80.69%, Top-5: 96.03%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.68%
[Alpha=0.30] Top-5 Accuracy: 96.02%
Result: Top-1: 80.68%, Top-5: 96.02%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.63%
[Alpha=0.30] Top-5 Accuracy: 96.02%
Result: Top-1: 80.63%, Top-5: 96.02%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.54%
[Alpha=0.30] Top-5 Accuracy: 96.00%
Result: Top-1: 80.54%, Top-5: 96.00%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.67%
[Alpha=0.30] Top-5 Accuracy: 95.96%
Result: Top-1: 80.67%, Top-5: 95.96%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.66%
[Alpha=0.30] Top-5 Accuracy: 95.97%
Result: Top-1: 80.66%, Top-5: 95.97%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 79.70%
[Alpha=0.30] Top-5 Accuracy: 95.90%
Result: Top-1: 79.70%, Top-5: 95.90%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.35%
[Alpha=0.30] Top-5 Accuracy: 95.94%
Result: Top-1: 80.35%, Top-5: 95.94%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.18%
[Alpha=0.30] Top-5 Accuracy: 95.90%
Result: Top-1: 80.18%, Top-5: 95.90%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.47%
[Alpha=0.30] Top-5 Accuracy: 95.95%
Result: Top-1: 80.47%, Top-5: 95.95%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.33%
[Alpha=0.30] Top-5 Accuracy: 95.97%
Result: Top-1: 80.33%, Top-5: 95.97%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.42%
[Alpha=0.30] Top-5 Accuracy: 95.93%
Result: Top-1: 80.42%, Top-5: 95.93%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.55%
[Alpha=0.30] Top-5 Accuracy: 96.00%
Result: Top-1: 80.55%, Top-5: 96.00%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.34%
[Alpha=0.30] Top-5 Accuracy: 95.92%
Result: Top-1: 80.34%, Top-5: 95.92%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.60%
[Alpha=0.30] Top-5 Accuracy: 95.96%
Result: Top-1: 80.60%, Top-5: 95.96%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.46%
[Alpha=0.30] Top-5 Accuracy: 95.96%
Result: Top-1: 80.46%, Top-5: 95.96%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 78.76%
[Alpha=0.30] Top-5 Accuracy: 95.69%
Result: Top-1: 78.76%, Top-5: 95.69%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 79.94%
[Alpha=0.30] Top-5 Accuracy: 95.63%
Result: Top-1: 79.94%, Top-5: 95.63%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 79.99%
[Alpha=0.30] Top-5 Accuracy: 95.83%
Result: Top-1: 79.99%, Top-5: 95.83%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 79.90%
[Alpha=0.30] Top-5 Accuracy: 95.83%
Result: Top-1: 79.90%, Top-5: 95.83%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.11%
[Alpha=0.30] Top-5 Accuracy: 95.87%
Result: Top-1: 80.11%, Top-5: 95.87%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 79.24%
[Alpha=0.30] Top-5 Accuracy: 95.64%
Result: Top-1: 79.24%, Top-5: 95.64%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.21%
[Alpha=0.30] Top-5 Accuracy: 95.92%
Result: Top-1: 80.21%, Top-5: 95.92%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 79.89%
[Alpha=0.30] Top-5 Accuracy: 95.87%
Result: Top-1: 79.89%, Top-5: 95.87%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 79.98%
[Alpha=0.30] Top-5 Accuracy: 95.76%
Result: Top-1: 79.98%, Top-5: 95.76%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.26%
[Alpha=0.30] Top-5 Accuracy: 95.95%
Result: Top-1: 80.26%, Top-5: 95.95%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 58.38%
[Alpha=0.30] Top-5 Accuracy: 82.04%
Result: Top-1: 58.38%, Top-5: 82.04%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 76.81%
[Alpha=0.30] Top-5 Accuracy: 93.32%
Result: Top-1: 76.81%, Top-5: 93.32%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 71.80%
[Alpha=0.30] Top-5 Accuracy: 91.33%
Result: Top-1: 71.80%, Top-5: 91.33%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 75.24%
[Alpha=0.30] Top-5 Accuracy: 93.18%
Result: Top-1: 75.24%, Top-5: 93.18%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 78.01%
[Alpha=0.30] Top-5 Accuracy: 95.23%
Result: Top-1: 78.01%, Top-5: 95.23%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 78.54%
[Alpha=0.30] Top-5 Accuracy: 95.01%
Result: Top-1: 78.54%, Top-5: 95.01%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 78.98%
[Alpha=0.30] Top-5 Accuracy: 94.87%
Result: Top-1: 78.98%, Top-5: 94.87%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 78.25%
[Alpha=0.30] Top-5 Accuracy: 95.06%
Result: Top-1: 78.25%, Top-5: 95.06%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 78.94%
[Alpha=0.30] Top-5 Accuracy: 94.95%
Result: Top-1: 78.94%, Top-5: 94.95%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 77.93%
[Alpha=0.30] Top-5 Accuracy: 94.04%
Result: Top-1: 77.93%, Top-5: 94.04%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.62%
[Alpha=0.40] Top-5 Accuracy: 96.05%
Result: Top-1: 80.62%, Top-5: 96.05%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.68%
[Alpha=0.40] Top-5 Accuracy: 96.04%
Result: Top-1: 80.68%, Top-5: 96.04%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.68%
[Alpha=0.40] Top-5 Accuracy: 96.03%
Result: Top-1: 80.68%, Top-5: 96.03%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.70%
[Alpha=0.40] Top-5 Accuracy: 96.02%
Result: Top-1: 80.70%, Top-5: 96.02%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.70%
[Alpha=0.40] Top-5 Accuracy: 96.04%
Result: Top-1: 80.70%, Top-5: 96.04%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.72%
[Alpha=0.40] Top-5 Accuracy: 96.02%
Result: Top-1: 80.72%, Top-5: 96.02%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.72%
[Alpha=0.40] Top-5 Accuracy: 96.05%
Result: Top-1: 80.72%, Top-5: 96.05%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.70%
[Alpha=0.40] Top-5 Accuracy: 96.03%
Result: Top-1: 80.70%, Top-5: 96.03%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.67%
[Alpha=0.40] Top-5 Accuracy: 96.03%
Result: Top-1: 80.67%, Top-5: 96.03%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.71%
[Alpha=0.40] Top-5 Accuracy: 96.03%
Result: Top-1: 80.71%, Top-5: 96.03%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.53%
[Alpha=0.40] Top-5 Accuracy: 96.04%
Result: Top-1: 80.53%, Top-5: 96.04%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.63%
[Alpha=0.40] Top-5 Accuracy: 96.05%
Result: Top-1: 80.63%, Top-5: 96.05%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.64%
[Alpha=0.40] Top-5 Accuracy: 96.02%
Result: Top-1: 80.64%, Top-5: 96.02%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.71%
[Alpha=0.40] Top-5 Accuracy: 96.00%
Result: Top-1: 80.71%, Top-5: 96.00%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.63%
[Alpha=0.40] Top-5 Accuracy: 95.99%
Result: Top-1: 80.63%, Top-5: 95.99%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.71%
[Alpha=0.40] Top-5 Accuracy: 95.98%
Result: Top-1: 80.71%, Top-5: 95.98%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.60%
[Alpha=0.40] Top-5 Accuracy: 95.96%
Result: Top-1: 80.60%, Top-5: 95.96%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.58%
[Alpha=0.40] Top-5 Accuracy: 96.02%
Result: Top-1: 80.58%, Top-5: 96.02%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.69%
[Alpha=0.40] Top-5 Accuracy: 96.02%
Result: Top-1: 80.69%, Top-5: 96.02%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.60%
[Alpha=0.40] Top-5 Accuracy: 96.01%
Result: Top-1: 80.60%, Top-5: 96.01%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.88%
[Alpha=0.40] Top-5 Accuracy: 95.89%
Result: Top-1: 79.88%, Top-5: 95.89%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.51%
[Alpha=0.40] Top-5 Accuracy: 95.98%
Result: Top-1: 80.51%, Top-5: 95.98%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.68%
[Alpha=0.40] Top-5 Accuracy: 95.98%
Result: Top-1: 80.68%, Top-5: 95.98%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.43%
[Alpha=0.40] Top-5 Accuracy: 95.96%
Result: Top-1: 80.43%, Top-5: 95.96%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.62%
[Alpha=0.40] Top-5 Accuracy: 95.98%
Result: Top-1: 80.62%, Top-5: 95.98%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.57%
[Alpha=0.40] Top-5 Accuracy: 96.00%
Result: Top-1: 80.57%, Top-5: 96.00%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.51%
[Alpha=0.40] Top-5 Accuracy: 95.96%
Result: Top-1: 80.51%, Top-5: 95.96%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.43%
[Alpha=0.40] Top-5 Accuracy: 95.98%
Result: Top-1: 80.43%, Top-5: 95.98%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.56%
[Alpha=0.40] Top-5 Accuracy: 95.94%
Result: Top-1: 80.56%, Top-5: 95.94%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.49%
[Alpha=0.40] Top-5 Accuracy: 95.96%
Result: Top-1: 80.49%, Top-5: 95.96%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 78.93%
[Alpha=0.40] Top-5 Accuracy: 95.77%
Result: Top-1: 78.93%, Top-5: 95.77%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.00%
[Alpha=0.40] Top-5 Accuracy: 95.87%
Result: Top-1: 80.00%, Top-5: 95.87%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.66%
[Alpha=0.40] Top-5 Accuracy: 95.77%
Result: Top-1: 79.66%, Top-5: 95.77%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.09%
[Alpha=0.40] Top-5 Accuracy: 95.85%
Result: Top-1: 80.09%, Top-5: 95.85%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.11%
[Alpha=0.40] Top-5 Accuracy: 95.91%
Result: Top-1: 80.11%, Top-5: 95.91%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.12%
[Alpha=0.40] Top-5 Accuracy: 95.88%
Result: Top-1: 80.12%, Top-5: 95.88%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.25%
[Alpha=0.40] Top-5 Accuracy: 95.96%
Result: Top-1: 80.25%, Top-5: 95.96%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.86%
[Alpha=0.40] Top-5 Accuracy: 95.80%
Result: Top-1: 79.86%, Top-5: 95.80%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.30%
[Alpha=0.40] Top-5 Accuracy: 95.91%
Result: Top-1: 80.30%, Top-5: 95.91%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.15%
[Alpha=0.40] Top-5 Accuracy: 95.87%
Result: Top-1: 80.15%, Top-5: 95.87%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 77.31%
[Alpha=0.40] Top-5 Accuracy: 95.33%
Result: Top-1: 77.31%, Top-5: 95.33%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.24%
[Alpha=0.40] Top-5 Accuracy: 95.39%
Result: Top-1: 79.24%, Top-5: 95.39%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.29%
[Alpha=0.40] Top-5 Accuracy: 95.64%
Result: Top-1: 79.29%, Top-5: 95.64%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.31%
[Alpha=0.40] Top-5 Accuracy: 95.63%
Result: Top-1: 79.31%, Top-5: 95.63%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.45%
[Alpha=0.40] Top-5 Accuracy: 95.77%
Result: Top-1: 79.45%, Top-5: 95.77%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 78.15%
[Alpha=0.40] Top-5 Accuracy: 95.35%
Result: Top-1: 78.15%, Top-5: 95.35%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.77%
[Alpha=0.40] Top-5 Accuracy: 95.79%
Result: Top-1: 79.77%, Top-5: 95.79%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.18%
[Alpha=0.40] Top-5 Accuracy: 95.70%
Result: Top-1: 79.18%, Top-5: 95.70%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.30%
[Alpha=0.40] Top-5 Accuracy: 95.63%
Result: Top-1: 79.30%, Top-5: 95.63%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 79.86%
[Alpha=0.40] Top-5 Accuracy: 95.84%
Result: Top-1: 79.86%, Top-5: 95.84%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.34%
[Alpha=0.40] Top-5 Accuracy: 77.70%
Result: Top-1: 55.34%, Top-5: 77.70%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 75.35%
[Alpha=0.40] Top-5 Accuracy: 92.85%
Result: Top-1: 75.35%, Top-5: 92.85%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 68.83%
[Alpha=0.40] Top-5 Accuracy: 89.52%
Result: Top-1: 68.83%, Top-5: 89.52%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 73.79%
[Alpha=0.40] Top-5 Accuracy: 91.96%
Result: Top-1: 73.79%, Top-5: 91.96%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 76.51%
[Alpha=0.40] Top-5 Accuracy: 94.67%
Result: Top-1: 76.51%, Top-5: 94.67%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 77.58%
[Alpha=0.40] Top-5 Accuracy: 94.67%
Result: Top-1: 77.58%, Top-5: 94.67%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 78.23%
[Alpha=0.40] Top-5 Accuracy: 94.57%
Result: Top-1: 78.23%, Top-5: 94.57%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 76.66%
[Alpha=0.40] Top-5 Accuracy: 94.65%
Result: Top-1: 76.66%, Top-5: 94.65%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 77.95%
[Alpha=0.40] Top-5 Accuracy: 94.64%
Result: Top-1: 77.95%, Top-5: 94.64%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 77.13%
[Alpha=0.40] Top-5 Accuracy: 93.53%
Result: Top-1: 77.13%, Top-5: 93.53%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.49%
[Alpha=0.50] Top-5 Accuracy: 96.04%
Result: Top-1: 80.49%, Top-5: 96.04%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.60%
[Alpha=0.50] Top-5 Accuracy: 96.02%
Result: Top-1: 80.60%, Top-5: 96.02%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.59%
[Alpha=0.50] Top-5 Accuracy: 96.02%
Result: Top-1: 80.59%, Top-5: 96.02%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.61%
[Alpha=0.50] Top-5 Accuracy: 96.01%
Result: Top-1: 80.61%, Top-5: 96.01%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.63%
[Alpha=0.50] Top-5 Accuracy: 96.02%
Result: Top-1: 80.63%, Top-5: 96.02%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.62%
[Alpha=0.50] Top-5 Accuracy: 96.01%
Result: Top-1: 80.62%, Top-5: 96.01%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.63%
[Alpha=0.50] Top-5 Accuracy: 96.02%
Result: Top-1: 80.63%, Top-5: 96.02%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.63%
[Alpha=0.50] Top-5 Accuracy: 96.02%
Result: Top-1: 80.63%, Top-5: 96.02%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.61%
[Alpha=0.50] Top-5 Accuracy: 96.02%
Result: Top-1: 80.61%, Top-5: 96.02%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.64%
[Alpha=0.50] Top-5 Accuracy: 96.02%
Result: Top-1: 80.64%, Top-5: 96.02%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.31%
[Alpha=0.50] Top-5 Accuracy: 96.00%
Result: Top-1: 80.31%, Top-5: 96.00%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.54%
[Alpha=0.50] Top-5 Accuracy: 96.04%
Result: Top-1: 80.54%, Top-5: 96.04%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.56%
[Alpha=0.50] Top-5 Accuracy: 96.02%
Result: Top-1: 80.56%, Top-5: 96.02%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.60%
[Alpha=0.50] Top-5 Accuracy: 95.99%
Result: Top-1: 80.60%, Top-5: 95.99%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.53%
[Alpha=0.50] Top-5 Accuracy: 95.98%
Result: Top-1: 80.53%, Top-5: 95.98%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.64%
[Alpha=0.50] Top-5 Accuracy: 95.97%
Result: Top-1: 80.64%, Top-5: 95.97%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.52%
[Alpha=0.50] Top-5 Accuracy: 95.94%
Result: Top-1: 80.52%, Top-5: 95.94%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.46%
[Alpha=0.50] Top-5 Accuracy: 96.00%
Result: Top-1: 80.46%, Top-5: 96.00%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.54%
[Alpha=0.50] Top-5 Accuracy: 96.01%
Result: Top-1: 80.54%, Top-5: 96.01%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.52%
[Alpha=0.50] Top-5 Accuracy: 95.99%
Result: Top-1: 80.52%, Top-5: 95.99%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.36%
[Alpha=0.50] Top-5 Accuracy: 95.81%
Result: Top-1: 79.36%, Top-5: 95.81%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.35%
[Alpha=0.50] Top-5 Accuracy: 95.96%
Result: Top-1: 80.35%, Top-5: 95.96%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.44%
[Alpha=0.50] Top-5 Accuracy: 95.96%
Result: Top-1: 80.44%, Top-5: 95.96%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.22%
[Alpha=0.50] Top-5 Accuracy: 95.91%
Result: Top-1: 80.22%, Top-5: 95.91%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.44%
[Alpha=0.50] Top-5 Accuracy: 95.94%
Result: Top-1: 80.44%, Top-5: 95.94%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.41%
[Alpha=0.50] Top-5 Accuracy: 95.97%
Result: Top-1: 80.41%, Top-5: 95.97%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.35%
[Alpha=0.50] Top-5 Accuracy: 95.93%
Result: Top-1: 80.35%, Top-5: 95.93%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.22%
[Alpha=0.50] Top-5 Accuracy: 95.93%
Result: Top-1: 80.22%, Top-5: 95.93%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.36%
[Alpha=0.50] Top-5 Accuracy: 95.90%
Result: Top-1: 80.36%, Top-5: 95.90%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 80.29%
[Alpha=0.50] Top-5 Accuracy: 95.92%
Result: Top-1: 80.29%, Top-5: 95.92%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 77.91%
[Alpha=0.50] Top-5 Accuracy: 95.55%
Result: Top-1: 77.91%, Top-5: 95.55%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.49%
[Alpha=0.50] Top-5 Accuracy: 95.76%
Result: Top-1: 79.49%, Top-5: 95.76%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.00%
[Alpha=0.50] Top-5 Accuracy: 95.70%
Result: Top-1: 79.00%, Top-5: 95.70%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.64%
[Alpha=0.50] Top-5 Accuracy: 95.73%
Result: Top-1: 79.64%, Top-5: 95.73%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.71%
[Alpha=0.50] Top-5 Accuracy: 95.80%
Result: Top-1: 79.71%, Top-5: 95.80%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.67%
[Alpha=0.50] Top-5 Accuracy: 95.81%
Result: Top-1: 79.67%, Top-5: 95.81%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.84%
[Alpha=0.50] Top-5 Accuracy: 95.85%
Result: Top-1: 79.84%, Top-5: 95.85%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.35%
[Alpha=0.50] Top-5 Accuracy: 95.64%
Result: Top-1: 79.35%, Top-5: 95.64%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.85%
[Alpha=0.50] Top-5 Accuracy: 95.80%
Result: Top-1: 79.85%, Top-5: 95.80%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.65%
[Alpha=0.50] Top-5 Accuracy: 95.79%
Result: Top-1: 79.65%, Top-5: 95.79%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 75.59%
[Alpha=0.50] Top-5 Accuracy: 94.76%
Result: Top-1: 75.59%, Top-5: 94.76%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 78.28%
[Alpha=0.50] Top-5 Accuracy: 95.16%
Result: Top-1: 78.28%, Top-5: 95.16%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 78.37%
[Alpha=0.50] Top-5 Accuracy: 95.40%
Result: Top-1: 78.37%, Top-5: 95.40%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 78.52%
[Alpha=0.50] Top-5 Accuracy: 95.41%
Result: Top-1: 78.52%, Top-5: 95.41%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 78.75%
[Alpha=0.50] Top-5 Accuracy: 95.63%
Result: Top-1: 78.75%, Top-5: 95.63%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 77.09%
[Alpha=0.50] Top-5 Accuracy: 94.85%
Result: Top-1: 77.09%, Top-5: 94.85%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.09%
[Alpha=0.50] Top-5 Accuracy: 95.62%
Result: Top-1: 79.09%, Top-5: 95.62%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 78.13%
[Alpha=0.50] Top-5 Accuracy: 95.50%
Result: Top-1: 78.13%, Top-5: 95.50%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 78.37%
[Alpha=0.50] Top-5 Accuracy: 95.48%
Result: Top-1: 78.37%, Top-5: 95.48%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 79.27%
[Alpha=0.50] Top-5 Accuracy: 95.72%
Result: Top-1: 79.27%, Top-5: 95.72%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 51.36%
[Alpha=0.50] Top-5 Accuracy: 74.62%
Result: Top-1: 51.36%, Top-5: 74.62%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 73.66%
[Alpha=0.50] Top-5 Accuracy: 92.19%
Result: Top-1: 73.66%, Top-5: 92.19%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 65.85%
[Alpha=0.50] Top-5 Accuracy: 87.78%
Result: Top-1: 65.85%, Top-5: 87.78%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 72.01%
[Alpha=0.50] Top-5 Accuracy: 90.90%
Result: Top-1: 72.01%, Top-5: 90.90%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 74.67%
[Alpha=0.50] Top-5 Accuracy: 93.81%
Result: Top-1: 74.67%, Top-5: 93.81%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 76.34%
[Alpha=0.50] Top-5 Accuracy: 94.08%
Result: Top-1: 76.34%, Top-5: 94.08%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 77.25%
[Alpha=0.50] Top-5 Accuracy: 94.23%
Result: Top-1: 77.25%, Top-5: 94.23%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 74.96%
[Alpha=0.50] Top-5 Accuracy: 94.06%
Result: Top-1: 74.96%, Top-5: 94.06%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 76.80%
[Alpha=0.50] Top-5 Accuracy: 94.14%
Result: Top-1: 76.80%, Top-5: 94.14%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 76.07%
[Alpha=0.50] Top-5 Accuracy: 93.06%
Result: Top-1: 76.07%, Top-5: 93.06%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.35%
[Alpha=0.60] Top-5 Accuracy: 96.01%
Result: Top-1: 80.35%, Top-5: 96.01%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.51%
[Alpha=0.60] Top-5 Accuracy: 95.99%
Result: Top-1: 80.51%, Top-5: 95.99%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.50%
[Alpha=0.60] Top-5 Accuracy: 95.99%
Result: Top-1: 80.50%, Top-5: 95.99%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.51%
[Alpha=0.60] Top-5 Accuracy: 95.97%
Result: Top-1: 80.51%, Top-5: 95.97%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.52%
[Alpha=0.60] Top-5 Accuracy: 95.98%
Result: Top-1: 80.52%, Top-5: 95.98%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.49%
[Alpha=0.60] Top-5 Accuracy: 95.98%
Result: Top-1: 80.49%, Top-5: 95.98%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.53%
[Alpha=0.60] Top-5 Accuracy: 95.99%
Result: Top-1: 80.53%, Top-5: 95.99%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.53%
[Alpha=0.60] Top-5 Accuracy: 95.99%
Result: Top-1: 80.53%, Top-5: 95.99%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.52%
[Alpha=0.60] Top-5 Accuracy: 96.00%
Result: Top-1: 80.52%, Top-5: 96.00%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.54%
[Alpha=0.60] Top-5 Accuracy: 95.98%
Result: Top-1: 80.54%, Top-5: 95.98%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.11%
[Alpha=0.60] Top-5 Accuracy: 95.96%
Result: Top-1: 80.11%, Top-5: 95.96%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.37%
[Alpha=0.60] Top-5 Accuracy: 96.03%
Result: Top-1: 80.37%, Top-5: 96.03%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.44%
[Alpha=0.60] Top-5 Accuracy: 96.02%
Result: Top-1: 80.44%, Top-5: 96.02%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.42%
[Alpha=0.60] Top-5 Accuracy: 95.98%
Result: Top-1: 80.42%, Top-5: 95.98%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.33%
[Alpha=0.60] Top-5 Accuracy: 95.92%
Result: Top-1: 80.33%, Top-5: 95.92%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.48%
[Alpha=0.60] Top-5 Accuracy: 95.94%
Result: Top-1: 80.48%, Top-5: 95.94%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.32%
[Alpha=0.60] Top-5 Accuracy: 95.91%
Result: Top-1: 80.32%, Top-5: 95.91%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.30%
[Alpha=0.60] Top-5 Accuracy: 96.01%
Result: Top-1: 80.30%, Top-5: 96.01%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.37%
[Alpha=0.60] Top-5 Accuracy: 95.97%
Result: Top-1: 80.37%, Top-5: 95.97%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 80.34%
[Alpha=0.60] Top-5 Accuracy: 95.95%
Result: Top-1: 80.34%, Top-5: 95.95%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 78.77%
[Alpha=0.60] Top-5 Accuracy: 95.72%
Result: Top-1: 78.77%, Top-5: 95.72%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=25
============================================================
slurmstepd-jnfat06: error: *** JOB 1659765 ON jnfat06 CANCELLED AT 2025-09-12T13:43:23 ***
