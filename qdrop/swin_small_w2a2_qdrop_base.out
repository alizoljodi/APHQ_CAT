Starting Swin-Small W2A2 QDROP experiment at Thu Sep 11 10:45:03 AM CEST 2025
2025-09-11 10:45:09,624 - INFO - Starting multi-seed experiment
2025-09-11 10:45:09,624 - INFO - Architecture: swin_small
2025-09-11 10:45:09,625 - INFO - Weight bits: 2
2025-09-11 10:45:09,625 - INFO - Activation bits: 2
2025-09-11 10:45:09,625 - INFO - Seeds: [1001, 1002, 1003]
2025-09-11 10:45:09,625 - INFO - Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
2025-09-11 10:45:09,625 - INFO - Cluster numbers: [8, 16, 32, 64, 128, 256]
2025-09-11 10:45:09,625 - INFO - PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]
2025-09-11 10:45:09,625 - INFO - Output directory: ./experiment_results/swin_small_w2_a2_20250911_104509
2025-09-11 10:45:09,625 - INFO - Checking basic requirements...
2025-09-11 10:45:09,625 - INFO - Basic checks passed
2025-09-11 10:45:09,625 - INFO - 
Starting experiments for 3 seeds...
2025-09-11 10:45:09,625 - INFO - Total parameter combinations: 600
2025-09-11 10:45:09,625 - INFO - Total experiments: 1800
2025-09-11 10:45:09,625 - INFO - 
============================================================
2025-09-11 10:45:09,625 - INFO - Running experiment 1/3 for seed 1001
2025-09-11 10:45:09,625 - INFO - ============================================================
2025-09-11 10:45:09,626 - INFO - Running experiment for seed 1001
2025-09-11 10:45:09,626 - INFO - Command: /home/alz07xz/project/PD-Quant/pd_quant/bin/python ../test_quant.py --model swin_small --w_bit 2 --a_bit 2 --seed 1001 --config ../configs/4bit/qdrop_baseline.py --dataset /home/alz07xz/imagenet --calib-size 1000 --calib-batch-size 32 --val-batch-size 500 --num-workers 8 --device cuda --alpha-list 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 --num-clusters-list 8 16 32 64 128 256 --pca-dim-list 1 25 50 75 100 125 150 175 200 220 --calibrate --optimize
2025-09-11 10:45:09,626 - INFO - Working directory: /home/alz07xz/project/APHQ_CAT/qdrop
2025-09-11 10:56:07 - start the process.
Namespace(model='swin_small', config='../configs/4bit/qdrop_baseline.py', dataset='/home/alz07xz/imagenet', val_batch_size=500, num_workers=8, device='cuda', reconstruct_mlp=False, load_reconstruct_checkpoint=False, test_reconstruct_checkpoint=False, calibrate=True, load_calibrate_checkpoint=None, test_calibrate_checkpoint=False, optimize=True, load_optimize_checkpoint='', test_optimize_checkpoint=False, print_freq=10, seed=1001, alpha_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], num_clusters_list=[8, 16, 32, 64, 128, 256], pca_dim_list=[1, 25, 50, 75, 100, 125, 150, 175, 200, 220], w_bit=2, a_bit=2, calib_size=1000, calib_batch_size=32)
deefe /home/alz07xz/project/APHQ_CAT/configs/4bit
Successfully imported Config class!
optim_size: 1024
calib_size: 1000
optim_batch_size: 32
calib_batch_size: 32
w_bit: 2
a_bit: 2
qconv_a_bit: 8
qhead_a_bit: 4
calib_metric: mse
matmul_head_channel_wise: True
token_channel_wise: False
eq_n: 128
search_round: 3
keep_gpu: True
optim_metric: mse
use_mean_hessian: False
temp: 20
recon_metric: hessian_perturb
pct: 0.99
optim_mode: qdrop
drop_prob: 0.5
reconstruct_mlp: False
Building model ...
/home/alz07xz/project/PD-Quant/pd_quant/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
No checkpoint found at './checkpoint/vit_raw/swin_small_patch4_window7_224.bin'
Loading pretrained weights from Hugging Face hub (timm/swin_small_patch4_window7_224.ms_in22k_ft_in1k)
[timm/swin_small_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Building validation dataloader ...
Validating on test set on fp ...
Test: [0/100]	Time 18.213 (18.213)	Loss 0.4363 (0.4363)	Prec@1 91.600 (91.600)	Prec@5 98.200 (98.200)
Test: [10/100]	Time 0.756 (3.213)	Loss 0.4835 (0.5209)	Prec@1 90.200 (87.800)	Prec@5 98.400 (98.073)
Test: [20/100]	Time 0.761 (2.076)	Loss 0.6876 (0.5743)	Prec@1 82.000 (86.400)	Prec@5 97.200 (97.743)
Test: [30/100]	Time 0.764 (1.719)	Loss 0.5090 (0.6038)	Prec@1 88.000 (85.516)	Prec@5 99.200 (97.677)
Test: [40/100]	Time 0.763 (1.607)	Loss 0.7823 (0.5960)	Prec@1 79.400 (85.780)	Prec@5 96.800 (97.741)
Test: [50/100]	Time 0.762 (1.713)	Loss 0.9962 (0.6402)	Prec@1 76.000 (84.569)	Prec@5 93.200 (97.353)
Test: [60/100]	Time 0.762 (1.601)	Loss 0.6332 (0.6450)	Prec@1 86.000 (84.538)	Prec@5 96.200 (97.285)
Test: [70/100]	Time 0.769 (1.484)	Loss 0.7378 (0.6617)	Prec@1 82.200 (83.859)	Prec@5 97.600 (97.141)
Test: [80/100]	Time 0.770 (1.395)	Loss 0.5519 (0.6669)	Prec@1 87.000 (83.805)	Prec@5 97.800 (97.030)
Test: [90/100]	Time 0.770 (1.327)	Loss 1.0055 (0.6846)	Prec@1 74.800 (83.189)	Prec@5 94.400 (96.912)
 * Prec@1 83.316 Prec@5 96.976 Loss 0.680 Time 127.800
Wraping quantiztion modules (reparam: False, recon: False) ...
2025-09-11 10:59:00 - start mse guided calibration
  0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   1%|          | 1/149 [00:11<27:14, 11.05s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|          | 1/149 [00:11<27:14, 11.05s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|▏         | 2/149 [01:06<1:31:37, 37.40s/it]calibrating layers.0.blocks.0.attn.proj:   1%|▏         | 2/149 [01:06<1:31:37, 37.40s/it]calibrating layers.0.blocks.0.attn.proj:   2%|▏         | 3/149 [01:31<1:16:42, 31.53s/it]calibrating layers.0.blocks.0.attn.matmul1:   2%|▏         | 3/149 [01:31<1:16:42, 31.53s/it]calibrating layers.0.blocks.0.attn.matmul1:   3%|▎         | 4/149 [04:34<3:40:22, 91.19s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 4/149 [04:34<3:40:22, 91.19s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 5/149 [09:45<6:49:45, 170.73s/it]calibrating layers.0.blocks.0.mlp.fc1:   3%|▎         | 5/149 [09:45<6:49:45, 170.73s/it]     calibrating layers.0.blocks.0.mlp.fc1:   4%|▍         | 6/149 [11:30<5:53:04, 148.14s/it]calibrating layers.0.blocks.0.mlp.fc2:   4%|▍         | 6/149 [11:30<5:53:04, 148.14s/it]calibrating layers.0.blocks.0.mlp.fc2:   5%|▍         | 7/149 [13:32<5:30:29, 139.65s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▍         | 7/149 [13:32<5:30:29, 139.65s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▌         | 8/149 [14:28<4:25:37, 113.03s/it]calibrating layers.0.blocks.1.attn.proj:   5%|▌         | 8/149 [14:28<4:25:37, 113.03s/it]calibrating layers.0.blocks.1.attn.proj:   6%|▌         | 9/149 [14:53<3:19:17, 85.41s/it] calibrating layers.0.blocks.1.attn.matmul1:   6%|▌         | 9/149 [14:53<3:19:17, 85.41s/it]calibrating layers.0.blocks.1.attn.matmul1:   7%|▋         | 10/149 [17:56<4:28:05, 115.72s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 10/149 [17:56<4:28:05, 115.72s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 11/149 [23:09<6:44:41, 175.95s/it]calibrating layers.0.blocks.1.mlp.fc1:   7%|▋         | 11/149 [23:09<6:44:41, 175.95s/it]     calibrating layers.0.blocks.1.mlp.fc1:   8%|▊         | 12/149 [24:54<5:52:28, 154.37s/it]calibrating layers.0.blocks.1.mlp.fc2:   8%|▊         | 12/149 [24:54<5:52:28, 154.37s/it]calibrating layers.0.blocks.1.mlp.fc2:   9%|▊         | 13/149 [26:56<5:27:52, 144.65s/it]calibrating layers.1.downsample.reduction:   9%|▊         | 13/149 [26:56<5:27:52, 144.65s/it]calibrating layers.1.downsample.reduction:   9%|▉         | 14/149 [27:26<4:07:27, 109.98s/it]calibrating layers.1.blocks.0.attn.qkv:   9%|▉         | 14/149 [27:26<4:07:27, 109.98s/it]   calibrating layers.1.blocks.0.attn.qkv:  10%|█         | 15/149 [27:58<3:13:33, 86.67s/it] calibrating layers.1.blocks.0.attn.proj:  10%|█         | 15/149 [27:58<3:13:33, 86.67s/it]calibrating layers.1.blocks.0.attn.proj:  11%|█         | 16/149 [28:16<2:25:47, 65.77s/it]calibrating layers.1.blocks.0.attn.matmul1:  11%|█         | 16/149 [28:16<2:25:47, 65.77s/it]calibrating layers.1.blocks.0.attn.matmul1:  11%|█▏        | 17/149 [28:44<1:59:45, 54.44s/it]calibrating layers.1.blocks.0.attn.matmul2:  11%|█▏        | 17/149 [28:44<1:59:45, 54.44s/it]calibrating layers.1.blocks.0.attn.matmul2:  12%|█▏        | 18/149 [29:20<1:46:55, 48.97s/it]calibrating layers.1.blocks.0.mlp.fc1:  12%|█▏        | 18/149 [29:20<1:46:55, 48.97s/it]     calibrating layers.1.blocks.0.mlp.fc1:  13%|█▎        | 19/149 [30:14<1:49:38, 50.61s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 19/149 [30:14<1:49:38, 50.61s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 20/149 [31:16<1:55:52, 53.90s/it]calibrating layers.1.blocks.1.attn.qkv:  13%|█▎        | 20/149 [31:16<1:55:52, 53.90s/it]calibrating layers.1.blocks.1.attn.qkv:  14%|█▍        | 21/149 [31:50<1:42:18, 47.95s/it]calibrating layers.1.blocks.1.attn.proj:  14%|█▍        | 21/149 [31:50<1:42:18, 47.95s/it]calibrating layers.1.blocks.1.attn.proj:  15%|█▍        | 22/149 [32:08<1:22:15, 38.87s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▍        | 22/149 [32:08<1:22:15, 38.87s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▌        | 23/149 [32:36<1:15:00, 35.72s/it]calibrating layers.1.blocks.1.attn.matmul2:  15%|█▌        | 23/149 [32:36<1:15:00, 35.72s/it]calibrating layers.1.blocks.1.attn.matmul2:  16%|█▌        | 24/149 [33:12<1:14:30, 35.76s/it]calibrating layers.1.blocks.1.mlp.fc1:  16%|█▌        | 24/149 [33:12<1:14:30, 35.76s/it]     calibrating layers.1.blocks.1.mlp.fc1:  17%|█▋        | 25/149 [34:06<1:25:15, 41.25s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 25/149 [34:06<1:25:15, 41.25s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 26/149 [35:08<1:37:07, 47.37s/it]calibrating layers.2.downsample.reduction:  17%|█▋        | 26/149 [35:08<1:37:07, 47.37s/it]calibrating layers.2.downsample.reduction:  18%|█▊        | 27/149 [35:24<1:17:35, 38.16s/it]calibrating layers.2.blocks.0.attn.qkv:  18%|█▊        | 27/149 [35:24<1:17:35, 38.16s/it]   calibrating layers.2.blocks.0.attn.qkv:  19%|█▉        | 28/149 [35:45<1:06:11, 32.82s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 28/149 [35:45<1:06:11, 32.82s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 29/149 [35:55<52:15, 26.13s/it]  calibrating layers.2.blocks.0.attn.matmul1:  19%|█▉        | 29/149 [35:55<52:15, 26.13s/it]calibrating layers.2.blocks.0.attn.matmul1:  20%|██        | 30/149 [36:08<44:03, 22.21s/it]calibrating layers.2.blocks.0.attn.matmul2:  20%|██        | 30/149 [36:08<44:03, 22.21s/it]calibrating layers.2.blocks.0.attn.matmul2:  21%|██        | 31/149 [36:22<38:21, 19.50s/it]calibrating layers.2.blocks.0.mlp.fc1:  21%|██        | 31/149 [36:22<38:21, 19.50s/it]     calibrating layers.2.blocks.0.mlp.fc1:  21%|██▏       | 32/149 [36:52<44:39, 22.90s/it]calibrating layers.2.blocks.0.mlp.fc2:  21%|██▏       | 32/149 [36:52<44:39, 22.90s/it]calibrating layers.2.blocks.0.mlp.fc2:  22%|██▏       | 33/149 [37:23<48:54, 25.30s/it]calibrating layers.2.blocks.1.attn.qkv:  22%|██▏       | 33/149 [37:23<48:54, 25.30s/it]calibrating layers.2.blocks.1.attn.qkv:  23%|██▎       | 34/149 [37:44<45:42, 23.85s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 34/149 [37:44<45:42, 23.85s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 35/149 [37:54<37:43, 19.85s/it]calibrating layers.2.blocks.1.attn.matmul1:  23%|██▎       | 35/149 [37:54<37:43, 19.85s/it]calibrating layers.2.blocks.1.attn.matmul1:  24%|██▍       | 36/149 [38:07<33:37, 17.85s/it]calibrating layers.2.blocks.1.attn.matmul2:  24%|██▍       | 36/149 [38:07<33:37, 17.85s/it]calibrating layers.2.blocks.1.attn.matmul2:  25%|██▍       | 37/149 [38:21<30:42, 16.45s/it]calibrating layers.2.blocks.1.mlp.fc1:  25%|██▍       | 37/149 [38:21<30:42, 16.45s/it]     calibrating layers.2.blocks.1.mlp.fc1:  26%|██▌       | 38/149 [38:51<38:23, 20.75s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 38/149 [38:51<38:23, 20.75s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 39/149 [39:22<43:44, 23.86s/it]calibrating layers.2.blocks.2.attn.qkv:  26%|██▌       | 39/149 [39:22<43:44, 23.86s/it]calibrating layers.2.blocks.2.attn.qkv:  27%|██▋       | 40/149 [39:43<41:32, 22.87s/it]calibrating layers.2.blocks.2.attn.proj:  27%|██▋       | 40/149 [39:43<41:32, 22.87s/it]calibrating layers.2.blocks.2.attn.proj:  28%|██▊       | 41/149 [39:54<34:29, 19.16s/it]calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 41/149 [39:54<34:29, 19.16s/it]calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 42/149 [40:07<30:58, 17.37s/it]calibrating layers.2.blocks.2.attn.matmul2:  28%|██▊       | 42/149 [40:07<30:58, 17.37s/it]calibrating layers.2.blocks.2.attn.matmul2:  29%|██▉       | 43/149 [40:20<28:30, 16.13s/it]calibrating layers.2.blocks.2.mlp.fc1:  29%|██▉       | 43/149 [40:20<28:30, 16.13s/it]     calibrating layers.2.blocks.2.mlp.fc1:  30%|██▉       | 44/149 [40:51<35:57, 20.55s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|██▉       | 44/149 [40:51<35:57, 20.55s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|███       | 45/149 [41:22<41:03, 23.69s/it]calibrating layers.2.blocks.3.attn.qkv:  30%|███       | 45/149 [41:22<41:03, 23.69s/it]calibrating layers.2.blocks.3.attn.qkv:  31%|███       | 46/149 [41:42<39:02, 22.74s/it]calibrating layers.2.blocks.3.attn.proj:  31%|███       | 46/149 [41:42<39:02, 22.74s/it]calibrating layers.2.blocks.3.attn.proj:  32%|███▏      | 47/149 [41:53<32:25, 19.07s/it]calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 47/149 [41:53<32:25, 19.07s/it]calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 48/149 [42:06<29:01, 17.24s/it]calibrating layers.2.blocks.3.attn.matmul2:  32%|███▏      | 48/149 [42:06<29:01, 17.24s/it]calibrating layers.2.blocks.3.attn.matmul2:  33%|███▎      | 49/149 [42:19<26:47, 16.07s/it]calibrating layers.2.blocks.3.mlp.fc1:  33%|███▎      | 49/149 [42:19<26:47, 16.07s/it]     calibrating layers.2.blocks.3.mlp.fc1:  34%|███▎      | 50/149 [42:50<33:47, 20.48s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▎      | 50/149 [42:50<33:47, 20.48s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▍      | 51/149 [43:21<38:29, 23.56s/it]calibrating layers.2.blocks.4.attn.qkv:  34%|███▍      | 51/149 [43:21<38:29, 23.56s/it]calibrating layers.2.blocks.4.attn.qkv:  35%|███▍      | 52/149 [43:41<36:24, 22.52s/it]calibrating layers.2.blocks.4.attn.proj:  35%|███▍      | 52/149 [43:41<36:24, 22.52s/it]calibrating layers.2.blocks.4.attn.proj:  36%|███▌      | 53/149 [43:51<30:15, 18.91s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 53/149 [43:51<30:15, 18.91s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 54/149 [44:04<27:06, 17.12s/it]calibrating layers.2.blocks.4.attn.matmul2:  36%|███▌      | 54/149 [44:04<27:06, 17.12s/it]calibrating layers.2.blocks.4.attn.matmul2:  37%|███▋      | 55/149 [44:18<25:00, 15.96s/it]calibrating layers.2.blocks.4.mlp.fc1:  37%|███▋      | 55/149 [44:18<25:00, 15.96s/it]     calibrating layers.2.blocks.4.mlp.fc1:  38%|███▊      | 56/149 [44:49<31:47, 20.51s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 56/149 [44:49<31:47, 20.51s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 57/149 [45:20<36:16, 23.65s/it]calibrating layers.2.blocks.5.attn.qkv:  38%|███▊      | 57/149 [45:20<36:16, 23.65s/it]calibrating layers.2.blocks.5.attn.qkv:  39%|███▉      | 58/149 [45:40<34:23, 22.68s/it]calibrating layers.2.blocks.5.attn.proj:  39%|███▉      | 58/149 [45:40<34:23, 22.68s/it]calibrating layers.2.blocks.5.attn.proj:  40%|███▉      | 59/149 [45:51<28:31, 19.02s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|███▉      | 59/149 [45:51<28:31, 19.02s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|████      | 60/149 [46:03<25:28, 17.17s/it]calibrating layers.2.blocks.5.attn.matmul2:  40%|████      | 60/149 [46:03<25:28, 17.17s/it]calibrating layers.2.blocks.5.attn.matmul2:  41%|████      | 61/149 [46:16<23:23, 15.95s/it]calibrating layers.2.blocks.5.mlp.fc1:  41%|████      | 61/149 [46:16<23:23, 15.95s/it]     calibrating layers.2.blocks.5.mlp.fc1:  42%|████▏     | 62/149 [46:47<29:32, 20.37s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 62/149 [46:47<29:32, 20.37s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 63/149 [47:18<33:46, 23.56s/it]calibrating layers.2.blocks.6.attn.qkv:  42%|████▏     | 63/149 [47:18<33:46, 23.56s/it]calibrating layers.2.blocks.6.attn.qkv:  43%|████▎     | 64/149 [47:39<32:02, 22.62s/it]calibrating layers.2.blocks.6.attn.proj:  43%|████▎     | 64/149 [47:39<32:02, 22.62s/it]calibrating layers.2.blocks.6.attn.proj:  44%|████▎     | 65/149 [47:49<26:34, 18.98s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▎     | 65/149 [47:49<26:34, 18.98s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▍     | 66/149 [48:02<23:47, 17.20s/it]calibrating layers.2.blocks.6.attn.matmul2:  44%|████▍     | 66/149 [48:02<23:47, 17.20s/it]calibrating layers.2.blocks.6.attn.matmul2:  45%|████▍     | 67/149 [48:15<21:48, 15.96s/it]calibrating layers.2.blocks.6.mlp.fc1:  45%|████▍     | 67/149 [48:15<21:48, 15.96s/it]     calibrating layers.2.blocks.6.mlp.fc1:  46%|████▌     | 68/149 [48:46<27:23, 20.30s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▌     | 68/149 [48:46<27:23, 20.30s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▋     | 69/149 [49:16<31:10, 23.38s/it]calibrating layers.2.blocks.7.attn.qkv:  46%|████▋     | 69/149 [49:16<31:10, 23.38s/it]calibrating layers.2.blocks.7.attn.qkv:  47%|████▋     | 70/149 [49:36<29:27, 22.38s/it]calibrating layers.2.blocks.7.attn.proj:  47%|████▋     | 70/149 [49:36<29:27, 22.38s/it]calibrating layers.2.blocks.7.attn.proj:  48%|████▊     | 71/149 [49:47<24:23, 18.77s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 71/149 [49:47<24:23, 18.77s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 72/149 [49:59<21:30, 16.76s/it]calibrating layers.2.blocks.7.attn.matmul2:  48%|████▊     | 72/149 [49:59<21:30, 16.76s/it]calibrating layers.2.blocks.7.attn.matmul2:  49%|████▉     | 73/149 [50:12<19:51, 15.68s/it]calibrating layers.2.blocks.7.mlp.fc1:  49%|████▉     | 73/149 [50:12<19:51, 15.68s/it]     calibrating layers.2.blocks.7.mlp.fc1:  50%|████▉     | 74/149 [50:42<25:13, 20.18s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|████▉     | 74/149 [50:42<25:13, 20.18s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|█████     | 75/149 [51:13<28:47, 23.35s/it]calibrating layers.2.blocks.8.attn.qkv:  50%|█████     | 75/149 [51:13<28:47, 23.35s/it]calibrating layers.2.blocks.8.attn.qkv:  51%|█████     | 76/149 [51:33<27:11, 22.35s/it]calibrating layers.2.blocks.8.attn.proj:  51%|█████     | 76/149 [51:33<27:11, 22.35s/it]calibrating layers.2.blocks.8.attn.proj:  52%|█████▏    | 77/149 [51:44<22:30, 18.76s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 77/149 [51:44<22:30, 18.76s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 78/149 [51:56<19:48, 16.73s/it]calibrating layers.2.blocks.8.attn.matmul2:  52%|█████▏    | 78/149 [51:56<19:48, 16.73s/it]calibrating layers.2.blocks.8.attn.matmul2:  53%|█████▎    | 79/149 [52:09<18:15, 15.65s/it]calibrating layers.2.blocks.8.mlp.fc1:  53%|█████▎    | 79/149 [52:09<18:15, 15.65s/it]     calibrating layers.2.blocks.8.mlp.fc1:  54%|█████▎    | 80/149 [52:40<23:14, 20.21s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▎    | 80/149 [52:40<23:14, 20.21s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▍    | 81/149 [53:11<26:36, 23.48s/it]calibrating layers.2.blocks.9.attn.qkv:  54%|█████▍    | 81/149 [53:11<26:36, 23.48s/it]calibrating layers.2.blocks.9.attn.qkv:  55%|█████▌    | 82/149 [53:31<25:12, 22.58s/it]calibrating layers.2.blocks.9.attn.proj:  55%|█████▌    | 82/149 [53:31<25:12, 22.58s/it]calibrating layers.2.blocks.9.attn.proj:  56%|█████▌    | 83/149 [53:42<20:51, 18.96s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▌    | 83/149 [53:42<20:51, 18.96s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▋    | 84/149 [53:55<18:38, 17.21s/it]calibrating layers.2.blocks.9.attn.matmul2:  56%|█████▋    | 84/149 [53:55<18:38, 17.21s/it]calibrating layers.2.blocks.9.attn.matmul2:  57%|█████▋    | 85/149 [54:08<17:07, 16.06s/it]calibrating layers.2.blocks.9.mlp.fc1:  57%|█████▋    | 85/149 [54:08<17:07, 16.06s/it]     calibrating layers.2.blocks.9.mlp.fc1:  58%|█████▊    | 86/149 [54:39<21:30, 20.48s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 86/149 [54:39<21:30, 20.48s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 87/149 [55:10<24:28, 23.69s/it]calibrating layers.2.blocks.10.attn.qkv:  58%|█████▊    | 87/149 [55:10<24:28, 23.69s/it]calibrating layers.2.blocks.10.attn.qkv:  59%|█████▉    | 88/149 [55:31<23:06, 22.73s/it]calibrating layers.2.blocks.10.attn.proj:  59%|█████▉    | 88/149 [55:31<23:06, 22.73s/it]calibrating layers.2.blocks.10.attn.proj:  60%|█████▉    | 89/149 [55:41<19:03, 19.06s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|█████▉    | 89/149 [55:41<19:03, 19.06s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|██████    | 90/149 [55:54<17:03, 17.34s/it]calibrating layers.2.blocks.10.attn.matmul2:  60%|██████    | 90/149 [55:54<17:03, 17.34s/it]calibrating layers.2.blocks.10.attn.matmul2:  61%|██████    | 91/149 [56:07<15:29, 16.03s/it]calibrating layers.2.blocks.10.mlp.fc1:  61%|██████    | 91/149 [56:07<15:29, 16.03s/it]     calibrating layers.2.blocks.10.mlp.fc1:  62%|██████▏   | 92/149 [56:38<19:25, 20.44s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 92/149 [56:38<19:25, 20.44s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 93/149 [57:09<22:02, 23.61s/it]calibrating layers.2.blocks.11.attn.qkv:  62%|██████▏   | 93/149 [57:09<22:02, 23.61s/it]calibrating layers.2.blocks.11.attn.qkv:  63%|██████▎   | 94/149 [57:30<20:47, 22.67s/it]calibrating layers.2.blocks.11.attn.proj:  63%|██████▎   | 94/149 [57:30<20:47, 22.67s/it]calibrating layers.2.blocks.11.attn.proj:  64%|██████▍   | 95/149 [57:40<17:07, 19.03s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 95/149 [57:40<17:07, 19.03s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 96/149 [57:52<15:01, 17.00s/it]calibrating layers.2.blocks.11.attn.matmul2:  64%|██████▍   | 96/149 [57:52<15:01, 17.00s/it]calibrating layers.2.blocks.11.attn.matmul2:  65%|██████▌   | 97/149 [58:06<13:43, 15.84s/it]calibrating layers.2.blocks.11.mlp.fc1:  65%|██████▌   | 97/149 [58:06<13:43, 15.84s/it]     calibrating layers.2.blocks.11.mlp.fc1:  66%|██████▌   | 98/149 [58:36<17:15, 20.30s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▌   | 98/149 [58:36<17:15, 20.30s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▋   | 99/149 [59:08<19:39, 23.59s/it]calibrating layers.2.blocks.12.attn.qkv:  66%|██████▋   | 99/149 [59:08<19:39, 23.59s/it]calibrating layers.2.blocks.12.attn.qkv:  67%|██████▋   | 100/149 [59:28<18:30, 22.65s/it]calibrating layers.2.blocks.12.attn.proj:  67%|██████▋   | 100/149 [59:28<18:30, 22.65s/it]calibrating layers.2.blocks.12.attn.proj:  68%|██████▊   | 101/149 [59:39<15:12, 19.00s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 101/149 [59:39<15:12, 19.00s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 102/149 [59:52<13:28, 17.20s/it]calibrating layers.2.blocks.12.attn.matmul2:  68%|██████▊   | 102/149 [59:52<13:28, 17.20s/it]calibrating layers.2.blocks.12.attn.matmul2:  69%|██████▉   | 103/149 [1:00:04<12:11, 15.91s/it]calibrating layers.2.blocks.12.mlp.fc1:  69%|██████▉   | 103/149 [1:00:04<12:11, 15.91s/it]     calibrating layers.2.blocks.12.mlp.fc1:  70%|██████▉   | 104/149 [1:00:35<15:15, 20.34s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|██████▉   | 104/149 [1:00:35<15:15, 20.34s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|███████   | 105/149 [1:01:06<17:18, 23.60s/it]calibrating layers.2.blocks.13.attn.qkv:  70%|███████   | 105/149 [1:01:06<17:18, 23.60s/it]calibrating layers.2.blocks.13.attn.qkv:  71%|███████   | 106/149 [1:01:26<16:10, 22.58s/it]calibrating layers.2.blocks.13.attn.proj:  71%|███████   | 106/149 [1:01:26<16:10, 22.58s/it]calibrating layers.2.blocks.13.attn.proj:  72%|███████▏  | 107/149 [1:01:37<13:14, 18.91s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 107/149 [1:01:37<13:14, 18.91s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 108/149 [1:01:50<11:42, 17.14s/it]calibrating layers.2.blocks.13.attn.matmul2:  72%|███████▏  | 108/149 [1:01:50<11:42, 17.14s/it]calibrating layers.2.blocks.13.attn.matmul2:  73%|███████▎  | 109/149 [1:02:03<10:37, 15.93s/it]calibrating layers.2.blocks.13.mlp.fc1:  73%|███████▎  | 109/149 [1:02:03<10:37, 15.93s/it]     calibrating layers.2.blocks.13.mlp.fc1:  74%|███████▍  | 110/149 [1:02:34<13:13, 20.36s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 110/149 [1:02:34<13:13, 20.36s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 111/149 [1:03:05<14:58, 23.64s/it]calibrating layers.2.blocks.14.attn.qkv:  74%|███████▍  | 111/149 [1:03:05<14:58, 23.64s/it]calibrating layers.2.blocks.14.attn.qkv:  75%|███████▌  | 112/149 [1:03:25<13:59, 22.68s/it]calibrating layers.2.blocks.14.attn.proj:  75%|███████▌  | 112/149 [1:03:25<13:59, 22.68s/it]calibrating layers.2.blocks.14.attn.proj:  76%|███████▌  | 113/149 [1:03:36<11:24, 19.01s/it]calibrating layers.2.blocks.14.attn.matmul1:  76%|███████▌  | 113/149 [1:03:36<11:24, 19.01s/it]calibrating layers.2.blocks.14.attn.matmul1:  77%|███████▋  | 114/149 [1:03:49<10:04, 17.27s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 114/149 [1:03:49<10:04, 17.27s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 115/149 [1:04:02<09:07, 16.11s/it]calibrating layers.2.blocks.14.mlp.fc1:  77%|███████▋  | 115/149 [1:04:02<09:07, 16.11s/it]     calibrating layers.2.blocks.14.mlp.fc1:  78%|███████▊  | 116/149 [1:04:33<11:18, 20.57s/it]calibrating layers.2.blocks.14.mlp.fc2:  78%|███████▊  | 116/149 [1:04:33<11:18, 20.57s/it]calibrating layers.2.blocks.14.mlp.fc2:  79%|███████▊  | 117/149 [1:05:05<12:40, 23.77s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▊  | 117/149 [1:05:05<12:40, 23.77s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▉  | 118/149 [1:05:25<11:48, 22.84s/it]calibrating layers.2.blocks.15.attn.proj:  79%|███████▉  | 118/149 [1:05:25<11:48, 22.84s/it]calibrating layers.2.blocks.15.attn.proj:  80%|███████▉  | 119/149 [1:05:36<09:35, 19.19s/it]calibrating layers.2.blocks.15.attn.matmul1:  80%|███████▉  | 119/149 [1:05:36<09:35, 19.19s/it]calibrating layers.2.blocks.15.attn.matmul1:  81%|████████  | 120/149 [1:05:49<08:25, 17.42s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 120/149 [1:05:49<08:25, 17.42s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 121/149 [1:06:03<07:34, 16.25s/it]calibrating layers.2.blocks.15.mlp.fc1:  81%|████████  | 121/149 [1:06:03<07:34, 16.25s/it]     calibrating layers.2.blocks.15.mlp.fc1:  82%|████████▏ | 122/149 [1:06:34<09:17, 20.65s/it]calibrating layers.2.blocks.15.mlp.fc2:  82%|████████▏ | 122/149 [1:06:34<09:17, 20.65s/it]calibrating layers.2.blocks.15.mlp.fc2:  83%|████████▎ | 123/149 [1:07:05<10:19, 23.82s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 123/149 [1:07:05<10:19, 23.82s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 124/149 [1:07:26<09:32, 22.88s/it]calibrating layers.2.blocks.16.attn.proj:  83%|████████▎ | 124/149 [1:07:26<09:32, 22.88s/it]calibrating layers.2.blocks.16.attn.proj:  84%|████████▍ | 125/149 [1:07:36<07:40, 19.18s/it]calibrating layers.2.blocks.16.attn.matmul1:  84%|████████▍ | 125/149 [1:07:36<07:40, 19.18s/it]calibrating layers.2.blocks.16.attn.matmul1:  85%|████████▍ | 126/149 [1:07:50<06:40, 17.43s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▍ | 126/149 [1:07:50<06:40, 17.43s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▌ | 127/149 [1:08:03<05:56, 16.19s/it]calibrating layers.2.blocks.16.mlp.fc1:  85%|████████▌ | 127/149 [1:08:03<05:56, 16.19s/it]     calibrating layers.2.blocks.16.mlp.fc1:  86%|████████▌ | 128/149 [1:08:33<07:09, 20.45s/it]calibrating layers.2.blocks.16.mlp.fc2:  86%|████████▌ | 128/149 [1:08:33<07:09, 20.45s/it]calibrating layers.2.blocks.16.mlp.fc2:  87%|████████▋ | 129/149 [1:09:04<07:50, 23.51s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 129/149 [1:09:04<07:50, 23.51s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 130/149 [1:09:24<07:07, 22.52s/it]calibrating layers.2.blocks.17.attn.proj:  87%|████████▋ | 130/149 [1:09:24<07:07, 22.52s/it]calibrating layers.2.blocks.17.attn.proj:  88%|████████▊ | 131/149 [1:09:35<05:40, 18.90s/it]calibrating layers.2.blocks.17.attn.matmul1:  88%|████████▊ | 131/149 [1:09:35<05:40, 18.90s/it]calibrating layers.2.blocks.17.attn.matmul1:  89%|████████▊ | 132/149 [1:09:47<04:47, 16.94s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▊ | 132/149 [1:09:47<04:47, 16.94s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▉ | 133/149 [1:10:00<04:14, 15.88s/it]calibrating layers.2.blocks.17.mlp.fc1:  89%|████████▉ | 133/149 [1:10:00<04:14, 15.88s/it]     calibrating layers.2.blocks.17.mlp.fc1:  90%|████████▉ | 134/149 [1:10:31<05:05, 20.34s/it]calibrating layers.2.blocks.17.mlp.fc2:  90%|████████▉ | 134/149 [1:10:31<05:05, 20.34s/it]calibrating layers.2.blocks.17.mlp.fc2:  91%|█████████ | 135/149 [1:11:02<05:31, 23.64s/it]calibrating layers.3.downsample.reduction:  91%|█████████ | 135/149 [1:11:02<05:31, 23.64s/it]calibrating layers.3.downsample.reduction:  91%|█████████▏| 136/149 [1:11:14<04:20, 20.05s/it]calibrating layers.3.blocks.0.attn.qkv:  91%|█████████▏| 136/149 [1:11:14<04:20, 20.05s/it]   calibrating layers.3.blocks.0.attn.qkv:  92%|█████████▏| 137/149 [1:11:29<03:43, 18.65s/it]calibrating layers.3.blocks.0.attn.proj:  92%|█████████▏| 137/149 [1:11:29<03:43, 18.65s/it]calibrating layers.3.blocks.0.attn.proj:  93%|█████████▎| 138/149 [1:11:37<02:46, 15.18s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 138/149 [1:11:37<02:46, 15.18s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 139/149 [1:11:49<02:22, 14.27s/it]calibrating layers.3.blocks.0.attn.matmul2:  93%|█████████▎| 139/149 [1:11:49<02:22, 14.27s/it]calibrating layers.3.blocks.0.attn.matmul2:  94%|█████████▍| 140/149 [1:12:01<02:02, 13.56s/it]calibrating layers.3.blocks.0.mlp.fc1:  94%|█████████▍| 140/149 [1:12:01<02:02, 13.56s/it]     calibrating layers.3.blocks.0.mlp.fc1:  95%|█████████▍| 141/149 [1:12:21<02:05, 15.66s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▍| 141/149 [1:12:21<02:05, 15.66s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▌| 142/149 [1:12:42<02:00, 17.28s/it]calibrating layers.3.blocks.1.attn.qkv:  95%|█████████▌| 142/149 [1:12:42<02:00, 17.28s/it]calibrating layers.3.blocks.1.attn.qkv:  96%|█████████▌| 143/149 [1:12:57<01:39, 16.65s/it]calibrating layers.3.blocks.1.attn.proj:  96%|█████████▌| 143/149 [1:12:57<01:39, 16.65s/it]calibrating layers.3.blocks.1.attn.proj:  97%|█████████▋| 144/149 [1:13:05<01:08, 13.80s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 144/149 [1:13:05<01:08, 13.80s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 145/149 [1:13:17<00:53, 13.39s/it]calibrating layers.3.blocks.1.attn.matmul2:  97%|█████████▋| 145/149 [1:13:17<00:53, 13.39s/it]calibrating layers.3.blocks.1.attn.matmul2:  98%|█████████▊| 146/149 [1:13:29<00:39, 13.00s/it]calibrating layers.3.blocks.1.mlp.fc1:  98%|█████████▊| 146/149 [1:13:29<00:39, 13.00s/it]     calibrating layers.3.blocks.1.mlp.fc1:  99%|█████████▊| 147/149 [1:13:50<00:30, 15.34s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▊| 147/149 [1:13:50<00:30, 15.34s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▉| 148/149 [1:14:11<00:17, 17.14s/it]calibrating head.fc:  99%|█████████▉| 148/149 [1:14:11<00:17, 17.14s/it]                  calibrating head.fc: 100%|██████████| 149/149 [1:14:14<00:00, 12.91s/it]calibrating head.fc: 100%|██████████| 149/149 [1:14:14<00:00, 29.90s/it]
2025-09-11 12:13:20 - mse guided calibration finished.
Saving checkpoint to ./checkpoint/quant_result/20250911_1056/swin_small_w2_a2_calibsize_1000_mse.pth
Validating after calibration ...
Test: [0/100]	Time 18.957 (18.957)	Loss 6.9917 (6.9917)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.000)
Test: [10/100]	Time 1.751 (3.317)	Loss 6.8465 (7.3054)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.000)
Test: [20/100]	Time 1.748 (2.570)	Loss 7.2961 (7.2516)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.410)
Test: [30/100]	Time 1.750 (2.307)	Loss 7.2801 (7.2489)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.277)
Test: [40/100]	Time 1.754 (2.171)	Loss 7.2321 (7.2431)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.224)
Test: [50/100]	Time 1.750 (2.089)	Loss 6.6705 (7.1570)	Prec@1 0.000 (0.000)	Prec@5 6.800 (0.384)
Test: [60/100]	Time 1.750 (2.034)	Loss 7.0002 (7.1155)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.325)
Test: [70/100]	Time 1.754 (1.994)	Loss 6.6844 (7.0863)	Prec@1 0.000 (0.000)	Prec@5 1.000 (0.437)
Test: [80/100]	Time 1.751 (1.964)	Loss 6.8526 (7.0555)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.509)
Test: [90/100]	Time 1.748 (1.941)	Loss 6.9282 (7.0381)	Prec@1 0.000 (0.000)	Prec@5 0.000 (0.468)
 * Prec@1 0.100 Prec@5 0.528 Loss 7.023 Time 192.544
Building calibrator ...
2025-09-11 12:16:53 - start mse guided block reconstruction
reconstructing patch_embed ...
initializing raw input and raw output ...
adaround training for patch_embed ...
wraping quantizers in patch_embed ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.056 (rec:0.056, round:0.000)	b=0.00	count=500
Total loss:	0.038 (rec:0.038, round:0.000)	b=0.00	count=1000
Total loss:	0.030 (rec:0.030, round:0.000)	b=0.00	count=1500
Total loss:	0.023 (rec:0.023, round:0.000)	b=0.00	count=2000
Total loss:	0.016 (rec:0.016, round:0.000)	b=0.00	count=2500
Total loss:	0.017 (rec:0.017, round:0.000)	b=0.00	count=3000
Total loss:	0.013 (rec:0.013, round:0.000)	b=0.00	count=3500
Total loss:	43.218 (rec:0.011, round:43.207)	b=20.00	count=4000
Total loss:	28.389 (rec:0.017, round:28.372)	b=19.44	count=4500
Total loss:	26.670 (rec:0.016, round:26.653)	b=18.88	count=5000
Total loss:	25.629 (rec:0.015, round:25.614)	b=18.31	count=5500
Total loss:	24.525 (rec:0.015, round:24.511)	b=17.75	count=6000
Total loss:	23.549 (rec:0.009, round:23.540)	b=17.19	count=6500
Total loss:	22.664 (rec:0.014, round:22.651)	b=16.62	count=7000
Total loss:	21.645 (rec:0.009, round:21.636)	b=16.06	count=7500
Total loss:	20.533 (rec:0.008, round:20.525)	b=15.50	count=8000
Total loss:	19.518 (rec:0.013, round:19.505)	b=14.94	count=8500
Total loss:	18.436 (rec:0.011, round:18.425)	b=14.38	count=9000
Total loss:	17.464 (rec:0.017, round:17.447)	b=13.81	count=9500
Total loss:	16.150 (rec:0.015, round:16.135)	b=13.25	count=10000
Total loss:	15.085 (rec:0.017, round:15.068)	b=12.69	count=10500
Total loss:	13.922 (rec:0.016, round:13.906)	b=12.12	count=11000
Total loss:	12.828 (rec:0.023, round:12.806)	b=11.56	count=11500
Total loss:	11.540 (rec:0.029, round:11.511)	b=11.00	count=12000
Total loss:	10.369 (rec:0.022, round:10.346)	b=10.44	count=12500
Total loss:	9.265 (rec:0.031, round:9.234)	b=9.88	count=13000
Total loss:	8.081 (rec:0.032, round:8.049)	b=9.31	count=13500
Total loss:	6.795 (rec:0.053, round:6.742)	b=8.75	count=14000
Total loss:	5.596 (rec:0.037, round:5.559)	b=8.19	count=14500
Total loss:	4.484 (rec:0.066, round:4.418)	b=7.62	count=15000
Total loss:	3.457 (rec:0.067, round:3.391)	b=7.06	count=15500
Total loss:	2.547 (rec:0.076, round:2.470)	b=6.50	count=16000
Total loss:	1.733 (rec:0.104, round:1.629)	b=5.94	count=16500
Total loss:	0.890 (rec:0.122, round:0.767)	b=5.38	count=17000
Total loss:	0.540 (rec:0.129, round:0.410)	b=4.81	count=17500
Total loss:	0.399 (rec:0.151, round:0.247)	b=4.25	count=18000
Total loss:	0.325 (rec:0.157, round:0.168)	b=3.69	count=18500
Total loss:	0.330 (rec:0.244, round:0.086)	b=3.12	count=19000
Total loss:	0.208 (rec:0.179, round:0.029)	b=2.56	count=19500
Total loss:	0.203 (rec:0.194, round:0.010)	b=2.00	count=20000
finished reconstructing patch_embed.
reconstructing layers.0.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.0.blocks.0 ...
wraping quantizers in layers.0.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.983 (rec:0.983, round:0.000)	b=0.00	count=500
Total loss:	0.751 (rec:0.751, round:0.000)	b=0.00	count=1000
Total loss:	0.732 (rec:0.732, round:0.000)	b=0.00	count=1500
Total loss:	0.643 (rec:0.643, round:0.000)	b=0.00	count=2000
Total loss:	0.552 (rec:0.552, round:0.000)	b=0.00	count=2500
Total loss:	0.601 (rec:0.601, round:0.000)	b=0.00	count=3000
Total loss:	0.629 (rec:0.629, round:0.000)	b=0.00	count=3500
Total loss:	854.541 (rec:0.661, round:853.880)	b=20.00	count=4000
Total loss:	399.434 (rec:0.601, round:398.833)	b=19.44	count=4500
Total loss:	356.505 (rec:0.602, round:355.903)	b=18.88	count=5000
Total loss:	324.830 (rec:0.595, round:324.235)	b=18.31	count=5500
Total loss:	297.497 (rec:0.570, round:296.927)	b=17.75	count=6000
Total loss:	273.817 (rec:0.597, round:273.221)	b=17.19	count=6500
Total loss:	252.622 (rec:0.617, round:252.005)	b=16.62	count=7000
Total loss:	234.002 (rec:0.572, round:233.430)	b=16.06	count=7500
Total loss:	216.340 (rec:0.586, round:215.753)	b=15.50	count=8000
Total loss:	200.153 (rec:0.600, round:199.553)	b=14.94	count=8500
Total loss:	185.018 (rec:0.528, round:184.489)	b=14.38	count=9000
Total loss:	172.073 (rec:0.587, round:171.486)	b=13.81	count=9500
Total loss:	159.762 (rec:0.564, round:159.197)	b=13.25	count=10000
Total loss:	147.939 (rec:0.553, round:147.386)	b=12.69	count=10500
Total loss:	136.149 (rec:0.616, round:135.533)	b=12.12	count=11000
Total loss:	125.189 (rec:0.647, round:124.542)	b=11.56	count=11500
Total loss:	113.576 (rec:0.598, round:112.978)	b=11.00	count=12000
Total loss:	102.197 (rec:0.583, round:101.614)	b=10.44	count=12500
Total loss:	90.727 (rec:0.606, round:90.120)	b=9.88	count=13000
Total loss:	79.575 (rec:0.604, round:78.971)	b=9.31	count=13500
Total loss:	68.469 (rec:0.621, round:67.848)	b=8.75	count=14000
Total loss:	57.345 (rec:0.567, round:56.779)	b=8.19	count=14500
Total loss:	47.238 (rec:0.538, round:46.700)	b=7.62	count=15000
Total loss:	37.306 (rec:0.674, round:36.633)	b=7.06	count=15500
Total loss:	27.564 (rec:0.660, round:26.904)	b=6.50	count=16000
Total loss:	18.869 (rec:0.672, round:18.197)	b=5.94	count=16500
Total loss:	11.691 (rec:0.600, round:11.092)	b=5.38	count=17000
Total loss:	6.697 (rec:0.733, round:5.964)	b=4.81	count=17500
Total loss:	3.456 (rec:0.736, round:2.721)	b=4.25	count=18000
Total loss:	1.546 (rec:0.756, round:0.790)	b=3.69	count=18500
Total loss:	0.815 (rec:0.643, round:0.172)	b=3.12	count=19000
Total loss:	0.830 (rec:0.805, round:0.025)	b=2.56	count=19500
Total loss:	0.669 (rec:0.668, round:0.001)	b=2.00	count=20000
finished reconstructing layers.0.blocks.0.
reconstructing layers.0.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.0.blocks.1 ...
wraping quantizers in layers.0.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.508 (rec:1.508, round:0.000)	b=0.00	count=500
Total loss:	1.378 (rec:1.378, round:0.000)	b=0.00	count=1000
Total loss:	1.220 (rec:1.220, round:0.000)	b=0.00	count=1500
Total loss:	1.232 (rec:1.232, round:0.000)	b=0.00	count=2000
Total loss:	1.175 (rec:1.175, round:0.000)	b=0.00	count=2500
Total loss:	1.289 (rec:1.289, round:0.000)	b=0.00	count=3000
Total loss:	1.149 (rec:1.149, round:0.000)	b=0.00	count=3500
Total loss:	867.126 (rec:1.048, round:866.078)	b=20.00	count=4000
Total loss:	432.257 (rec:1.099, round:431.158)	b=19.44	count=4500
Total loss:	389.472 (rec:1.133, round:388.338)	b=18.88	count=5000
Total loss:	360.313 (rec:1.121, round:359.193)	b=18.31	count=5500
Total loss:	333.455 (rec:1.126, round:332.328)	b=17.75	count=6000
Total loss:	311.135 (rec:1.110, round:310.025)	b=17.19	count=6500
Total loss:	290.352 (rec:1.135, round:289.217)	b=16.62	count=7000
Total loss:	271.413 (rec:1.125, round:270.288)	b=16.06	count=7500
Total loss:	254.048 (rec:1.172, round:252.876)	b=15.50	count=8000
Total loss:	238.760 (rec:1.124, round:237.636)	b=14.94	count=8500
Total loss:	224.461 (rec:1.077, round:223.385)	b=14.38	count=9000
Total loss:	210.728 (rec:1.190, round:209.538)	b=13.81	count=9500
Total loss:	196.483 (rec:1.083, round:195.399)	b=13.25	count=10000
Total loss:	182.994 (rec:1.137, round:181.857)	b=12.69	count=10500
Total loss:	169.929 (rec:1.146, round:168.783)	b=12.12	count=11000
Total loss:	157.123 (rec:1.243, round:155.880)	b=11.56	count=11500
Total loss:	144.096 (rec:1.280, round:142.816)	b=11.00	count=12000
Total loss:	131.728 (rec:1.139, round:130.590)	b=10.44	count=12500
Total loss:	119.211 (rec:1.130, round:118.082)	b=9.88	count=13000
Total loss:	106.804 (rec:1.181, round:105.623)	b=9.31	count=13500
Total loss:	94.128 (rec:1.201, round:92.927)	b=8.75	count=14000
Total loss:	81.257 (rec:1.106, round:80.150)	b=8.19	count=14500
Total loss:	68.126 (rec:1.132, round:66.994)	b=7.62	count=15000
Total loss:	54.807 (rec:1.174, round:53.633)	b=7.06	count=15500
Total loss:	41.558 (rec:1.212, round:40.346)	b=6.50	count=16000
Total loss:	29.346 (rec:1.372, round:27.974)	b=5.94	count=16500
Total loss:	19.044 (rec:1.269, round:17.775)	b=5.38	count=17000
Total loss:	10.834 (rec:1.234, round:9.600)	b=4.81	count=17500
Total loss:	5.632 (rec:1.263, round:4.369)	b=4.25	count=18000
Total loss:	2.788 (rec:1.254, round:1.533)	b=3.69	count=18500
Total loss:	1.584 (rec:1.266, round:0.319)	b=3.12	count=19000
Total loss:	1.283 (rec:1.244, round:0.039)	b=2.56	count=19500
Total loss:	1.287 (rec:1.286, round:0.002)	b=2.00	count=20000
finished reconstructing layers.0.blocks.1.
reconstructing layers.1.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.downsample ...
wraping quantizers in layers.1.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.662 (rec:1.662, round:0.000)	b=0.00	count=500
Total loss:	1.504 (rec:1.504, round:0.000)	b=0.00	count=1000
Total loss:	1.547 (rec:1.547, round:0.000)	b=0.00	count=1500
Total loss:	1.581 (rec:1.581, round:0.000)	b=0.00	count=2000
Total loss:	1.524 (rec:1.524, round:0.000)	b=0.00	count=2500
Total loss:	1.427 (rec:1.427, round:0.000)	b=0.00	count=3000
Total loss:	1.494 (rec:1.494, round:0.000)	b=0.00	count=3500
Total loss:	596.626 (rec:1.492, round:595.134)	b=20.00	count=4000
Total loss:	322.918 (rec:1.500, round:321.418)	b=19.44	count=4500
Total loss:	298.288 (rec:1.506, round:296.783)	b=18.88	count=5000
Total loss:	282.865 (rec:1.507, round:281.358)	b=18.31	count=5500
Total loss:	270.949 (rec:1.466, round:269.483)	b=17.75	count=6000
Total loss:	260.767 (rec:1.463, round:259.304)	b=17.19	count=6500
Total loss:	251.080 (rec:1.476, round:249.604)	b=16.62	count=7000
Total loss:	242.305 (rec:1.478, round:240.827)	b=16.06	count=7500
Total loss:	233.875 (rec:1.509, round:232.366)	b=15.50	count=8000
Total loss:	225.473 (rec:1.529, round:223.944)	b=14.94	count=8500
Total loss:	217.253 (rec:1.498, round:215.754)	b=14.38	count=9000
Total loss:	208.674 (rec:1.497, round:207.178)	b=13.81	count=9500
Total loss:	199.868 (rec:1.540, round:198.328)	b=13.25	count=10000
Total loss:	190.889 (rec:1.461, round:189.428)	b=12.69	count=10500
Total loss:	181.703 (rec:1.455, round:180.248)	b=12.12	count=11000
Total loss:	171.820 (rec:1.463, round:170.357)	b=11.56	count=11500
Total loss:	161.912 (rec:1.546, round:160.366)	b=11.00	count=12000
Total loss:	151.179 (rec:1.491, round:149.688)	b=10.44	count=12500
Total loss:	140.258 (rec:1.523, round:138.735)	b=9.88	count=13000
Total loss:	128.317 (rec:1.472, round:126.845)	b=9.31	count=13500
Total loss:	115.656 (rec:1.557, round:114.099)	b=8.75	count=14000
Total loss:	102.232 (rec:1.526, round:100.705)	b=8.19	count=14500
Total loss:	87.766 (rec:1.557, round:86.209)	b=7.62	count=15000
Total loss:	72.551 (rec:1.569, round:70.982)	b=7.06	count=15500
Total loss:	57.521 (rec:1.593, round:55.928)	b=6.50	count=16000
Total loss:	42.687 (rec:1.555, round:41.131)	b=5.94	count=16500
Total loss:	28.900 (rec:1.620, round:27.280)	b=5.38	count=17000
Total loss:	17.787 (rec:1.611, round:16.176)	b=4.81	count=17500
Total loss:	9.587 (rec:1.627, round:7.960)	b=4.25	count=18000
Total loss:	4.663 (rec:1.649, round:3.015)	b=3.69	count=18500
Total loss:	2.470 (rec:1.645, round:0.825)	b=3.12	count=19000
Total loss:	1.750 (rec:1.638, round:0.112)	b=2.56	count=19500
Total loss:	1.677 (rec:1.670, round:0.008)	b=2.00	count=20000
finished reconstructing layers.1.downsample.
reconstructing layers.1.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.blocks.0 ...
wraping quantizers in layers.1.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.194 (rec:1.194, round:0.000)	b=0.00	count=500
Total loss:	1.049 (rec:1.049, round:0.000)	b=0.00	count=1000
Total loss:	1.039 (rec:1.039, round:0.000)	b=0.00	count=1500
Total loss:	0.944 (rec:0.944, round:0.000)	b=0.00	count=2000
Total loss:	0.959 (rec:0.959, round:0.000)	b=0.00	count=2500
Total loss:	0.936 (rec:0.936, round:0.000)	b=0.00	count=3000
Total loss:	0.929 (rec:0.929, round:0.000)	b=0.00	count=3500
Total loss:	3716.861 (rec:0.903, round:3715.958)	b=20.00	count=4000
Total loss:	1768.262 (rec:0.906, round:1767.357)	b=19.44	count=4500
Total loss:	1599.974 (rec:0.909, round:1599.065)	b=18.88	count=5000
Total loss:	1478.422 (rec:0.923, round:1477.499)	b=18.31	count=5500
Total loss:	1372.535 (rec:0.892, round:1371.643)	b=17.75	count=6000
Total loss:	1275.987 (rec:0.906, round:1275.081)	b=17.19	count=6500
Total loss:	1184.950 (rec:0.949, round:1184.001)	b=16.62	count=7000
Total loss:	1102.809 (rec:0.924, round:1101.885)	b=16.06	count=7500
Total loss:	1026.645 (rec:0.892, round:1025.752)	b=15.50	count=8000
Total loss:	953.386 (rec:0.919, round:952.467)	b=14.94	count=8500
Total loss:	885.750 (rec:0.900, round:884.850)	b=14.38	count=9000
Total loss:	823.533 (rec:0.952, round:822.581)	b=13.81	count=9500
Total loss:	762.313 (rec:0.899, round:761.414)	b=13.25	count=10000
Total loss:	704.062 (rec:0.916, round:703.145)	b=12.69	count=10500
Total loss:	648.182 (rec:0.905, round:647.277)	b=12.12	count=11000
Total loss:	593.338 (rec:0.919, round:592.420)	b=11.56	count=11500
Total loss:	537.873 (rec:0.921, round:536.952)	b=11.00	count=12000
Total loss:	483.038 (rec:0.923, round:482.115)	b=10.44	count=12500
Total loss:	429.003 (rec:0.928, round:428.075)	b=9.88	count=13000
Total loss:	374.508 (rec:0.921, round:373.587)	b=9.31	count=13500
Total loss:	321.580 (rec:0.918, round:320.663)	b=8.75	count=14000
Total loss:	269.094 (rec:0.885, round:268.209)	b=8.19	count=14500
Total loss:	217.619 (rec:0.924, round:216.695)	b=7.62	count=15000
Total loss:	166.868 (rec:0.939, round:165.930)	b=7.06	count=15500
Total loss:	120.514 (rec:0.936, round:119.578)	b=6.50	count=16000
Total loss:	79.139 (rec:0.973, round:78.166)	b=5.94	count=16500
Total loss:	46.174 (rec:0.934, round:45.240)	b=5.38	count=17000
Total loss:	22.767 (rec:0.935, round:21.833)	b=4.81	count=17500
Total loss:	8.727 (rec:0.914, round:7.813)	b=4.25	count=18000
Total loss:	3.041 (rec:0.951, round:2.090)	b=3.69	count=18500
Total loss:	1.340 (rec:0.923, round:0.417)	b=3.12	count=19000
Total loss:	0.997 (rec:0.958, round:0.039)	b=2.56	count=19500
Total loss:	0.943 (rec:0.943, round:0.000)	b=2.00	count=20000
finished reconstructing layers.1.blocks.0.
reconstructing layers.1.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.blocks.1 ...
wraping quantizers in layers.1.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.248 (rec:1.248, round:0.000)	b=0.00	count=500
Total loss:	1.103 (rec:1.103, round:0.000)	b=0.00	count=1000
Total loss:	1.058 (rec:1.058, round:0.000)	b=0.00	count=1500
Total loss:	1.071 (rec:1.071, round:0.000)	b=0.00	count=2000
Total loss:	1.046 (rec:1.046, round:0.000)	b=0.00	count=2500
Total loss:	1.021 (rec:1.021, round:0.000)	b=0.00	count=3000
Total loss:	1.004 (rec:1.004, round:0.000)	b=0.00	count=3500
Total loss:	3755.888 (rec:0.967, round:3754.921)	b=20.00	count=4000
Total loss:	1820.173 (rec:0.975, round:1819.198)	b=19.44	count=4500
Total loss:	1655.527 (rec:0.966, round:1654.561)	b=18.88	count=5000
Total loss:	1536.161 (rec:0.967, round:1535.194)	b=18.31	count=5500
Total loss:	1428.717 (rec:0.988, round:1427.728)	b=17.75	count=6000
Total loss:	1332.878 (rec:0.988, round:1331.890)	b=17.19	count=6500
Total loss:	1242.107 (rec:0.970, round:1241.137)	b=16.62	count=7000
Total loss:	1156.430 (rec:0.980, round:1155.450)	b=16.06	count=7500
Total loss:	1077.377 (rec:0.983, round:1076.393)	b=15.50	count=8000
Total loss:	1003.859 (rec:0.966, round:1002.893)	b=14.94	count=8500
Total loss:	934.253 (rec:1.006, round:933.246)	b=14.38	count=9000
Total loss:	867.644 (rec:0.997, round:866.647)	b=13.81	count=9500
Total loss:	803.210 (rec:0.976, round:802.234)	b=13.25	count=10000
Total loss:	742.319 (rec:0.959, round:741.360)	b=12.69	count=10500
Total loss:	682.847 (rec:0.982, round:681.865)	b=12.12	count=11000
Total loss:	625.068 (rec:1.013, round:624.055)	b=11.56	count=11500
Total loss:	569.128 (rec:0.961, round:568.167)	b=11.00	count=12000
Total loss:	513.620 (rec:0.998, round:512.622)	b=10.44	count=12500
Total loss:	458.043 (rec:0.990, round:457.052)	b=9.88	count=13000
Total loss:	404.849 (rec:0.994, round:403.855)	b=9.31	count=13500
Total loss:	350.025 (rec:0.984, round:349.041)	b=8.75	count=14000
Total loss:	296.944 (rec:0.982, round:295.961)	b=8.19	count=14500
Total loss:	243.453 (rec:1.015, round:242.439)	b=7.62	count=15000
Total loss:	191.136 (rec:0.985, round:190.151)	b=7.06	count=15500
Total loss:	141.233 (rec:0.976, round:140.257)	b=6.50	count=16000
Total loss:	96.208 (rec:1.045, round:95.162)	b=5.94	count=16500
Total loss:	57.999 (rec:0.980, round:57.019)	b=5.38	count=17000
Total loss:	29.937 (rec:1.032, round:28.905)	b=4.81	count=17500
Total loss:	12.295 (rec:1.050, round:11.245)	b=4.25	count=18000
Total loss:	4.143 (rec:1.037, round:3.106)	b=3.69	count=18500
Total loss:	1.425 (rec:0.967, round:0.457)	b=3.12	count=19000
Total loss:	1.053 (rec:1.016, round:0.037)	b=2.56	count=19500
Total loss:	0.979 (rec:0.978, round:0.001)	b=2.00	count=20000
finished reconstructing layers.1.blocks.1.
reconstructing layers.2.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.downsample ...
wraping quantizers in layers.2.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.745 (rec:1.745, round:0.000)	b=0.00	count=500
Total loss:	1.582 (rec:1.582, round:0.000)	b=0.00	count=1000
Total loss:	1.578 (rec:1.578, round:0.000)	b=0.00	count=1500
Total loss:	1.550 (rec:1.550, round:0.000)	b=0.00	count=2000
Total loss:	1.540 (rec:1.540, round:0.000)	b=0.00	count=2500
Total loss:	1.469 (rec:1.469, round:0.000)	b=0.00	count=3000
Total loss:	1.461 (rec:1.461, round:0.000)	b=0.00	count=3500
Total loss:	2364.163 (rec:1.475, round:2362.687)	b=20.00	count=4000
Total loss:	1158.684 (rec:1.483, round:1157.202)	b=19.44	count=4500
Total loss:	1061.301 (rec:1.496, round:1059.805)	b=18.88	count=5000
Total loss:	996.953 (rec:1.409, round:995.544)	b=18.31	count=5500
Total loss:	944.794 (rec:1.473, round:943.321)	b=17.75	count=6000
Total loss:	900.046 (rec:1.512, round:898.534)	b=17.19	count=6500
Total loss:	858.871 (rec:1.484, round:857.386)	b=16.62	count=7000
Total loss:	819.763 (rec:1.443, round:818.321)	b=16.06	count=7500
Total loss:	784.119 (rec:1.490, round:782.629)	b=15.50	count=8000
Total loss:	749.067 (rec:1.412, round:747.655)	b=14.94	count=8500
Total loss:	715.064 (rec:1.481, round:713.583)	b=14.38	count=9000
Total loss:	682.224 (rec:1.465, round:680.759)	b=13.81	count=9500
Total loss:	649.132 (rec:1.491, round:647.641)	b=13.25	count=10000
Total loss:	615.203 (rec:1.479, round:613.724)	b=12.69	count=10500
Total loss:	581.004 (rec:1.555, round:579.450)	b=12.12	count=11000
Total loss:	546.706 (rec:1.523, round:545.182)	b=11.56	count=11500
Total loss:	509.955 (rec:1.509, round:508.446)	b=11.00	count=12000
Total loss:	472.117 (rec:1.493, round:470.624)	b=10.44	count=12500
Total loss:	433.278 (rec:1.468, round:431.810)	b=9.88	count=13000
Total loss:	393.246 (rec:1.556, round:391.690)	b=9.31	count=13500
Total loss:	352.218 (rec:1.546, round:350.672)	b=8.75	count=14000
Total loss:	309.393 (rec:1.497, round:307.897)	b=8.19	count=14500
Total loss:	264.244 (rec:1.488, round:262.756)	b=7.62	count=15000
Total loss:	217.752 (rec:1.536, round:216.216)	b=7.06	count=15500
Total loss:	169.669 (rec:1.554, round:168.115)	b=6.50	count=16000
Total loss:	123.464 (rec:1.545, round:121.919)	b=5.94	count=16500
Total loss:	80.742 (rec:1.588, round:79.154)	b=5.38	count=17000
Total loss:	45.927 (rec:1.642, round:44.285)	b=4.81	count=17500
Total loss:	21.951 (rec:1.544, round:20.406)	b=4.25	count=18000
Total loss:	8.395 (rec:1.561, round:6.834)	b=3.69	count=18500
Total loss:	3.087 (rec:1.574, round:1.513)	b=3.12	count=19000
Total loss:	1.778 (rec:1.622, round:0.156)	b=2.56	count=19500
Total loss:	1.607 (rec:1.604, round:0.003)	b=2.00	count=20000
finished reconstructing layers.2.downsample.
reconstructing layers.2.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.0 ...
wraping quantizers in layers.2.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.570 (rec:1.570, round:0.000)	b=0.00	count=500
Total loss:	1.457 (rec:1.457, round:0.000)	b=0.00	count=1000
Total loss:	1.444 (rec:1.444, round:0.000)	b=0.00	count=1500
Total loss:	1.408 (rec:1.408, round:0.000)	b=0.00	count=2000
Total loss:	1.389 (rec:1.389, round:0.000)	b=0.00	count=2500
Total loss:	1.383 (rec:1.383, round:0.000)	b=0.00	count=3000
Total loss:	1.358 (rec:1.358, round:0.000)	b=0.00	count=3500
Total loss:	15461.768 (rec:1.320, round:15460.447)	b=20.00	count=4000
Total loss:	7001.405 (rec:1.373, round:7000.032)	b=19.44	count=4500
Total loss:	6398.864 (rec:1.320, round:6397.544)	b=18.88	count=5000
Total loss:	5976.250 (rec:1.268, round:5974.982)	b=18.31	count=5500
Total loss:	5601.165 (rec:1.304, round:5599.860)	b=17.75	count=6000
Total loss:	5251.551 (rec:1.341, round:5250.210)	b=17.19	count=6500
Total loss:	4921.275 (rec:1.335, round:4919.940)	b=16.62	count=7000
Total loss:	4603.078 (rec:1.286, round:4601.792)	b=16.06	count=7500
Total loss:	4295.990 (rec:1.339, round:4294.650)	b=15.50	count=8000
Total loss:	4001.801 (rec:1.287, round:4000.515)	b=14.94	count=8500
Total loss:	3722.609 (rec:1.294, round:3721.314)	b=14.38	count=9000
Total loss:	3452.195 (rec:1.291, round:3450.904)	b=13.81	count=9500
Total loss:	3188.989 (rec:1.269, round:3187.721)	b=13.25	count=10000
Total loss:	2936.029 (rec:1.269, round:2934.759)	b=12.69	count=10500
Total loss:	2690.524 (rec:1.315, round:2689.209)	b=12.12	count=11000
Total loss:	2450.156 (rec:1.317, round:2448.839)	b=11.56	count=11500
Total loss:	2216.755 (rec:1.312, round:2215.442)	b=11.00	count=12000
Total loss:	1989.713 (rec:1.303, round:1988.409)	b=10.44	count=12500
Total loss:	1767.317 (rec:1.292, round:1766.026)	b=9.88	count=13000
Total loss:	1549.210 (rec:1.287, round:1547.924)	b=9.31	count=13500
Total loss:	1337.134 (rec:1.309, round:1335.825)	b=8.75	count=14000
Total loss:	1134.158 (rec:1.319, round:1132.839)	b=8.19	count=14500
Total loss:	938.110 (rec:1.321, round:936.789)	b=7.62	count=15000
Total loss:	748.182 (rec:1.377, round:746.805)	b=7.06	count=15500
Total loss:	565.922 (rec:1.310, round:564.612)	b=6.50	count=16000
Total loss:	393.491 (rec:1.352, round:392.139)	b=5.94	count=16500
Total loss:	231.643 (rec:1.322, round:230.321)	b=5.38	count=17000
Total loss:	100.312 (rec:1.393, round:98.919)	b=4.81	count=17500
Total loss:	32.535 (rec:1.360, round:31.175)	b=4.25	count=18000
Total loss:	8.718 (rec:1.338, round:7.380)	b=3.69	count=18500
Total loss:	2.391 (rec:1.334, round:1.057)	b=3.12	count=19000
Total loss:	1.399 (rec:1.330, round:0.069)	b=2.56	count=19500
Total loss:	1.331 (rec:1.329, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.0.
reconstructing layers.2.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.1 ...
wraping quantizers in layers.2.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.553 (rec:1.553, round:0.000)	b=0.00	count=500
Total loss:	1.528 (rec:1.528, round:0.000)	b=0.00	count=1000
Total loss:	1.466 (rec:1.466, round:0.000)	b=0.00	count=1500
Total loss:	1.494 (rec:1.494, round:0.000)	b=0.00	count=2000
Total loss:	1.467 (rec:1.467, round:0.000)	b=0.00	count=2500
Total loss:	1.491 (rec:1.491, round:0.000)	b=0.00	count=3000
Total loss:	1.437 (rec:1.437, round:0.000)	b=0.00	count=3500
Total loss:	15554.024 (rec:1.395, round:15552.629)	b=20.00	count=4000
Total loss:	7303.044 (rec:1.396, round:7301.648)	b=19.44	count=4500
Total loss:	6701.148 (rec:1.390, round:6699.758)	b=18.88	count=5000
Total loss:	6277.717 (rec:1.378, round:6276.340)	b=18.31	count=5500
Total loss:	5903.962 (rec:1.379, round:5902.583)	b=17.75	count=6000
Total loss:	5555.592 (rec:1.414, round:5554.178)	b=17.19	count=6500
Total loss:	5228.142 (rec:1.366, round:5226.776)	b=16.62	count=7000
Total loss:	4911.939 (rec:1.412, round:4910.527)	b=16.06	count=7500
Total loss:	4605.461 (rec:1.372, round:4604.089)	b=15.50	count=8000
Total loss:	4312.897 (rec:1.441, round:4311.457)	b=14.94	count=8500
Total loss:	4028.778 (rec:1.369, round:4027.409)	b=14.38	count=9000
Total loss:	3750.536 (rec:1.400, round:3749.136)	b=13.81	count=9500
Total loss:	3480.385 (rec:1.393, round:3478.992)	b=13.25	count=10000
Total loss:	3218.486 (rec:1.379, round:3217.107)	b=12.69	count=10500
Total loss:	2960.324 (rec:1.361, round:2958.963)	b=12.12	count=11000
Total loss:	2704.832 (rec:1.388, round:2703.443)	b=11.56	count=11500
Total loss:	2456.908 (rec:1.359, round:2455.550)	b=11.00	count=12000
Total loss:	2212.954 (rec:1.374, round:2211.581)	b=10.44	count=12500
Total loss:	1975.758 (rec:1.406, round:1974.352)	b=9.88	count=13000
Total loss:	1742.969 (rec:1.401, round:1741.568)	b=9.31	count=13500
Total loss:	1514.813 (rec:1.418, round:1513.396)	b=8.75	count=14000
Total loss:	1291.844 (rec:1.372, round:1290.471)	b=8.19	count=14500
Total loss:	1073.472 (rec:1.415, round:1072.057)	b=7.62	count=15000
Total loss:	859.456 (rec:1.391, round:858.065)	b=7.06	count=15500
Total loss:	656.110 (rec:1.371, round:654.739)	b=6.50	count=16000
Total loss:	464.200 (rec:1.426, round:462.774)	b=5.94	count=16500
Total loss:	280.145 (rec:1.423, round:278.722)	b=5.38	count=17000
Total loss:	124.740 (rec:1.412, round:123.328)	b=4.81	count=17500
Total loss:	39.684 (rec:1.444, round:38.240)	b=4.25	count=18000
Total loss:	9.864 (rec:1.414, round:8.449)	b=3.69	count=18500
Total loss:	2.646 (rec:1.399, round:1.247)	b=3.12	count=19000
Total loss:	1.538 (rec:1.440, round:0.098)	b=2.56	count=19500
Total loss:	1.463 (rec:1.462, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.1.
reconstructing layers.2.blocks.2 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.2 ...
wraping quantizers in layers.2.blocks.2 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.742 (rec:1.742, round:0.000)	b=0.00	count=500
Total loss:	1.594 (rec:1.594, round:0.000)	b=0.00	count=1000
Total loss:	1.566 (rec:1.566, round:0.000)	b=0.00	count=1500
Total loss:	1.574 (rec:1.574, round:0.000)	b=0.00	count=2000
Total loss:	1.522 (rec:1.522, round:0.000)	b=0.00	count=2500
Total loss:	1.548 (rec:1.548, round:0.000)	b=0.00	count=3000
Total loss:	1.536 (rec:1.536, round:0.000)	b=0.00	count=3500
Total loss:	15608.469 (rec:1.473, round:15606.996)	b=20.00	count=4000
Total loss:	7478.468 (rec:1.474, round:7476.994)	b=19.44	count=4500
Total loss:	6867.861 (rec:1.461, round:6866.400)	b=18.88	count=5000
Total loss:	6438.274 (rec:1.491, round:6436.783)	b=18.31	count=5500
Total loss:	6060.619 (rec:1.452, round:6059.167)	b=17.75	count=6000
Total loss:	5709.155 (rec:1.518, round:5707.638)	b=17.19	count=6500
Total loss:	5370.887 (rec:1.440, round:5369.447)	b=16.62	count=7000
Total loss:	5047.626 (rec:1.454, round:5046.172)	b=16.06	count=7500
Total loss:	4740.647 (rec:1.468, round:4739.179)	b=15.50	count=8000
Total loss:	4439.386 (rec:1.481, round:4437.905)	b=14.94	count=8500
Total loss:	4147.932 (rec:1.480, round:4146.452)	b=14.38	count=9000
Total loss:	3864.857 (rec:1.470, round:3863.387)	b=13.81	count=9500
Total loss:	3589.625 (rec:1.472, round:3588.152)	b=13.25	count=10000
Total loss:	3322.387 (rec:1.489, round:3320.897)	b=12.69	count=10500
Total loss:	3058.183 (rec:1.521, round:3056.663)	b=12.12	count=11000
Total loss:	2799.309 (rec:1.490, round:2797.819)	b=11.56	count=11500
Total loss:	2545.156 (rec:1.476, round:2543.680)	b=11.00	count=12000
Total loss:	2295.832 (rec:1.471, round:2294.361)	b=10.44	count=12500
Total loss:	2052.211 (rec:1.462, round:2050.749)	b=9.88	count=13000
Total loss:	1814.081 (rec:1.467, round:1812.613)	b=9.31	count=13500
Total loss:	1580.676 (rec:1.465, round:1579.211)	b=8.75	count=14000
Total loss:	1351.074 (rec:1.473, round:1349.602)	b=8.19	count=14500
Total loss:	1126.123 (rec:1.458, round:1124.665)	b=7.62	count=15000
Total loss:	908.933 (rec:1.467, round:907.466)	b=7.06	count=15500
Total loss:	698.131 (rec:1.486, round:696.645)	b=6.50	count=16000
Total loss:	498.821 (rec:1.477, round:497.345)	b=5.94	count=16500
Total loss:	308.033 (rec:1.476, round:306.557)	b=5.38	count=17000
Total loss:	139.454 (rec:1.472, round:137.982)	b=4.81	count=17500
Total loss:	43.465 (rec:1.499, round:41.966)	b=4.25	count=18000
Total loss:	11.061 (rec:1.505, round:9.556)	b=3.69	count=18500
Total loss:	2.845 (rec:1.501, round:1.344)	b=3.12	count=19000
Total loss:	1.587 (rec:1.499, round:0.088)	b=2.56	count=19500
Total loss:	1.504 (rec:1.502, round:0.002)	b=2.00	count=20000
finished reconstructing layers.2.blocks.2.
reconstructing layers.2.blocks.3 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.3 ...
wraping quantizers in layers.2.blocks.3 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.653 (rec:1.653, round:0.000)	b=0.00	count=500
Total loss:	1.609 (rec:1.609, round:0.000)	b=0.00	count=1000
Total loss:	1.567 (rec:1.567, round:0.000)	b=0.00	count=1500
Total loss:	1.528 (rec:1.528, round:0.000)	b=0.00	count=2000
Total loss:	1.552 (rec:1.552, round:0.000)	b=0.00	count=2500
Total loss:	1.588 (rec:1.588, round:0.000)	b=0.00	count=3000
Total loss:	1.522 (rec:1.522, round:0.000)	b=0.00	count=3500
Total loss:	15673.492 (rec:1.547, round:15671.945)	b=20.00	count=4000
Total loss:	7478.125 (rec:1.548, round:7476.577)	b=19.44	count=4500
Total loss:	6872.006 (rec:1.534, round:6870.473)	b=18.88	count=5000
Total loss:	6446.517 (rec:1.485, round:6445.032)	b=18.31	count=5500
Total loss:	6069.291 (rec:1.510, round:6067.781)	b=17.75	count=6000
Total loss:	5716.379 (rec:1.534, round:5714.845)	b=17.19	count=6500
Total loss:	5378.998 (rec:1.491, round:5377.506)	b=16.62	count=7000
Total loss:	5057.008 (rec:1.474, round:5055.535)	b=16.06	count=7500
Total loss:	4742.491 (rec:1.469, round:4741.022)	b=15.50	count=8000
Total loss:	4440.039 (rec:1.506, round:4438.533)	b=14.94	count=8500
Total loss:	4145.390 (rec:1.486, round:4143.904)	b=14.38	count=9000
Total loss:	3856.687 (rec:1.456, round:3855.231)	b=13.81	count=9500
Total loss:	3579.527 (rec:1.447, round:3578.080)	b=13.25	count=10000
Total loss:	3306.480 (rec:1.495, round:3304.986)	b=12.69	count=10500
Total loss:	3039.578 (rec:1.506, round:3038.072)	b=12.12	count=11000
Total loss:	2776.323 (rec:1.539, round:2774.784)	b=11.56	count=11500
Total loss:	2521.845 (rec:1.531, round:2520.314)	b=11.00	count=12000
Total loss:	2271.803 (rec:1.489, round:2270.314)	b=10.44	count=12500
Total loss:	2027.972 (rec:1.522, round:2026.450)	b=9.88	count=13000
Total loss:	1790.541 (rec:1.528, round:1789.014)	b=9.31	count=13500
Total loss:	1557.493 (rec:1.505, round:1555.988)	b=8.75	count=14000
Total loss:	1329.152 (rec:1.475, round:1327.677)	b=8.19	count=14500
Total loss:	1108.427 (rec:1.507, round:1106.921)	b=7.62	count=15000
Total loss:	896.655 (rec:1.519, round:895.136)	b=7.06	count=15500
Total loss:	691.445 (rec:1.533, round:689.913)	b=6.50	count=16000
Total loss:	493.550 (rec:1.529, round:492.021)	b=5.94	count=16500
Total loss:	302.861 (rec:1.485, round:301.376)	b=5.38	count=17000
Total loss:	136.126 (rec:1.527, round:134.599)	b=4.81	count=17500
Total loss:	45.031 (rec:1.566, round:43.465)	b=4.25	count=18000
Total loss:	11.689 (rec:1.517, round:10.172)	b=3.69	count=18500
Total loss:	2.893 (rec:1.530, round:1.363)	b=3.12	count=19000
Total loss:	1.625 (rec:1.547, round:0.078)	b=2.56	count=19500
Total loss:	1.514 (rec:1.512, round:0.002)	b=2.00	count=20000
finished reconstructing layers.2.blocks.3.
reconstructing layers.2.blocks.4 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.4 ...
wraping quantizers in layers.2.blocks.4 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.742 (rec:1.742, round:0.000)	b=0.00	count=500
Total loss:	1.738 (rec:1.738, round:0.000)	b=0.00	count=1000
Total loss:	1.708 (rec:1.708, round:0.000)	b=0.00	count=1500
Total loss:	1.654 (rec:1.654, round:0.000)	b=0.00	count=2000
Total loss:	1.632 (rec:1.632, round:0.000)	b=0.00	count=2500
Total loss:	1.624 (rec:1.624, round:0.000)	b=0.00	count=3000
Total loss:	1.594 (rec:1.594, round:0.000)	b=0.00	count=3500
Total loss:	15790.929 (rec:1.634, round:15789.295)	b=20.00	count=4000
Total loss:	7724.220 (rec:1.638, round:7722.582)	b=19.44	count=4500
Total loss:	7115.215 (rec:1.601, round:7113.614)	b=18.88	count=5000
Total loss:	6692.967 (rec:1.593, round:6691.374)	b=18.31	count=5500
Total loss:	6323.150 (rec:1.610, round:6321.540)	b=17.75	count=6000
Total loss:	5976.107 (rec:1.564, round:5974.543)	b=17.19	count=6500
Total loss:	5645.222 (rec:1.566, round:5643.656)	b=16.62	count=7000
Total loss:	5320.308 (rec:1.592, round:5318.716)	b=16.06	count=7500
Total loss:	5008.191 (rec:1.622, round:5006.569)	b=15.50	count=8000
Total loss:	4705.968 (rec:1.572, round:4704.396)	b=14.94	count=8500
Total loss:	4408.779 (rec:1.585, round:4407.194)	b=14.38	count=9000
Total loss:	4118.295 (rec:1.580, round:4116.715)	b=13.81	count=9500
Total loss:	3832.433 (rec:1.561, round:3830.872)	b=13.25	count=10000
Total loss:	3555.413 (rec:1.592, round:3553.821)	b=12.69	count=10500
Total loss:	3281.501 (rec:1.572, round:3279.930)	b=12.12	count=11000
Total loss:	3011.853 (rec:1.594, round:3010.259)	b=11.56	count=11500
Total loss:	2744.986 (rec:1.599, round:2743.386)	b=11.00	count=12000
Total loss:	2482.705 (rec:1.594, round:2481.112)	b=10.44	count=12500
Total loss:	2225.222 (rec:1.563, round:2223.659)	b=9.88	count=13000
Total loss:	1971.330 (rec:1.566, round:1969.764)	b=9.31	count=13500
Total loss:	1720.502 (rec:1.593, round:1718.909)	b=8.75	count=14000
Total loss:	1476.462 (rec:1.580, round:1474.882)	b=8.19	count=14500
Total loss:	1236.583 (rec:1.585, round:1234.999)	b=7.62	count=15000
Total loss:	1002.972 (rec:1.582, round:1001.390)	b=7.06	count=15500
Total loss:	777.674 (rec:1.599, round:776.074)	b=6.50	count=16000
Total loss:	561.175 (rec:1.583, round:559.592)	b=5.94	count=16500
Total loss:	352.654 (rec:1.577, round:351.078)	b=5.38	count=17000
Total loss:	167.027 (rec:1.588, round:165.438)	b=4.81	count=17500
Total loss:	55.107 (rec:1.612, round:53.495)	b=4.25	count=18000
Total loss:	13.931 (rec:1.602, round:12.329)	b=3.69	count=18500
Total loss:	3.295 (rec:1.600, round:1.695)	b=3.12	count=19000
Total loss:	1.723 (rec:1.607, round:0.115)	b=2.56	count=19500
Total loss:	1.617 (rec:1.615, round:0.002)	b=2.00	count=20000
finished reconstructing layers.2.blocks.4.
reconstructing layers.2.blocks.5 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.5 ...
wraping quantizers in layers.2.blocks.5 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.773 (rec:1.773, round:0.000)	b=0.00	count=500
Total loss:	1.701 (rec:1.701, round:0.000)	b=0.00	count=1000
Total loss:	1.649 (rec:1.649, round:0.000)	b=0.00	count=1500
Total loss:	1.672 (rec:1.672, round:0.000)	b=0.00	count=2000
Total loss:	1.588 (rec:1.588, round:0.000)	b=0.00	count=2500
Total loss:	1.622 (rec:1.622, round:0.000)	b=0.00	count=3000
Total loss:	1.636 (rec:1.636, round:0.000)	b=0.00	count=3500
Total loss:	15814.107 (rec:1.551, round:15812.557)	b=20.00	count=4000
Total loss:	7756.685 (rec:1.643, round:7755.041)	b=19.44	count=4500
Total loss:	7150.057 (rec:1.603, round:7148.454)	b=18.88	count=5000
Total loss:	6733.306 (rec:1.589, round:6731.717)	b=18.31	count=5500
Total loss:	6365.009 (rec:1.564, round:6363.445)	b=17.75	count=6000
Total loss:	6023.284 (rec:1.598, round:6021.686)	b=17.19	count=6500
Total loss:	5696.926 (rec:1.584, round:5695.343)	b=16.62	count=7000
Total loss:	5380.513 (rec:1.596, round:5378.917)	b=16.06	count=7500
Total loss:	5075.444 (rec:1.597, round:5073.847)	b=15.50	count=8000
Total loss:	4776.958 (rec:1.599, round:4775.359)	b=14.94	count=8500
Total loss:	4483.430 (rec:1.540, round:4481.890)	b=14.38	count=9000
Total loss:	4197.608 (rec:1.573, round:4196.036)	b=13.81	count=9500
Total loss:	3915.321 (rec:1.552, round:3913.770)	b=13.25	count=10000
Total loss:	3633.867 (rec:1.586, round:3632.281)	b=12.69	count=10500
Total loss:	3356.338 (rec:1.604, round:3354.734)	b=12.12	count=11000
Total loss:	3082.938 (rec:1.564, round:3081.374)	b=11.56	count=11500
Total loss:	2813.923 (rec:1.550, round:2812.373)	b=11.00	count=12000
Total loss:	2549.356 (rec:1.559, round:2547.798)	b=10.44	count=12500
Total loss:	2288.659 (rec:1.560, round:2287.099)	b=9.88	count=13000
Total loss:	2031.560 (rec:1.578, round:2029.982)	b=9.31	count=13500
Total loss:	1772.245 (rec:1.559, round:1770.686)	b=8.75	count=14000
Total loss:	1519.732 (rec:1.577, round:1518.155)	b=8.19	count=14500
Total loss:	1272.221 (rec:1.584, round:1270.637)	b=7.62	count=15000
Total loss:	1031.461 (rec:1.576, round:1029.885)	b=7.06	count=15500
Total loss:	796.496 (rec:1.621, round:794.875)	b=6.50	count=16000
Total loss:	572.665 (rec:1.593, round:571.072)	b=5.94	count=16500
Total loss:	357.518 (rec:1.632, round:355.886)	b=5.38	count=17000
Total loss:	167.104 (rec:1.580, round:165.523)	b=4.81	count=17500
Total loss:	56.503 (rec:1.595, round:54.908)	b=4.25	count=18000
Total loss:	14.485 (rec:1.636, round:12.849)	b=3.69	count=18500
Total loss:	3.273 (rec:1.590, round:1.683)	b=3.12	count=19000
Total loss:	1.653 (rec:1.560, round:0.093)	b=2.56	count=19500
Total loss:	1.593 (rec:1.593, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.5.
reconstructing layers.2.blocks.6 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.6 ...
wraping quantizers in layers.2.blocks.6 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.405 (rec:1.405, round:0.000)	b=0.00	count=500
Total loss:	1.336 (rec:1.336, round:0.000)	b=0.00	count=1000
Total loss:	1.293 (rec:1.293, round:0.000)	b=0.00	count=1500
Total loss:	1.254 (rec:1.254, round:0.000)	b=0.00	count=2000
Total loss:	1.248 (rec:1.248, round:0.000)	b=0.00	count=2500
Total loss:	1.250 (rec:1.250, round:0.000)	b=0.00	count=3000
Total loss:	1.243 (rec:1.243, round:0.000)	b=0.00	count=3500
Total loss:	15785.577 (rec:1.213, round:15784.363)	b=20.00	count=4000
Total loss:	7549.435 (rec:1.191, round:7548.243)	b=19.44	count=4500
Total loss:	6940.701 (rec:1.217, round:6939.483)	b=18.88	count=5000
Total loss:	6508.153 (rec:1.203, round:6506.951)	b=18.31	count=5500
Total loss:	6129.491 (rec:1.194, round:6128.297)	b=17.75	count=6000
Total loss:	5772.401 (rec:1.228, round:5771.174)	b=17.19	count=6500
Total loss:	5430.608 (rec:1.201, round:5429.407)	b=16.62	count=7000
Total loss:	5102.634 (rec:1.224, round:5101.410)	b=16.06	count=7500
Total loss:	4785.353 (rec:1.167, round:4784.186)	b=15.50	count=8000
Total loss:	4476.760 (rec:1.211, round:4475.549)	b=14.94	count=8500
Total loss:	4176.422 (rec:1.166, round:4175.256)	b=14.38	count=9000
Total loss:	3888.252 (rec:1.197, round:3887.055)	b=13.81	count=9500
Total loss:	3607.821 (rec:1.202, round:3606.619)	b=13.25	count=10000
Total loss:	3333.508 (rec:1.217, round:3332.290)	b=12.69	count=10500
Total loss:	3062.157 (rec:1.188, round:3060.969)	b=12.12	count=11000
Total loss:	2802.579 (rec:1.215, round:2801.364)	b=11.56	count=11500
Total loss:	2547.722 (rec:1.231, round:2546.492)	b=11.00	count=12000
Total loss:	2298.647 (rec:1.216, round:2297.431)	b=10.44	count=12500
Total loss:	2049.655 (rec:1.226, round:2048.429)	b=9.88	count=13000
Total loss:	1805.681 (rec:1.242, round:1804.440)	b=9.31	count=13500
Total loss:	1564.776 (rec:1.184, round:1563.591)	b=8.75	count=14000
Total loss:	1328.461 (rec:1.206, round:1327.256)	b=8.19	count=14500
Total loss:	1102.925 (rec:1.237, round:1101.688)	b=7.62	count=15000
Total loss:	884.136 (rec:1.224, round:882.912)	b=7.06	count=15500
Total loss:	673.258 (rec:1.224, round:672.033)	b=6.50	count=16000
Total loss:	474.284 (rec:1.194, round:473.090)	b=5.94	count=16500
Total loss:	296.284 (rec:1.267, round:295.018)	b=5.38	count=17000
Total loss:	153.044 (rec:1.236, round:151.808)	b=4.81	count=17500
Total loss:	60.261 (rec:1.230, round:59.031)	b=4.25	count=18000
Total loss:	16.583 (rec:1.227, round:15.356)	b=3.69	count=18500
Total loss:	3.405 (rec:1.221, round:2.184)	b=3.12	count=19000
Total loss:	1.385 (rec:1.235, round:0.150)	b=2.56	count=19500
Total loss:	1.257 (rec:1.243, round:0.014)	b=2.00	count=20000
finished reconstructing layers.2.blocks.6.
reconstructing layers.2.blocks.7 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.7 ...
wraping quantizers in layers.2.blocks.7 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.697 (rec:1.697, round:0.000)	b=0.00	count=500
Total loss:	1.626 (rec:1.626, round:0.000)	b=0.00	count=1000
Total loss:	1.604 (rec:1.604, round:0.000)	b=0.00	count=1500
Total loss:	1.615 (rec:1.615, round:0.000)	b=0.00	count=2000
Total loss:	1.546 (rec:1.546, round:0.000)	b=0.00	count=2500
Total loss:	1.534 (rec:1.534, round:0.000)	b=0.00	count=3000
Total loss:	1.530 (rec:1.530, round:0.000)	b=0.00	count=3500
Total loss:	15620.483 (rec:1.534, round:15618.949)	b=20.00	count=4000
Total loss:	7592.204 (rec:1.517, round:7590.687)	b=19.44	count=4500
Total loss:	6990.038 (rec:1.497, round:6988.541)	b=18.88	count=5000
Total loss:	6575.078 (rec:1.486, round:6573.592)	b=18.31	count=5500
Total loss:	6216.437 (rec:1.473, round:6214.963)	b=17.75	count=6000
Total loss:	5881.113 (rec:1.507, round:5879.606)	b=17.19	count=6500
Total loss:	5558.474 (rec:1.451, round:5557.022)	b=16.62	count=7000
Total loss:	5249.983 (rec:1.530, round:5248.453)	b=16.06	count=7500
Total loss:	4951.469 (rec:1.486, round:4949.983)	b=15.50	count=8000
Total loss:	4656.120 (rec:1.495, round:4654.625)	b=14.94	count=8500
Total loss:	4366.029 (rec:1.445, round:4364.584)	b=14.38	count=9000
Total loss:	4086.398 (rec:1.501, round:4084.897)	b=13.81	count=9500
Total loss:	3808.529 (rec:1.455, round:3807.074)	b=13.25	count=10000
Total loss:	3537.537 (rec:1.479, round:3536.058)	b=12.69	count=10500
Total loss:	3271.913 (rec:1.477, round:3270.437)	b=12.12	count=11000
Total loss:	3008.160 (rec:1.475, round:3006.685)	b=11.56	count=11500
Total loss:	2744.322 (rec:1.485, round:2742.837)	b=11.00	count=12000
Total loss:	2483.548 (rec:1.473, round:2482.074)	b=10.44	count=12500
Total loss:	2228.359 (rec:1.468, round:2226.891)	b=9.88	count=13000
Total loss:	1973.527 (rec:1.500, round:1972.027)	b=9.31	count=13500
Total loss:	1721.764 (rec:1.522, round:1720.242)	b=8.75	count=14000
Total loss:	1476.239 (rec:1.494, round:1474.745)	b=8.19	count=14500
Total loss:	1234.934 (rec:1.448, round:1233.486)	b=7.62	count=15000
Total loss:	1000.252 (rec:1.506, round:998.746)	b=7.06	count=15500
Total loss:	772.410 (rec:1.449, round:770.962)	b=6.50	count=16000
Total loss:	554.221 (rec:1.462, round:552.759)	b=5.94	count=16500
Total loss:	356.961 (rec:1.520, round:355.441)	b=5.38	count=17000
Total loss:	193.965 (rec:1.495, round:192.470)	b=4.81	count=17500
Total loss:	83.997 (rec:1.518, round:82.479)	b=4.25	count=18000
Total loss:	23.816 (rec:1.506, round:22.310)	b=3.69	count=18500
Total loss:	4.374 (rec:1.514, round:2.859)	b=3.12	count=19000
Total loss:	1.635 (rec:1.493, round:0.141)	b=2.56	count=19500
Total loss:	1.521 (rec:1.520, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.7.
reconstructing layers.2.blocks.8 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.8 ...
wraping quantizers in layers.2.blocks.8 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.507 (rec:1.507, round:0.000)	b=0.00	count=500
Total loss:	1.397 (rec:1.397, round:0.000)	b=0.00	count=1000
Total loss:	1.280 (rec:1.280, round:0.000)	b=0.00	count=1500
Total loss:	1.166 (rec:1.166, round:0.000)	b=0.00	count=2000
Total loss:	1.140 (rec:1.140, round:0.000)	b=0.00	count=2500
Total loss:	1.228 (rec:1.228, round:0.000)	b=0.00	count=3000
Total loss:	1.144 (rec:1.144, round:0.000)	b=0.00	count=3500
Total loss:	15674.513 (rec:1.089, round:15673.424)	b=20.00	count=4000
Total loss:	7542.081 (rec:1.090, round:7540.991)	b=19.44	count=4500
Total loss:	6927.549 (rec:1.102, round:6926.447)	b=18.88	count=5000
Total loss:	6500.895 (rec:1.061, round:6499.833)	b=18.31	count=5500
Total loss:	6124.413 (rec:1.041, round:6123.372)	b=17.75	count=6000
Total loss:	5776.272 (rec:1.043, round:5775.229)	b=17.19	count=6500
Total loss:	5440.624 (rec:1.087, round:5439.537)	b=16.62	count=7000
Total loss:	5120.121 (rec:1.032, round:5119.089)	b=16.06	count=7500
Total loss:	4816.187 (rec:1.053, round:4815.134)	b=15.50	count=8000
Total loss:	4522.848 (rec:1.079, round:4521.770)	b=14.94	count=8500
Total loss:	4238.612 (rec:1.042, round:4237.570)	b=14.38	count=9000
Total loss:	3961.466 (rec:1.071, round:3960.395)	b=13.81	count=9500
Total loss:	3695.299 (rec:0.992, round:3694.307)	b=13.25	count=10000
Total loss:	3432.979 (rec:1.029, round:3431.950)	b=12.69	count=10500
Total loss:	3179.334 (rec:1.006, round:3178.328)	b=12.12	count=11000
Total loss:	2930.192 (rec:1.052, round:2929.140)	b=11.56	count=11500
Total loss:	2686.110 (rec:1.068, round:2685.042)	b=11.00	count=12000
Total loss:	2444.978 (rec:1.057, round:2443.921)	b=10.44	count=12500
Total loss:	2207.138 (rec:1.000, round:2206.138)	b=9.88	count=13000
Total loss:	1971.464 (rec:1.014, round:1970.450)	b=9.31	count=13500
Total loss:	1736.863 (rec:1.085, round:1735.778)	b=8.75	count=14000
Total loss:	1505.871 (rec:1.043, round:1504.828)	b=8.19	count=14500
Total loss:	1277.919 (rec:1.051, round:1276.868)	b=7.62	count=15000
Total loss:	1055.646 (rec:1.077, round:1054.569)	b=7.06	count=15500
Total loss:	839.974 (rec:0.983, round:838.991)	b=6.50	count=16000
Total loss:	640.373 (rec:0.993, round:639.380)	b=5.94	count=16500
Total loss:	455.236 (rec:1.011, round:454.225)	b=5.38	count=17000
Total loss:	301.087 (rec:1.008, round:300.079)	b=4.81	count=17500
Total loss:	177.705 (rec:1.004, round:176.700)	b=4.25	count=18000
Total loss:	85.870 (rec:1.055, round:84.815)	b=3.69	count=18500
Total loss:	25.615 (rec:0.998, round:24.617)	b=3.12	count=19000
Total loss:	3.277 (rec:0.996, round:2.281)	b=2.56	count=19500
Total loss:	1.008 (rec:0.966, round:0.042)	b=2.00	count=20000
finished reconstructing layers.2.blocks.8.
reconstructing layers.2.blocks.9 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.9 ...
wraping quantizers in layers.2.blocks.9 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.889 (rec:1.889, round:0.000)	b=0.00	count=500
Total loss:	1.764 (rec:1.764, round:0.000)	b=0.00	count=1000
Total loss:	1.612 (rec:1.612, round:0.000)	b=0.00	count=1500
Total loss:	1.304 (rec:1.304, round:0.000)	b=0.00	count=2000
Total loss:	1.399 (rec:1.399, round:0.000)	b=0.00	count=2500
Total loss:	1.432 (rec:1.432, round:0.000)	b=0.00	count=3000
Total loss:	1.330 (rec:1.330, round:0.000)	b=0.00	count=3500
Total loss:	15414.774 (rec:1.235, round:15413.539)	b=20.00	count=4000
Total loss:	7360.911 (rec:1.400, round:7359.511)	b=19.44	count=4500
Total loss:	6756.362 (rec:1.354, round:6755.008)	b=18.88	count=5000
Total loss:	6332.796 (rec:1.257, round:6331.539)	b=18.31	count=5500
Total loss:	5958.065 (rec:1.204, round:5956.861)	b=17.75	count=6000
Total loss:	5605.970 (rec:1.256, round:5604.713)	b=17.19	count=6500
Total loss:	5273.630 (rec:1.344, round:5272.286)	b=16.62	count=7000
Total loss:	4951.275 (rec:1.253, round:4950.021)	b=16.06	count=7500
Total loss:	4642.659 (rec:1.223, round:4641.436)	b=15.50	count=8000
Total loss:	4347.609 (rec:1.300, round:4346.309)	b=14.94	count=8500
Total loss:	4059.415 (rec:1.191, round:4058.225)	b=14.38	count=9000
Total loss:	3782.260 (rec:1.232, round:3781.028)	b=13.81	count=9500
Total loss:	3510.899 (rec:1.189, round:3509.710)	b=13.25	count=10000
Total loss:	3251.817 (rec:1.169, round:3250.649)	b=12.69	count=10500
Total loss:	3001.247 (rec:1.226, round:3000.021)	b=12.12	count=11000
Total loss:	2756.370 (rec:1.134, round:2755.236)	b=11.56	count=11500
Total loss:	2517.629 (rec:1.101, round:2516.528)	b=11.00	count=12000
Total loss:	2282.798 (rec:1.257, round:2281.541)	b=10.44	count=12500
Total loss:	2051.594 (rec:1.225, round:2050.368)	b=9.88	count=13000
Total loss:	1825.787 (rec:1.111, round:1824.676)	b=9.31	count=13500
Total loss:	1603.802 (rec:1.092, round:1602.710)	b=8.75	count=14000
Total loss:	1386.378 (rec:1.225, round:1385.153)	b=8.19	count=14500
Total loss:	1174.173 (rec:1.125, round:1173.048)	b=7.62	count=15000
Total loss:	968.546 (rec:1.162, round:967.384)	b=7.06	count=15500
Total loss:	770.284 (rec:1.108, round:769.175)	b=6.50	count=16000
Total loss:	583.510 (rec:1.195, round:582.315)	b=5.94	count=16500
Total loss:	418.162 (rec:1.110, round:417.052)	b=5.38	count=17000
Total loss:	279.405 (rec:1.118, round:278.287)	b=4.81	count=17500
Total loss:	168.152 (rec:1.140, round:167.013)	b=4.25	count=18000
Total loss:	85.383 (rec:1.072, round:84.311)	b=3.69	count=18500
Total loss:	27.600 (rec:1.168, round:26.433)	b=3.12	count=19000
Total loss:	3.597 (rec:1.156, round:2.441)	b=2.56	count=19500
Total loss:	1.202 (rec:1.136, round:0.066)	b=2.00	count=20000
finished reconstructing layers.2.blocks.9.
reconstructing layers.2.blocks.10 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.10 ...
wraping quantizers in layers.2.blocks.10 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.752 (rec:1.752, round:0.000)	b=0.00	count=500
Total loss:	1.244 (rec:1.244, round:0.000)	b=0.00	count=1000
Total loss:	1.127 (rec:1.127, round:0.000)	b=0.00	count=1500
Total loss:	1.101 (rec:1.101, round:0.000)	b=0.00	count=2000
Total loss:	1.114 (rec:1.114, round:0.000)	b=0.00	count=2500
Total loss:	1.085 (rec:1.085, round:0.000)	b=0.00	count=3000
Total loss:	1.071 (rec:1.071, round:0.000)	b=0.00	count=3500
Total loss:	15170.429 (rec:1.033, round:15169.396)	b=20.00	count=4000
Total loss:	7018.846 (rec:1.135, round:7017.711)	b=19.44	count=4500
Total loss:	6423.579 (rec:1.073, round:6422.506)	b=18.88	count=5000
Total loss:	6008.194 (rec:0.976, round:6007.218)	b=18.31	count=5500
Total loss:	5637.332 (rec:1.087, round:5636.245)	b=17.75	count=6000
Total loss:	5289.826 (rec:1.014, round:5288.812)	b=17.19	count=6500
Total loss:	4961.222 (rec:1.057, round:4960.166)	b=16.62	count=7000
Total loss:	4648.579 (rec:0.982, round:4647.598)	b=16.06	count=7500
Total loss:	4347.403 (rec:0.965, round:4346.438)	b=15.50	count=8000
Total loss:	4059.298 (rec:1.009, round:4058.289)	b=14.94	count=8500
Total loss:	3786.896 (rec:0.921, round:3785.975)	b=14.38	count=9000
Total loss:	3520.357 (rec:1.045, round:3519.312)	b=13.81	count=9500
Total loss:	3260.690 (rec:0.956, round:3259.734)	b=13.25	count=10000
Total loss:	3011.067 (rec:0.988, round:3010.079)	b=12.69	count=10500
Total loss:	2769.270 (rec:0.912, round:2768.358)	b=12.12	count=11000
Total loss:	2536.037 (rec:0.951, round:2535.086)	b=11.56	count=11500
Total loss:	2311.483 (rec:0.944, round:2310.539)	b=11.00	count=12000
Total loss:	2090.358 (rec:0.979, round:2089.379)	b=10.44	count=12500
Total loss:	1870.925 (rec:1.065, round:1869.860)	b=9.88	count=13000
Total loss:	1658.008 (rec:0.980, round:1657.028)	b=9.31	count=13500
Total loss:	1449.679 (rec:0.929, round:1448.750)	b=8.75	count=14000
Total loss:	1244.350 (rec:0.962, round:1243.389)	b=8.19	count=14500
Total loss:	1045.632 (rec:0.965, round:1044.667)	b=7.62	count=15000
Total loss:	850.278 (rec:0.923, round:849.355)	b=7.06	count=15500
Total loss:	668.053 (rec:0.978, round:667.075)	b=6.50	count=16000
Total loss:	499.581 (rec:0.974, round:498.608)	b=5.94	count=16500
Total loss:	354.691 (rec:0.963, round:353.727)	b=5.38	count=17000
Total loss:	233.289 (rec:0.920, round:232.369)	b=4.81	count=17500
Total loss:	137.289 (rec:0.916, round:136.373)	b=4.25	count=18000
Total loss:	65.552 (rec:0.937, round:64.615)	b=3.69	count=18500
Total loss:	20.901 (rec:0.988, round:19.913)	b=3.12	count=19000
Total loss:	3.729 (rec:0.949, round:2.780)	b=2.56	count=19500
Total loss:	1.047 (rec:0.894, round:0.154)	b=2.00	count=20000
finished reconstructing layers.2.blocks.10.
reconstructing layers.2.blocks.11 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.11 ...
wraping quantizers in layers.2.blocks.11 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.781 (rec:1.781, round:0.000)	b=0.00	count=500
Total loss:	1.129 (rec:1.129, round:0.000)	b=0.00	count=1000
Total loss:	0.911 (rec:0.911, round:0.000)	b=0.00	count=1500
Total loss:	0.813 (rec:0.813, round:0.000)	b=0.00	count=2000
Total loss:	0.763 (rec:0.763, round:0.000)	b=0.00	count=2500
Total loss:	0.778 (rec:0.778, round:0.000)	b=0.00	count=3000
Total loss:	0.747 (rec:0.747, round:0.000)	b=0.00	count=3500
Total loss:	14608.760 (rec:0.686, round:14608.074)	b=20.00	count=4000
Total loss:	6386.594 (rec:0.725, round:6385.869)	b=19.44	count=4500
Total loss:	5804.310 (rec:0.691, round:5803.619)	b=18.88	count=5000
Total loss:	5390.801 (rec:0.692, round:5390.109)	b=18.31	count=5500
Total loss:	5020.950 (rec:0.705, round:5020.245)	b=17.75	count=6000
Total loss:	4678.188 (rec:0.672, round:4677.517)	b=17.19	count=6500
Total loss:	4353.132 (rec:0.696, round:4352.437)	b=16.62	count=7000
Total loss:	4049.914 (rec:0.688, round:4049.226)	b=16.06	count=7500
Total loss:	3759.246 (rec:0.693, round:3758.553)	b=15.50	count=8000
Total loss:	3483.205 (rec:0.650, round:3482.555)	b=14.94	count=8500
Total loss:	3218.989 (rec:0.740, round:3218.249)	b=14.38	count=9000
Total loss:	2969.059 (rec:0.645, round:2968.413)	b=13.81	count=9500
Total loss:	2731.592 (rec:0.699, round:2730.893)	b=13.25	count=10000
Total loss:	2505.034 (rec:0.713, round:2504.321)	b=12.69	count=10500
Total loss:	2283.951 (rec:0.637, round:2283.314)	b=12.12	count=11000
Total loss:	2076.287 (rec:0.658, round:2075.629)	b=11.56	count=11500
Total loss:	1871.341 (rec:0.666, round:1870.676)	b=11.00	count=12000
Total loss:	1671.991 (rec:0.700, round:1671.291)	b=10.44	count=12500
Total loss:	1484.253 (rec:0.690, round:1483.563)	b=9.88	count=13000
Total loss:	1298.904 (rec:0.668, round:1298.237)	b=9.31	count=13500
Total loss:	1119.659 (rec:0.616, round:1119.043)	b=8.75	count=14000
Total loss:	948.417 (rec:0.710, round:947.707)	b=8.19	count=14500
Total loss:	784.019 (rec:0.646, round:783.373)	b=7.62	count=15000
Total loss:	621.282 (rec:0.649, round:620.632)	b=7.06	count=15500
Total loss:	468.597 (rec:0.640, round:467.958)	b=6.50	count=16000
Total loss:	330.572 (rec:0.635, round:329.938)	b=5.94	count=16500
Total loss:	214.659 (rec:0.662, round:213.998)	b=5.38	count=17000
Total loss:	129.155 (rec:0.681, round:128.474)	b=4.81	count=17500
Total loss:	67.253 (rec:0.645, round:66.608)	b=4.25	count=18000
Total loss:	27.949 (rec:0.645, round:27.304)	b=3.69	count=18500
Total loss:	7.098 (rec:0.643, round:6.455)	b=3.12	count=19000
Total loss:	1.269 (rec:0.671, round:0.598)	b=2.56	count=19500
Total loss:	0.668 (rec:0.653, round:0.016)	b=2.00	count=20000
finished reconstructing layers.2.blocks.11.
reconstructing layers.2.blocks.12 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.12 ...
wraping quantizers in layers.2.blocks.12 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.728 (rec:1.728, round:0.000)	b=0.00	count=500
Total loss:	1.366 (rec:1.366, round:0.000)	b=0.00	count=1000
Total loss:	0.977 (rec:0.977, round:0.000)	b=0.00	count=1500
Total loss:	0.831 (rec:0.831, round:0.000)	b=0.00	count=2000
Total loss:	0.789 (rec:0.789, round:0.000)	b=0.00	count=2500
Total loss:	0.724 (rec:0.724, round:0.000)	b=0.00	count=3000
Total loss:	0.680 (rec:0.680, round:0.000)	b=0.00	count=3500
Total loss:	14285.113 (rec:0.692, round:14284.422)	b=20.00	count=4000
Total loss:	6194.705 (rec:0.652, round:6194.053)	b=19.44	count=4500
Total loss:	5640.495 (rec:0.625, round:5639.871)	b=18.88	count=5000
Total loss:	5243.440 (rec:0.596, round:5242.844)	b=18.31	count=5500
Total loss:	4888.830 (rec:0.633, round:4888.197)	b=17.75	count=6000
Total loss:	4553.380 (rec:0.629, round:4552.751)	b=17.19	count=6500
Total loss:	4235.268 (rec:0.594, round:4234.674)	b=16.62	count=7000
Total loss:	3934.052 (rec:0.611, round:3933.441)	b=16.06	count=7500
Total loss:	3649.823 (rec:0.601, round:3649.222)	b=15.50	count=8000
Total loss:	3377.053 (rec:0.602, round:3376.450)	b=14.94	count=8500
Total loss:	3118.530 (rec:0.604, round:3117.926)	b=14.38	count=9000
Total loss:	2873.529 (rec:0.585, round:2872.943)	b=13.81	count=9500
Total loss:	2641.409 (rec:0.605, round:2640.804)	b=13.25	count=10000
Total loss:	2421.591 (rec:0.599, round:2420.992)	b=12.69	count=10500
Total loss:	2205.428 (rec:0.585, round:2204.844)	b=12.12	count=11000
Total loss:	2001.113 (rec:0.603, round:2000.510)	b=11.56	count=11500
Total loss:	1804.475 (rec:0.546, round:1803.928)	b=11.00	count=12000
Total loss:	1614.359 (rec:0.592, round:1613.767)	b=10.44	count=12500
Total loss:	1430.063 (rec:0.557, round:1429.506)	b=9.88	count=13000
Total loss:	1254.582 (rec:0.568, round:1254.013)	b=9.31	count=13500
Total loss:	1085.040 (rec:0.588, round:1084.452)	b=8.75	count=14000
Total loss:	920.809 (rec:0.549, round:920.260)	b=8.19	count=14500
Total loss:	760.897 (rec:0.571, round:760.327)	b=7.62	count=15000
Total loss:	607.050 (rec:0.584, round:606.466)	b=7.06	count=15500
Total loss:	463.834 (rec:0.583, round:463.251)	b=6.50	count=16000
Total loss:	330.150 (rec:0.541, round:329.609)	b=5.94	count=16500
Total loss:	216.303 (rec:0.564, round:215.739)	b=5.38	count=17000
Total loss:	129.854 (rec:0.603, round:129.251)	b=4.81	count=17500
Total loss:	69.142 (rec:0.572, round:68.571)	b=4.25	count=18000
Total loss:	28.618 (rec:0.578, round:28.041)	b=3.69	count=18500
Total loss:	7.792 (rec:0.576, round:7.215)	b=3.12	count=19000
Total loss:	1.508 (rec:0.600, round:0.908)	b=2.56	count=19500
Total loss:	0.622 (rec:0.582, round:0.040)	b=2.00	count=20000
finished reconstructing layers.2.blocks.12.
reconstructing layers.2.blocks.13 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.13 ...
wraping quantizers in layers.2.blocks.13 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.821 (rec:1.821, round:0.000)	b=0.00	count=500
Total loss:	1.467 (rec:1.467, round:0.000)	b=0.00	count=1000
Total loss:	1.282 (rec:1.282, round:0.000)	b=0.00	count=1500
Total loss:	1.118 (rec:1.118, round:0.000)	b=0.00	count=2000
Total loss:	1.001 (rec:1.001, round:0.000)	b=0.00	count=2500
Total loss:	0.952 (rec:0.952, round:0.000)	b=0.00	count=3000
Total loss:	0.876 (rec:0.876, round:0.000)	b=0.00	count=3500
Total loss:	13849.374 (rec:0.883, round:13848.490)	b=20.00	count=4000
Total loss:	5759.017 (rec:0.771, round:5758.246)	b=19.44	count=4500
Total loss:	5201.824 (rec:0.796, round:5201.028)	b=18.88	count=5000
Total loss:	4791.316 (rec:0.789, round:4790.528)	b=18.31	count=5500
Total loss:	4426.324 (rec:0.793, round:4425.532)	b=17.75	count=6000
Total loss:	4088.711 (rec:0.784, round:4087.927)	b=17.19	count=6500
Total loss:	3775.068 (rec:0.823, round:3774.244)	b=16.62	count=7000
Total loss:	3485.596 (rec:0.824, round:3484.772)	b=16.06	count=7500
Total loss:	3211.943 (rec:0.798, round:3211.146)	b=15.50	count=8000
Total loss:	2960.948 (rec:0.750, round:2960.197)	b=14.94	count=8500
Total loss:	2723.034 (rec:0.821, round:2722.214)	b=14.38	count=9000
Total loss:	2497.310 (rec:0.809, round:2496.501)	b=13.81	count=9500
Total loss:	2283.859 (rec:0.754, round:2283.105)	b=13.25	count=10000
Total loss:	2082.079 (rec:0.831, round:2081.248)	b=12.69	count=10500
Total loss:	1890.463 (rec:0.781, round:1889.682)	b=12.12	count=11000
Total loss:	1708.844 (rec:0.807, round:1708.037)	b=11.56	count=11500
Total loss:	1534.763 (rec:0.796, round:1533.968)	b=11.00	count=12000
Total loss:	1366.702 (rec:0.775, round:1365.927)	b=10.44	count=12500
Total loss:	1204.294 (rec:0.761, round:1203.533)	b=9.88	count=13000
Total loss:	1048.101 (rec:0.789, round:1047.312)	b=9.31	count=13500
Total loss:	897.443 (rec:0.741, round:896.703)	b=8.75	count=14000
Total loss:	752.076 (rec:0.823, round:751.253)	b=8.19	count=14500
Total loss:	612.187 (rec:0.721, round:611.467)	b=7.62	count=15000
Total loss:	479.014 (rec:0.771, round:478.243)	b=7.06	count=15500
Total loss:	352.747 (rec:0.754, round:351.994)	b=6.50	count=16000
Total loss:	238.277 (rec:0.751, round:237.527)	b=5.94	count=16500
Total loss:	144.628 (rec:0.809, round:143.819)	b=5.38	count=17000
Total loss:	78.781 (rec:0.768, round:78.013)	b=4.81	count=17500
Total loss:	38.384 (rec:0.790, round:37.594)	b=4.25	count=18000
Total loss:	15.075 (rec:0.759, round:14.317)	b=3.69	count=18500
Total loss:	4.033 (rec:0.850, round:3.183)	b=3.12	count=19000
Total loss:	1.036 (rec:0.781, round:0.256)	b=2.56	count=19500
Total loss:	0.767 (rec:0.762, round:0.005)	b=2.00	count=20000
finished reconstructing layers.2.blocks.13.
reconstructing layers.2.blocks.14 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.14 ...
wraping quantizers in layers.2.blocks.14 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.658 (rec:0.658, round:0.000)	b=0.00	count=500
Total loss:	0.487 (rec:0.487, round:0.000)	b=0.00	count=1000
Total loss:	0.479 (rec:0.479, round:0.000)	b=0.00	count=1500
Total loss:	0.391 (rec:0.391, round:0.000)	b=0.00	count=2000
Total loss:	0.351 (rec:0.351, round:0.000)	b=0.00	count=2500
Total loss:	0.347 (rec:0.347, round:0.000)	b=0.00	count=3000
Total loss:	0.344 (rec:0.344, round:0.000)	b=0.00	count=3500
Total loss:	16130.306 (rec:0.313, round:16129.993)	b=20.00	count=4000
Total loss:	7590.515 (rec:0.321, round:7590.194)	b=19.44	count=4500
Total loss:	6979.761 (rec:0.319, round:6979.442)	b=18.88	count=5000
Total loss:	6552.482 (rec:0.311, round:6552.172)	b=18.31	count=5500
Total loss:	6176.658 (rec:0.286, round:6176.373)	b=17.75	count=6000
Total loss:	5813.259 (rec:0.287, round:5812.972)	b=17.19	count=6500
Total loss:	5456.472 (rec:0.294, round:5456.178)	b=16.62	count=7000
Total loss:	5103.426 (rec:0.282, round:5103.144)	b=16.06	count=7500
Total loss:	4753.046 (rec:0.293, round:4752.753)	b=15.50	count=8000
Total loss:	4410.370 (rec:0.285, round:4410.085)	b=14.94	count=8500
Total loss:	4073.087 (rec:0.276, round:4072.811)	b=14.38	count=9000
Total loss:	3742.645 (rec:0.281, round:3742.364)	b=13.81	count=9500
Total loss:	3419.228 (rec:0.274, round:3418.954)	b=13.25	count=10000
Total loss:	3107.673 (rec:0.295, round:3107.378)	b=12.69	count=10500
Total loss:	2807.589 (rec:0.275, round:2807.314)	b=12.12	count=11000
Total loss:	2522.976 (rec:0.283, round:2522.693)	b=11.56	count=11500
Total loss:	2250.532 (rec:0.279, round:2250.253)	b=11.00	count=12000
Total loss:	1988.680 (rec:0.301, round:1988.379)	b=10.44	count=12500
Total loss:	1737.991 (rec:0.286, round:1737.706)	b=9.88	count=13000
Total loss:	1500.905 (rec:0.277, round:1500.628)	b=9.31	count=13500
Total loss:	1277.884 (rec:0.278, round:1277.606)	b=8.75	count=14000
Total loss:	1067.614 (rec:0.273, round:1067.341)	b=8.19	count=14500
Total loss:	869.700 (rec:0.289, round:869.410)	b=7.62	count=15000
Total loss:	684.600 (rec:0.275, round:684.325)	b=7.06	count=15500
Total loss:	515.131 (rec:0.273, round:514.859)	b=6.50	count=16000
Total loss:	365.755 (rec:0.278, round:365.478)	b=5.94	count=16500
Total loss:	235.951 (rec:0.279, round:235.671)	b=5.38	count=17000
Total loss:	128.325 (rec:0.283, round:128.042)	b=4.81	count=17500
Total loss:	50.228 (rec:0.280, round:49.948)	b=4.25	count=18000
Total loss:	11.354 (rec:0.284, round:11.070)	b=3.69	count=18500
Total loss:	1.799 (rec:0.295, round:1.504)	b=3.12	count=19000
Total loss:	0.388 (rec:0.280, round:0.108)	b=2.56	count=19500
Total loss:	0.268 (rec:0.268, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.14.
reconstructing layers.2.blocks.15 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.15 ...
wraping quantizers in layers.2.blocks.15 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.464 (rec:0.464, round:0.000)	b=0.00	count=500
Total loss:	0.355 (rec:0.355, round:0.000)	b=0.00	count=1000
Total loss:	0.337 (rec:0.337, round:0.000)	b=0.00	count=1500
Total loss:	0.323 (rec:0.323, round:0.000)	b=0.00	count=2000
Total loss:	0.291 (rec:0.291, round:0.000)	b=0.00	count=2500
Total loss:	0.286 (rec:0.286, round:0.000)	b=0.00	count=3000
Total loss:	0.262 (rec:0.262, round:0.000)	b=0.00	count=3500
Total loss:	16279.054 (rec:0.256, round:16278.798)	b=20.00	count=4000
Total loss:	7697.375 (rec:0.245, round:7697.130)	b=19.44	count=4500
Total loss:	7081.530 (rec:0.243, round:7081.287)	b=18.88	count=5000
Total loss:	6660.317 (rec:0.249, round:6660.068)	b=18.31	count=5500
Total loss:	6283.208 (rec:0.237, round:6282.971)	b=17.75	count=6000
Total loss:	5918.405 (rec:0.235, round:5918.170)	b=17.19	count=6500
Total loss:	5558.299 (rec:0.236, round:5558.063)	b=16.62	count=7000
Total loss:	5200.873 (rec:0.227, round:5200.646)	b=16.06	count=7500
Total loss:	4850.855 (rec:0.228, round:4850.627)	b=15.50	count=8000
Total loss:	4504.086 (rec:0.223, round:4503.863)	b=14.94	count=8500
Total loss:	4164.848 (rec:0.234, round:4164.614)	b=14.38	count=9000
Total loss:	3830.448 (rec:0.224, round:3830.225)	b=13.81	count=9500
Total loss:	3505.285 (rec:0.224, round:3505.061)	b=13.25	count=10000
Total loss:	3189.478 (rec:0.224, round:3189.254)	b=12.69	count=10500
Total loss:	2885.652 (rec:0.226, round:2885.426)	b=12.12	count=11000
Total loss:	2592.865 (rec:0.224, round:2592.641)	b=11.56	count=11500
Total loss:	2314.225 (rec:0.209, round:2314.016)	b=11.00	count=12000
Total loss:	2044.492 (rec:0.225, round:2044.266)	b=10.44	count=12500
Total loss:	1787.730 (rec:0.217, round:1787.514)	b=9.88	count=13000
Total loss:	1544.481 (rec:0.216, round:1544.265)	b=9.31	count=13500
Total loss:	1314.516 (rec:0.222, round:1314.294)	b=8.75	count=14000
Total loss:	1095.596 (rec:0.220, round:1095.375)	b=8.19	count=14500
Total loss:	891.155 (rec:0.226, round:890.929)	b=7.62	count=15000
Total loss:	703.612 (rec:0.220, round:703.392)	b=7.06	count=15500
Total loss:	530.550 (rec:0.221, round:530.329)	b=6.50	count=16000
Total loss:	376.585 (rec:0.213, round:376.373)	b=5.94	count=16500
Total loss:	244.699 (rec:0.224, round:244.475)	b=5.38	count=17000
Total loss:	137.438 (rec:0.217, round:137.221)	b=4.81	count=17500
Total loss:	55.670 (rec:0.216, round:55.454)	b=4.25	count=18000
Total loss:	12.400 (rec:0.223, round:12.177)	b=3.69	count=18500
Total loss:	1.601 (rec:0.228, round:1.373)	b=3.12	count=19000
Total loss:	0.292 (rec:0.219, round:0.072)	b=2.56	count=19500
Total loss:	0.229 (rec:0.228, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.15.
reconstructing layers.2.blocks.16 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.16 ...
wraping quantizers in layers.2.blocks.16 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.560 (rec:0.560, round:0.000)	b=0.00	count=500
Total loss:	0.471 (rec:0.471, round:0.000)	b=0.00	count=1000
Total loss:	0.424 (rec:0.424, round:0.000)	b=0.00	count=1500
Total loss:	0.405 (rec:0.405, round:0.000)	b=0.00	count=2000
Total loss:	0.392 (rec:0.392, round:0.000)	b=0.00	count=2500
Total loss:	0.374 (rec:0.374, round:0.000)	b=0.00	count=3000
Total loss:	0.363 (rec:0.363, round:0.000)	b=0.00	count=3500
Total loss:	16318.612 (rec:0.364, round:16318.248)	b=20.00	count=4000
Total loss:	7781.827 (rec:0.337, round:7781.490)	b=19.44	count=4500
Total loss:	7177.729 (rec:0.339, round:7177.390)	b=18.88	count=5000
Total loss:	6759.568 (rec:0.336, round:6759.232)	b=18.31	count=5500
Total loss:	6385.286 (rec:0.347, round:6384.939)	b=17.75	count=6000
Total loss:	6022.676 (rec:0.345, round:6022.331)	b=17.19	count=6500
Total loss:	5672.388 (rec:0.331, round:5672.056)	b=16.62	count=7000
Total loss:	5322.390 (rec:0.342, round:5322.048)	b=16.06	count=7500
Total loss:	4974.068 (rec:0.327, round:4973.741)	b=15.50	count=8000
Total loss:	4627.563 (rec:0.324, round:4627.239)	b=14.94	count=8500
Total loss:	4285.535 (rec:0.322, round:4285.213)	b=14.38	count=9000
Total loss:	3954.557 (rec:0.325, round:3954.232)	b=13.81	count=9500
Total loss:	3630.742 (rec:0.325, round:3630.417)	b=13.25	count=10000
Total loss:	3315.292 (rec:0.320, round:3314.972)	b=12.69	count=10500
Total loss:	3010.277 (rec:0.331, round:3009.946)	b=12.12	count=11000
Total loss:	2713.440 (rec:0.311, round:2713.129)	b=11.56	count=11500
Total loss:	2428.737 (rec:0.317, round:2428.420)	b=11.00	count=12000
Total loss:	2155.855 (rec:0.324, round:2155.532)	b=10.44	count=12500
Total loss:	1898.754 (rec:0.328, round:1898.427)	b=9.88	count=13000
Total loss:	1650.708 (rec:0.309, round:1650.399)	b=9.31	count=13500
Total loss:	1411.598 (rec:0.320, round:1411.277)	b=8.75	count=14000
Total loss:	1184.478 (rec:0.321, round:1184.156)	b=8.19	count=14500
Total loss:	971.508 (rec:0.319, round:971.188)	b=7.62	count=15000
Total loss:	772.147 (rec:0.321, round:771.826)	b=7.06	count=15500
Total loss:	587.934 (rec:0.312, round:587.622)	b=6.50	count=16000
Total loss:	420.872 (rec:0.319, round:420.553)	b=5.94	count=16500
Total loss:	271.725 (rec:0.306, round:271.418)	b=5.38	count=17000
Total loss:	152.414 (rec:0.324, round:152.090)	b=4.81	count=17500
Total loss:	61.810 (rec:0.322, round:61.489)	b=4.25	count=18000
Total loss:	14.992 (rec:0.324, round:14.667)	b=3.69	count=18500
Total loss:	2.090 (rec:0.321, round:1.769)	b=3.12	count=19000
Total loss:	0.433 (rec:0.320, round:0.112)	b=2.56	count=19500
Total loss:	0.324 (rec:0.322, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.16.
reconstructing layers.2.blocks.17 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.17 ...
wraping quantizers in layers.2.blocks.17 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.431 (rec:1.431, round:0.000)	b=0.00	count=500
Total loss:	1.224 (rec:1.224, round:0.000)	b=0.00	count=1000
Total loss:	1.160 (rec:1.160, round:0.000)	b=0.00	count=1500
Total loss:	1.112 (rec:1.112, round:0.000)	b=0.00	count=2000
Total loss:	1.127 (rec:1.127, round:0.000)	b=0.00	count=2500
Total loss:	1.093 (rec:1.093, round:0.000)	b=0.00	count=3000
Total loss:	1.063 (rec:1.063, round:0.000)	b=0.00	count=3500
Total loss:	16027.385 (rec:1.024, round:16026.361)	b=20.00	count=4000
Total loss:	7749.879 (rec:1.022, round:7748.857)	b=19.44	count=4500
Total loss:	7143.183 (rec:1.000, round:7142.184)	b=18.88	count=5000
Total loss:	6722.979 (rec:1.030, round:6721.949)	b=18.31	count=5500
Total loss:	6348.259 (rec:0.990, round:6347.269)	b=17.75	count=6000
Total loss:	5998.250 (rec:0.976, round:5997.273)	b=17.19	count=6500
Total loss:	5658.685 (rec:0.989, round:5657.697)	b=16.62	count=7000
Total loss:	5327.893 (rec:0.947, round:5326.946)	b=16.06	count=7500
Total loss:	5006.840 (rec:0.954, round:5005.886)	b=15.50	count=8000
Total loss:	4692.487 (rec:0.964, round:4691.522)	b=14.94	count=8500
Total loss:	4386.079 (rec:0.975, round:4385.104)	b=14.38	count=9000
Total loss:	4087.254 (rec:0.967, round:4086.287)	b=13.81	count=9500
Total loss:	3792.815 (rec:0.966, round:3791.849)	b=13.25	count=10000
Total loss:	3506.958 (rec:0.943, round:3506.014)	b=12.69	count=10500
Total loss:	3228.213 (rec:0.950, round:3227.264)	b=12.12	count=11000
Total loss:	2954.621 (rec:0.968, round:2953.653)	b=11.56	count=11500
Total loss:	2687.134 (rec:0.939, round:2686.195)	b=11.00	count=12000
Total loss:	2426.449 (rec:0.940, round:2425.509)	b=10.44	count=12500
Total loss:	2171.121 (rec:0.949, round:2170.172)	b=9.88	count=13000
Total loss:	1924.308 (rec:0.932, round:1923.376)	b=9.31	count=13500
Total loss:	1683.004 (rec:0.947, round:1682.057)	b=8.75	count=14000
Total loss:	1446.243 (rec:0.971, round:1445.272)	b=8.19	count=14500
Total loss:	1217.572 (rec:0.951, round:1216.621)	b=7.62	count=15000
Total loss:	997.134 (rec:0.941, round:996.193)	b=7.06	count=15500
Total loss:	787.030 (rec:0.910, round:786.121)	b=6.50	count=16000
Total loss:	591.016 (rec:0.944, round:590.072)	b=5.94	count=16500
Total loss:	411.160 (rec:0.960, round:410.200)	b=5.38	count=17000
Total loss:	254.634 (rec:0.976, round:253.658)	b=4.81	count=17500
Total loss:	130.257 (rec:0.925, round:129.331)	b=4.25	count=18000
Total loss:	48.024 (rec:0.991, round:47.033)	b=3.69	count=18500
Total loss:	9.963 (rec:0.954, round:9.009)	b=3.12	count=19000
Total loss:	1.631 (rec:0.960, round:0.671)	b=2.56	count=19500
Total loss:	0.995 (rec:0.953, round:0.042)	b=2.00	count=20000
finished reconstructing layers.2.blocks.17.
reconstructing layers.3.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.downsample ...
wraping quantizers in layers.3.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.593 (rec:1.593, round:0.000)	b=0.00	count=500
Total loss:	1.365 (rec:1.365, round:0.000)	b=0.00	count=1000
Total loss:	1.305 (rec:1.305, round:0.000)	b=0.00	count=1500
Total loss:	1.272 (rec:1.272, round:0.000)	b=0.00	count=2000
Total loss:	1.212 (rec:1.212, round:0.000)	b=0.00	count=2500
Total loss:	1.211 (rec:1.211, round:0.000)	b=0.00	count=3000
Total loss:	1.231 (rec:1.231, round:0.000)	b=0.00	count=3500
Total loss:	10660.255 (rec:1.255, round:10659.000)	b=20.00	count=4000
Total loss:	5345.604 (rec:1.204, round:5344.400)	b=19.44	count=4500
Total loss:	4969.204 (rec:1.271, round:4967.933)	b=18.88	count=5000
Total loss:	4738.926 (rec:1.222, round:4737.704)	b=18.31	count=5500
Total loss:	4550.316 (rec:1.218, round:4549.098)	b=17.75	count=6000
Total loss:	4377.555 (rec:1.227, round:4376.328)	b=17.19	count=6500
Total loss:	4217.011 (rec:1.176, round:4215.834)	b=16.62	count=7000
Total loss:	4058.330 (rec:1.166, round:4057.165)	b=16.06	count=7500
Total loss:	3904.486 (rec:1.164, round:3903.322)	b=15.50	count=8000
Total loss:	3751.665 (rec:1.318, round:3750.347)	b=14.94	count=8500
Total loss:	3599.074 (rec:1.211, round:3597.863)	b=14.38	count=9000
Total loss:	3446.720 (rec:1.179, round:3445.541)	b=13.81	count=9500
Total loss:	3292.674 (rec:1.194, round:3291.479)	b=13.25	count=10000
Total loss:	3134.700 (rec:1.217, round:3133.483)	b=12.69	count=10500
Total loss:	2972.117 (rec:1.181, round:2970.936)	b=12.12	count=11000
Total loss:	2804.531 (rec:1.201, round:2803.329)	b=11.56	count=11500
Total loss:	2633.322 (rec:1.254, round:2632.068)	b=11.00	count=12000
Total loss:	2453.738 (rec:1.223, round:2452.515)	b=10.44	count=12500
Total loss:	2272.208 (rec:1.199, round:2271.009)	b=9.88	count=13000
Total loss:	2081.145 (rec:1.245, round:2079.900)	b=9.31	count=13500
Total loss:	1878.127 (rec:1.273, round:1876.854)	b=8.75	count=14000
Total loss:	1669.378 (rec:1.319, round:1668.059)	b=8.19	count=14500
Total loss:	1450.469 (rec:1.265, round:1449.203)	b=7.62	count=15000
Total loss:	1224.220 (rec:1.295, round:1222.925)	b=7.06	count=15500
Total loss:	991.614 (rec:1.262, round:990.352)	b=6.50	count=16000
Total loss:	754.991 (rec:1.285, round:753.705)	b=5.94	count=16500
Total loss:	521.099 (rec:1.214, round:519.885)	b=5.38	count=17000
Total loss:	300.546 (rec:1.254, round:299.292)	b=4.81	count=17500
Total loss:	128.374 (rec:1.235, round:127.139)	b=4.25	count=18000
Total loss:	33.690 (rec:1.296, round:32.394)	b=3.69	count=18500
Total loss:	5.043 (rec:1.265, round:3.778)	b=3.12	count=19000
Total loss:	1.482 (rec:1.257, round:0.225)	b=2.56	count=19500
Total loss:	1.272 (rec:1.258, round:0.014)	b=2.00	count=20000
finished reconstructing layers.3.downsample.
reconstructing layers.3.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.blocks.0 ...
wraping quantizers in layers.3.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.636 (rec:1.636, round:0.000)	b=0.00	count=500
Total loss:	1.340 (rec:1.340, round:0.000)	b=0.00	count=1000
Total loss:	1.264 (rec:1.264, round:0.000)	b=0.00	count=1500
Total loss:	1.165 (rec:1.165, round:0.000)	b=0.00	count=2000
Total loss:	1.158 (rec:1.158, round:0.000)	b=0.00	count=2500
Total loss:	1.159 (rec:1.159, round:0.000)	b=0.00	count=3000
Total loss:	1.152 (rec:1.152, round:0.000)	b=0.00	count=3500
Total loss:	65820.078 (rec:1.053, round:65819.023)	b=20.00	count=4000
Total loss:	32185.426 (rec:1.020, round:32184.406)	b=19.44	count=4500
Total loss:	29824.836 (rec:0.996, round:29823.840)	b=18.88	count=5000
Total loss:	28302.953 (rec:1.004, round:28301.949)	b=18.31	count=5500
Total loss:	27000.551 (rec:0.992, round:26999.559)	b=17.75	count=6000
Total loss:	25776.498 (rec:0.962, round:25775.535)	b=17.19	count=6500
Total loss:	24591.279 (rec:0.948, round:24590.332)	b=16.62	count=7000
Total loss:	23428.746 (rec:0.959, round:23427.787)	b=16.06	count=7500
Total loss:	22275.539 (rec:0.946, round:22274.594)	b=15.50	count=8000
Total loss:	21123.369 (rec:0.921, round:21122.449)	b=14.94	count=8500
Total loss:	19972.535 (rec:0.970, round:19971.564)	b=14.38	count=9000
Total loss:	18815.588 (rec:0.919, round:18814.668)	b=13.81	count=9500
Total loss:	17654.826 (rec:0.942, round:17653.885)	b=13.25	count=10000
Total loss:	16490.779 (rec:0.887, round:16489.893)	b=12.69	count=10500
Total loss:	15332.231 (rec:0.910, round:15331.322)	b=12.12	count=11000
Total loss:	14169.260 (rec:0.930, round:14168.329)	b=11.56	count=11500
Total loss:	13005.048 (rec:0.942, round:13004.105)	b=11.00	count=12000
Total loss:	11844.907 (rec:0.884, round:11844.023)	b=10.44	count=12500
Total loss:	10690.955 (rec:0.857, round:10690.098)	b=9.88	count=13000
Total loss:	9539.438 (rec:0.873, round:9538.564)	b=9.31	count=13500
Total loss:	8401.713 (rec:0.883, round:8400.830)	b=8.75	count=14000
Total loss:	7275.734 (rec:0.876, round:7274.858)	b=8.19	count=14500
Total loss:	6165.949 (rec:0.906, round:6165.043)	b=7.62	count=15000
Total loss:	5080.570 (rec:0.893, round:5079.677)	b=7.06	count=15500
Total loss:	4037.473 (rec:0.894, round:4036.579)	b=6.50	count=16000
Total loss:	3043.907 (rec:0.911, round:3042.996)	b=5.94	count=16500
Total loss:	2124.200 (rec:0.865, round:2123.335)	b=5.38	count=17000
Total loss:	1318.796 (rec:0.918, round:1317.878)	b=4.81	count=17500
Total loss:	664.312 (rec:0.879, round:663.433)	b=4.25	count=18000
Total loss:	221.987 (rec:0.909, round:221.079)	b=3.69	count=18500
Total loss:	33.632 (rec:0.897, round:32.735)	b=3.12	count=19000
Total loss:	2.526 (rec:0.914, round:1.612)	b=2.56	count=19500
Total loss:	0.965 (rec:0.932, round:0.034)	b=2.00	count=20000
finished reconstructing layers.3.blocks.0.
reconstructing layers.3.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.blocks.1 ...
wraping quantizers in layers.3.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.522 (rec:1.522, round:0.000)	b=0.00	count=500
Total loss:	1.435 (rec:1.435, round:0.000)	b=0.00	count=1000
Total loss:	1.370 (rec:1.370, round:0.000)	b=0.00	count=1500
Total loss:	1.356 (rec:1.356, round:0.000)	b=0.00	count=2000
Total loss:	1.329 (rec:1.329, round:0.000)	b=0.00	count=2500
Total loss:	1.226 (rec:1.226, round:0.000)	b=0.00	count=3000
Total loss:	1.231 (rec:1.231, round:0.000)	b=0.00	count=3500
Total loss:	65038.188 (rec:1.225, round:65036.961)	b=20.00	count=4000
Total loss:	31842.547 (rec:1.179, round:31841.367)	b=19.44	count=4500
Total loss:	29482.867 (rec:1.175, round:29481.691)	b=18.88	count=5000
Total loss:	27926.625 (rec:1.113, round:27925.512)	b=18.31	count=5500
Total loss:	26574.756 (rec:1.057, round:26573.699)	b=17.75	count=6000
Total loss:	25307.111 (rec:1.056, round:25306.055)	b=17.19	count=6500
Total loss:	24075.406 (rec:1.183, round:24074.225)	b=16.62	count=7000
Total loss:	22864.668 (rec:1.108, round:22863.561)	b=16.06	count=7500
Total loss:	21667.643 (rec:1.088, round:21666.555)	b=15.50	count=8000
Total loss:	20479.691 (rec:1.024, round:20478.668)	b=14.94	count=8500
Total loss:	19298.217 (rec:0.993, round:19297.223)	b=14.38	count=9000
Total loss:	18110.596 (rec:1.072, round:18109.523)	b=13.81	count=9500
Total loss:	16927.795 (rec:1.009, round:16926.785)	b=13.25	count=10000
Total loss:	15748.001 (rec:1.047, round:15746.954)	b=12.69	count=10500
Total loss:	14571.147 (rec:1.022, round:14570.125)	b=12.12	count=11000
Total loss:	13400.944 (rec:0.995, round:13399.949)	b=11.56	count=11500
Total loss:	12243.583 (rec:1.017, round:12242.566)	b=11.00	count=12000
Total loss:	11093.938 (rec:0.955, round:11092.982)	b=10.44	count=12500
Total loss:	9960.074 (rec:1.033, round:9959.041)	b=9.88	count=13000
Total loss:	8842.056 (rec:1.009, round:8841.047)	b=9.31	count=13500
Total loss:	7738.453 (rec:1.043, round:7737.410)	b=8.75	count=14000
Total loss:	6659.622 (rec:0.957, round:6658.666)	b=8.19	count=14500
Total loss:	5600.259 (rec:0.965, round:5599.294)	b=7.62	count=15000
Total loss:	4577.047 (rec:1.007, round:4576.040)	b=7.06	count=15500
Total loss:	3600.424 (rec:0.986, round:3599.438)	b=6.50	count=16000
Total loss:	2681.296 (rec:1.066, round:2680.230)	b=5.94	count=16500
Total loss:	1848.993 (rec:1.022, round:1847.971)	b=5.38	count=17000
Total loss:	1127.825 (rec:1.026, round:1126.799)	b=4.81	count=17500
Total loss:	554.609 (rec:1.025, round:553.584)	b=4.25	count=18000
Total loss:	182.699 (rec:1.033, round:181.666)	b=3.69	count=18500
Total loss:	29.634 (rec:1.019, round:28.615)	b=3.12	count=19000
Total loss:	3.170 (rec:1.056, round:2.113)	b=2.56	count=19500
Total loss:	1.168 (rec:1.042, round:0.125)	b=2.00	count=20000
finished reconstructing layers.3.blocks.1.
reconstructing head ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for head ...
wraping quantizers in head ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.573 (rec:1.573, round:0.000)	b=0.00	count=500
Total loss:	1.498 (rec:1.498, round:0.000)	b=0.00	count=1000
Total loss:	1.450 (rec:1.450, round:0.000)	b=0.00	count=1500
Total loss:	1.265 (rec:1.265, round:0.000)	b=0.00	count=2000
Total loss:	1.072 (rec:1.072, round:0.000)	b=0.00	count=2500
Total loss:	0.967 (rec:0.967, round:0.000)	b=0.00	count=3000
Total loss:	0.975 (rec:0.975, round:0.000)	b=0.00	count=3500
Total loss:	6630.570 (rec:0.650, round:6629.919)	b=20.00	count=4000
Total loss:	3365.463 (rec:0.666, round:3364.797)	b=19.44	count=4500
Total loss:	3078.360 (rec:0.663, round:3077.697)	b=18.88	count=5000
Total loss:	2875.705 (rec:0.653, round:2875.052)	b=18.31	count=5500
Total loss:	2693.681 (rec:0.445, round:2693.236)	b=17.75	count=6000
Total loss:	2522.711 (rec:0.438, round:2522.273)	b=17.19	count=6500
Total loss:	2356.596 (rec:0.367, round:2356.230)	b=16.62	count=7000
Total loss:	2194.100 (rec:0.508, round:2193.592)	b=16.06	count=7500
Total loss:	2035.874 (rec:0.413, round:2035.461)	b=15.50	count=8000
Total loss:	1881.249 (rec:0.354, round:1880.895)	b=14.94	count=8500
Total loss:	1729.017 (rec:0.321, round:1728.696)	b=14.38	count=9000
Total loss:	1581.824 (rec:0.366, round:1581.458)	b=13.81	count=9500
Total loss:	1439.681 (rec:0.330, round:1439.351)	b=13.25	count=10000
Total loss:	1303.409 (rec:0.252, round:1303.157)	b=12.69	count=10500
Total loss:	1173.578 (rec:0.333, round:1173.245)	b=12.12	count=11000
Total loss:	1049.883 (rec:0.303, round:1049.580)	b=11.56	count=11500
Total loss:	931.640 (rec:0.289, round:931.352)	b=11.00	count=12000
Total loss:	821.963 (rec:0.268, round:821.695)	b=10.44	count=12500
Total loss:	719.497 (rec:0.281, round:719.215)	b=9.88	count=13000
Total loss:	621.932 (rec:0.234, round:621.698)	b=9.31	count=13500
Total loss:	531.461 (rec:0.209, round:531.252)	b=8.75	count=14000
Total loss:	448.107 (rec:0.265, round:447.842)	b=8.19	count=14500
Total loss:	372.117 (rec:0.306, round:371.811)	b=7.62	count=15000
Total loss:	301.381 (rec:0.218, round:301.163)	b=7.06	count=15500
Total loss:	238.230 (rec:0.233, round:237.997)	b=6.50	count=16000
Total loss:	179.848 (rec:0.258, round:179.590)	b=5.94	count=16500
Total loss:	127.912 (rec:0.229, round:127.683)	b=5.38	count=17000
Total loss:	83.788 (rec:0.195, round:83.593)	b=4.81	count=17500
Total loss:	47.706 (rec:0.234, round:47.471)	b=4.25	count=18000
Total loss:	21.420 (rec:0.234, round:21.186)	b=3.69	count=18500
Total loss:	6.385 (rec:0.273, round:6.112)	b=3.12	count=19000
Total loss:	0.998 (rec:0.268, round:0.730)	b=2.56	count=19500
Total loss:	0.238 (rec:0.210, round:0.028)	b=2.00	count=20000
finished reconstructing head.
2025-09-11 14:48:35 - mse guided block reconstruction finished.
Saving checkpoint to ./checkpoint/quant_result/20250911_1056/swin_small_w2_a2_optimsize_1024_mse_qdrop.pth
Validating on calibration set after block reconstruction ...
Test: [0/32]	Time 0.501 (0.501)	Loss 4.4180 (4.4180)	Prec@1 65.625 (65.625)	Prec@5 75.000 (75.000)
Test: [10/32]	Time 0.065 (0.105)	Loss 3.9653 (4.0766)	Prec@1 75.000 (64.773)	Prec@5 84.375 (82.102)
Test: [20/32]	Time 0.066 (0.086)	Loss 3.9364 (4.0387)	Prec@1 56.250 (65.625)	Prec@5 87.500 (83.780)
Test: [30/32]	Time 0.066 (0.080)	Loss 3.9531 (4.0433)	Prec@1 68.750 (65.222)	Prec@5 87.500 (83.972)
 * Prec@1 65.039 Prec@5 84.180 Loss 4.029 Time 2.652
Validating on test set after block reconstruction ...
Test: [0/100]	Time 4.866 (4.866)	Loss 5.6204 (5.6204)	Prec@1 13.800 (13.800)	Prec@5 34.400 (34.400)
Test: [10/100]	Time 1.744 (2.029)	Loss 5.5036 (5.7068)	Prec@1 24.000 (14.364)	Prec@5 39.200 (31.491)
Test: [20/100]	Time 1.750 (1.895)	Loss 4.1026 (5.3195)	Prec@1 21.400 (16.057)	Prec@5 55.400 (36.152)
Test: [30/100]	Time 1.747 (1.847)	Loss 5.0704 (5.0663)	Prec@1 16.600 (17.465)	Prec@5 39.800 (40.226)
Test: [40/100]	Time 1.752 (1.823)	Loss 5.5336 (5.2548)	Prec@1 6.600 (15.961)	Prec@5 29.200 (36.766)
Test: [50/100]	Time 1.750 (1.809)	Loss 5.9251 (5.3539)	Prec@1 10.200 (14.984)	Prec@5 21.800 (34.714)
Test: [60/100]	Time 1.750 (1.799)	Loss 5.7582 (5.4221)	Prec@1 11.600 (14.439)	Prec@5 27.200 (33.341)
Test: [70/100]	Time 1.751 (1.792)	Loss 5.7373 (5.4648)	Prec@1 8.800 (13.876)	Prec@5 24.800 (32.211)
Test: [80/100]	Time 1.751 (1.787)	Loss 6.0386 (5.5146)	Prec@1 6.600 (13.398)	Prec@5 15.600 (31.114)
Test: [90/100]	Time 1.750 (1.783)	Loss 5.7137 (5.5435)	Prec@1 12.400 (13.024)	Prec@5 22.600 (30.382)
 * Prec@1 12.850 Prec@5 30.040 Loss 5.571 Time 178.236
2025-09-11 14:51:36 - finished the process.
Extracting logits from quantized and full-precision models...
Testing combinations:
  Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
  Cluster numbers: [8, 16, 32, 64, 128, 256]
  PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.84%
[Alpha=0.10] Top-5 Accuracy: 30.04%
Result: Top-1: 12.84%, Top-5: 30.04%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.79%
[Alpha=0.10] Top-5 Accuracy: 29.99%
Result: Top-1: 12.79%, Top-5: 29.99%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.78%
[Alpha=0.10] Top-5 Accuracy: 29.94%
Result: Top-1: 12.78%, Top-5: 29.94%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.75%
[Alpha=0.10] Top-5 Accuracy: 29.95%
Result: Top-1: 12.75%, Top-5: 29.95%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.76%
[Alpha=0.10] Top-5 Accuracy: 29.96%
Result: Top-1: 12.76%, Top-5: 29.96%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.75%
[Alpha=0.10] Top-5 Accuracy: 29.95%
Result: Top-1: 12.75%, Top-5: 29.95%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.78%
[Alpha=0.10] Top-5 Accuracy: 29.93%
Result: Top-1: 12.78%, Top-5: 29.93%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.76%
[Alpha=0.10] Top-5 Accuracy: 29.92%
Result: Top-1: 12.76%, Top-5: 29.92%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.79%
[Alpha=0.10] Top-5 Accuracy: 29.99%
Result: Top-1: 12.79%, Top-5: 29.99%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.83%
[Alpha=0.10] Top-5 Accuracy: 30.00%
Result: Top-1: 12.83%, Top-5: 30.00%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.79%
[Alpha=0.10] Top-5 Accuracy: 29.98%
Result: Top-1: 12.79%, Top-5: 29.98%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.77%
[Alpha=0.10] Top-5 Accuracy: 29.94%
Result: Top-1: 12.77%, Top-5: 29.94%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.79%
[Alpha=0.10] Top-5 Accuracy: 29.94%
Result: Top-1: 12.79%, Top-5: 29.94%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.78%
[Alpha=0.10] Top-5 Accuracy: 29.95%
Result: Top-1: 12.78%, Top-5: 29.95%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.77%
[Alpha=0.10] Top-5 Accuracy: 29.96%
Result: Top-1: 12.77%, Top-5: 29.96%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.83%
[Alpha=0.10] Top-5 Accuracy: 29.94%
Result: Top-1: 12.83%, Top-5: 29.94%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.81%
[Alpha=0.10] Top-5 Accuracy: 29.94%
Result: Top-1: 12.81%, Top-5: 29.94%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.79%
[Alpha=0.10] Top-5 Accuracy: 29.93%
Result: Top-1: 12.79%, Top-5: 29.93%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.79%
[Alpha=0.10] Top-5 Accuracy: 29.96%
Result: Top-1: 12.79%, Top-5: 29.96%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.79%
[Alpha=0.10] Top-5 Accuracy: 29.94%
Result: Top-1: 12.79%, Top-5: 29.94%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.78%
[Alpha=0.10] Top-5 Accuracy: 29.94%
Result: Top-1: 12.78%, Top-5: 29.94%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.74%
[Alpha=0.10] Top-5 Accuracy: 29.87%
Result: Top-1: 12.74%, Top-5: 29.87%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.76%
[Alpha=0.10] Top-5 Accuracy: 29.88%
Result: Top-1: 12.76%, Top-5: 29.88%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.74%
[Alpha=0.10] Top-5 Accuracy: 29.91%
Result: Top-1: 12.74%, Top-5: 29.91%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.78%
[Alpha=0.10] Top-5 Accuracy: 29.90%
Result: Top-1: 12.78%, Top-5: 29.90%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.70%
[Alpha=0.10] Top-5 Accuracy: 29.91%
Result: Top-1: 12.70%, Top-5: 29.91%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.76%
[Alpha=0.10] Top-5 Accuracy: 29.89%
Result: Top-1: 12.76%, Top-5: 29.89%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.76%
[Alpha=0.10] Top-5 Accuracy: 29.96%
Result: Top-1: 12.76%, Top-5: 29.96%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.76%
[Alpha=0.10] Top-5 Accuracy: 29.89%
Result: Top-1: 12.76%, Top-5: 29.89%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.76%
[Alpha=0.10] Top-5 Accuracy: 29.91%
Result: Top-1: 12.76%, Top-5: 29.91%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.73%
[Alpha=0.10] Top-5 Accuracy: 29.88%
Result: Top-1: 12.73%, Top-5: 29.88%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.69%
[Alpha=0.10] Top-5 Accuracy: 29.85%
Result: Top-1: 12.69%, Top-5: 29.85%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.72%
[Alpha=0.10] Top-5 Accuracy: 30.00%
Result: Top-1: 12.72%, Top-5: 30.00%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.71%
[Alpha=0.10] Top-5 Accuracy: 29.84%
Result: Top-1: 12.71%, Top-5: 29.84%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.71%
[Alpha=0.10] Top-5 Accuracy: 29.88%
Result: Top-1: 12.71%, Top-5: 29.88%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.73%
[Alpha=0.10] Top-5 Accuracy: 29.84%
Result: Top-1: 12.73%, Top-5: 29.84%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.70%
[Alpha=0.10] Top-5 Accuracy: 29.84%
Result: Top-1: 12.70%, Top-5: 29.84%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.75%
[Alpha=0.10] Top-5 Accuracy: 29.88%
Result: Top-1: 12.75%, Top-5: 29.88%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.67%
[Alpha=0.10] Top-5 Accuracy: 29.83%
Result: Top-1: 12.67%, Top-5: 29.83%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.75%
[Alpha=0.10] Top-5 Accuracy: 29.92%
Result: Top-1: 12.75%, Top-5: 29.92%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.74%
[Alpha=0.10] Top-5 Accuracy: 29.89%
Result: Top-1: 12.74%, Top-5: 29.89%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.73%
[Alpha=0.10] Top-5 Accuracy: 29.83%
Result: Top-1: 12.73%, Top-5: 29.83%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.69%
[Alpha=0.10] Top-5 Accuracy: 29.89%
Result: Top-1: 12.69%, Top-5: 29.89%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.73%
[Alpha=0.10] Top-5 Accuracy: 29.88%
Result: Top-1: 12.73%, Top-5: 29.88%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.77%
[Alpha=0.10] Top-5 Accuracy: 29.90%
Result: Top-1: 12.77%, Top-5: 29.90%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.71%
[Alpha=0.10] Top-5 Accuracy: 29.92%
Result: Top-1: 12.71%, Top-5: 29.92%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.64%
[Alpha=0.10] Top-5 Accuracy: 29.94%
Result: Top-1: 12.64%, Top-5: 29.94%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.76%
[Alpha=0.10] Top-5 Accuracy: 29.86%
Result: Top-1: 12.76%, Top-5: 29.86%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.66%
[Alpha=0.10] Top-5 Accuracy: 29.82%
Result: Top-1: 12.66%, Top-5: 29.82%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.72%
[Alpha=0.10] Top-5 Accuracy: 29.90%
Result: Top-1: 12.72%, Top-5: 29.90%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.66%
[Alpha=0.10] Top-5 Accuracy: 29.85%
Result: Top-1: 12.66%, Top-5: 29.85%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.46%
[Alpha=0.10] Top-5 Accuracy: 29.71%
Result: Top-1: 12.46%, Top-5: 29.71%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.58%
[Alpha=0.10] Top-5 Accuracy: 29.87%
Result: Top-1: 12.58%, Top-5: 29.87%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.51%
[Alpha=0.10] Top-5 Accuracy: 29.67%
Result: Top-1: 12.51%, Top-5: 29.67%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.41%
[Alpha=0.10] Top-5 Accuracy: 29.37%
Result: Top-1: 12.41%, Top-5: 29.37%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.65%
[Alpha=0.10] Top-5 Accuracy: 29.66%
Result: Top-1: 12.65%, Top-5: 29.66%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.57%
[Alpha=0.10] Top-5 Accuracy: 29.74%
Result: Top-1: 12.57%, Top-5: 29.74%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.54%
[Alpha=0.10] Top-5 Accuracy: 29.54%
Result: Top-1: 12.54%, Top-5: 29.54%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.43%
[Alpha=0.10] Top-5 Accuracy: 29.57%
Result: Top-1: 12.43%, Top-5: 29.57%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 12.59%
[Alpha=0.10] Top-5 Accuracy: 29.83%
Result: Top-1: 12.59%, Top-5: 29.83%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.20%
[Alpha=0.20] Top-5 Accuracy: 29.55%
Result: Top-1: 12.20%, Top-5: 29.55%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.21%
[Alpha=0.20] Top-5 Accuracy: 29.46%
Result: Top-1: 12.21%, Top-5: 29.46%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.09%
[Alpha=0.20] Top-5 Accuracy: 29.36%
Result: Top-1: 12.09%, Top-5: 29.36%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.08%
[Alpha=0.20] Top-5 Accuracy: 29.37%
Result: Top-1: 12.08%, Top-5: 29.37%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.08%
[Alpha=0.20] Top-5 Accuracy: 29.47%
Result: Top-1: 12.08%, Top-5: 29.47%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.14%
[Alpha=0.20] Top-5 Accuracy: 29.43%
Result: Top-1: 12.14%, Top-5: 29.43%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.09%
[Alpha=0.20] Top-5 Accuracy: 29.30%
Result: Top-1: 12.09%, Top-5: 29.30%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.10%
[Alpha=0.20] Top-5 Accuracy: 29.33%
Result: Top-1: 12.10%, Top-5: 29.33%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.15%
[Alpha=0.20] Top-5 Accuracy: 29.46%
Result: Top-1: 12.15%, Top-5: 29.46%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.25%
[Alpha=0.20] Top-5 Accuracy: 29.54%
Result: Top-1: 12.25%, Top-5: 29.54%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.13%
[Alpha=0.20] Top-5 Accuracy: 29.51%
Result: Top-1: 12.13%, Top-5: 29.51%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.97%
[Alpha=0.20] Top-5 Accuracy: 29.32%
Result: Top-1: 11.97%, Top-5: 29.32%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.97%
[Alpha=0.20] Top-5 Accuracy: 29.35%
Result: Top-1: 11.97%, Top-5: 29.35%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.88%
[Alpha=0.20] Top-5 Accuracy: 29.43%
Result: Top-1: 11.88%, Top-5: 29.43%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.95%
[Alpha=0.20] Top-5 Accuracy: 29.39%
Result: Top-1: 11.95%, Top-5: 29.39%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.97%
[Alpha=0.20] Top-5 Accuracy: 29.38%
Result: Top-1: 11.97%, Top-5: 29.38%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.07%
[Alpha=0.20] Top-5 Accuracy: 29.49%
Result: Top-1: 12.07%, Top-5: 29.49%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.89%
[Alpha=0.20] Top-5 Accuracy: 29.25%
Result: Top-1: 11.89%, Top-5: 29.25%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.95%
[Alpha=0.20] Top-5 Accuracy: 29.32%
Result: Top-1: 11.95%, Top-5: 29.32%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.92%
[Alpha=0.20] Top-5 Accuracy: 29.34%
Result: Top-1: 11.92%, Top-5: 29.34%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.12%
[Alpha=0.20] Top-5 Accuracy: 29.49%
Result: Top-1: 12.12%, Top-5: 29.49%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.78%
[Alpha=0.20] Top-5 Accuracy: 29.14%
Result: Top-1: 11.78%, Top-5: 29.14%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.88%
[Alpha=0.20] Top-5 Accuracy: 29.35%
Result: Top-1: 11.88%, Top-5: 29.35%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.70%
[Alpha=0.20] Top-5 Accuracy: 29.17%
Result: Top-1: 11.70%, Top-5: 29.17%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.80%
[Alpha=0.20] Top-5 Accuracy: 29.21%
Result: Top-1: 11.80%, Top-5: 29.21%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.75%
[Alpha=0.20] Top-5 Accuracy: 29.13%
Result: Top-1: 11.75%, Top-5: 29.13%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.80%
[Alpha=0.20] Top-5 Accuracy: 29.22%
Result: Top-1: 11.80%, Top-5: 29.22%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.77%
[Alpha=0.20] Top-5 Accuracy: 29.24%
Result: Top-1: 11.77%, Top-5: 29.24%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.88%
[Alpha=0.20] Top-5 Accuracy: 29.23%
Result: Top-1: 11.88%, Top-5: 29.23%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.73%
[Alpha=0.20] Top-5 Accuracy: 29.17%
Result: Top-1: 11.73%, Top-5: 29.17%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 12.02%
[Alpha=0.20] Top-5 Accuracy: 29.42%
Result: Top-1: 12.02%, Top-5: 29.42%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.71%
[Alpha=0.20] Top-5 Accuracy: 29.06%
Result: Top-1: 11.71%, Top-5: 29.06%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.69%
[Alpha=0.20] Top-5 Accuracy: 29.23%
Result: Top-1: 11.69%, Top-5: 29.23%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.70%
[Alpha=0.20] Top-5 Accuracy: 29.12%
Result: Top-1: 11.70%, Top-5: 29.12%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.67%
[Alpha=0.20] Top-5 Accuracy: 29.15%
Result: Top-1: 11.67%, Top-5: 29.15%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.69%
[Alpha=0.20] Top-5 Accuracy: 29.15%
Result: Top-1: 11.69%, Top-5: 29.15%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.61%
[Alpha=0.20] Top-5 Accuracy: 29.13%
Result: Top-1: 11.61%, Top-5: 29.13%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.66%
[Alpha=0.20] Top-5 Accuracy: 29.20%
Result: Top-1: 11.66%, Top-5: 29.20%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.34%
[Alpha=0.20] Top-5 Accuracy: 28.81%
Result: Top-1: 11.34%, Top-5: 28.81%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.68%
[Alpha=0.20] Top-5 Accuracy: 29.23%
Result: Top-1: 11.68%, Top-5: 29.23%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.93%
[Alpha=0.20] Top-5 Accuracy: 29.28%
Result: Top-1: 11.93%, Top-5: 29.28%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.56%
[Alpha=0.20] Top-5 Accuracy: 28.85%
Result: Top-1: 11.56%, Top-5: 28.85%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.54%
[Alpha=0.20] Top-5 Accuracy: 29.01%
Result: Top-1: 11.54%, Top-5: 29.01%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.68%
[Alpha=0.20] Top-5 Accuracy: 28.98%
Result: Top-1: 11.68%, Top-5: 28.98%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.55%
[Alpha=0.20] Top-5 Accuracy: 28.90%
Result: Top-1: 11.55%, Top-5: 28.90%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.68%
[Alpha=0.20] Top-5 Accuracy: 28.97%
Result: Top-1: 11.68%, Top-5: 28.97%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.47%
[Alpha=0.20] Top-5 Accuracy: 29.14%
Result: Top-1: 11.47%, Top-5: 29.14%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.43%
[Alpha=0.20] Top-5 Accuracy: 28.98%
Result: Top-1: 11.43%, Top-5: 28.98%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.35%
[Alpha=0.20] Top-5 Accuracy: 28.65%
Result: Top-1: 11.35%, Top-5: 28.65%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.52%
[Alpha=0.20] Top-5 Accuracy: 28.99%
Result: Top-1: 11.52%, Top-5: 28.99%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.72%
[Alpha=0.20] Top-5 Accuracy: 28.86%
Result: Top-1: 11.72%, Top-5: 28.86%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.01%
[Alpha=0.20] Top-5 Accuracy: 28.48%
Result: Top-1: 11.01%, Top-5: 28.48%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.15%
[Alpha=0.20] Top-5 Accuracy: 28.44%
Result: Top-1: 11.15%, Top-5: 28.44%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.23%
[Alpha=0.20] Top-5 Accuracy: 28.48%
Result: Top-1: 11.23%, Top-5: 28.48%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 10.80%
[Alpha=0.20] Top-5 Accuracy: 27.92%
Result: Top-1: 10.80%, Top-5: 27.92%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.23%
[Alpha=0.20] Top-5 Accuracy: 28.50%
Result: Top-1: 11.23%, Top-5: 28.50%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.23%
[Alpha=0.20] Top-5 Accuracy: 28.67%
Result: Top-1: 11.23%, Top-5: 28.67%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.13%
[Alpha=0.20] Top-5 Accuracy: 28.17%
Result: Top-1: 11.13%, Top-5: 28.17%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.04%
[Alpha=0.20] Top-5 Accuracy: 28.29%
Result: Top-1: 11.04%, Top-5: 28.29%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 11.29%
[Alpha=0.20] Top-5 Accuracy: 28.60%
Result: Top-1: 11.29%, Top-5: 28.60%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.99%
[Alpha=0.30] Top-5 Accuracy: 28.60%
Result: Top-1: 10.99%, Top-5: 28.60%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.93%
[Alpha=0.30] Top-5 Accuracy: 28.51%
Result: Top-1: 10.93%, Top-5: 28.51%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.71%
[Alpha=0.30] Top-5 Accuracy: 28.41%
Result: Top-1: 10.71%, Top-5: 28.41%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.74%
[Alpha=0.30] Top-5 Accuracy: 28.45%
Result: Top-1: 10.74%, Top-5: 28.45%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.87%
[Alpha=0.30] Top-5 Accuracy: 28.46%
Result: Top-1: 10.87%, Top-5: 28.46%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.85%
[Alpha=0.30] Top-5 Accuracy: 28.54%
Result: Top-1: 10.85%, Top-5: 28.54%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.74%
[Alpha=0.30] Top-5 Accuracy: 28.41%
Result: Top-1: 10.74%, Top-5: 28.41%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.72%
[Alpha=0.30] Top-5 Accuracy: 28.41%
Result: Top-1: 10.72%, Top-5: 28.41%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.93%
[Alpha=0.30] Top-5 Accuracy: 28.53%
Result: Top-1: 10.93%, Top-5: 28.53%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 11.07%
[Alpha=0.30] Top-5 Accuracy: 28.53%
Result: Top-1: 11.07%, Top-5: 28.53%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.99%
[Alpha=0.30] Top-5 Accuracy: 28.60%
Result: Top-1: 10.99%, Top-5: 28.60%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.65%
[Alpha=0.30] Top-5 Accuracy: 28.36%
Result: Top-1: 10.65%, Top-5: 28.36%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.61%
[Alpha=0.30] Top-5 Accuracy: 28.38%
Result: Top-1: 10.61%, Top-5: 28.38%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.57%
[Alpha=0.30] Top-5 Accuracy: 28.39%
Result: Top-1: 10.57%, Top-5: 28.39%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.62%
[Alpha=0.30] Top-5 Accuracy: 28.39%
Result: Top-1: 10.62%, Top-5: 28.39%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.74%
[Alpha=0.30] Top-5 Accuracy: 28.39%
Result: Top-1: 10.74%, Top-5: 28.39%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.88%
[Alpha=0.30] Top-5 Accuracy: 28.61%
Result: Top-1: 10.88%, Top-5: 28.61%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.49%
[Alpha=0.30] Top-5 Accuracy: 28.23%
Result: Top-1: 10.49%, Top-5: 28.23%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.63%
[Alpha=0.30] Top-5 Accuracy: 28.38%
Result: Top-1: 10.63%, Top-5: 28.38%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.51%
[Alpha=0.30] Top-5 Accuracy: 28.25%
Result: Top-1: 10.51%, Top-5: 28.25%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.88%
[Alpha=0.30] Top-5 Accuracy: 28.48%
Result: Top-1: 10.88%, Top-5: 28.48%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.10%
[Alpha=0.30] Top-5 Accuracy: 27.82%
Result: Top-1: 10.10%, Top-5: 27.82%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.47%
[Alpha=0.30] Top-5 Accuracy: 28.37%
Result: Top-1: 10.47%, Top-5: 28.37%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.95%
[Alpha=0.30] Top-5 Accuracy: 27.89%
Result: Top-1: 9.95%, Top-5: 27.89%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.33%
[Alpha=0.30] Top-5 Accuracy: 28.03%
Result: Top-1: 10.33%, Top-5: 28.03%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.17%
[Alpha=0.30] Top-5 Accuracy: 28.09%
Result: Top-1: 10.17%, Top-5: 28.09%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.25%
[Alpha=0.30] Top-5 Accuracy: 27.99%
Result: Top-1: 10.25%, Top-5: 27.99%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.24%
[Alpha=0.30] Top-5 Accuracy: 28.07%
Result: Top-1: 10.24%, Top-5: 28.07%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.30%
[Alpha=0.30] Top-5 Accuracy: 27.98%
Result: Top-1: 10.30%, Top-5: 27.98%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.88%
[Alpha=0.30] Top-5 Accuracy: 27.86%
Result: Top-1: 9.88%, Top-5: 27.86%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.83%
[Alpha=0.30] Top-5 Accuracy: 28.33%
Result: Top-1: 10.83%, Top-5: 28.33%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.96%
[Alpha=0.30] Top-5 Accuracy: 27.75%
Result: Top-1: 9.96%, Top-5: 27.75%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.11%
[Alpha=0.30] Top-5 Accuracy: 27.97%
Result: Top-1: 10.11%, Top-5: 27.97%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.04%
[Alpha=0.30] Top-5 Accuracy: 27.80%
Result: Top-1: 10.04%, Top-5: 27.80%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.06%
[Alpha=0.30] Top-5 Accuracy: 27.91%
Result: Top-1: 10.06%, Top-5: 27.91%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.17%
[Alpha=0.30] Top-5 Accuracy: 27.94%
Result: Top-1: 10.17%, Top-5: 27.94%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.12%
[Alpha=0.30] Top-5 Accuracy: 27.92%
Result: Top-1: 10.12%, Top-5: 27.92%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.98%
[Alpha=0.30] Top-5 Accuracy: 27.91%
Result: Top-1: 9.98%, Top-5: 27.91%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.49%
[Alpha=0.30] Top-5 Accuracy: 27.26%
Result: Top-1: 9.49%, Top-5: 27.26%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.95%
[Alpha=0.30] Top-5 Accuracy: 27.87%
Result: Top-1: 9.95%, Top-5: 27.87%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.51%
[Alpha=0.30] Top-5 Accuracy: 28.02%
Result: Top-1: 10.51%, Top-5: 28.02%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.71%
[Alpha=0.30] Top-5 Accuracy: 27.28%
Result: Top-1: 9.71%, Top-5: 27.28%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.72%
[Alpha=0.30] Top-5 Accuracy: 27.28%
Result: Top-1: 9.72%, Top-5: 27.28%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.96%
[Alpha=0.30] Top-5 Accuracy: 27.35%
Result: Top-1: 9.96%, Top-5: 27.35%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.81%
[Alpha=0.30] Top-5 Accuracy: 27.38%
Result: Top-1: 9.81%, Top-5: 27.38%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.93%
[Alpha=0.30] Top-5 Accuracy: 27.55%
Result: Top-1: 9.93%, Top-5: 27.55%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.56%
[Alpha=0.30] Top-5 Accuracy: 27.53%
Result: Top-1: 9.56%, Top-5: 27.53%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.71%
[Alpha=0.30] Top-5 Accuracy: 27.47%
Result: Top-1: 9.71%, Top-5: 27.47%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.17%
[Alpha=0.30] Top-5 Accuracy: 26.68%
Result: Top-1: 9.17%, Top-5: 26.68%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.74%
[Alpha=0.30] Top-5 Accuracy: 27.64%
Result: Top-1: 9.74%, Top-5: 27.64%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 10.09%
[Alpha=0.30] Top-5 Accuracy: 27.22%
Result: Top-1: 10.09%, Top-5: 27.22%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.13%
[Alpha=0.30] Top-5 Accuracy: 26.41%
Result: Top-1: 9.13%, Top-5: 26.41%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 8.88%
[Alpha=0.30] Top-5 Accuracy: 26.14%
Result: Top-1: 8.88%, Top-5: 26.14%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.29%
[Alpha=0.30] Top-5 Accuracy: 26.55%
Result: Top-1: 9.29%, Top-5: 26.55%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 8.73%
[Alpha=0.30] Top-5 Accuracy: 25.81%
Result: Top-1: 8.73%, Top-5: 25.81%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.01%
[Alpha=0.30] Top-5 Accuracy: 26.45%
Result: Top-1: 9.01%, Top-5: 26.45%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.11%
[Alpha=0.30] Top-5 Accuracy: 26.76%
Result: Top-1: 9.11%, Top-5: 26.76%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 8.98%
[Alpha=0.30] Top-5 Accuracy: 26.26%
Result: Top-1: 8.98%, Top-5: 26.26%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 8.96%
[Alpha=0.30] Top-5 Accuracy: 26.28%
Result: Top-1: 8.96%, Top-5: 26.28%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 9.13%
[Alpha=0.30] Top-5 Accuracy: 26.76%
Result: Top-1: 9.13%, Top-5: 26.76%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.84%
[Alpha=0.40] Top-5 Accuracy: 26.57%
Result: Top-1: 8.84%, Top-5: 26.57%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.84%
[Alpha=0.40] Top-5 Accuracy: 26.55%
Result: Top-1: 8.84%, Top-5: 26.55%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.53%
[Alpha=0.40] Top-5 Accuracy: 26.25%
Result: Top-1: 8.53%, Top-5: 26.25%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.52%
[Alpha=0.40] Top-5 Accuracy: 26.30%
Result: Top-1: 8.52%, Top-5: 26.30%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.71%
[Alpha=0.40] Top-5 Accuracy: 26.49%
Result: Top-1: 8.71%, Top-5: 26.49%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.68%
[Alpha=0.40] Top-5 Accuracy: 26.47%
Result: Top-1: 8.68%, Top-5: 26.47%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.50%
[Alpha=0.40] Top-5 Accuracy: 26.25%
Result: Top-1: 8.50%, Top-5: 26.25%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.51%
[Alpha=0.40] Top-5 Accuracy: 26.30%
Result: Top-1: 8.51%, Top-5: 26.30%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.79%
[Alpha=0.40] Top-5 Accuracy: 26.74%
Result: Top-1: 8.79%, Top-5: 26.74%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.97%
[Alpha=0.40] Top-5 Accuracy: 26.64%
Result: Top-1: 8.97%, Top-5: 26.64%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.66%
[Alpha=0.40] Top-5 Accuracy: 26.50%
Result: Top-1: 8.66%, Top-5: 26.50%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.59%
[Alpha=0.40] Top-5 Accuracy: 26.44%
Result: Top-1: 8.59%, Top-5: 26.44%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.48%
[Alpha=0.40] Top-5 Accuracy: 26.48%
Result: Top-1: 8.48%, Top-5: 26.48%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.44%
[Alpha=0.40] Top-5 Accuracy: 26.50%
Result: Top-1: 8.44%, Top-5: 26.50%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.43%
[Alpha=0.40] Top-5 Accuracy: 26.37%
Result: Top-1: 8.43%, Top-5: 26.37%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.69%
[Alpha=0.40] Top-5 Accuracy: 26.41%
Result: Top-1: 8.69%, Top-5: 26.41%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.91%
[Alpha=0.40] Top-5 Accuracy: 26.72%
Result: Top-1: 8.91%, Top-5: 26.72%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.19%
[Alpha=0.40] Top-5 Accuracy: 26.17%
Result: Top-1: 8.19%, Top-5: 26.17%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.52%
[Alpha=0.40] Top-5 Accuracy: 26.42%
Result: Top-1: 8.52%, Top-5: 26.42%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.34%
[Alpha=0.40] Top-5 Accuracy: 26.28%
Result: Top-1: 8.34%, Top-5: 26.28%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.56%
[Alpha=0.40] Top-5 Accuracy: 26.36%
Result: Top-1: 8.56%, Top-5: 26.36%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.59%
[Alpha=0.40] Top-5 Accuracy: 25.49%
Result: Top-1: 7.59%, Top-5: 25.49%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.18%
[Alpha=0.40] Top-5 Accuracy: 26.11%
Result: Top-1: 8.18%, Top-5: 26.11%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.45%
[Alpha=0.40] Top-5 Accuracy: 25.58%
Result: Top-1: 7.45%, Top-5: 25.58%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.82%
[Alpha=0.40] Top-5 Accuracy: 25.73%
Result: Top-1: 7.82%, Top-5: 25.73%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.65%
[Alpha=0.40] Top-5 Accuracy: 25.73%
Result: Top-1: 7.65%, Top-5: 25.73%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.88%
[Alpha=0.40] Top-5 Accuracy: 25.87%
Result: Top-1: 7.88%, Top-5: 25.87%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.50%
[Alpha=0.40] Top-5 Accuracy: 25.72%
Result: Top-1: 7.50%, Top-5: 25.72%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.63%
[Alpha=0.40] Top-5 Accuracy: 25.49%
Result: Top-1: 7.63%, Top-5: 25.49%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.37%
[Alpha=0.40] Top-5 Accuracy: 25.63%
Result: Top-1: 7.37%, Top-5: 25.63%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.45%
[Alpha=0.40] Top-5 Accuracy: 26.18%
Result: Top-1: 8.45%, Top-5: 26.18%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.12%
[Alpha=0.40] Top-5 Accuracy: 25.27%
Result: Top-1: 7.12%, Top-5: 25.27%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.41%
[Alpha=0.40] Top-5 Accuracy: 25.51%
Result: Top-1: 7.41%, Top-5: 25.51%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.56%
[Alpha=0.40] Top-5 Accuracy: 25.34%
Result: Top-1: 7.56%, Top-5: 25.34%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.47%
[Alpha=0.40] Top-5 Accuracy: 25.59%
Result: Top-1: 7.47%, Top-5: 25.59%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.45%
[Alpha=0.40] Top-5 Accuracy: 25.44%
Result: Top-1: 7.45%, Top-5: 25.44%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.40%
[Alpha=0.40] Top-5 Accuracy: 25.37%
Result: Top-1: 7.40%, Top-5: 25.37%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.28%
[Alpha=0.40] Top-5 Accuracy: 25.52%
Result: Top-1: 7.28%, Top-5: 25.52%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.81%
[Alpha=0.40] Top-5 Accuracy: 24.61%
Result: Top-1: 6.81%, Top-5: 24.61%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.32%
[Alpha=0.40] Top-5 Accuracy: 25.63%
Result: Top-1: 7.32%, Top-5: 25.63%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 8.19%
[Alpha=0.40] Top-5 Accuracy: 25.64%
Result: Top-1: 8.19%, Top-5: 25.64%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.11%
[Alpha=0.40] Top-5 Accuracy: 24.60%
Result: Top-1: 7.11%, Top-5: 24.60%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.83%
[Alpha=0.40] Top-5 Accuracy: 24.51%
Result: Top-1: 6.83%, Top-5: 24.51%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.22%
[Alpha=0.40] Top-5 Accuracy: 24.77%
Result: Top-1: 7.22%, Top-5: 24.77%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.05%
[Alpha=0.40] Top-5 Accuracy: 24.75%
Result: Top-1: 7.05%, Top-5: 24.75%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.18%
[Alpha=0.40] Top-5 Accuracy: 24.74%
Result: Top-1: 7.18%, Top-5: 24.74%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.92%
[Alpha=0.40] Top-5 Accuracy: 25.02%
Result: Top-1: 6.92%, Top-5: 25.02%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.09%
[Alpha=0.40] Top-5 Accuracy: 24.79%
Result: Top-1: 7.09%, Top-5: 24.79%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.49%
[Alpha=0.40] Top-5 Accuracy: 23.84%
Result: Top-1: 6.49%, Top-5: 23.84%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.09%
[Alpha=0.40] Top-5 Accuracy: 25.10%
Result: Top-1: 7.09%, Top-5: 25.10%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 7.72%
[Alpha=0.40] Top-5 Accuracy: 24.43%
Result: Top-1: 7.72%, Top-5: 24.43%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.51%
[Alpha=0.40] Top-5 Accuracy: 23.45%
Result: Top-1: 6.51%, Top-5: 23.45%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.01%
[Alpha=0.40] Top-5 Accuracy: 22.81%
Result: Top-1: 6.01%, Top-5: 22.81%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.52%
[Alpha=0.40] Top-5 Accuracy: 23.58%
Result: Top-1: 6.52%, Top-5: 23.58%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.04%
[Alpha=0.40] Top-5 Accuracy: 22.73%
Result: Top-1: 6.04%, Top-5: 22.73%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.05%
[Alpha=0.40] Top-5 Accuracy: 23.38%
Result: Top-1: 6.05%, Top-5: 23.38%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.52%
[Alpha=0.40] Top-5 Accuracy: 23.54%
Result: Top-1: 6.52%, Top-5: 23.54%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.22%
[Alpha=0.40] Top-5 Accuracy: 23.21%
Result: Top-1: 6.22%, Top-5: 23.21%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.16%
[Alpha=0.40] Top-5 Accuracy: 23.35%
Result: Top-1: 6.16%, Top-5: 23.35%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 6.51%
[Alpha=0.40] Top-5 Accuracy: 23.51%
Result: Top-1: 6.51%, Top-5: 23.51%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.51%
[Alpha=0.50] Top-5 Accuracy: 23.64%
Result: Top-1: 5.51%, Top-5: 23.64%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.67%
[Alpha=0.50] Top-5 Accuracy: 23.62%
Result: Top-1: 5.67%, Top-5: 23.62%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.46%
[Alpha=0.50] Top-5 Accuracy: 23.24%
Result: Top-1: 5.46%, Top-5: 23.24%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.52%
[Alpha=0.50] Top-5 Accuracy: 23.26%
Result: Top-1: 5.52%, Top-5: 23.26%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.64%
[Alpha=0.50] Top-5 Accuracy: 23.60%
Result: Top-1: 5.64%, Top-5: 23.60%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.71%
[Alpha=0.50] Top-5 Accuracy: 23.47%
Result: Top-1: 5.71%, Top-5: 23.47%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.46%
[Alpha=0.50] Top-5 Accuracy: 23.19%
Result: Top-1: 5.46%, Top-5: 23.19%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.45%
[Alpha=0.50] Top-5 Accuracy: 23.22%
Result: Top-1: 5.45%, Top-5: 23.22%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.68%
[Alpha=0.50] Top-5 Accuracy: 23.80%
Result: Top-1: 5.68%, Top-5: 23.80%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.97%
[Alpha=0.50] Top-5 Accuracy: 23.78%
Result: Top-1: 5.97%, Top-5: 23.78%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.36%
[Alpha=0.50] Top-5 Accuracy: 23.47%
Result: Top-1: 5.36%, Top-5: 23.47%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.87%
[Alpha=0.50] Top-5 Accuracy: 23.47%
Result: Top-1: 5.87%, Top-5: 23.47%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.82%
[Alpha=0.50] Top-5 Accuracy: 23.43%
Result: Top-1: 5.82%, Top-5: 23.43%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.64%
[Alpha=0.50] Top-5 Accuracy: 23.42%
Result: Top-1: 5.64%, Top-5: 23.42%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.59%
[Alpha=0.50] Top-5 Accuracy: 23.26%
Result: Top-1: 5.59%, Top-5: 23.26%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.76%
[Alpha=0.50] Top-5 Accuracy: 23.38%
Result: Top-1: 5.76%, Top-5: 23.38%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 6.11%
[Alpha=0.50] Top-5 Accuracy: 23.93%
Result: Top-1: 6.11%, Top-5: 23.93%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.27%
[Alpha=0.50] Top-5 Accuracy: 23.09%
Result: Top-1: 5.27%, Top-5: 23.09%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.68%
[Alpha=0.50] Top-5 Accuracy: 23.22%
Result: Top-1: 5.68%, Top-5: 23.22%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.54%
[Alpha=0.50] Top-5 Accuracy: 23.22%
Result: Top-1: 5.54%, Top-5: 23.22%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.34%
[Alpha=0.50] Top-5 Accuracy: 23.34%
Result: Top-1: 5.34%, Top-5: 23.34%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.57%
[Alpha=0.50] Top-5 Accuracy: 22.12%
Result: Top-1: 4.57%, Top-5: 22.12%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.18%
[Alpha=0.50] Top-5 Accuracy: 23.00%
Result: Top-1: 5.18%, Top-5: 23.00%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.17%
[Alpha=0.50] Top-5 Accuracy: 22.27%
Result: Top-1: 4.17%, Top-5: 22.27%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.72%
[Alpha=0.50] Top-5 Accuracy: 22.40%
Result: Top-1: 4.72%, Top-5: 22.40%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.49%
[Alpha=0.50] Top-5 Accuracy: 22.55%
Result: Top-1: 4.49%, Top-5: 22.55%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.73%
[Alpha=0.50] Top-5 Accuracy: 22.67%
Result: Top-1: 4.73%, Top-5: 22.67%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.14%
[Alpha=0.50] Top-5 Accuracy: 22.41%
Result: Top-1: 4.14%, Top-5: 22.41%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.30%
[Alpha=0.50] Top-5 Accuracy: 22.27%
Result: Top-1: 4.30%, Top-5: 22.27%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.29%
[Alpha=0.50] Top-5 Accuracy: 22.27%
Result: Top-1: 4.29%, Top-5: 22.27%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.21%
[Alpha=0.50] Top-5 Accuracy: 23.10%
Result: Top-1: 5.21%, Top-5: 23.10%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.19%
[Alpha=0.50] Top-5 Accuracy: 21.72%
Result: Top-1: 4.19%, Top-5: 21.72%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.29%
[Alpha=0.50] Top-5 Accuracy: 22.44%
Result: Top-1: 4.29%, Top-5: 22.44%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.71%
[Alpha=0.50] Top-5 Accuracy: 22.02%
Result: Top-1: 4.71%, Top-5: 22.02%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.49%
[Alpha=0.50] Top-5 Accuracy: 22.27%
Result: Top-1: 4.49%, Top-5: 22.27%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.23%
[Alpha=0.50] Top-5 Accuracy: 22.16%
Result: Top-1: 4.23%, Top-5: 22.16%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.33%
[Alpha=0.50] Top-5 Accuracy: 22.10%
Result: Top-1: 4.33%, Top-5: 22.10%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.28%
[Alpha=0.50] Top-5 Accuracy: 22.05%
Result: Top-1: 4.28%, Top-5: 22.05%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 3.91%
[Alpha=0.50] Top-5 Accuracy: 21.08%
Result: Top-1: 3.91%, Top-5: 21.08%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.62%
[Alpha=0.50] Top-5 Accuracy: 22.29%
Result: Top-1: 4.62%, Top-5: 22.29%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.26%
[Alpha=0.50] Top-5 Accuracy: 22.30%
Result: Top-1: 5.26%, Top-5: 22.30%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.40%
[Alpha=0.50] Top-5 Accuracy: 21.13%
Result: Top-1: 4.40%, Top-5: 21.13%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.19%
[Alpha=0.50] Top-5 Accuracy: 21.17%
Result: Top-1: 4.19%, Top-5: 21.17%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.61%
[Alpha=0.50] Top-5 Accuracy: 21.33%
Result: Top-1: 4.61%, Top-5: 21.33%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.45%
[Alpha=0.50] Top-5 Accuracy: 21.09%
Result: Top-1: 4.45%, Top-5: 21.09%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.68%
[Alpha=0.50] Top-5 Accuracy: 21.31%
Result: Top-1: 4.68%, Top-5: 21.31%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.44%
[Alpha=0.50] Top-5 Accuracy: 21.56%
Result: Top-1: 4.44%, Top-5: 21.56%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.62%
[Alpha=0.50] Top-5 Accuracy: 21.11%
Result: Top-1: 4.62%, Top-5: 21.11%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 3.98%
[Alpha=0.50] Top-5 Accuracy: 20.32%
Result: Top-1: 3.98%, Top-5: 20.32%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.64%
[Alpha=0.50] Top-5 Accuracy: 21.68%
Result: Top-1: 4.64%, Top-5: 21.68%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 5.05%
[Alpha=0.50] Top-5 Accuracy: 20.99%
Result: Top-1: 5.05%, Top-5: 20.99%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.08%
[Alpha=0.50] Top-5 Accuracy: 19.88%
Result: Top-1: 4.08%, Top-5: 19.88%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 3.81%
[Alpha=0.50] Top-5 Accuracy: 19.17%
Result: Top-1: 3.81%, Top-5: 19.17%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.12%
[Alpha=0.50] Top-5 Accuracy: 20.08%
Result: Top-1: 4.12%, Top-5: 20.08%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 3.82%
[Alpha=0.50] Top-5 Accuracy: 19.16%
Result: Top-1: 3.82%, Top-5: 19.16%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 3.58%
[Alpha=0.50] Top-5 Accuracy: 19.94%
Result: Top-1: 3.58%, Top-5: 19.94%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.18%
[Alpha=0.50] Top-5 Accuracy: 19.85%
Result: Top-1: 4.18%, Top-5: 19.85%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 3.73%
[Alpha=0.50] Top-5 Accuracy: 19.65%
Result: Top-1: 3.73%, Top-5: 19.65%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 3.81%
[Alpha=0.50] Top-5 Accuracy: 19.56%
Result: Top-1: 3.81%, Top-5: 19.56%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 4.26%
[Alpha=0.50] Top-5 Accuracy: 19.73%
Result: Top-1: 4.26%, Top-5: 19.73%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.03%
[Alpha=0.60] Top-5 Accuracy: 20.14%
Result: Top-1: 3.03%, Top-5: 20.14%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.07%
[Alpha=0.60] Top-5 Accuracy: 19.93%
Result: Top-1: 3.07%, Top-5: 19.93%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 2.97%
[Alpha=0.60] Top-5 Accuracy: 19.45%
Result: Top-1: 2.97%, Top-5: 19.45%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 2.98%
[Alpha=0.60] Top-5 Accuracy: 19.44%
Result: Top-1: 2.98%, Top-5: 19.44%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.09%
[Alpha=0.60] Top-5 Accuracy: 19.93%
Result: Top-1: 3.09%, Top-5: 19.93%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.18%
[Alpha=0.60] Top-5 Accuracy: 19.64%
Result: Top-1: 3.18%, Top-5: 19.64%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 2.96%
[Alpha=0.60] Top-5 Accuracy: 19.46%
Result: Top-1: 2.96%, Top-5: 19.46%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 2.98%
[Alpha=0.60] Top-5 Accuracy: 19.51%
Result: Top-1: 2.98%, Top-5: 19.51%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.05%
[Alpha=0.60] Top-5 Accuracy: 19.89%
Result: Top-1: 3.05%, Top-5: 19.89%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.35%
[Alpha=0.60] Top-5 Accuracy: 20.17%
Result: Top-1: 3.35%, Top-5: 20.17%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 2.96%
[Alpha=0.60] Top-5 Accuracy: 19.96%
Result: Top-1: 2.96%, Top-5: 19.96%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.34%
[Alpha=0.60] Top-5 Accuracy: 19.73%
Result: Top-1: 3.34%, Top-5: 19.73%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.36%
[Alpha=0.60] Top-5 Accuracy: 19.78%
Result: Top-1: 3.36%, Top-5: 19.78%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.13%
[Alpha=0.60] Top-5 Accuracy: 19.64%
Result: Top-1: 3.13%, Top-5: 19.64%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.11%
[Alpha=0.60] Top-5 Accuracy: 19.43%
Result: Top-1: 3.11%, Top-5: 19.43%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.27%
[Alpha=0.60] Top-5 Accuracy: 19.76%
Result: Top-1: 3.27%, Top-5: 19.76%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.67%
[Alpha=0.60] Top-5 Accuracy: 20.17%
Result: Top-1: 3.67%, Top-5: 20.17%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 2.69%
[Alpha=0.60] Top-5 Accuracy: 19.30%
Result: Top-1: 2.69%, Top-5: 19.30%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 3.09%
[Alpha=0.60] Top-5 Accuracy: 19.37%
Result: Top-1: 3.09%, Top-5: 19.37%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 2.99%
[Alpha=0.60] Top-5 Accuracy: 19.40%
Result: Top-1: 2.99%, Top-5: 19.40%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 2.99%
[Alpha=0.60] Top-5 Accuracy: 19.75%
Result: Top-1: 2.99%, Top-5: 19.75%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=25
============================================================
slurmstepd-jnfat06: error: *** JOB 1659764 ON jnfat06 CANCELLED AT 2025-09-12T13:43:21 ***
