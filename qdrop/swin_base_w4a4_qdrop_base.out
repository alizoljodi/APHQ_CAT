Starting Swin-Base W4A4 QDROP experiment at Thu Sep 11 10:45:01 AM CEST 2025
2025-09-11 10:45:09,625 - INFO - Starting multi-seed experiment
2025-09-11 10:45:09,625 - INFO - Architecture: swin_base
2025-09-11 10:45:09,625 - INFO - Weight bits: 4
2025-09-11 10:45:09,625 - INFO - Activation bits: 4
2025-09-11 10:45:09,625 - INFO - Seeds: [1001, 1002, 1003]
2025-09-11 10:45:09,625 - INFO - Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
2025-09-11 10:45:09,625 - INFO - Cluster numbers: [8, 16, 32, 64, 128, 256]
2025-09-11 10:45:09,625 - INFO - PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]
2025-09-11 10:45:09,625 - INFO - Output directory: ./experiment_results/swin_base_w4_a4_20250911_104509
2025-09-11 10:45:09,625 - INFO - Checking basic requirements...
2025-09-11 10:45:09,626 - INFO - Basic checks passed
2025-09-11 10:45:09,626 - INFO - 
Starting experiments for 3 seeds...
2025-09-11 10:45:09,626 - INFO - Total parameter combinations: 600
2025-09-11 10:45:09,626 - INFO - Total experiments: 1800
2025-09-11 10:45:09,626 - INFO - 
============================================================
2025-09-11 10:45:09,626 - INFO - Running experiment 1/3 for seed 1001
2025-09-11 10:45:09,626 - INFO - ============================================================
2025-09-11 10:45:09,626 - INFO - Running experiment for seed 1001
2025-09-11 10:45:09,626 - INFO - Command: /home/alz07xz/project/PD-Quant/pd_quant/bin/python ../test_quant.py --model swin_base --w_bit 4 --a_bit 4 --seed 1001 --config ../configs/4bit/qdrop_baseline.py --dataset /home/alz07xz/imagenet --calib-size 1000 --calib-batch-size 32 --val-batch-size 500 --num-workers 8 --device cuda --alpha-list 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 --num-clusters-list 8 16 32 64 128 256 --pca-dim-list 1 25 50 75 100 125 150 175 200 220 --calibrate --optimize
2025-09-11 10:45:09,626 - INFO - Working directory: /home/alz07xz/project/APHQ_CAT/qdrop
2025-09-11 10:54:07 - start the process.
Namespace(model='swin_base', config='../configs/4bit/qdrop_baseline.py', dataset='/home/alz07xz/imagenet', val_batch_size=500, num_workers=8, device='cuda', reconstruct_mlp=False, load_reconstruct_checkpoint=False, test_reconstruct_checkpoint=False, calibrate=True, load_calibrate_checkpoint=None, test_calibrate_checkpoint=False, optimize=True, load_optimize_checkpoint='', test_optimize_checkpoint=False, print_freq=10, seed=1001, alpha_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], num_clusters_list=[8, 16, 32, 64, 128, 256], pca_dim_list=[1, 25, 50, 75, 100, 125, 150, 175, 200, 220], w_bit=4, a_bit=4, calib_size=1000, calib_batch_size=32)
deefe /home/alz07xz/project/APHQ_CAT/configs/4bit
Successfully imported Config class!
optim_size: 1024
calib_size: 1000
optim_batch_size: 32
calib_batch_size: 32
w_bit: 4
a_bit: 4
qconv_a_bit: 8
qhead_a_bit: 4
calib_metric: mse
matmul_head_channel_wise: True
token_channel_wise: False
eq_n: 128
search_round: 3
keep_gpu: True
optim_metric: mse
use_mean_hessian: False
temp: 20
recon_metric: hessian_perturb
pct: 0.99
optim_mode: qdrop
drop_prob: 0.5
reconstruct_mlp: False
Building model ...
/home/alz07xz/project/PD-Quant/pd_quant/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
No checkpoint found at './checkpoint/vit_raw/swin_base_patch4_window7_224.bin'
Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)
[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Building validation dataloader ...
Validating on test set on fp ...
Test: [0/100]	Time 15.567 (15.567)	Loss 0.4076 (0.4076)	Prec@1 91.800 (91.800)	Prec@5 98.400 (98.400)
Test: [10/100]	Time 1.047 (2.471)	Loss 0.4707 (0.5107)	Prec@1 91.600 (88.745)	Prec@5 98.800 (98.491)
Test: [20/100]	Time 1.059 (1.896)	Loss 0.5991 (0.5373)	Prec@1 86.000 (88.381)	Prec@5 98.000 (98.171)
Test: [30/100]	Time 1.063 (1.625)	Loss 0.4928 (0.5636)	Prec@1 88.200 (87.555)	Prec@5 99.400 (98.129)
Test: [40/100]	Time 1.065 (1.488)	Loss 0.7451 (0.5610)	Prec@1 82.400 (87.663)	Prec@5 97.000 (98.185)
Test: [50/100]	Time 1.062 (1.405)	Loss 0.9181 (0.6040)	Prec@1 77.800 (86.451)	Prec@5 94.800 (97.808)
Test: [60/100]	Time 1.066 (1.349)	Loss 0.5948 (0.6094)	Prec@1 87.200 (86.338)	Prec@5 96.600 (97.764)
Test: [70/100]	Time 2.548 (1.359)	Loss 0.6936 (0.6248)	Prec@1 84.200 (85.859)	Prec@5 97.800 (97.668)
Test: [80/100]	Time 1.065 (1.340)	Loss 0.4770 (0.6272)	Prec@1 88.400 (85.780)	Prec@5 99.200 (97.602)
Test: [90/100]	Time 1.065 (1.362)	Loss 0.9203 (0.6428)	Prec@1 77.000 (85.305)	Prec@5 95.400 (97.525)
 * Prec@1 85.274 Prec@5 97.568 Loss 0.641 Time 140.722
Wraping quantiztion modules (reparam: False, recon: False) ...
2025-09-11 10:57:08 - start mse guided calibration
  0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   0%|          | 0/149 [00:00<?, ?it/s]calibrating patch_embed.proj:   1%|          | 1/149 [00:14<36:00, 14.60s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|          | 1/149 [00:14<36:00, 14.60s/it]calibrating layers.0.blocks.0.attn.qkv:   1%|▏         | 2/149 [01:33<2:07:59, 52.24s/it]calibrating layers.0.blocks.0.attn.proj:   1%|▏         | 2/149 [01:33<2:07:59, 52.24s/it]calibrating layers.0.blocks.0.attn.proj:   2%|▏         | 3/149 [02:12<1:53:18, 46.56s/it]calibrating layers.0.blocks.0.attn.matmul1:   2%|▏         | 3/149 [02:12<1:53:18, 46.56s/it]calibrating layers.0.blocks.0.attn.matmul1:   3%|▎         | 4/149 [07:29<6:10:19, 153.24s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 4/149 [07:29<6:10:19, 153.24s/it]calibrating layers.0.blocks.0.attn.matmul2:   3%|▎         | 5/149 [12:49<8:31:26, 213.10s/it]calibrating layers.0.blocks.0.mlp.fc1:   3%|▎         | 5/149 [12:49<8:31:26, 213.10s/it]     calibrating layers.0.blocks.0.mlp.fc1:   4%|▍         | 6/149 [15:48<8:00:22, 201.56s/it]calibrating layers.0.blocks.0.mlp.fc2:   4%|▍         | 6/149 [15:48<8:00:22, 201.56s/it]calibrating layers.0.blocks.0.mlp.fc2:   5%|▍         | 7/149 [18:55<7:45:44, 196.79s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▍         | 7/149 [18:55<7:45:44, 196.79s/it]calibrating layers.0.blocks.1.attn.qkv:   5%|▌         | 8/149 [20:13<6:14:13, 159.25s/it]calibrating layers.0.blocks.1.attn.proj:   5%|▌         | 8/149 [20:13<6:14:13, 159.25s/it]calibrating layers.0.blocks.1.attn.proj:   6%|▌         | 9/149 [20:54<4:44:36, 121.98s/it]calibrating layers.0.blocks.1.attn.matmul1:   6%|▌         | 9/149 [20:54<4:44:36, 121.98s/it]calibrating layers.0.blocks.1.attn.matmul1:   7%|▋         | 10/149 [26:09<7:00:54, 181.69s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 10/149 [26:09<7:00:54, 181.69s/it]calibrating layers.0.blocks.1.attn.matmul2:   7%|▋         | 11/149 [31:26<8:33:22, 223.21s/it]calibrating layers.0.blocks.1.mlp.fc1:   7%|▋         | 11/149 [31:26<8:33:22, 223.21s/it]     calibrating layers.0.blocks.1.mlp.fc1:   8%|▊         | 12/149 [34:26<7:59:17, 209.91s/it]calibrating layers.0.blocks.1.mlp.fc2:   8%|▊         | 12/149 [34:26<7:59:17, 209.91s/it]calibrating layers.0.blocks.1.mlp.fc2:   9%|▊         | 13/149 [37:33<7:40:24, 203.12s/it]calibrating layers.1.downsample.reduction:   9%|▊         | 13/149 [37:33<7:40:24, 203.12s/it]calibrating layers.1.downsample.reduction:   9%|▉         | 14/149 [38:15<5:47:21, 154.38s/it]calibrating layers.1.blocks.0.attn.qkv:   9%|▉         | 14/149 [38:15<5:47:21, 154.38s/it]   calibrating layers.1.blocks.0.attn.qkv:  10%|█         | 15/149 [39:01<4:31:38, 121.63s/it]calibrating layers.1.blocks.0.attn.proj:  10%|█         | 15/149 [39:01<4:31:38, 121.63s/it]calibrating layers.1.blocks.0.attn.proj:  11%|█         | 16/149 [39:24<3:23:42, 91.90s/it] calibrating layers.1.blocks.0.attn.matmul1:  11%|█         | 16/149 [39:24<3:23:42, 91.90s/it]calibrating layers.1.blocks.0.attn.matmul1:  11%|█▏        | 17/149 [40:00<2:45:35, 75.27s/it]calibrating layers.1.blocks.0.attn.matmul2:  11%|█▏        | 17/149 [40:00<2:45:35, 75.27s/it]calibrating layers.1.blocks.0.attn.matmul2:  12%|█▏        | 18/149 [40:52<2:28:38, 68.08s/it]calibrating layers.1.blocks.0.mlp.fc1:  12%|█▏        | 18/149 [40:52<2:28:38, 68.08s/it]     calibrating layers.1.blocks.0.mlp.fc1:  13%|█▎        | 19/149 [42:21<2:41:13, 74.41s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 19/149 [42:21<2:41:13, 74.41s/it]calibrating layers.1.blocks.0.mlp.fc2:  13%|█▎        | 20/149 [43:53<2:51:44, 79.88s/it]calibrating layers.1.blocks.1.attn.qkv:  13%|█▎        | 20/149 [43:53<2:51:44, 79.88s/it]calibrating layers.1.blocks.1.attn.qkv:  14%|█▍        | 21/149 [44:41<2:29:47, 70.22s/it]calibrating layers.1.blocks.1.attn.proj:  14%|█▍        | 21/149 [44:41<2:29:47, 70.22s/it]calibrating layers.1.blocks.1.attn.proj:  15%|█▍        | 22/149 [45:05<1:59:23, 56.40s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▍        | 22/149 [45:05<1:59:23, 56.40s/it]calibrating layers.1.blocks.1.attn.matmul1:  15%|█▌        | 23/149 [45:42<1:46:18, 50.62s/it]calibrating layers.1.blocks.1.attn.matmul2:  15%|█▌        | 23/149 [45:42<1:46:18, 50.62s/it]calibrating layers.1.blocks.1.attn.matmul2:  16%|█▌        | 24/149 [46:34<1:46:00, 50.88s/it]calibrating layers.1.blocks.1.mlp.fc1:  16%|█▌        | 24/149 [46:34<1:46:00, 50.88s/it]     calibrating layers.1.blocks.1.mlp.fc1:  17%|█▋        | 25/149 [48:03<2:08:40, 62.26s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 25/149 [48:03<2:08:40, 62.26s/it]calibrating layers.1.blocks.1.mlp.fc2:  17%|█▋        | 26/149 [49:34<2:25:49, 71.13s/it]calibrating layers.2.downsample.reduction:  17%|█▋        | 26/149 [49:34<2:25:49, 71.13s/it]calibrating layers.2.downsample.reduction:  18%|█▊        | 27/149 [50:00<1:56:46, 57.43s/it]calibrating layers.2.blocks.0.attn.qkv:  18%|█▊        | 27/149 [50:00<1:56:46, 57.43s/it]   calibrating layers.2.blocks.0.attn.qkv:  19%|█▉        | 28/149 [50:32<1:40:29, 49.83s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 28/149 [50:32<1:40:29, 49.83s/it]calibrating layers.2.blocks.0.attn.proj:  19%|█▉        | 29/149 [50:47<1:18:33, 39.28s/it]calibrating layers.2.blocks.0.attn.matmul1:  19%|█▉        | 29/149 [50:47<1:18:33, 39.28s/it]calibrating layers.2.blocks.0.attn.matmul1:  20%|██        | 30/149 [51:04<1:04:50, 32.70s/it]calibrating layers.2.blocks.0.attn.matmul2:  20%|██        | 30/149 [51:04<1:04:50, 32.70s/it]calibrating layers.2.blocks.0.attn.matmul2:  21%|██        | 31/149 [51:22<55:28, 28.20s/it]  calibrating layers.2.blocks.0.mlp.fc1:  21%|██        | 31/149 [51:22<55:28, 28.20s/it]     calibrating layers.2.blocks.0.mlp.fc1:  21%|██▏       | 32/149 [52:10<1:06:56, 34.33s/it]calibrating layers.2.blocks.0.mlp.fc2:  21%|██▏       | 32/149 [52:10<1:06:56, 34.33s/it]calibrating layers.2.blocks.0.mlp.fc2:  22%|██▏       | 33/149 [53:00<1:15:12, 38.90s/it]calibrating layers.2.blocks.1.attn.qkv:  22%|██▏       | 33/149 [53:00<1:15:12, 38.90s/it]calibrating layers.2.blocks.1.attn.qkv:  23%|██▎       | 34/149 [53:32<1:10:41, 36.88s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 34/149 [53:32<1:10:41, 36.88s/it]calibrating layers.2.blocks.1.attn.proj:  23%|██▎       | 35/149 [53:47<57:29, 30.26s/it]  calibrating layers.2.blocks.1.attn.matmul1:  23%|██▎       | 35/149 [53:47<57:29, 30.26s/it]calibrating layers.2.blocks.1.attn.matmul1:  24%|██▍       | 36/149 [54:04<49:37, 26.35s/it]calibrating layers.2.blocks.1.attn.matmul2:  24%|██▍       | 36/149 [54:04<49:37, 26.35s/it]calibrating layers.2.blocks.1.attn.matmul2:  25%|██▍       | 37/149 [54:22<44:17, 23.73s/it]calibrating layers.2.blocks.1.mlp.fc1:  25%|██▍       | 37/149 [54:22<44:17, 23.73s/it]     calibrating layers.2.blocks.1.mlp.fc1:  26%|██▌       | 38/149 [55:10<57:39, 31.17s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 38/149 [55:10<57:39, 31.17s/it]calibrating layers.2.blocks.1.mlp.fc2:  26%|██▌       | 39/149 [56:00<1:07:17, 36.70s/it]calibrating layers.2.blocks.2.attn.qkv:  26%|██▌       | 39/149 [56:00<1:07:17, 36.70s/it]calibrating layers.2.blocks.2.attn.qkv:  27%|██▋       | 40/149 [56:32<1:04:19, 35.41s/it]calibrating layers.2.blocks.2.attn.proj:  27%|██▋       | 40/149 [56:32<1:04:19, 35.41s/it]calibrating layers.2.blocks.2.attn.proj:  28%|██▊       | 41/149 [56:47<52:35, 29.22s/it]  calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 41/149 [56:47<52:35, 29.22s/it]calibrating layers.2.blocks.2.attn.matmul1:  28%|██▊       | 42/149 [57:05<45:50, 25.70s/it]calibrating layers.2.blocks.2.attn.matmul2:  28%|██▊       | 42/149 [57:05<45:50, 25.70s/it]calibrating layers.2.blocks.2.attn.matmul2:  29%|██▉       | 43/149 [57:22<41:14, 23.34s/it]calibrating layers.2.blocks.2.mlp.fc1:  29%|██▉       | 43/149 [57:22<41:14, 23.34s/it]     calibrating layers.2.blocks.2.mlp.fc1:  30%|██▉       | 44/149 [58:11<53:58, 30.84s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|██▉       | 44/149 [58:11<53:58, 30.84s/it]calibrating layers.2.blocks.2.mlp.fc2:  30%|███       | 45/149 [59:00<1:03:04, 36.39s/it]calibrating layers.2.blocks.3.attn.qkv:  30%|███       | 45/149 [59:00<1:03:04, 36.39s/it]calibrating layers.2.blocks.3.attn.qkv:  31%|███       | 46/149 [59:32<1:00:13, 35.08s/it]calibrating layers.2.blocks.3.attn.proj:  31%|███       | 46/149 [59:32<1:00:13, 35.08s/it]calibrating layers.2.blocks.3.attn.proj:  32%|███▏      | 47/149 [59:47<49:14, 28.96s/it]  calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 47/149 [59:47<49:14, 28.96s/it]calibrating layers.2.blocks.3.attn.matmul1:  32%|███▏      | 48/149 [1:00:04<42:51, 25.46s/it]calibrating layers.2.blocks.3.attn.matmul2:  32%|███▏      | 48/149 [1:00:04<42:51, 25.46s/it]calibrating layers.2.blocks.3.attn.matmul2:  33%|███▎      | 49/149 [1:00:22<38:35, 23.16s/it]calibrating layers.2.blocks.3.mlp.fc1:  33%|███▎      | 49/149 [1:00:22<38:35, 23.16s/it]     calibrating layers.2.blocks.3.mlp.fc1:  34%|███▎      | 50/149 [1:01:11<50:53, 30.84s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▎      | 50/149 [1:01:11<50:53, 30.84s/it]calibrating layers.2.blocks.3.mlp.fc2:  34%|███▍      | 51/149 [1:02:01<59:44, 36.58s/it]calibrating layers.2.blocks.4.attn.qkv:  34%|███▍      | 51/149 [1:02:01<59:44, 36.58s/it]calibrating layers.2.blocks.4.attn.qkv:  35%|███▍      | 52/149 [1:02:33<57:11, 35.38s/it]calibrating layers.2.blocks.4.attn.proj:  35%|███▍      | 52/149 [1:02:33<57:11, 35.38s/it]calibrating layers.2.blocks.4.attn.proj:  36%|███▌      | 53/149 [1:02:48<46:44, 29.21s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 53/149 [1:02:48<46:44, 29.21s/it]calibrating layers.2.blocks.4.attn.matmul1:  36%|███▌      | 54/149 [1:03:05<40:36, 25.65s/it]calibrating layers.2.blocks.4.attn.matmul2:  36%|███▌      | 54/149 [1:03:05<40:36, 25.65s/it]calibrating layers.2.blocks.4.attn.matmul2:  37%|███▋      | 55/149 [1:03:23<36:29, 23.29s/it]calibrating layers.2.blocks.4.mlp.fc1:  37%|███▋      | 55/149 [1:03:23<36:29, 23.29s/it]     calibrating layers.2.blocks.4.mlp.fc1:  38%|███▊      | 56/149 [1:04:12<47:57, 30.94s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 56/149 [1:04:12<47:57, 30.94s/it]calibrating layers.2.blocks.4.mlp.fc2:  38%|███▊      | 57/149 [1:05:02<56:04, 36.57s/it]calibrating layers.2.blocks.5.attn.qkv:  38%|███▊      | 57/149 [1:05:02<56:04, 36.57s/it]calibrating layers.2.blocks.5.attn.qkv:  39%|███▉      | 58/149 [1:05:34<53:35, 35.34s/it]calibrating layers.2.blocks.5.attn.proj:  39%|███▉      | 58/149 [1:05:34<53:35, 35.34s/it]calibrating layers.2.blocks.5.attn.proj:  40%|███▉      | 59/149 [1:05:49<43:47, 29.19s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|███▉      | 59/149 [1:05:49<43:47, 29.19s/it]calibrating layers.2.blocks.5.attn.matmul1:  40%|████      | 60/149 [1:06:06<38:04, 25.67s/it]calibrating layers.2.blocks.5.attn.matmul2:  40%|████      | 60/149 [1:06:06<38:04, 25.67s/it]calibrating layers.2.blocks.5.attn.matmul2:  41%|████      | 61/149 [1:06:24<34:15, 23.36s/it]calibrating layers.2.blocks.5.mlp.fc1:  41%|████      | 61/149 [1:06:24<34:15, 23.36s/it]     calibrating layers.2.blocks.5.mlp.fc1:  42%|████▏     | 62/149 [1:07:13<44:55, 30.98s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 62/149 [1:07:13<44:55, 30.98s/it]calibrating layers.2.blocks.5.mlp.fc2:  42%|████▏     | 63/149 [1:08:03<52:26, 36.58s/it]calibrating layers.2.blocks.6.attn.qkv:  42%|████▏     | 63/149 [1:08:03<52:26, 36.58s/it]calibrating layers.2.blocks.6.attn.qkv:  43%|████▎     | 64/149 [1:08:35<50:00, 35.30s/it]calibrating layers.2.blocks.6.attn.proj:  43%|████▎     | 64/149 [1:08:35<50:00, 35.30s/it]calibrating layers.2.blocks.6.attn.proj:  44%|████▎     | 65/149 [1:08:50<40:49, 29.16s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▎     | 65/149 [1:08:50<40:49, 29.16s/it]calibrating layers.2.blocks.6.attn.matmul1:  44%|████▍     | 66/149 [1:09:07<35:22, 25.57s/it]calibrating layers.2.blocks.6.attn.matmul2:  44%|████▍     | 66/149 [1:09:07<35:22, 25.57s/it]calibrating layers.2.blocks.6.attn.matmul2:  45%|████▍     | 67/149 [1:09:24<31:36, 23.13s/it]calibrating layers.2.blocks.6.mlp.fc1:  45%|████▍     | 67/149 [1:09:24<31:36, 23.13s/it]     calibrating layers.2.blocks.6.mlp.fc1:  46%|████▌     | 68/149 [1:10:13<41:36, 30.82s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▌     | 68/149 [1:10:13<41:36, 30.82s/it]calibrating layers.2.blocks.6.mlp.fc2:  46%|████▋     | 69/149 [1:11:03<48:41, 36.52s/it]calibrating layers.2.blocks.7.attn.qkv:  46%|████▋     | 69/149 [1:11:03<48:41, 36.52s/it]calibrating layers.2.blocks.7.attn.qkv:  47%|████▋     | 70/149 [1:11:36<46:31, 35.34s/it]calibrating layers.2.blocks.7.attn.proj:  47%|████▋     | 70/149 [1:11:36<46:31, 35.34s/it]calibrating layers.2.blocks.7.attn.proj:  48%|████▊     | 71/149 [1:11:50<37:56, 29.18s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 71/149 [1:11:50<37:56, 29.18s/it]calibrating layers.2.blocks.7.attn.matmul1:  48%|████▊     | 72/149 [1:12:08<32:51, 25.60s/it]calibrating layers.2.blocks.7.attn.matmul2:  48%|████▊     | 72/149 [1:12:08<32:51, 25.60s/it]calibrating layers.2.blocks.7.attn.matmul2:  49%|████▉     | 73/149 [1:12:26<29:32, 23.32s/it]calibrating layers.2.blocks.7.mlp.fc1:  49%|████▉     | 73/149 [1:12:26<29:32, 23.32s/it]     calibrating layers.2.blocks.7.mlp.fc1:  50%|████▉     | 74/149 [1:13:14<38:36, 30.89s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|████▉     | 74/149 [1:13:14<38:36, 30.89s/it]calibrating layers.2.blocks.7.mlp.fc2:  50%|█████     | 75/149 [1:14:04<45:03, 36.53s/it]calibrating layers.2.blocks.8.attn.qkv:  50%|█████     | 75/149 [1:14:04<45:03, 36.53s/it]calibrating layers.2.blocks.8.attn.qkv:  51%|█████     | 76/149 [1:14:37<43:02, 35.38s/it]calibrating layers.2.blocks.8.attn.proj:  51%|█████     | 76/149 [1:14:37<43:02, 35.38s/it]calibrating layers.2.blocks.8.attn.proj:  52%|█████▏    | 77/149 [1:14:52<35:04, 29.23s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 77/149 [1:14:52<35:04, 29.23s/it]calibrating layers.2.blocks.8.attn.matmul1:  52%|█████▏    | 78/149 [1:15:09<30:22, 25.67s/it]calibrating layers.2.blocks.8.attn.matmul2:  52%|█████▏    | 78/149 [1:15:09<30:22, 25.67s/it]calibrating layers.2.blocks.8.attn.matmul2:  53%|█████▎    | 79/149 [1:15:26<27:03, 23.19s/it]calibrating layers.2.blocks.8.mlp.fc1:  53%|█████▎    | 79/149 [1:15:26<27:03, 23.19s/it]     calibrating layers.2.blocks.8.mlp.fc1:  54%|█████▎    | 80/149 [1:16:15<35:30, 30.88s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▎    | 80/149 [1:16:15<35:30, 30.88s/it]calibrating layers.2.blocks.8.mlp.fc2:  54%|█████▍    | 81/149 [1:17:05<41:26, 36.56s/it]calibrating layers.2.blocks.9.attn.qkv:  54%|█████▍    | 81/149 [1:17:05<41:26, 36.56s/it]calibrating layers.2.blocks.9.attn.qkv:  55%|█████▌    | 82/149 [1:17:37<39:25, 35.30s/it]calibrating layers.2.blocks.9.attn.proj:  55%|█████▌    | 82/149 [1:17:37<39:25, 35.30s/it]calibrating layers.2.blocks.9.attn.proj:  56%|█████▌    | 83/149 [1:17:52<32:02, 29.13s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▌    | 83/149 [1:17:52<32:02, 29.13s/it]calibrating layers.2.blocks.9.attn.matmul1:  56%|█████▋    | 84/149 [1:18:09<27:45, 25.62s/it]calibrating layers.2.blocks.9.attn.matmul2:  56%|█████▋    | 84/149 [1:18:09<27:45, 25.62s/it]calibrating layers.2.blocks.9.attn.matmul2:  57%|█████▋    | 85/149 [1:18:27<24:40, 23.14s/it]calibrating layers.2.blocks.9.mlp.fc1:  57%|█████▋    | 85/149 [1:18:27<24:40, 23.14s/it]     calibrating layers.2.blocks.9.mlp.fc1:  58%|█████▊    | 86/149 [1:19:15<32:12, 30.68s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 86/149 [1:19:15<32:12, 30.68s/it]calibrating layers.2.blocks.9.mlp.fc2:  58%|█████▊    | 87/149 [1:20:05<37:31, 36.31s/it]calibrating layers.2.blocks.10.attn.qkv:  58%|█████▊    | 87/149 [1:20:05<37:31, 36.31s/it]calibrating layers.2.blocks.10.attn.qkv:  59%|█████▉    | 88/149 [1:20:37<35:42, 35.12s/it]calibrating layers.2.blocks.10.attn.proj:  59%|█████▉    | 88/149 [1:20:37<35:42, 35.12s/it]calibrating layers.2.blocks.10.attn.proj:  60%|█████▉    | 89/149 [1:20:52<29:02, 29.04s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|█████▉    | 89/149 [1:20:52<29:02, 29.04s/it]calibrating layers.2.blocks.10.attn.matmul1:  60%|██████    | 90/149 [1:21:09<25:07, 25.55s/it]calibrating layers.2.blocks.10.attn.matmul2:  60%|██████    | 90/149 [1:21:09<25:07, 25.55s/it]calibrating layers.2.blocks.10.attn.matmul2:  61%|██████    | 91/149 [1:21:27<22:22, 23.15s/it]calibrating layers.2.blocks.10.mlp.fc1:  61%|██████    | 91/149 [1:21:27<22:22, 23.15s/it]     calibrating layers.2.blocks.10.mlp.fc1:  62%|██████▏   | 92/149 [1:22:15<29:11, 30.74s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 92/149 [1:22:15<29:11, 30.74s/it]calibrating layers.2.blocks.10.mlp.fc2:  62%|██████▏   | 93/149 [1:23:05<33:56, 36.36s/it]calibrating layers.2.blocks.11.attn.qkv:  62%|██████▏   | 93/149 [1:23:05<33:56, 36.36s/it]calibrating layers.2.blocks.11.attn.qkv:  63%|██████▎   | 94/149 [1:23:37<32:14, 35.17s/it]calibrating layers.2.blocks.11.attn.proj:  63%|██████▎   | 94/149 [1:23:37<32:14, 35.17s/it]calibrating layers.2.blocks.11.attn.proj:  64%|██████▍   | 95/149 [1:23:52<26:07, 29.03s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 95/149 [1:23:52<26:07, 29.03s/it]calibrating layers.2.blocks.11.attn.matmul1:  64%|██████▍   | 96/149 [1:24:09<22:37, 25.61s/it]calibrating layers.2.blocks.11.attn.matmul2:  64%|██████▍   | 96/149 [1:24:09<22:37, 25.61s/it]calibrating layers.2.blocks.11.attn.matmul2:  65%|██████▌   | 97/149 [1:24:27<20:07, 23.22s/it]calibrating layers.2.blocks.11.mlp.fc1:  65%|██████▌   | 97/149 [1:24:27<20:07, 23.22s/it]     calibrating layers.2.blocks.11.mlp.fc1:  66%|██████▌   | 98/149 [1:25:16<26:16, 30.92s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▌   | 98/149 [1:25:16<26:16, 30.92s/it]calibrating layers.2.blocks.11.mlp.fc2:  66%|██████▋   | 99/149 [1:26:06<30:30, 36.60s/it]calibrating layers.2.blocks.12.attn.qkv:  66%|██████▋   | 99/149 [1:26:06<30:30, 36.60s/it]calibrating layers.2.blocks.12.attn.qkv:  67%|██████▋   | 100/149 [1:26:38<28:53, 35.37s/it]calibrating layers.2.blocks.12.attn.proj:  67%|██████▋   | 100/149 [1:26:38<28:53, 35.37s/it]calibrating layers.2.blocks.12.attn.proj:  68%|██████▊   | 101/149 [1:26:53<23:21, 29.19s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 101/149 [1:26:53<23:21, 29.19s/it]calibrating layers.2.blocks.12.attn.matmul1:  68%|██████▊   | 102/149 [1:27:10<20:04, 25.63s/it]calibrating layers.2.blocks.12.attn.matmul2:  68%|██████▊   | 102/149 [1:27:10<20:04, 25.63s/it]calibrating layers.2.blocks.12.attn.matmul2:  69%|██████▉   | 103/149 [1:27:28<17:44, 23.15s/it]calibrating layers.2.blocks.12.mlp.fc1:  69%|██████▉   | 103/149 [1:27:28<17:44, 23.15s/it]     calibrating layers.2.blocks.12.mlp.fc1:  70%|██████▉   | 104/149 [1:28:16<23:04, 30.78s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|██████▉   | 104/149 [1:28:16<23:04, 30.78s/it]calibrating layers.2.blocks.12.mlp.fc2:  70%|███████   | 105/149 [1:29:06<26:39, 36.35s/it]calibrating layers.2.blocks.13.attn.qkv:  70%|███████   | 105/149 [1:29:06<26:39, 36.35s/it]calibrating layers.2.blocks.13.attn.qkv:  71%|███████   | 106/149 [1:29:38<25:09, 35.11s/it]calibrating layers.2.blocks.13.attn.proj:  71%|███████   | 106/149 [1:29:38<25:09, 35.11s/it]calibrating layers.2.blocks.13.attn.proj:  72%|███████▏  | 107/149 [1:29:53<20:18, 29.01s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 107/149 [1:29:53<20:18, 29.01s/it]calibrating layers.2.blocks.13.attn.matmul1:  72%|███████▏  | 108/149 [1:30:10<17:26, 25.52s/it]calibrating layers.2.blocks.13.attn.matmul2:  72%|███████▏  | 108/149 [1:30:10<17:26, 25.52s/it]calibrating layers.2.blocks.13.attn.matmul2:  73%|███████▎  | 109/149 [1:30:28<15:28, 23.22s/it]calibrating layers.2.blocks.13.mlp.fc1:  73%|███████▎  | 109/149 [1:30:28<15:28, 23.22s/it]     calibrating layers.2.blocks.13.mlp.fc1:  74%|███████▍  | 110/149 [1:31:16<20:02, 30.82s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 110/149 [1:31:16<20:02, 30.82s/it]calibrating layers.2.blocks.13.mlp.fc2:  74%|███████▍  | 111/149 [1:32:06<23:08, 36.53s/it]calibrating layers.2.blocks.14.attn.qkv:  74%|███████▍  | 111/149 [1:32:06<23:08, 36.53s/it]calibrating layers.2.blocks.14.attn.qkv:  75%|███████▌  | 112/149 [1:32:39<21:47, 35.35s/it]calibrating layers.2.blocks.14.attn.proj:  75%|███████▌  | 112/149 [1:32:39<21:47, 35.35s/it]calibrating layers.2.blocks.14.attn.proj:  76%|███████▌  | 113/149 [1:32:54<17:30, 29.17s/it]calibrating layers.2.blocks.14.attn.matmul1:  76%|███████▌  | 113/149 [1:32:54<17:30, 29.17s/it]calibrating layers.2.blocks.14.attn.matmul1:  77%|███████▋  | 114/149 [1:33:11<14:59, 25.69s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 114/149 [1:33:11<14:59, 25.69s/it]calibrating layers.2.blocks.14.attn.matmul2:  77%|███████▋  | 115/149 [1:33:29<13:13, 23.33s/it]calibrating layers.2.blocks.14.mlp.fc1:  77%|███████▋  | 115/149 [1:33:29<13:13, 23.33s/it]     calibrating layers.2.blocks.14.mlp.fc1:  78%|███████▊  | 116/149 [1:34:18<17:00, 30.92s/it]calibrating layers.2.blocks.14.mlp.fc2:  78%|███████▊  | 116/149 [1:34:18<17:00, 30.92s/it]calibrating layers.2.blocks.14.mlp.fc2:  79%|███████▊  | 117/149 [1:35:07<19:30, 36.59s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▊  | 117/149 [1:35:07<19:30, 36.59s/it]calibrating layers.2.blocks.15.attn.qkv:  79%|███████▉  | 118/149 [1:35:40<18:16, 35.38s/it]calibrating layers.2.blocks.15.attn.proj:  79%|███████▉  | 118/149 [1:35:40<18:16, 35.38s/it]calibrating layers.2.blocks.15.attn.proj:  80%|███████▉  | 119/149 [1:35:55<14:36, 29.22s/it]calibrating layers.2.blocks.15.attn.matmul1:  80%|███████▉  | 119/149 [1:35:55<14:36, 29.22s/it]calibrating layers.2.blocks.15.attn.matmul1:  81%|████████  | 120/149 [1:36:12<12:24, 25.67s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 120/149 [1:36:12<12:24, 25.67s/it]calibrating layers.2.blocks.15.attn.matmul2:  81%|████████  | 121/149 [1:36:30<10:53, 23.35s/it]calibrating layers.2.blocks.15.mlp.fc1:  81%|████████  | 121/149 [1:36:30<10:53, 23.35s/it]     calibrating layers.2.blocks.15.mlp.fc1:  82%|████████▏ | 122/149 [1:37:19<13:56, 30.97s/it]calibrating layers.2.blocks.15.mlp.fc2:  82%|████████▏ | 122/149 [1:37:19<13:56, 30.97s/it]calibrating layers.2.blocks.15.mlp.fc2:  83%|████████▎ | 123/149 [1:38:09<15:50, 36.57s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 123/149 [1:38:09<15:50, 36.57s/it]calibrating layers.2.blocks.16.attn.qkv:  83%|████████▎ | 124/149 [1:38:41<14:43, 35.32s/it]calibrating layers.2.blocks.16.attn.proj:  83%|████████▎ | 124/149 [1:38:41<14:43, 35.32s/it]calibrating layers.2.blocks.16.attn.proj:  84%|████████▍ | 125/149 [1:38:56<11:39, 29.13s/it]calibrating layers.2.blocks.16.attn.matmul1:  84%|████████▍ | 125/149 [1:38:56<11:39, 29.13s/it]calibrating layers.2.blocks.16.attn.matmul1:  85%|████████▍ | 126/149 [1:39:13<09:49, 25.62s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▍ | 126/149 [1:39:13<09:49, 25.62s/it]calibrating layers.2.blocks.16.attn.matmul2:  85%|████████▌ | 127/149 [1:39:31<08:30, 23.22s/it]calibrating layers.2.blocks.16.mlp.fc1:  85%|████████▌ | 127/149 [1:39:31<08:30, 23.22s/it]     calibrating layers.2.blocks.16.mlp.fc1:  86%|████████▌ | 128/149 [1:40:20<10:49, 30.94s/it]calibrating layers.2.blocks.16.mlp.fc2:  86%|████████▌ | 128/149 [1:40:20<10:49, 30.94s/it]calibrating layers.2.blocks.16.mlp.fc2:  87%|████████▋ | 129/149 [1:41:09<12:10, 36.55s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 129/149 [1:41:09<12:10, 36.55s/it]calibrating layers.2.blocks.17.attn.qkv:  87%|████████▋ | 130/149 [1:41:42<11:10, 35.31s/it]calibrating layers.2.blocks.17.attn.proj:  87%|████████▋ | 130/149 [1:41:42<11:10, 35.31s/it]calibrating layers.2.blocks.17.attn.proj:  88%|████████▊ | 131/149 [1:41:56<08:44, 29.15s/it]calibrating layers.2.blocks.17.attn.matmul1:  88%|████████▊ | 131/149 [1:41:56<08:44, 29.15s/it]calibrating layers.2.blocks.17.attn.matmul1:  89%|████████▊ | 132/149 [1:42:14<07:16, 25.66s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▊ | 132/149 [1:42:14<07:16, 25.66s/it]calibrating layers.2.blocks.17.attn.matmul2:  89%|████████▉ | 133/149 [1:42:32<06:13, 23.37s/it]calibrating layers.2.blocks.17.mlp.fc1:  89%|████████▉ | 133/149 [1:42:32<06:13, 23.37s/it]     calibrating layers.2.blocks.17.mlp.fc1:  90%|████████▉ | 134/149 [1:43:21<07:43, 30.92s/it]calibrating layers.2.blocks.17.mlp.fc2:  90%|████████▉ | 134/149 [1:43:21<07:43, 30.92s/it]calibrating layers.2.blocks.17.mlp.fc2:  91%|█████████ | 135/149 [1:44:10<08:30, 36.48s/it]calibrating layers.3.downsample.reduction:  91%|█████████ | 135/149 [1:44:10<08:30, 36.48s/it]calibrating layers.3.downsample.reduction:  91%|█████████▏| 136/149 [1:44:27<06:38, 30.68s/it]calibrating layers.3.blocks.0.attn.qkv:  91%|█████████▏| 136/149 [1:44:27<06:38, 30.68s/it]   calibrating layers.3.blocks.0.attn.qkv:  92%|█████████▏| 137/149 [1:44:53<05:49, 29.09s/it]calibrating layers.3.blocks.0.attn.proj:  92%|█████████▏| 137/149 [1:44:53<05:49, 29.09s/it]calibrating layers.3.blocks.0.attn.proj:  93%|█████████▎| 138/149 [1:45:04<04:20, 23.70s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 138/149 [1:45:04<04:20, 23.70s/it]calibrating layers.3.blocks.0.attn.matmul1:  93%|█████████▎| 139/149 [1:45:21<03:36, 21.65s/it]calibrating layers.3.blocks.0.attn.matmul2:  93%|█████████▎| 139/149 [1:45:21<03:36, 21.65s/it]calibrating layers.3.blocks.0.attn.matmul2:  94%|█████████▍| 140/149 [1:45:36<02:59, 19.93s/it]calibrating layers.3.blocks.0.mlp.fc1:  94%|█████████▍| 140/149 [1:45:36<02:59, 19.93s/it]     calibrating layers.3.blocks.0.mlp.fc1:  95%|█████████▍| 141/149 [1:46:09<03:08, 23.57s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▍| 141/149 [1:46:09<03:08, 23.57s/it]calibrating layers.3.blocks.0.mlp.fc2:  95%|█████████▌| 142/149 [1:46:41<03:03, 26.23s/it]calibrating layers.3.blocks.1.attn.qkv:  95%|█████████▌| 142/149 [1:46:41<03:03, 26.23s/it]calibrating layers.3.blocks.1.attn.qkv:  96%|█████████▌| 143/149 [1:47:06<02:35, 25.98s/it]calibrating layers.3.blocks.1.attn.proj:  96%|█████████▌| 143/149 [1:47:06<02:35, 25.98s/it]calibrating layers.3.blocks.1.attn.proj:  97%|█████████▋| 144/149 [1:47:17<01:47, 21.51s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 144/149 [1:47:17<01:47, 21.51s/it]calibrating layers.3.blocks.1.attn.matmul1:  97%|█████████▋| 145/149 [1:47:34<01:20, 20.08s/it]calibrating layers.3.blocks.1.attn.matmul2:  97%|█████████▋| 145/149 [1:47:34<01:20, 20.08s/it]calibrating layers.3.blocks.1.attn.matmul2:  98%|█████████▊| 146/149 [1:47:50<00:56, 18.83s/it]calibrating layers.3.blocks.1.mlp.fc1:  98%|█████████▊| 146/149 [1:47:50<00:56, 18.83s/it]     calibrating layers.3.blocks.1.mlp.fc1:  99%|█████████▊| 147/149 [1:48:22<00:45, 22.80s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▊| 147/149 [1:48:22<00:45, 22.80s/it]calibrating layers.3.blocks.1.mlp.fc2:  99%|█████████▉| 148/149 [1:48:55<00:25, 25.78s/it]calibrating head.fc:  99%|█████████▉| 148/149 [1:48:55<00:25, 25.78s/it]                  calibrating head.fc: 100%|██████████| 149/149 [1:48:59<00:00, 19.25s/it]calibrating head.fc: 100%|██████████| 149/149 [1:48:59<00:00, 43.89s/it]
2025-09-11 12:46:23 - mse guided calibration finished.
Saving checkpoint to ./checkpoint/quant_result/20250911_1054/swin_base_w4_a4_calibsize_1000_mse.pth
Validating after calibration ...
Test: [0/100]	Time 5.350 (5.350)	Loss 1.0320 (1.0320)	Prec@1 89.200 (89.200)	Prec@5 97.600 (97.600)
Test: [10/100]	Time 2.369 (2.643)	Loss 1.0456 (1.1400)	Prec@1 85.800 (84.891)	Prec@5 96.600 (97.073)
Test: [20/100]	Time 2.373 (2.514)	Loss 1.4605 (1.2148)	Prec@1 79.000 (83.400)	Prec@5 97.400 (96.619)
Test: [30/100]	Time 2.373 (2.468)	Loss 1.1066 (1.2517)	Prec@1 83.800 (82.542)	Prec@5 98.400 (96.574)
Test: [40/100]	Time 2.376 (2.445)	Loss 1.2477 (1.2226)	Prec@1 78.600 (82.980)	Prec@5 95.200 (96.683)
Test: [50/100]	Time 2.371 (2.430)	Loss 1.5509 (1.2538)	Prec@1 73.600 (81.631)	Prec@5 90.800 (96.157)
Test: [60/100]	Time 2.368 (2.421)	Loss 1.1925 (1.2543)	Prec@1 82.400 (81.380)	Prec@5 94.800 (95.948)
Test: [70/100]	Time 2.368 (2.413)	Loss 1.4678 (1.2748)	Prec@1 76.600 (80.631)	Prec@5 94.600 (95.707)
Test: [80/100]	Time 2.369 (2.408)	Loss 1.1318 (1.2823)	Prec@1 84.000 (80.395)	Prec@5 96.400 (95.469)
Test: [90/100]	Time 2.373 (2.404)	Loss 1.4581 (1.3003)	Prec@1 72.400 (79.736)	Prec@5 92.200 (95.308)
 * Prec@1 79.842 Prec@5 95.424 Loss 1.286 Time 240.271
Building calibrator ...
2025-09-11 12:50:28 - start mse guided block reconstruction
reconstructing patch_embed ...
initializing raw input and raw output ...
adaround training for patch_embed ...
wraping quantizers in patch_embed ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.134 (rec:0.134, round:0.000)	b=0.00	count=500
Total loss:	0.092 (rec:0.092, round:0.000)	b=0.00	count=1000
Total loss:	0.050 (rec:0.050, round:0.000)	b=0.00	count=1500
Total loss:	0.043 (rec:0.043, round:0.000)	b=0.00	count=2000
Total loss:	0.033 (rec:0.033, round:0.000)	b=0.00	count=2500
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=3000
Total loss:	0.026 (rec:0.026, round:0.000)	b=0.00	count=3500
Total loss:	57.818 (rec:0.014, round:57.803)	b=20.00	count=4000
Total loss:	37.671 (rec:0.030, round:37.641)	b=19.44	count=4500
Total loss:	34.911 (rec:0.020, round:34.891)	b=18.88	count=5000
Total loss:	33.435 (rec:0.023, round:33.412)	b=18.31	count=5500
Total loss:	31.987 (rec:0.026, round:31.962)	b=17.75	count=6000
Total loss:	30.500 (rec:0.021, round:30.480)	b=17.19	count=6500
Total loss:	29.080 (rec:0.020, round:29.060)	b=16.62	count=7000
Total loss:	27.648 (rec:0.023, round:27.625)	b=16.06	count=7500
Total loss:	26.268 (rec:0.017, round:26.251)	b=15.50	count=8000
Total loss:	24.625 (rec:0.019, round:24.607)	b=14.94	count=8500
Total loss:	23.176 (rec:0.029, round:23.147)	b=14.38	count=9000
Total loss:	21.403 (rec:0.040, round:21.363)	b=13.81	count=9500
Total loss:	20.037 (rec:0.032, round:20.004)	b=13.25	count=10000
Total loss:	18.457 (rec:0.035, round:18.422)	b=12.69	count=10500
Total loss:	16.863 (rec:0.034, round:16.830)	b=12.12	count=11000
Total loss:	15.207 (rec:0.046, round:15.161)	b=11.56	count=11500
Total loss:	13.500 (rec:0.037, round:13.463)	b=11.00	count=12000
Total loss:	12.013 (rec:0.049, round:11.964)	b=10.44	count=12500
Total loss:	10.310 (rec:0.057, round:10.253)	b=9.88	count=13000
Total loss:	9.014 (rec:0.062, round:8.952)	b=9.31	count=13500
Total loss:	7.833 (rec:0.082, round:7.752)	b=8.75	count=14000
Total loss:	6.605 (rec:0.076, round:6.529)	b=8.19	count=14500
Total loss:	5.477 (rec:0.081, round:5.395)	b=7.62	count=15000
Total loss:	4.444 (rec:0.133, round:4.311)	b=7.06	count=15500
Total loss:	3.431 (rec:0.135, round:3.297)	b=6.50	count=16000
Total loss:	2.645 (rec:0.153, round:2.493)	b=5.94	count=16500
Total loss:	2.052 (rec:0.201, round:1.851)	b=5.38	count=17000
Total loss:	1.593 (rec:0.237, round:1.356)	b=4.81	count=17500
Total loss:	1.297 (rec:0.286, round:1.011)	b=4.25	count=18000
Total loss:	1.061 (rec:0.321, round:0.740)	b=3.69	count=18500
Total loss:	0.830 (rec:0.310, round:0.521)	b=3.12	count=19000
Total loss:	0.717 (rec:0.394, round:0.323)	b=2.56	count=19500
Total loss:	0.662 (rec:0.444, round:0.218)	b=2.00	count=20000
finished reconstructing patch_embed.
reconstructing layers.0.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.0.blocks.0 ...
wraping quantizers in layers.0.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.236 (rec:1.236, round:0.000)	b=0.00	count=500
Total loss:	1.145 (rec:1.145, round:0.000)	b=0.00	count=1000
Total loss:	1.115 (rec:1.115, round:0.000)	b=0.00	count=1500
Total loss:	0.968 (rec:0.968, round:0.000)	b=0.00	count=2000
Total loss:	0.985 (rec:0.985, round:0.000)	b=0.00	count=2500
Total loss:	0.883 (rec:0.883, round:0.000)	b=0.00	count=3000
Total loss:	0.906 (rec:0.906, round:0.000)	b=0.00	count=3500
Total loss:	1584.931 (rec:0.842, round:1584.089)	b=20.00	count=4000
Total loss:	752.212 (rec:0.883, round:751.329)	b=19.44	count=4500
Total loss:	665.112 (rec:0.817, round:664.296)	b=18.88	count=5000
Total loss:	597.243 (rec:0.816, round:596.427)	b=18.31	count=5500
Total loss:	538.312 (rec:0.767, round:537.546)	b=17.75	count=6000
Total loss:	486.379 (rec:0.843, round:485.536)	b=17.19	count=6500
Total loss:	439.552 (rec:0.772, round:438.780)	b=16.62	count=7000
Total loss:	398.627 (rec:0.777, round:397.850)	b=16.06	count=7500
Total loss:	361.211 (rec:0.783, round:360.427)	b=15.50	count=8000
Total loss:	327.427 (rec:0.766, round:326.661)	b=14.94	count=8500
Total loss:	296.830 (rec:0.776, round:296.054)	b=14.38	count=9000
Total loss:	269.459 (rec:0.749, round:268.710)	b=13.81	count=9500
Total loss:	244.680 (rec:0.719, round:243.961)	b=13.25	count=10000
Total loss:	221.327 (rec:0.762, round:220.565)	b=12.69	count=10500
Total loss:	198.851 (rec:0.741, round:198.110)	b=12.12	count=11000
Total loss:	178.708 (rec:0.734, round:177.975)	b=11.56	count=11500
Total loss:	158.112 (rec:0.741, round:157.371)	b=11.00	count=12000
Total loss:	139.149 (rec:0.747, round:138.402)	b=10.44	count=12500
Total loss:	121.431 (rec:0.768, round:120.663)	b=9.88	count=13000
Total loss:	103.494 (rec:0.756, round:102.738)	b=9.31	count=13500
Total loss:	86.528 (rec:0.721, round:85.807)	b=8.75	count=14000
Total loss:	70.361 (rec:0.795, round:69.566)	b=8.19	count=14500
Total loss:	54.733 (rec:0.800, round:53.934)	b=7.62	count=15000
Total loss:	41.402 (rec:0.744, round:40.657)	b=7.06	count=15500
Total loss:	30.117 (rec:0.747, round:29.370)	b=6.50	count=16000
Total loss:	20.318 (rec:0.719, round:19.599)	b=5.94	count=16500
Total loss:	12.372 (rec:0.773, round:11.599)	b=5.38	count=17000
Total loss:	6.957 (rec:0.780, round:6.177)	b=4.81	count=17500
Total loss:	3.292 (rec:0.754, round:2.539)	b=4.25	count=18000
Total loss:	1.610 (rec:0.779, round:0.832)	b=3.69	count=18500
Total loss:	1.022 (rec:0.786, round:0.235)	b=3.12	count=19000
Total loss:	0.794 (rec:0.745, round:0.049)	b=2.56	count=19500
Total loss:	0.758 (rec:0.752, round:0.006)	b=2.00	count=20000
finished reconstructing layers.0.blocks.0.
reconstructing layers.0.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.0.blocks.1 ...
wraping quantizers in layers.0.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.691 (rec:1.691, round:0.000)	b=0.00	count=500
Total loss:	1.444 (rec:1.444, round:0.000)	b=0.00	count=1000
Total loss:	1.475 (rec:1.475, round:0.000)	b=0.00	count=1500
Total loss:	1.339 (rec:1.339, round:0.000)	b=0.00	count=2000
Total loss:	1.513 (rec:1.513, round:0.000)	b=0.00	count=2500
Total loss:	1.435 (rec:1.435, round:0.000)	b=0.00	count=3000
Total loss:	1.395 (rec:1.395, round:0.000)	b=0.00	count=3500
Total loss:	1605.195 (rec:1.469, round:1603.726)	b=20.00	count=4000
Total loss:	814.760 (rec:1.384, round:813.376)	b=19.44	count=4500
Total loss:	733.740 (rec:1.340, round:732.400)	b=18.88	count=5000
Total loss:	673.739 (rec:1.392, round:672.347)	b=18.31	count=5500
Total loss:	621.199 (rec:1.442, round:619.757)	b=17.75	count=6000
Total loss:	573.615 (rec:1.367, round:572.248)	b=17.19	count=6500
Total loss:	530.242 (rec:1.309, round:528.932)	b=16.62	count=7000
Total loss:	490.723 (rec:1.473, round:489.249)	b=16.06	count=7500
Total loss:	454.638 (rec:1.406, round:453.233)	b=15.50	count=8000
Total loss:	421.797 (rec:1.405, round:420.392)	b=14.94	count=8500
Total loss:	389.961 (rec:1.426, round:388.535)	b=14.38	count=9000
Total loss:	360.892 (rec:1.495, round:359.397)	b=13.81	count=9500
Total loss:	332.416 (rec:1.335, round:331.082)	b=13.25	count=10000
Total loss:	305.890 (rec:1.333, round:304.558)	b=12.69	count=10500
Total loss:	281.000 (rec:1.419, round:279.581)	b=12.12	count=11000
Total loss:	256.953 (rec:1.380, round:255.573)	b=11.56	count=11500
Total loss:	233.015 (rec:1.392, round:231.623)	b=11.00	count=12000
Total loss:	208.749 (rec:1.384, round:207.365)	b=10.44	count=12500
Total loss:	185.317 (rec:1.420, round:183.897)	b=9.88	count=13000
Total loss:	161.583 (rec:1.343, round:160.240)	b=9.31	count=13500
Total loss:	138.292 (rec:1.497, round:136.795)	b=8.75	count=14000
Total loss:	115.534 (rec:1.451, round:114.083)	b=8.19	count=14500
Total loss:	94.074 (rec:1.412, round:92.662)	b=7.62	count=15000
Total loss:	73.250 (rec:1.374, round:71.876)	b=7.06	count=15500
Total loss:	52.732 (rec:1.394, round:51.338)	b=6.50	count=16000
Total loss:	35.315 (rec:1.427, round:33.888)	b=5.94	count=16500
Total loss:	21.242 (rec:1.404, round:19.837)	b=5.38	count=17000
Total loss:	11.338 (rec:1.484, round:9.854)	b=4.81	count=17500
Total loss:	5.220 (rec:1.371, round:3.849)	b=4.25	count=18000
Total loss:	2.438 (rec:1.368, round:1.070)	b=3.69	count=18500
Total loss:	1.449 (rec:1.285, round:0.163)	b=3.12	count=19000
Total loss:	1.465 (rec:1.453, round:0.011)	b=2.56	count=19500
Total loss:	1.414 (rec:1.414, round:0.000)	b=2.00	count=20000
finished reconstructing layers.0.blocks.1.
reconstructing layers.1.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.downsample ...
wraping quantizers in layers.1.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.690 (rec:1.690, round:0.000)	b=0.00	count=500
Total loss:	1.747 (rec:1.747, round:0.000)	b=0.00	count=1000
Total loss:	1.700 (rec:1.700, round:0.000)	b=0.00	count=1500
Total loss:	1.736 (rec:1.736, round:0.000)	b=0.00	count=2000
Total loss:	1.811 (rec:1.811, round:0.000)	b=0.00	count=2500
Total loss:	1.684 (rec:1.684, round:0.000)	b=0.00	count=3000
Total loss:	1.682 (rec:1.682, round:0.000)	b=0.00	count=3500
Total loss:	1038.690 (rec:1.576, round:1037.114)	b=20.00	count=4000
Total loss:	542.244 (rec:1.588, round:540.656)	b=19.44	count=4500
Total loss:	490.520 (rec:1.595, round:488.925)	b=18.88	count=5000
Total loss:	456.463 (rec:1.586, round:454.877)	b=18.31	count=5500
Total loss:	427.695 (rec:1.631, round:426.064)	b=17.75	count=6000
Total loss:	402.055 (rec:1.724, round:400.331)	b=17.19	count=6500
Total loss:	377.753 (rec:1.585, round:376.168)	b=16.62	count=7000
Total loss:	356.179 (rec:1.639, round:354.540)	b=16.06	count=7500
Total loss:	335.334 (rec:1.692, round:333.642)	b=15.50	count=8000
Total loss:	315.630 (rec:1.609, round:314.022)	b=14.94	count=8500
Total loss:	296.455 (rec:1.609, round:294.846)	b=14.38	count=9000
Total loss:	277.911 (rec:1.609, round:276.302)	b=13.81	count=9500
Total loss:	259.503 (rec:1.591, round:257.912)	b=13.25	count=10000
Total loss:	241.805 (rec:1.699, round:240.106)	b=12.69	count=10500
Total loss:	223.717 (rec:1.629, round:222.088)	b=12.12	count=11000
Total loss:	205.649 (rec:1.627, round:204.022)	b=11.56	count=11500
Total loss:	188.245 (rec:1.685, round:186.560)	b=11.00	count=12000
Total loss:	170.002 (rec:1.692, round:168.310)	b=10.44	count=12500
Total loss:	151.142 (rec:1.710, round:149.432)	b=9.88	count=13000
Total loss:	131.579 (rec:1.681, round:129.898)	b=9.31	count=13500
Total loss:	112.633 (rec:1.646, round:110.987)	b=8.75	count=14000
Total loss:	94.323 (rec:1.729, round:92.594)	b=8.19	count=14500
Total loss:	75.558 (rec:1.616, round:73.942)	b=7.62	count=15000
Total loss:	58.240 (rec:1.724, round:56.516)	b=7.06	count=15500
Total loss:	41.863 (rec:1.663, round:40.200)	b=6.50	count=16000
Total loss:	27.519 (rec:1.754, round:25.765)	b=5.94	count=16500
Total loss:	16.723 (rec:1.487, round:15.236)	b=5.38	count=17000
Total loss:	9.158 (rec:1.625, round:7.533)	b=4.81	count=17500
Total loss:	4.885 (rec:1.624, round:3.261)	b=4.25	count=18000
Total loss:	2.875 (rec:1.672, round:1.203)	b=3.69	count=18500
Total loss:	2.060 (rec:1.733, round:0.327)	b=3.12	count=19000
Total loss:	1.577 (rec:1.518, round:0.059)	b=2.56	count=19500
Total loss:	1.776 (rec:1.772, round:0.004)	b=2.00	count=20000
finished reconstructing layers.1.downsample.
reconstructing layers.1.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.blocks.0 ...
wraping quantizers in layers.1.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.872 (rec:1.872, round:0.000)	b=0.00	count=500
Total loss:	1.772 (rec:1.772, round:0.000)	b=0.00	count=1000
Total loss:	1.806 (rec:1.806, round:0.000)	b=0.00	count=1500
Total loss:	1.667 (rec:1.667, round:0.000)	b=0.00	count=2000
Total loss:	1.707 (rec:1.707, round:0.000)	b=0.00	count=2500
Total loss:	1.693 (rec:1.693, round:0.000)	b=0.00	count=3000
Total loss:	1.775 (rec:1.775, round:0.000)	b=0.00	count=3500
Total loss:	6785.254 (rec:1.740, round:6783.514)	b=20.00	count=4000
Total loss:	3399.841 (rec:1.668, round:3398.173)	b=19.44	count=4500
Total loss:	3106.318 (rec:1.895, round:3104.423)	b=18.88	count=5000
Total loss:	2901.551 (rec:1.743, round:2899.807)	b=18.31	count=5500
Total loss:	2723.382 (rec:1.766, round:2721.615)	b=17.75	count=6000
Total loss:	2560.231 (rec:1.821, round:2558.410)	b=17.19	count=6500
Total loss:	2407.323 (rec:1.838, round:2405.485)	b=16.62	count=7000
Total loss:	2261.983 (rec:1.824, round:2260.159)	b=16.06	count=7500
Total loss:	2121.520 (rec:1.902, round:2119.618)	b=15.50	count=8000
Total loss:	1986.128 (rec:1.722, round:1984.406)	b=14.94	count=8500
Total loss:	1854.487 (rec:1.737, round:1852.751)	b=14.38	count=9000
Total loss:	1727.594 (rec:1.728, round:1725.865)	b=13.81	count=9500
Total loss:	1603.966 (rec:1.761, round:1602.205)	b=13.25	count=10000
Total loss:	1481.762 (rec:1.758, round:1480.005)	b=12.69	count=10500
Total loss:	1359.682 (rec:1.742, round:1357.940)	b=12.12	count=11000
Total loss:	1240.453 (rec:1.961, round:1238.492)	b=11.56	count=11500
Total loss:	1122.317 (rec:1.839, round:1120.477)	b=11.00	count=12000
Total loss:	1003.147 (rec:1.724, round:1001.424)	b=10.44	count=12500
Total loss:	887.131 (rec:1.766, round:885.365)	b=9.88	count=13000
Total loss:	774.212 (rec:1.959, round:772.253)	b=9.31	count=13500
Total loss:	659.757 (rec:1.811, round:657.947)	b=8.75	count=14000
Total loss:	548.987 (rec:1.907, round:547.080)	b=8.19	count=14500
Total loss:	438.744 (rec:1.869, round:436.875)	b=7.62	count=15000
Total loss:	334.481 (rec:1.860, round:332.620)	b=7.06	count=15500
Total loss:	239.432 (rec:1.824, round:237.609)	b=6.50	count=16000
Total loss:	154.860 (rec:1.851, round:153.008)	b=5.94	count=16500
Total loss:	86.745 (rec:1.702, round:85.043)	b=5.38	count=17000
Total loss:	40.728 (rec:1.760, round:38.968)	b=4.81	count=17500
Total loss:	15.341 (rec:1.732, round:13.610)	b=4.25	count=18000
Total loss:	4.690 (rec:1.721, round:2.969)	b=3.69	count=18500
Total loss:	2.045 (rec:1.693, round:0.353)	b=3.12	count=19000
Total loss:	1.779 (rec:1.769, round:0.010)	b=2.56	count=19500
Total loss:	1.805 (rec:1.805, round:0.000)	b=2.00	count=20000
finished reconstructing layers.1.blocks.0.
reconstructing layers.1.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.1.blocks.1 ...
wraping quantizers in layers.1.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.822 (rec:1.822, round:0.000)	b=0.00	count=500
Total loss:	1.807 (rec:1.807, round:0.000)	b=0.00	count=1000
Total loss:	1.721 (rec:1.721, round:0.000)	b=0.00	count=1500
Total loss:	1.617 (rec:1.617, round:0.000)	b=0.00	count=2000
Total loss:	1.810 (rec:1.810, round:0.000)	b=0.00	count=2500
Total loss:	1.644 (rec:1.644, round:0.000)	b=0.00	count=3000
Total loss:	1.791 (rec:1.791, round:0.000)	b=0.00	count=3500
Total loss:	6894.688 (rec:1.677, round:6893.011)	b=20.00	count=4000
Total loss:	3470.271 (rec:1.711, round:3468.560)	b=19.44	count=4500
Total loss:	3183.001 (rec:1.629, round:3181.373)	b=18.88	count=5000
Total loss:	2984.256 (rec:1.682, round:2982.574)	b=18.31	count=5500
Total loss:	2812.680 (rec:1.718, round:2810.962)	b=17.75	count=6000
Total loss:	2653.835 (rec:1.736, round:2652.099)	b=17.19	count=6500
Total loss:	2504.008 (rec:1.733, round:2502.275)	b=16.62	count=7000
Total loss:	2358.733 (rec:1.695, round:2357.038)	b=16.06	count=7500
Total loss:	2219.389 (rec:1.754, round:2217.635)	b=15.50	count=8000
Total loss:	2085.202 (rec:1.733, round:2083.470)	b=14.94	count=8500
Total loss:	1951.869 (rec:1.644, round:1950.225)	b=14.38	count=9000
Total loss:	1824.782 (rec:1.798, round:1822.984)	b=13.81	count=9500
Total loss:	1697.487 (rec:1.676, round:1695.811)	b=13.25	count=10000
Total loss:	1573.163 (rec:1.720, round:1571.444)	b=12.69	count=10500
Total loss:	1450.043 (rec:1.683, round:1448.360)	b=12.12	count=11000
Total loss:	1329.217 (rec:1.692, round:1327.525)	b=11.56	count=11500
Total loss:	1209.921 (rec:1.704, round:1208.217)	b=11.00	count=12000
Total loss:	1090.548 (rec:1.748, round:1088.799)	b=10.44	count=12500
Total loss:	970.237 (rec:1.720, round:968.517)	b=9.88	count=13000
Total loss:	852.117 (rec:1.668, round:850.449)	b=9.31	count=13500
Total loss:	735.122 (rec:1.721, round:733.401)	b=8.75	count=14000
Total loss:	620.948 (rec:1.754, round:619.195)	b=8.19	count=14500
Total loss:	506.028 (rec:1.675, round:504.353)	b=7.62	count=15000
Total loss:	395.347 (rec:1.715, round:393.632)	b=7.06	count=15500
Total loss:	289.494 (rec:1.696, round:287.798)	b=6.50	count=16000
Total loss:	192.104 (rec:1.770, round:190.334)	b=5.94	count=16500
Total loss:	108.704 (rec:1.610, round:107.094)	b=5.38	count=17000
Total loss:	49.880 (rec:1.692, round:48.188)	b=4.81	count=17500
Total loss:	17.977 (rec:1.648, round:16.329)	b=4.25	count=18000
Total loss:	5.283 (rec:1.642, round:3.641)	b=3.69	count=18500
Total loss:	2.207 (rec:1.769, round:0.438)	b=3.12	count=19000
Total loss:	1.676 (rec:1.665, round:0.011)	b=2.56	count=19500
Total loss:	1.654 (rec:1.654, round:0.000)	b=2.00	count=20000
finished reconstructing layers.1.blocks.1.
reconstructing layers.2.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.downsample ...
wraping quantizers in layers.2.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	2.315 (rec:2.315, round:0.000)	b=0.00	count=500
Total loss:	1.903 (rec:1.903, round:0.000)	b=0.00	count=1000
Total loss:	2.138 (rec:2.138, round:0.000)	b=0.00	count=1500
Total loss:	2.042 (rec:2.042, round:0.000)	b=0.00	count=2000
Total loss:	1.912 (rec:1.912, round:0.000)	b=0.00	count=2500
Total loss:	1.950 (rec:1.950, round:0.000)	b=0.00	count=3000
Total loss:	1.856 (rec:1.856, round:0.000)	b=0.00	count=3500
Total loss:	4449.033 (rec:1.825, round:4447.208)	b=20.00	count=4000
Total loss:	2237.207 (rec:1.717, round:2235.490)	b=19.44	count=4500
Total loss:	2040.760 (rec:1.897, round:2038.864)	b=18.88	count=5000
Total loss:	1901.303 (rec:1.887, round:1899.415)	b=18.31	count=5500
Total loss:	1781.345 (rec:1.946, round:1779.399)	b=17.75	count=6000
Total loss:	1671.243 (rec:1.964, round:1669.279)	b=17.19	count=6500
Total loss:	1571.436 (rec:1.898, round:1569.538)	b=16.62	count=7000
Total loss:	1474.349 (rec:1.800, round:1472.549)	b=16.06	count=7500
Total loss:	1384.885 (rec:1.859, round:1383.026)	b=15.50	count=8000
Total loss:	1296.838 (rec:1.886, round:1294.952)	b=14.94	count=8500
Total loss:	1213.842 (rec:1.740, round:1212.102)	b=14.38	count=9000
Total loss:	1130.578 (rec:1.870, round:1128.708)	b=13.81	count=9500
Total loss:	1049.417 (rec:1.810, round:1047.607)	b=13.25	count=10000
Total loss:	970.021 (rec:1.803, round:968.218)	b=12.69	count=10500
Total loss:	893.339 (rec:1.779, round:891.560)	b=12.12	count=11000
Total loss:	817.392 (rec:1.875, round:815.517)	b=11.56	count=11500
Total loss:	741.213 (rec:1.839, round:739.373)	b=11.00	count=12000
Total loss:	664.934 (rec:1.870, round:663.063)	b=10.44	count=12500
Total loss:	588.403 (rec:1.977, round:586.426)	b=9.88	count=13000
Total loss:	512.602 (rec:1.766, round:510.836)	b=9.31	count=13500
Total loss:	437.435 (rec:1.911, round:435.524)	b=8.75	count=14000
Total loss:	363.245 (rec:1.956, round:361.289)	b=8.19	count=14500
Total loss:	291.474 (rec:1.901, round:289.573)	b=7.62	count=15000
Total loss:	223.262 (rec:1.914, round:221.348)	b=7.06	count=15500
Total loss:	158.936 (rec:1.919, round:157.016)	b=6.50	count=16000
Total loss:	104.947 (rec:1.865, round:103.082)	b=5.94	count=16500
Total loss:	61.007 (rec:1.939, round:59.067)	b=5.38	count=17000
Total loss:	30.163 (rec:1.810, round:28.353)	b=4.81	count=17500
Total loss:	13.448 (rec:1.991, round:11.457)	b=4.25	count=18000
Total loss:	5.172 (rec:1.929, round:3.243)	b=3.69	count=18500
Total loss:	2.522 (rec:1.924, round:0.598)	b=3.12	count=19000
Total loss:	1.856 (rec:1.793, round:0.063)	b=2.56	count=19500
Total loss:	1.761 (rec:1.760, round:0.002)	b=2.00	count=20000
finished reconstructing layers.2.downsample.
reconstructing layers.2.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.0 ...
wraping quantizers in layers.2.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.638 (rec:1.638, round:0.000)	b=0.00	count=500
Total loss:	1.861 (rec:1.861, round:0.000)	b=0.00	count=1000
Total loss:	1.877 (rec:1.877, round:0.000)	b=0.00	count=1500
Total loss:	1.653 (rec:1.653, round:0.000)	b=0.00	count=2000
Total loss:	1.765 (rec:1.765, round:0.000)	b=0.00	count=2500
Total loss:	1.704 (rec:1.704, round:0.000)	b=0.00	count=3000
Total loss:	1.775 (rec:1.775, round:0.000)	b=0.00	count=3500
Total loss:	28704.996 (rec:1.648, round:28703.348)	b=20.00	count=4000
Total loss:	13430.195 (rec:1.607, round:13428.588)	b=19.44	count=4500
Total loss:	12366.339 (rec:1.749, round:12364.590)	b=18.88	count=5000
Total loss:	11648.533 (rec:1.797, round:11646.736)	b=18.31	count=5500
Total loss:	11016.080 (rec:1.681, round:11014.399)	b=17.75	count=6000
Total loss:	10428.620 (rec:1.609, round:10427.012)	b=17.19	count=6500
Total loss:	9863.138 (rec:1.572, round:9861.566)	b=16.62	count=7000
Total loss:	9302.438 (rec:1.616, round:9300.822)	b=16.06	count=7500
Total loss:	8754.789 (rec:1.721, round:8753.068)	b=15.50	count=8000
Total loss:	8218.488 (rec:1.761, round:8216.728)	b=14.94	count=8500
Total loss:	7685.896 (rec:1.721, round:7684.175)	b=14.38	count=9000
Total loss:	7162.985 (rec:1.815, round:7161.171)	b=13.81	count=9500
Total loss:	6643.449 (rec:1.695, round:6641.754)	b=13.25	count=10000
Total loss:	6129.929 (rec:1.721, round:6128.208)	b=12.69	count=10500
Total loss:	5630.335 (rec:1.690, round:5628.646)	b=12.12	count=11000
Total loss:	5136.400 (rec:1.859, round:5134.542)	b=11.56	count=11500
Total loss:	4650.723 (rec:1.558, round:4649.165)	b=11.00	count=12000
Total loss:	4176.229 (rec:1.661, round:4174.569)	b=10.44	count=12500
Total loss:	3707.714 (rec:1.609, round:3706.105)	b=9.88	count=13000
Total loss:	3249.517 (rec:1.736, round:3247.780)	b=9.31	count=13500
Total loss:	2801.867 (rec:1.602, round:2800.265)	b=8.75	count=14000
Total loss:	2367.041 (rec:1.790, round:2365.251)	b=8.19	count=14500
Total loss:	1943.037 (rec:1.671, round:1941.366)	b=7.62	count=15000
Total loss:	1539.904 (rec:1.789, round:1538.115)	b=7.06	count=15500
Total loss:	1152.192 (rec:1.727, round:1150.464)	b=6.50	count=16000
Total loss:	775.935 (rec:1.774, round:774.161)	b=5.94	count=16500
Total loss:	410.388 (rec:1.642, round:408.746)	b=5.38	count=17000
Total loss:	151.657 (rec:1.805, round:149.852)	b=4.81	count=17500
Total loss:	40.509 (rec:1.573, round:38.937)	b=4.25	count=18000
Total loss:	8.588 (rec:1.729, round:6.859)	b=3.69	count=18500
Total loss:	2.349 (rec:1.675, round:0.674)	b=3.12	count=19000
Total loss:	1.855 (rec:1.832, round:0.023)	b=2.56	count=19500
Total loss:	1.697 (rec:1.697, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.0.
reconstructing layers.2.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.1 ...
wraping quantizers in layers.2.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.840 (rec:1.840, round:0.000)	b=0.00	count=500
Total loss:	1.855 (rec:1.855, round:0.000)	b=0.00	count=1000
Total loss:	1.759 (rec:1.759, round:0.000)	b=0.00	count=1500
Total loss:	1.889 (rec:1.889, round:0.000)	b=0.00	count=2000
Total loss:	1.754 (rec:1.754, round:0.000)	b=0.00	count=2500
Total loss:	1.829 (rec:1.829, round:0.000)	b=0.00	count=3000
Total loss:	1.826 (rec:1.826, round:0.000)	b=0.00	count=3500
Total loss:	28789.521 (rec:1.830, round:28787.691)	b=20.00	count=4000
Total loss:	13812.835 (rec:1.773, round:13811.062)	b=19.44	count=4500
Total loss:	12744.812 (rec:1.783, round:12743.029)	b=18.88	count=5000
Total loss:	12025.493 (rec:1.921, round:12023.572)	b=18.31	count=5500
Total loss:	11400.608 (rec:1.755, round:11398.854)	b=17.75	count=6000
Total loss:	10816.344 (rec:1.741, round:10814.603)	b=17.19	count=6500
Total loss:	10257.833 (rec:1.831, round:10256.002)	b=16.62	count=7000
Total loss:	9708.021 (rec:1.731, round:9706.290)	b=16.06	count=7500
Total loss:	9169.823 (rec:1.882, round:9167.941)	b=15.50	count=8000
Total loss:	8639.949 (rec:1.887, round:8638.062)	b=14.94	count=8500
Total loss:	8108.382 (rec:1.777, round:8106.605)	b=14.38	count=9000
Total loss:	7583.508 (rec:1.952, round:7581.557)	b=13.81	count=9500
Total loss:	7061.411 (rec:1.856, round:7059.554)	b=13.25	count=10000
Total loss:	6539.702 (rec:1.963, round:6537.739)	b=12.69	count=10500
Total loss:	6027.279 (rec:1.883, round:6025.396)	b=12.12	count=11000
Total loss:	5520.281 (rec:1.819, round:5518.462)	b=11.56	count=11500
Total loss:	5021.123 (rec:1.949, round:5019.173)	b=11.00	count=12000
Total loss:	4525.645 (rec:1.850, round:4523.795)	b=10.44	count=12500
Total loss:	4031.119 (rec:1.759, round:4029.360)	b=9.88	count=13000
Total loss:	3544.597 (rec:1.880, round:3542.717)	b=9.31	count=13500
Total loss:	3069.019 (rec:1.861, round:3067.158)	b=8.75	count=14000
Total loss:	2605.821 (rec:1.816, round:2604.005)	b=8.19	count=14500
Total loss:	2152.388 (rec:1.805, round:2150.583)	b=7.62	count=15000
Total loss:	1711.553 (rec:1.826, round:1709.728)	b=7.06	count=15500
Total loss:	1286.511 (rec:1.792, round:1284.719)	b=6.50	count=16000
Total loss:	874.071 (rec:1.758, round:872.313)	b=5.94	count=16500
Total loss:	472.049 (rec:1.797, round:470.252)	b=5.38	count=17000
Total loss:	176.615 (rec:1.799, round:174.816)	b=4.81	count=17500
Total loss:	48.339 (rec:1.805, round:46.535)	b=4.25	count=18000
Total loss:	10.323 (rec:1.814, round:8.509)	b=3.69	count=18500
Total loss:	2.672 (rec:1.855, round:0.817)	b=3.12	count=19000
Total loss:	1.928 (rec:1.883, round:0.045)	b=2.56	count=19500
Total loss:	1.861 (rec:1.860, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.1.
reconstructing layers.2.blocks.2 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.2 ...
wraping quantizers in layers.2.blocks.2 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.997 (rec:1.997, round:0.000)	b=0.00	count=500
Total loss:	1.970 (rec:1.970, round:0.000)	b=0.00	count=1000
Total loss:	1.957 (rec:1.957, round:0.000)	b=0.00	count=1500
Total loss:	1.864 (rec:1.864, round:0.000)	b=0.00	count=2000
Total loss:	1.940 (rec:1.940, round:0.000)	b=0.00	count=2500
Total loss:	1.867 (rec:1.867, round:0.000)	b=0.00	count=3000
Total loss:	2.007 (rec:2.007, round:0.000)	b=0.00	count=3500
Total loss:	28691.881 (rec:1.814, round:28690.066)	b=20.00	count=4000
Total loss:	13887.697 (rec:1.940, round:13885.758)	b=19.44	count=4500
Total loss:	12806.156 (rec:1.902, round:12804.254)	b=18.88	count=5000
Total loss:	12082.046 (rec:1.918, round:12080.128)	b=18.31	count=5500
Total loss:	11449.818 (rec:1.867, round:11447.951)	b=17.75	count=6000
Total loss:	10857.027 (rec:1.840, round:10855.187)	b=17.19	count=6500
Total loss:	10287.803 (rec:1.913, round:10285.891)	b=16.62	count=7000
Total loss:	9733.374 (rec:1.948, round:9731.426)	b=16.06	count=7500
Total loss:	9191.731 (rec:1.848, round:9189.884)	b=15.50	count=8000
Total loss:	8653.229 (rec:2.022, round:8651.207)	b=14.94	count=8500
Total loss:	8125.557 (rec:1.979, round:8123.578)	b=14.38	count=9000
Total loss:	7592.973 (rec:2.009, round:7590.964)	b=13.81	count=9500
Total loss:	7069.641 (rec:1.759, round:7067.882)	b=13.25	count=10000
Total loss:	6552.404 (rec:1.863, round:6550.541)	b=12.69	count=10500
Total loss:	6038.201 (rec:1.876, round:6036.325)	b=12.12	count=11000
Total loss:	5525.755 (rec:1.835, round:5523.920)	b=11.56	count=11500
Total loss:	5020.329 (rec:2.013, round:5018.315)	b=11.00	count=12000
Total loss:	4521.521 (rec:1.917, round:4519.604)	b=10.44	count=12500
Total loss:	4031.112 (rec:1.832, round:4029.280)	b=9.88	count=13000
Total loss:	3547.599 (rec:1.839, round:3545.760)	b=9.31	count=13500
Total loss:	3072.385 (rec:1.807, round:3070.579)	b=8.75	count=14000
Total loss:	2605.347 (rec:1.916, round:2603.431)	b=8.19	count=14500
Total loss:	2149.889 (rec:1.907, round:2147.982)	b=7.62	count=15000
Total loss:	1707.042 (rec:1.816, round:1705.226)	b=7.06	count=15500
Total loss:	1286.447 (rec:2.019, round:1284.428)	b=6.50	count=16000
Total loss:	876.842 (rec:1.935, round:874.907)	b=5.94	count=16500
Total loss:	486.945 (rec:1.917, round:485.027)	b=5.38	count=17000
Total loss:	199.107 (rec:1.843, round:197.264)	b=4.81	count=17500
Total loss:	60.620 (rec:1.865, round:58.755)	b=4.25	count=18000
Total loss:	13.283 (rec:1.912, round:11.371)	b=3.69	count=18500
Total loss:	2.892 (rec:1.951, round:0.941)	b=3.12	count=19000
Total loss:	2.024 (rec:2.006, round:0.018)	b=2.56	count=19500
Total loss:	1.855 (rec:1.855, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.2.
reconstructing layers.2.blocks.3 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.3 ...
wraping quantizers in layers.2.blocks.3 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.911 (rec:1.911, round:0.000)	b=0.00	count=500
Total loss:	1.865 (rec:1.865, round:0.000)	b=0.00	count=1000
Total loss:	1.908 (rec:1.908, round:0.000)	b=0.00	count=1500
Total loss:	1.853 (rec:1.853, round:0.000)	b=0.00	count=2000
Total loss:	1.779 (rec:1.779, round:0.000)	b=0.00	count=2500
Total loss:	1.892 (rec:1.892, round:0.000)	b=0.00	count=3000
Total loss:	1.672 (rec:1.672, round:0.000)	b=0.00	count=3500
Total loss:	28692.352 (rec:1.744, round:28690.607)	b=20.00	count=4000
Total loss:	13770.882 (rec:1.748, round:13769.134)	b=19.44	count=4500
Total loss:	12696.753 (rec:1.810, round:12694.943)	b=18.88	count=5000
Total loss:	11967.860 (rec:1.829, round:11966.031)	b=18.31	count=5500
Total loss:	11333.336 (rec:1.872, round:11331.464)	b=17.75	count=6000
Total loss:	10739.591 (rec:1.733, round:10737.857)	b=17.19	count=6500
Total loss:	10168.102 (rec:1.807, round:10166.294)	b=16.62	count=7000
Total loss:	9604.275 (rec:1.814, round:9602.461)	b=16.06	count=7500
Total loss:	9057.290 (rec:1.784, round:9055.506)	b=15.50	count=8000
Total loss:	8511.532 (rec:1.833, round:8509.699)	b=14.94	count=8500
Total loss:	7976.724 (rec:1.846, round:7974.878)	b=14.38	count=9000
Total loss:	7445.827 (rec:1.747, round:7444.080)	b=13.81	count=9500
Total loss:	6922.594 (rec:1.853, round:6920.741)	b=13.25	count=10000
Total loss:	6399.718 (rec:1.771, round:6397.947)	b=12.69	count=10500
Total loss:	5885.872 (rec:1.799, round:5884.073)	b=12.12	count=11000
Total loss:	5375.276 (rec:1.763, round:5373.514)	b=11.56	count=11500
Total loss:	4869.855 (rec:1.787, round:4868.068)	b=11.00	count=12000
Total loss:	4374.816 (rec:1.848, round:4372.968)	b=10.44	count=12500
Total loss:	3889.982 (rec:1.822, round:3888.160)	b=9.88	count=13000
Total loss:	3413.093 (rec:1.744, round:3411.348)	b=9.31	count=13500
Total loss:	2945.771 (rec:1.897, round:2943.874)	b=8.75	count=14000
Total loss:	2490.618 (rec:1.820, round:2488.799)	b=8.19	count=14500
Total loss:	2051.813 (rec:1.744, round:2050.069)	b=7.62	count=15000
Total loss:	1627.453 (rec:1.739, round:1625.715)	b=7.06	count=15500
Total loss:	1216.350 (rec:1.690, round:1214.660)	b=6.50	count=16000
Total loss:	822.234 (rec:1.791, round:820.443)	b=5.94	count=16500
Total loss:	458.158 (rec:1.785, round:456.373)	b=5.38	count=17000
Total loss:	191.429 (rec:1.850, round:189.580)	b=4.81	count=17500
Total loss:	59.678 (rec:1.895, round:57.783)	b=4.25	count=18000
Total loss:	12.886 (rec:1.745, round:11.140)	b=3.69	count=18500
Total loss:	2.695 (rec:1.770, round:0.925)	b=3.12	count=19000
Total loss:	1.880 (rec:1.860, round:0.019)	b=2.56	count=19500
Total loss:	1.770 (rec:1.770, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.3.
reconstructing layers.2.blocks.4 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.4 ...
wraping quantizers in layers.2.blocks.4 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.871 (rec:1.871, round:0.000)	b=0.00	count=500
Total loss:	1.901 (rec:1.901, round:0.000)	b=0.00	count=1000
Total loss:	1.799 (rec:1.799, round:0.000)	b=0.00	count=1500
Total loss:	1.754 (rec:1.754, round:0.000)	b=0.00	count=2000
Total loss:	1.820 (rec:1.820, round:0.000)	b=0.00	count=2500
Total loss:	1.859 (rec:1.859, round:0.000)	b=0.00	count=3000
Total loss:	1.728 (rec:1.728, round:0.000)	b=0.00	count=3500
Total loss:	28758.146 (rec:1.810, round:28756.336)	b=20.00	count=4000
Total loss:	13949.752 (rec:1.702, round:13948.050)	b=19.44	count=4500
Total loss:	12872.074 (rec:1.629, round:12870.445)	b=18.88	count=5000
Total loss:	12150.011 (rec:1.653, round:12148.357)	b=18.31	count=5500
Total loss:	11515.686 (rec:1.777, round:11513.908)	b=17.75	count=6000
Total loss:	10925.610 (rec:1.789, round:10923.821)	b=17.19	count=6500
Total loss:	10358.437 (rec:1.796, round:10356.641)	b=16.62	count=7000
Total loss:	9803.418 (rec:1.721, round:9801.697)	b=16.06	count=7500
Total loss:	9258.093 (rec:1.790, round:9256.303)	b=15.50	count=8000
Total loss:	8715.162 (rec:1.719, round:8713.443)	b=14.94	count=8500
Total loss:	8176.041 (rec:1.782, round:8174.258)	b=14.38	count=9000
Total loss:	7647.924 (rec:1.856, round:7646.068)	b=13.81	count=9500
Total loss:	7119.163 (rec:1.826, round:7117.336)	b=13.25	count=10000
Total loss:	6591.819 (rec:1.861, round:6589.958)	b=12.69	count=10500
Total loss:	6069.078 (rec:1.866, round:6067.212)	b=12.12	count=11000
Total loss:	5551.374 (rec:1.722, round:5549.652)	b=11.56	count=11500
Total loss:	5043.658 (rec:1.791, round:5041.867)	b=11.00	count=12000
Total loss:	4543.732 (rec:1.870, round:4541.862)	b=10.44	count=12500
Total loss:	4049.360 (rec:1.631, round:4047.729)	b=9.88	count=13000
Total loss:	3565.583 (rec:1.640, round:3563.943)	b=9.31	count=13500
Total loss:	3085.271 (rec:1.825, round:3083.447)	b=8.75	count=14000
Total loss:	2616.899 (rec:1.686, round:2615.214)	b=8.19	count=14500
Total loss:	2157.829 (rec:1.887, round:2155.942)	b=7.62	count=15000
Total loss:	1720.135 (rec:1.809, round:1718.327)	b=7.06	count=15500
Total loss:	1294.859 (rec:1.686, round:1293.173)	b=6.50	count=16000
Total loss:	885.861 (rec:1.821, round:884.041)	b=5.94	count=16500
Total loss:	513.225 (rec:1.803, round:511.422)	b=5.38	count=17000
Total loss:	232.142 (rec:1.837, round:230.305)	b=4.81	count=17500
Total loss:	79.042 (rec:1.871, round:77.171)	b=4.25	count=18000
Total loss:	16.996 (rec:1.725, round:15.271)	b=3.69	count=18500
Total loss:	2.989 (rec:1.668, round:1.321)	b=3.12	count=19000
Total loss:	1.840 (rec:1.811, round:0.029)	b=2.56	count=19500
Total loss:	1.816 (rec:1.816, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.4.
reconstructing layers.2.blocks.5 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.5 ...
wraping quantizers in layers.2.blocks.5 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.849 (rec:1.849, round:0.000)	b=0.00	count=500
Total loss:	2.263 (rec:2.263, round:0.000)	b=0.00	count=1000
Total loss:	1.898 (rec:1.898, round:0.000)	b=0.00	count=1500
Total loss:	1.818 (rec:1.818, round:0.000)	b=0.00	count=2000
Total loss:	1.785 (rec:1.785, round:0.000)	b=0.00	count=2500
Total loss:	1.985 (rec:1.985, round:0.000)	b=0.00	count=3000
Total loss:	1.922 (rec:1.922, round:0.000)	b=0.00	count=3500
Total loss:	28725.137 (rec:1.938, round:28723.199)	b=20.00	count=4000
Total loss:	13948.386 (rec:1.859, round:13946.526)	b=19.44	count=4500
Total loss:	12868.350 (rec:1.843, round:12866.506)	b=18.88	count=5000
Total loss:	12139.666 (rec:1.800, round:12137.865)	b=18.31	count=5500
Total loss:	11507.684 (rec:1.910, round:11505.773)	b=17.75	count=6000
Total loss:	10909.973 (rec:1.896, round:10908.076)	b=17.19	count=6500
Total loss:	10332.009 (rec:1.853, round:10330.156)	b=16.62	count=7000
Total loss:	9768.635 (rec:1.802, round:9766.832)	b=16.06	count=7500
Total loss:	9212.188 (rec:1.811, round:9210.378)	b=15.50	count=8000
Total loss:	8667.207 (rec:1.878, round:8665.329)	b=14.94	count=8500
Total loss:	8126.965 (rec:1.822, round:8125.144)	b=14.38	count=9000
Total loss:	7592.706 (rec:1.816, round:7590.891)	b=13.81	count=9500
Total loss:	7062.780 (rec:1.843, round:7060.937)	b=13.25	count=10000
Total loss:	6537.160 (rec:1.884, round:6535.276)	b=12.69	count=10500
Total loss:	6018.068 (rec:1.788, round:6016.281)	b=12.12	count=11000
Total loss:	5500.107 (rec:1.823, round:5498.284)	b=11.56	count=11500
Total loss:	4997.677 (rec:1.874, round:4995.803)	b=11.00	count=12000
Total loss:	4494.471 (rec:1.867, round:4492.604)	b=10.44	count=12500
Total loss:	4000.251 (rec:1.836, round:3998.415)	b=9.88	count=13000
Total loss:	3518.925 (rec:1.885, round:3517.040)	b=9.31	count=13500
Total loss:	3047.371 (rec:1.843, round:3045.527)	b=8.75	count=14000
Total loss:	2582.525 (rec:1.830, round:2580.695)	b=8.19	count=14500
Total loss:	2131.477 (rec:1.948, round:2129.530)	b=7.62	count=15000
Total loss:	1696.085 (rec:1.847, round:1694.238)	b=7.06	count=15500
Total loss:	1280.396 (rec:1.913, round:1278.482)	b=6.50	count=16000
Total loss:	889.050 (rec:1.792, round:887.257)	b=5.94	count=16500
Total loss:	537.594 (rec:1.683, round:535.911)	b=5.38	count=17000
Total loss:	261.061 (rec:1.836, round:259.225)	b=4.81	count=17500
Total loss:	91.082 (rec:1.934, round:89.148)	b=4.25	count=18000
Total loss:	19.932 (rec:1.831, round:18.101)	b=3.69	count=18500
Total loss:	3.282 (rec:1.830, round:1.452)	b=3.12	count=19000
Total loss:	1.812 (rec:1.782, round:0.029)	b=2.56	count=19500
Total loss:	1.828 (rec:1.828, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.5.
reconstructing layers.2.blocks.6 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.6 ...
wraping quantizers in layers.2.blocks.6 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.775 (rec:1.775, round:0.000)	b=0.00	count=500
Total loss:	1.858 (rec:1.858, round:0.000)	b=0.00	count=1000
Total loss:	1.955 (rec:1.955, round:0.000)	b=0.00	count=1500
Total loss:	1.860 (rec:1.860, round:0.000)	b=0.00	count=2000
Total loss:	1.790 (rec:1.790, round:0.000)	b=0.00	count=2500
Total loss:	1.916 (rec:1.916, round:0.000)	b=0.00	count=3000
Total loss:	1.749 (rec:1.749, round:0.000)	b=0.00	count=3500
Total loss:	28682.361 (rec:1.836, round:28680.525)	b=20.00	count=4000
Total loss:	13945.823 (rec:1.827, round:13943.996)	b=19.44	count=4500
Total loss:	12859.422 (rec:1.758, round:12857.664)	b=18.88	count=5000
Total loss:	12126.320 (rec:1.849, round:12124.472)	b=18.31	count=5500
Total loss:	11488.655 (rec:1.806, round:11486.850)	b=17.75	count=6000
Total loss:	10894.136 (rec:1.812, round:10892.324)	b=17.19	count=6500
Total loss:	10319.516 (rec:1.748, round:10317.768)	b=16.62	count=7000
Total loss:	9760.704 (rec:1.780, round:9758.924)	b=16.06	count=7500
Total loss:	9210.305 (rec:1.767, round:9208.538)	b=15.50	count=8000
Total loss:	8664.711 (rec:1.831, round:8662.880)	b=14.94	count=8500
Total loss:	8124.916 (rec:1.692, round:8123.224)	b=14.38	count=9000
Total loss:	7598.026 (rec:1.780, round:7596.246)	b=13.81	count=9500
Total loss:	7069.200 (rec:1.699, round:7067.500)	b=13.25	count=10000
Total loss:	6546.724 (rec:1.756, round:6544.968)	b=12.69	count=10500
Total loss:	6027.064 (rec:1.771, round:6025.292)	b=12.12	count=11000
Total loss:	5510.873 (rec:1.729, round:5509.144)	b=11.56	count=11500
Total loss:	5003.748 (rec:1.896, round:5001.851)	b=11.00	count=12000
Total loss:	4504.180 (rec:1.904, round:4502.276)	b=10.44	count=12500
Total loss:	4010.875 (rec:1.935, round:4008.941)	b=9.88	count=13000
Total loss:	3526.068 (rec:1.729, round:3524.339)	b=9.31	count=13500
Total loss:	3050.225 (rec:1.742, round:3048.483)	b=8.75	count=14000
Total loss:	2583.292 (rec:1.716, round:2581.575)	b=8.19	count=14500
Total loss:	2131.398 (rec:1.928, round:2129.470)	b=7.62	count=15000
Total loss:	1689.230 (rec:1.750, round:1687.480)	b=7.06	count=15500
Total loss:	1268.161 (rec:1.822, round:1266.339)	b=6.50	count=16000
Total loss:	878.153 (rec:1.767, round:876.386)	b=5.94	count=16500
Total loss:	530.914 (rec:1.810, round:529.104)	b=5.38	count=17000
Total loss:	262.178 (rec:1.791, round:260.387)	b=4.81	count=17500
Total loss:	97.481 (rec:1.700, round:95.780)	b=4.25	count=18000
Total loss:	22.614 (rec:1.913, round:20.701)	b=3.69	count=18500
Total loss:	3.625 (rec:1.790, round:1.834)	b=3.12	count=19000
Total loss:	1.897 (rec:1.862, round:0.035)	b=2.56	count=19500
Total loss:	1.725 (rec:1.725, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.6.
reconstructing layers.2.blocks.7 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.7 ...
wraping quantizers in layers.2.blocks.7 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.841 (rec:1.841, round:0.000)	b=0.00	count=500
Total loss:	1.943 (rec:1.943, round:0.000)	b=0.00	count=1000
Total loss:	1.991 (rec:1.991, round:0.000)	b=0.00	count=1500
Total loss:	1.898 (rec:1.898, round:0.000)	b=0.00	count=2000
Total loss:	1.871 (rec:1.871, round:0.000)	b=0.00	count=2500
Total loss:	1.855 (rec:1.855, round:0.000)	b=0.00	count=3000
Total loss:	1.834 (rec:1.834, round:0.000)	b=0.00	count=3500
Total loss:	28435.059 (rec:1.711, round:28433.348)	b=20.00	count=4000
Total loss:	13801.191 (rec:1.892, round:13799.299)	b=19.44	count=4500
Total loss:	12715.215 (rec:1.910, round:12713.305)	b=18.88	count=5000
Total loss:	11973.664 (rec:1.812, round:11971.852)	b=18.31	count=5500
Total loss:	11325.416 (rec:1.856, round:11323.561)	b=17.75	count=6000
Total loss:	10712.742 (rec:1.637, round:10711.104)	b=17.19	count=6500
Total loss:	10126.703 (rec:1.658, round:10125.045)	b=16.62	count=7000
Total loss:	9555.050 (rec:1.775, round:9553.275)	b=16.06	count=7500
Total loss:	8993.946 (rec:1.776, round:8992.170)	b=15.50	count=8000
Total loss:	8446.202 (rec:1.748, round:8444.454)	b=14.94	count=8500
Total loss:	7897.410 (rec:1.887, round:7895.522)	b=14.38	count=9000
Total loss:	7355.909 (rec:1.752, round:7354.157)	b=13.81	count=9500
Total loss:	6824.923 (rec:1.720, round:6823.203)	b=13.25	count=10000
Total loss:	6300.013 (rec:1.797, round:6298.216)	b=12.69	count=10500
Total loss:	5783.270 (rec:1.744, round:5781.526)	b=12.12	count=11000
Total loss:	5275.926 (rec:1.689, round:5274.237)	b=11.56	count=11500
Total loss:	4779.747 (rec:1.680, round:4778.066)	b=11.00	count=12000
Total loss:	4294.107 (rec:1.801, round:4292.306)	b=10.44	count=12500
Total loss:	3811.010 (rec:1.925, round:3809.085)	b=9.88	count=13000
Total loss:	3341.001 (rec:1.673, round:3339.329)	b=9.31	count=13500
Total loss:	2884.390 (rec:1.702, round:2882.688)	b=8.75	count=14000
Total loss:	2441.016 (rec:1.952, round:2439.064)	b=8.19	count=14500
Total loss:	2015.213 (rec:1.783, round:2013.430)	b=7.62	count=15000
Total loss:	1603.242 (rec:1.778, round:1601.464)	b=7.06	count=15500
Total loss:	1210.906 (rec:1.941, round:1208.965)	b=6.50	count=16000
Total loss:	847.001 (rec:1.741, round:845.260)	b=5.94	count=16500
Total loss:	525.950 (rec:1.746, round:524.203)	b=5.38	count=17000
Total loss:	276.808 (rec:1.627, round:275.181)	b=4.81	count=17500
Total loss:	115.689 (rec:1.790, round:113.899)	b=4.25	count=18000
Total loss:	29.953 (rec:1.712, round:28.242)	b=3.69	count=18500
Total loss:	4.398 (rec:1.755, round:2.643)	b=3.12	count=19000
Total loss:	1.762 (rec:1.682, round:0.080)	b=2.56	count=19500
Total loss:	1.793 (rec:1.791, round:0.002)	b=2.00	count=20000
finished reconstructing layers.2.blocks.7.
reconstructing layers.2.blocks.8 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.8 ...
wraping quantizers in layers.2.blocks.8 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.796 (rec:1.796, round:0.000)	b=0.00	count=500
Total loss:	1.648 (rec:1.648, round:0.000)	b=0.00	count=1000
Total loss:	1.772 (rec:1.772, round:0.000)	b=0.00	count=1500
Total loss:	1.753 (rec:1.753, round:0.000)	b=0.00	count=2000
Total loss:	1.654 (rec:1.654, round:0.000)	b=0.00	count=2500
Total loss:	1.686 (rec:1.686, round:0.000)	b=0.00	count=3000
Total loss:	1.563 (rec:1.563, round:0.000)	b=0.00	count=3500
Total loss:	28210.160 (rec:1.604, round:28208.557)	b=20.00	count=4000
Total loss:	13260.831 (rec:1.610, round:13259.221)	b=19.44	count=4500
Total loss:	12172.072 (rec:1.736, round:12170.336)	b=18.88	count=5000
Total loss:	11422.180 (rec:1.715, round:11420.464)	b=18.31	count=5500
Total loss:	10754.284 (rec:1.561, round:10752.723)	b=17.75	count=6000
Total loss:	10123.980 (rec:1.683, round:10122.297)	b=17.19	count=6500
Total loss:	9520.191 (rec:1.605, round:9518.586)	b=16.62	count=7000
Total loss:	8934.397 (rec:1.625, round:8932.772)	b=16.06	count=7500
Total loss:	8364.444 (rec:1.528, round:8362.916)	b=15.50	count=8000
Total loss:	7803.596 (rec:1.525, round:7802.070)	b=14.94	count=8500
Total loss:	7255.866 (rec:1.513, round:7254.354)	b=14.38	count=9000
Total loss:	6724.439 (rec:1.562, round:6722.877)	b=13.81	count=9500
Total loss:	6204.251 (rec:1.640, round:6202.611)	b=13.25	count=10000
Total loss:	5697.313 (rec:1.494, round:5695.819)	b=12.69	count=10500
Total loss:	5204.714 (rec:1.365, round:5203.349)	b=12.12	count=11000
Total loss:	4724.433 (rec:1.637, round:4722.796)	b=11.56	count=11500
Total loss:	4258.052 (rec:1.663, round:4256.390)	b=11.00	count=12000
Total loss:	3808.127 (rec:1.699, round:3806.428)	b=10.44	count=12500
Total loss:	3375.245 (rec:1.594, round:3373.651)	b=9.88	count=13000
Total loss:	2956.893 (rec:1.535, round:2955.358)	b=9.31	count=13500
Total loss:	2552.565 (rec:1.540, round:2551.026)	b=8.75	count=14000
Total loss:	2166.142 (rec:1.520, round:2164.623)	b=8.19	count=14500
Total loss:	1792.271 (rec:1.536, round:1790.736)	b=7.62	count=15000
Total loss:	1437.380 (rec:1.575, round:1435.806)	b=7.06	count=15500
Total loss:	1110.533 (rec:1.583, round:1108.951)	b=6.50	count=16000
Total loss:	815.590 (rec:1.454, round:814.135)	b=5.94	count=16500
Total loss:	553.673 (rec:1.399, round:552.274)	b=5.38	count=17000
Total loss:	338.872 (rec:1.538, round:337.334)	b=4.81	count=17500
Total loss:	181.135 (rec:1.501, round:179.634)	b=4.25	count=18000
Total loss:	73.188 (rec:1.547, round:71.641)	b=3.69	count=18500
Total loss:	13.296 (rec:1.470, round:11.826)	b=3.12	count=19000
Total loss:	2.146 (rec:1.512, round:0.634)	b=2.56	count=19500
Total loss:	1.556 (rec:1.545, round:0.011)	b=2.00	count=20000
finished reconstructing layers.2.blocks.8.
reconstructing layers.2.blocks.9 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.9 ...
wraping quantizers in layers.2.blocks.9 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	2.065 (rec:2.065, round:0.000)	b=0.00	count=500
Total loss:	2.004 (rec:2.004, round:0.000)	b=0.00	count=1000
Total loss:	1.673 (rec:1.673, round:0.000)	b=0.00	count=1500
Total loss:	1.818 (rec:1.818, round:0.000)	b=0.00	count=2000
Total loss:	1.909 (rec:1.909, round:0.000)	b=0.00	count=2500
Total loss:	2.072 (rec:2.072, round:0.000)	b=0.00	count=3000
Total loss:	1.935 (rec:1.935, round:0.000)	b=0.00	count=3500
Total loss:	27842.182 (rec:1.762, round:27840.420)	b=20.00	count=4000
Total loss:	12655.241 (rec:1.896, round:12653.345)	b=19.44	count=4500
Total loss:	11558.169 (rec:1.698, round:11556.471)	b=18.88	count=5000
Total loss:	10771.286 (rec:1.772, round:10769.515)	b=18.31	count=5500
Total loss:	10060.733 (rec:1.820, round:10058.913)	b=17.75	count=6000
Total loss:	9397.059 (rec:1.666, round:9395.393)	b=17.19	count=6500
Total loss:	8762.202 (rec:1.776, round:8760.426)	b=16.62	count=7000
Total loss:	8151.309 (rec:1.868, round:8149.441)	b=16.06	count=7500
Total loss:	7560.855 (rec:1.620, round:7559.235)	b=15.50	count=8000
Total loss:	6996.632 (rec:1.885, round:6994.747)	b=14.94	count=8500
Total loss:	6452.502 (rec:1.617, round:6450.885)	b=14.38	count=9000
Total loss:	5930.075 (rec:1.861, round:5928.214)	b=13.81	count=9500
Total loss:	5435.511 (rec:1.418, round:5434.093)	b=13.25	count=10000
Total loss:	4963.161 (rec:1.630, round:4961.531)	b=12.69	count=10500
Total loss:	4507.542 (rec:1.785, round:4505.757)	b=12.12	count=11000
Total loss:	4073.985 (rec:1.707, round:4072.278)	b=11.56	count=11500
Total loss:	3656.378 (rec:1.824, round:3654.555)	b=11.00	count=12000
Total loss:	3252.622 (rec:1.624, round:3250.997)	b=10.44	count=12500
Total loss:	2867.913 (rec:1.745, round:2866.168)	b=9.88	count=13000
Total loss:	2504.103 (rec:1.683, round:2502.419)	b=9.31	count=13500
Total loss:	2159.503 (rec:1.699, round:2157.805)	b=8.75	count=14000
Total loss:	1831.463 (rec:1.631, round:1829.832)	b=8.19	count=14500
Total loss:	1519.097 (rec:1.875, round:1517.222)	b=7.62	count=15000
Total loss:	1225.105 (rec:1.665, round:1223.440)	b=7.06	count=15500
Total loss:	952.368 (rec:1.857, round:950.511)	b=6.50	count=16000
Total loss:	704.869 (rec:1.708, round:703.161)	b=5.94	count=16500
Total loss:	488.114 (rec:1.444, round:486.671)	b=5.38	count=17000
Total loss:	308.118 (rec:1.651, round:306.467)	b=4.81	count=17500
Total loss:	167.554 (rec:1.645, round:165.909)	b=4.25	count=18000
Total loss:	70.935 (rec:1.705, round:69.230)	b=3.69	count=18500
Total loss:	18.608 (rec:1.505, round:17.103)	b=3.12	count=19000
Total loss:	3.333 (rec:1.696, round:1.637)	b=2.56	count=19500
Total loss:	1.816 (rec:1.770, round:0.045)	b=2.00	count=20000
finished reconstructing layers.2.blocks.9.
reconstructing layers.2.blocks.10 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.10 ...
wraping quantizers in layers.2.blocks.10 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.868 (rec:1.868, round:0.000)	b=0.00	count=500
Total loss:	1.619 (rec:1.619, round:0.000)	b=0.00	count=1000
Total loss:	1.618 (rec:1.618, round:0.000)	b=0.00	count=1500
Total loss:	1.585 (rec:1.585, round:0.000)	b=0.00	count=2000
Total loss:	1.630 (rec:1.630, round:0.000)	b=0.00	count=2500
Total loss:	1.743 (rec:1.743, round:0.000)	b=0.00	count=3000
Total loss:	1.279 (rec:1.279, round:0.000)	b=0.00	count=3500
Total loss:	27270.979 (rec:1.489, round:27269.488)	b=20.00	count=4000
Total loss:	11855.286 (rec:1.566, round:11853.720)	b=19.44	count=4500
Total loss:	10721.876 (rec:1.339, round:10720.537)	b=18.88	count=5000
Total loss:	9894.503 (rec:1.360, round:9893.143)	b=18.31	count=5500
Total loss:	9155.647 (rec:1.283, round:9154.364)	b=17.75	count=6000
Total loss:	8462.597 (rec:1.398, round:8461.199)	b=17.19	count=6500
Total loss:	7810.845 (rec:1.412, round:7809.434)	b=16.62	count=7000
Total loss:	7195.944 (rec:1.442, round:7194.503)	b=16.06	count=7500
Total loss:	6628.143 (rec:1.413, round:6626.729)	b=15.50	count=8000
Total loss:	6094.748 (rec:1.538, round:6093.209)	b=14.94	count=8500
Total loss:	5594.930 (rec:1.358, round:5593.572)	b=14.38	count=9000
Total loss:	5123.924 (rec:1.314, round:5122.609)	b=13.81	count=9500
Total loss:	4681.458 (rec:1.265, round:4680.193)	b=13.25	count=10000
Total loss:	4261.477 (rec:1.519, round:4259.958)	b=12.69	count=10500
Total loss:	3866.677 (rec:1.382, round:3865.295)	b=12.12	count=11000
Total loss:	3492.005 (rec:1.383, round:3490.622)	b=11.56	count=11500
Total loss:	3135.977 (rec:1.347, round:3134.630)	b=11.00	count=12000
Total loss:	2799.645 (rec:1.374, round:2798.271)	b=10.44	count=12500
Total loss:	2479.367 (rec:1.366, round:2478.000)	b=9.88	count=13000
Total loss:	2171.779 (rec:1.346, round:2170.432)	b=9.31	count=13500
Total loss:	1877.801 (rec:1.178, round:1876.623)	b=8.75	count=14000
Total loss:	1597.294 (rec:1.374, round:1595.920)	b=8.19	count=14500
Total loss:	1331.318 (rec:1.333, round:1329.984)	b=7.62	count=15000
Total loss:	1082.915 (rec:1.351, round:1081.564)	b=7.06	count=15500
Total loss:	851.489 (rec:1.417, round:850.072)	b=6.50	count=16000
Total loss:	640.463 (rec:1.390, round:639.072)	b=5.94	count=16500
Total loss:	449.544 (rec:1.341, round:448.203)	b=5.38	count=17000
Total loss:	290.145 (rec:1.434, round:288.711)	b=4.81	count=17500
Total loss:	159.204 (rec:1.495, round:157.709)	b=4.25	count=18000
Total loss:	60.718 (rec:1.387, round:59.331)	b=3.69	count=18500
Total loss:	10.381 (rec:1.488, round:8.893)	b=3.12	count=19000
Total loss:	1.527 (rec:1.205, round:0.322)	b=2.56	count=19500
Total loss:	1.395 (rec:1.387, round:0.008)	b=2.00	count=20000
finished reconstructing layers.2.blocks.10.
reconstructing layers.2.blocks.11 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.11 ...
wraping quantizers in layers.2.blocks.11 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.993 (rec:1.993, round:0.000)	b=0.00	count=500
Total loss:	1.628 (rec:1.628, round:0.000)	b=0.00	count=1000
Total loss:	1.594 (rec:1.594, round:0.000)	b=0.00	count=1500
Total loss:	1.698 (rec:1.698, round:0.000)	b=0.00	count=2000
Total loss:	1.733 (rec:1.733, round:0.000)	b=0.00	count=2500
Total loss:	1.471 (rec:1.471, round:0.000)	b=0.00	count=3000
Total loss:	1.595 (rec:1.595, round:0.000)	b=0.00	count=3500
Total loss:	26196.395 (rec:1.293, round:26195.102)	b=20.00	count=4000
Total loss:	10720.551 (rec:1.496, round:10719.056)	b=19.44	count=4500
Total loss:	9521.067 (rec:1.244, round:9519.823)	b=18.88	count=5000
Total loss:	8605.336 (rec:1.463, round:8603.872)	b=18.31	count=5500
Total loss:	7781.811 (rec:1.228, round:7780.583)	b=17.75	count=6000
Total loss:	7029.044 (rec:1.474, round:7027.570)	b=17.19	count=6500
Total loss:	6342.095 (rec:1.433, round:6340.661)	b=16.62	count=7000
Total loss:	5718.411 (rec:1.467, round:5716.944)	b=16.06	count=7500
Total loss:	5147.378 (rec:1.465, round:5145.913)	b=15.50	count=8000
Total loss:	4640.229 (rec:1.463, round:4638.766)	b=14.94	count=8500
Total loss:	4179.489 (rec:1.208, round:4178.282)	b=14.38	count=9000
Total loss:	3762.970 (rec:1.266, round:3761.704)	b=13.81	count=9500
Total loss:	3385.734 (rec:1.434, round:3384.300)	b=13.25	count=10000
Total loss:	3040.556 (rec:1.251, round:3039.305)	b=12.69	count=10500
Total loss:	2722.040 (rec:1.429, round:2720.611)	b=12.12	count=11000
Total loss:	2427.355 (rec:1.341, round:2426.014)	b=11.56	count=11500
Total loss:	2155.900 (rec:1.352, round:2154.549)	b=11.00	count=12000
Total loss:	1903.399 (rec:1.233, round:1902.166)	b=10.44	count=12500
Total loss:	1668.711 (rec:1.463, round:1667.248)	b=9.88	count=13000
Total loss:	1446.469 (rec:1.350, round:1445.119)	b=9.31	count=13500
Total loss:	1237.847 (rec:1.386, round:1236.462)	b=8.75	count=14000
Total loss:	1043.097 (rec:1.179, round:1041.918)	b=8.19	count=14500
Total loss:	862.301 (rec:1.575, round:860.726)	b=7.62	count=15000
Total loss:	693.547 (rec:1.358, round:692.188)	b=7.06	count=15500
Total loss:	537.009 (rec:1.159, round:535.850)	b=6.50	count=16000
Total loss:	396.865 (rec:1.196, round:395.669)	b=5.94	count=16500
Total loss:	274.951 (rec:1.387, round:273.565)	b=5.38	count=17000
Total loss:	172.874 (rec:1.350, round:171.524)	b=4.81	count=17500
Total loss:	93.190 (rec:1.217, round:91.972)	b=4.25	count=18000
Total loss:	38.108 (rec:1.318, round:36.790)	b=3.69	count=18500
Total loss:	8.966 (rec:1.321, round:7.645)	b=3.12	count=19000
Total loss:	2.119 (rec:1.502, round:0.617)	b=2.56	count=19500
Total loss:	1.195 (rec:1.164, round:0.032)	b=2.00	count=20000
finished reconstructing layers.2.blocks.11.
reconstructing layers.2.blocks.12 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.12 ...
wraping quantizers in layers.2.blocks.12 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.839 (rec:1.839, round:0.000)	b=0.00	count=500
Total loss:	1.909 (rec:1.909, round:0.000)	b=0.00	count=1000
Total loss:	1.733 (rec:1.733, round:0.000)	b=0.00	count=1500
Total loss:	1.947 (rec:1.947, round:0.000)	b=0.00	count=2000
Total loss:	1.758 (rec:1.758, round:0.000)	b=0.00	count=2500
Total loss:	1.929 (rec:1.929, round:0.000)	b=0.00	count=3000
Total loss:	1.775 (rec:1.775, round:0.000)	b=0.00	count=3500
Total loss:	26195.910 (rec:1.495, round:26194.414)	b=20.00	count=4000
Total loss:	10712.062 (rec:1.547, round:10710.515)	b=19.44	count=4500
Total loss:	9511.133 (rec:1.653, round:9509.480)	b=18.88	count=5000
Total loss:	8583.993 (rec:1.599, round:8582.395)	b=18.31	count=5500
Total loss:	7747.548 (rec:1.406, round:7746.143)	b=17.75	count=6000
Total loss:	6977.372 (rec:1.553, round:6975.819)	b=17.19	count=6500
Total loss:	6266.471 (rec:1.608, round:6264.863)	b=16.62	count=7000
Total loss:	5622.091 (rec:1.548, round:5620.543)	b=16.06	count=7500
Total loss:	5031.051 (rec:1.462, round:5029.589)	b=15.50	count=8000
Total loss:	4502.834 (rec:1.456, round:4501.378)	b=14.94	count=8500
Total loss:	4025.975 (rec:1.579, round:4024.396)	b=14.38	count=9000
Total loss:	3598.892 (rec:1.589, round:3597.304)	b=13.81	count=9500
Total loss:	3210.298 (rec:1.649, round:3208.649)	b=13.25	count=10000
Total loss:	2859.603 (rec:1.478, round:2858.125)	b=12.69	count=10500
Total loss:	2542.372 (rec:1.566, round:2540.806)	b=12.12	count=11000
Total loss:	2251.719 (rec:1.361, round:2250.358)	b=11.56	count=11500
Total loss:	1986.639 (rec:1.500, round:1985.139)	b=11.00	count=12000
Total loss:	1742.876 (rec:1.445, round:1741.431)	b=10.44	count=12500
Total loss:	1513.595 (rec:1.481, round:1512.114)	b=9.88	count=13000
Total loss:	1303.860 (rec:1.575, round:1302.286)	b=9.31	count=13500
Total loss:	1109.220 (rec:1.389, round:1107.832)	b=8.75	count=14000
Total loss:	925.553 (rec:1.475, round:924.079)	b=8.19	count=14500
Total loss:	758.308 (rec:1.441, round:756.868)	b=7.62	count=15000
Total loss:	604.987 (rec:1.387, round:603.600)	b=7.06	count=15500
Total loss:	463.652 (rec:1.421, round:462.231)	b=6.50	count=16000
Total loss:	338.257 (rec:1.363, round:336.894)	b=5.94	count=16500
Total loss:	229.874 (rec:1.585, round:228.289)	b=5.38	count=17000
Total loss:	140.302 (rec:1.419, round:138.883)	b=4.81	count=17500
Total loss:	71.769 (rec:1.347, round:70.422)	b=4.25	count=18000
Total loss:	24.014 (rec:1.430, round:22.584)	b=3.69	count=18500
Total loss:	4.923 (rec:1.482, round:3.441)	b=3.12	count=19000
Total loss:	1.661 (rec:1.508, round:0.154)	b=2.56	count=19500
Total loss:	1.366 (rec:1.363, round:0.003)	b=2.00	count=20000
finished reconstructing layers.2.blocks.12.
reconstructing layers.2.blocks.13 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.13 ...
wraping quantizers in layers.2.blocks.13 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.530 (rec:1.530, round:0.000)	b=0.00	count=500
Total loss:	1.433 (rec:1.433, round:0.000)	b=0.00	count=1000
Total loss:	1.457 (rec:1.457, round:0.000)	b=0.00	count=1500
Total loss:	1.146 (rec:1.146, round:0.000)	b=0.00	count=2000
Total loss:	1.358 (rec:1.358, round:0.000)	b=0.00	count=2500
Total loss:	1.242 (rec:1.242, round:0.000)	b=0.00	count=3000
Total loss:	1.172 (rec:1.172, round:0.000)	b=0.00	count=3500
Total loss:	25996.137 (rec:1.362, round:25994.773)	b=20.00	count=4000
Total loss:	10363.495 (rec:1.174, round:10362.321)	b=19.44	count=4500
Total loss:	9241.763 (rec:1.241, round:9240.521)	b=18.88	count=5000
Total loss:	8341.948 (rec:1.204, round:8340.744)	b=18.31	count=5500
Total loss:	7501.255 (rec:1.235, round:7500.021)	b=17.75	count=6000
Total loss:	6695.925 (rec:1.269, round:6694.656)	b=17.19	count=6500
Total loss:	5948.187 (rec:1.259, round:5946.928)	b=16.62	count=7000
Total loss:	5273.560 (rec:1.111, round:5272.450)	b=16.06	count=7500
Total loss:	4667.586 (rec:1.062, round:4666.523)	b=15.50	count=8000
Total loss:	4127.154 (rec:1.152, round:4126.002)	b=14.94	count=8500
Total loss:	3646.936 (rec:1.105, round:3645.831)	b=14.38	count=9000
Total loss:	3219.974 (rec:1.202, round:3218.772)	b=13.81	count=9500
Total loss:	2842.396 (rec:1.245, round:2841.151)	b=13.25	count=10000
Total loss:	2502.018 (rec:1.173, round:2500.845)	b=12.69	count=10500
Total loss:	2196.304 (rec:1.185, round:2195.119)	b=12.12	count=11000
Total loss:	1919.420 (rec:1.156, round:1918.264)	b=11.56	count=11500
Total loss:	1670.224 (rec:1.229, round:1668.995)	b=11.00	count=12000
Total loss:	1446.389 (rec:1.023, round:1445.366)	b=10.44	count=12500
Total loss:	1240.107 (rec:1.186, round:1238.921)	b=9.88	count=13000
Total loss:	1050.133 (rec:1.281, round:1048.852)	b=9.31	count=13500
Total loss:	877.477 (rec:1.222, round:876.255)	b=8.75	count=14000
Total loss:	719.585 (rec:1.155, round:718.430)	b=8.19	count=14500
Total loss:	577.413 (rec:1.108, round:576.305)	b=7.62	count=15000
Total loss:	448.644 (rec:1.053, round:447.591)	b=7.06	count=15500
Total loss:	332.567 (rec:1.076, round:331.491)	b=6.50	count=16000
Total loss:	231.877 (rec:1.204, round:230.673)	b=5.94	count=16500
Total loss:	147.516 (rec:1.179, round:146.337)	b=5.38	count=17000
Total loss:	82.127 (rec:1.041, round:81.086)	b=4.81	count=17500
Total loss:	36.428 (rec:1.251, round:35.177)	b=4.25	count=18000
Total loss:	11.057 (rec:1.073, round:9.983)	b=3.69	count=18500
Total loss:	2.550 (rec:1.045, round:1.505)	b=3.12	count=19000
Total loss:	1.296 (rec:1.209, round:0.088)	b=2.56	count=19500
Total loss:	1.119 (rec:1.118, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.13.
reconstructing layers.2.blocks.14 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.14 ...
wraping quantizers in layers.2.blocks.14 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.527 (rec:1.527, round:0.000)	b=0.00	count=500
Total loss:	1.216 (rec:1.216, round:0.000)	b=0.00	count=1000
Total loss:	1.088 (rec:1.088, round:0.000)	b=0.00	count=1500
Total loss:	1.315 (rec:1.315, round:0.000)	b=0.00	count=2000
Total loss:	1.137 (rec:1.137, round:0.000)	b=0.00	count=2500
Total loss:	1.064 (rec:1.064, round:0.000)	b=0.00	count=3000
Total loss:	1.045 (rec:1.045, round:0.000)	b=0.00	count=3500
Total loss:	27286.328 (rec:1.123, round:27285.205)	b=20.00	count=4000
Total loss:	11867.751 (rec:1.100, round:11866.650)	b=19.44	count=4500
Total loss:	10654.182 (rec:1.118, round:10653.064)	b=18.88	count=5000
Total loss:	9701.260 (rec:1.072, round:9700.188)	b=18.31	count=5500
Total loss:	8804.398 (rec:0.985, round:8803.414)	b=17.75	count=6000
Total loss:	7947.346 (rec:1.069, round:7946.277)	b=17.19	count=6500
Total loss:	7139.916 (rec:0.925, round:7138.990)	b=16.62	count=7000
Total loss:	6392.772 (rec:0.970, round:6391.802)	b=16.06	count=7500
Total loss:	5717.157 (rec:0.924, round:5716.233)	b=15.50	count=8000
Total loss:	5097.204 (rec:0.915, round:5096.289)	b=14.94	count=8500
Total loss:	4534.704 (rec:1.052, round:4533.652)	b=14.38	count=9000
Total loss:	4018.167 (rec:0.908, round:4017.259)	b=13.81	count=9500
Total loss:	3552.302 (rec:0.967, round:3551.335)	b=13.25	count=10000
Total loss:	3124.832 (rec:0.845, round:3123.987)	b=12.69	count=10500
Total loss:	2738.727 (rec:1.011, round:2737.716)	b=12.12	count=11000
Total loss:	2388.080 (rec:1.036, round:2387.044)	b=11.56	count=11500
Total loss:	2070.230 (rec:0.961, round:2069.269)	b=11.00	count=12000
Total loss:	1783.112 (rec:1.032, round:1782.079)	b=10.44	count=12500
Total loss:	1524.463 (rec:1.000, round:1523.463)	b=9.88	count=13000
Total loss:	1284.644 (rec:0.967, round:1283.677)	b=9.31	count=13500
Total loss:	1065.966 (rec:0.923, round:1065.044)	b=8.75	count=14000
Total loss:	867.985 (rec:0.948, round:867.036)	b=8.19	count=14500
Total loss:	686.959 (rec:0.885, round:686.074)	b=7.62	count=15000
Total loss:	528.673 (rec:0.960, round:527.713)	b=7.06	count=15500
Total loss:	387.992 (rec:1.072, round:386.920)	b=6.50	count=16000
Total loss:	267.498 (rec:1.073, round:266.425)	b=5.94	count=16500
Total loss:	166.982 (rec:1.003, round:165.979)	b=5.38	count=17000
Total loss:	90.296 (rec:1.026, round:89.271)	b=4.81	count=17500
Total loss:	36.938 (rec:0.903, round:36.035)	b=4.25	count=18000
Total loss:	9.258 (rec:0.904, round:8.353)	b=3.69	count=18500
Total loss:	1.898 (rec:1.099, round:0.800)	b=3.12	count=19000
Total loss:	0.996 (rec:0.959, round:0.037)	b=2.56	count=19500
Total loss:	1.044 (rec:1.044, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.14.
reconstructing layers.2.blocks.15 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.15 ...
wraping quantizers in layers.2.blocks.15 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.496 (rec:1.496, round:0.000)	b=0.00	count=500
Total loss:	1.003 (rec:1.003, round:0.000)	b=0.00	count=1000
Total loss:	1.074 (rec:1.074, round:0.000)	b=0.00	count=1500
Total loss:	1.021 (rec:1.021, round:0.000)	b=0.00	count=2000
Total loss:	0.884 (rec:0.884, round:0.000)	b=0.00	count=2500
Total loss:	1.042 (rec:1.042, round:0.000)	b=0.00	count=3000
Total loss:	0.886 (rec:0.886, round:0.000)	b=0.00	count=3500
Total loss:	27352.246 (rec:0.895, round:27351.352)	b=20.00	count=4000
Total loss:	12055.439 (rec:0.948, round:12054.492)	b=19.44	count=4500
Total loss:	10824.835 (rec:0.889, round:10823.946)	b=18.88	count=5000
Total loss:	9881.565 (rec:0.820, round:9880.746)	b=18.31	count=5500
Total loss:	8996.178 (rec:0.867, round:8995.311)	b=17.75	count=6000
Total loss:	8153.523 (rec:0.851, round:8152.672)	b=17.19	count=6500
Total loss:	7359.812 (rec:0.879, round:7358.934)	b=16.62	count=7000
Total loss:	6617.542 (rec:0.781, round:6616.761)	b=16.06	count=7500
Total loss:	5938.687 (rec:0.789, round:5937.898)	b=15.50	count=8000
Total loss:	5325.679 (rec:0.773, round:5324.906)	b=14.94	count=8500
Total loss:	4764.167 (rec:0.831, round:4763.336)	b=14.38	count=9000
Total loss:	4249.185 (rec:0.715, round:4248.469)	b=13.81	count=9500
Total loss:	3776.557 (rec:0.776, round:3775.781)	b=13.25	count=10000
Total loss:	3345.509 (rec:0.832, round:3344.677)	b=12.69	count=10500
Total loss:	2951.556 (rec:0.814, round:2950.743)	b=12.12	count=11000
Total loss:	2588.665 (rec:0.804, round:2587.861)	b=11.56	count=11500
Total loss:	2254.521 (rec:0.889, round:2253.632)	b=11.00	count=12000
Total loss:	1951.685 (rec:0.839, round:1950.846)	b=10.44	count=12500
Total loss:	1677.109 (rec:0.731, round:1676.379)	b=9.88	count=13000
Total loss:	1423.696 (rec:0.773, round:1422.923)	b=9.31	count=13500
Total loss:	1189.903 (rec:0.828, round:1189.075)	b=8.75	count=14000
Total loss:	976.940 (rec:0.902, round:976.038)	b=8.19	count=14500
Total loss:	784.371 (rec:0.871, round:783.500)	b=7.62	count=15000
Total loss:	609.318 (rec:0.861, round:608.457)	b=7.06	count=15500
Total loss:	451.762 (rec:0.837, round:450.925)	b=6.50	count=16000
Total loss:	316.705 (rec:0.765, round:315.940)	b=5.94	count=16500
Total loss:	202.535 (rec:0.889, round:201.646)	b=5.38	count=17000
Total loss:	111.529 (rec:0.857, round:110.672)	b=4.81	count=17500
Total loss:	47.254 (rec:0.820, round:46.435)	b=4.25	count=18000
Total loss:	11.553 (rec:0.852, round:10.701)	b=3.69	count=18500
Total loss:	2.002 (rec:0.836, round:1.166)	b=3.12	count=19000
Total loss:	0.811 (rec:0.764, round:0.047)	b=2.56	count=19500
Total loss:	0.757 (rec:0.757, round:0.000)	b=2.00	count=20000
finished reconstructing layers.2.blocks.15.
reconstructing layers.2.blocks.16 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.16 ...
wraping quantizers in layers.2.blocks.16 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.091 (rec:1.091, round:0.000)	b=0.00	count=500
Total loss:	1.134 (rec:1.134, round:0.000)	b=0.00	count=1000
Total loss:	0.897 (rec:0.897, round:0.000)	b=0.00	count=1500
Total loss:	1.026 (rec:1.026, round:0.000)	b=0.00	count=2000
Total loss:	0.861 (rec:0.861, round:0.000)	b=0.00	count=2500
Total loss:	0.905 (rec:0.905, round:0.000)	b=0.00	count=3000
Total loss:	0.881 (rec:0.881, round:0.000)	b=0.00	count=3500
Total loss:	27697.861 (rec:0.759, round:27697.102)	b=20.00	count=4000
Total loss:	12493.175 (rec:0.815, round:12492.359)	b=19.44	count=4500
Total loss:	11274.227 (rec:0.813, round:11273.414)	b=18.88	count=5000
Total loss:	10346.703 (rec:0.795, round:10345.908)	b=18.31	count=5500
Total loss:	9490.479 (rec:0.700, round:9489.780)	b=17.75	count=6000
Total loss:	8687.126 (rec:0.743, round:8686.384)	b=17.19	count=6500
Total loss:	7936.425 (rec:0.771, round:7935.654)	b=16.62	count=7000
Total loss:	7235.352 (rec:0.773, round:7234.579)	b=16.06	count=7500
Total loss:	6581.548 (rec:0.662, round:6580.886)	b=15.50	count=8000
Total loss:	5977.375 (rec:0.682, round:5976.692)	b=14.94	count=8500
Total loss:	5410.872 (rec:0.686, round:5410.187)	b=14.38	count=9000
Total loss:	4882.897 (rec:0.661, round:4882.236)	b=13.81	count=9500
Total loss:	4392.306 (rec:0.712, round:4391.594)	b=13.25	count=10000
Total loss:	3932.655 (rec:0.715, round:3931.940)	b=12.69	count=10500
Total loss:	3506.389 (rec:0.761, round:3505.628)	b=12.12	count=11000
Total loss:	3108.061 (rec:0.698, round:3107.363)	b=11.56	count=11500
Total loss:	2735.623 (rec:0.692, round:2734.931)	b=11.00	count=12000
Total loss:	2389.488 (rec:0.657, round:2388.831)	b=10.44	count=12500
Total loss:	2067.878 (rec:0.683, round:2067.195)	b=9.88	count=13000
Total loss:	1772.478 (rec:0.673, round:1771.806)	b=9.31	count=13500
Total loss:	1494.766 (rec:0.729, round:1494.036)	b=8.75	count=14000
Total loss:	1235.182 (rec:0.723, round:1234.459)	b=8.19	count=14500
Total loss:	999.854 (rec:0.745, round:999.109)	b=7.62	count=15000
Total loss:	786.679 (rec:0.713, round:785.967)	b=7.06	count=15500
Total loss:	594.045 (rec:0.677, round:593.368)	b=6.50	count=16000
Total loss:	422.867 (rec:0.743, round:422.124)	b=5.94	count=16500
Total loss:	275.205 (rec:0.698, round:274.507)	b=5.38	count=17000
Total loss:	155.006 (rec:0.717, round:154.289)	b=4.81	count=17500
Total loss:	65.191 (rec:0.750, round:64.440)	b=4.25	count=18000
Total loss:	13.582 (rec:0.634, round:12.947)	b=3.69	count=18500
Total loss:	1.779 (rec:0.725, round:1.054)	b=3.12	count=19000
Total loss:	0.768 (rec:0.708, round:0.060)	b=2.56	count=19500
Total loss:	0.692 (rec:0.690, round:0.001)	b=2.00	count=20000
finished reconstructing layers.2.blocks.16.
reconstructing layers.2.blocks.17 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.2.blocks.17 ...
wraping quantizers in layers.2.blocks.17 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.462 (rec:1.462, round:0.000)	b=0.00	count=500
Total loss:	1.571 (rec:1.571, round:0.000)	b=0.00	count=1000
Total loss:	1.112 (rec:1.112, round:0.000)	b=0.00	count=1500
Total loss:	1.450 (rec:1.450, round:0.000)	b=0.00	count=2000
Total loss:	1.409 (rec:1.409, round:0.000)	b=0.00	count=2500
Total loss:	1.261 (rec:1.261, round:0.000)	b=0.00	count=3000
Total loss:	1.300 (rec:1.300, round:0.000)	b=0.00	count=3500
Total loss:	27721.943 (rec:1.231, round:27720.713)	b=20.00	count=4000
Total loss:	12412.821 (rec:1.137, round:12411.685)	b=19.44	count=4500
Total loss:	11331.315 (rec:1.266, round:11330.050)	b=18.88	count=5000
Total loss:	10549.218 (rec:1.250, round:10547.968)	b=18.31	count=5500
Total loss:	9837.669 (rec:1.166, round:9836.503)	b=17.75	count=6000
Total loss:	9156.738 (rec:1.097, round:9155.642)	b=17.19	count=6500
Total loss:	8495.881 (rec:1.090, round:8494.791)	b=16.62	count=7000
Total loss:	7862.857 (rec:1.204, round:7861.653)	b=16.06	count=7500
Total loss:	7250.054 (rec:1.037, round:7249.017)	b=15.50	count=8000
Total loss:	6661.353 (rec:1.120, round:6660.232)	b=14.94	count=8500
Total loss:	6095.579 (rec:1.189, round:6094.391)	b=14.38	count=9000
Total loss:	5554.829 (rec:1.199, round:5553.630)	b=13.81	count=9500
Total loss:	5041.223 (rec:1.164, round:5040.059)	b=13.25	count=10000
Total loss:	4554.348 (rec:1.243, round:4553.105)	b=12.69	count=10500
Total loss:	4091.658 (rec:1.170, round:4090.487)	b=12.12	count=11000
Total loss:	3654.904 (rec:1.194, round:3653.710)	b=11.56	count=11500
Total loss:	3243.130 (rec:1.044, round:3242.086)	b=11.00	count=12000
Total loss:	2859.338 (rec:1.033, round:2858.305)	b=10.44	count=12500
Total loss:	2494.403 (rec:1.333, round:2493.070)	b=9.88	count=13000
Total loss:	2150.703 (rec:1.280, round:2149.423)	b=9.31	count=13500
Total loss:	1825.314 (rec:1.090, round:1824.224)	b=8.75	count=14000
Total loss:	1522.620 (rec:1.227, round:1521.393)	b=8.19	count=14500
Total loss:	1239.562 (rec:1.202, round:1238.361)	b=7.62	count=15000
Total loss:	977.775 (rec:1.196, round:976.578)	b=7.06	count=15500
Total loss:	742.584 (rec:1.287, round:741.296)	b=6.50	count=16000
Total loss:	533.262 (rec:1.334, round:531.928)	b=5.94	count=16500
Total loss:	348.470 (rec:1.104, round:347.366)	b=5.38	count=17000
Total loss:	197.892 (rec:1.160, round:196.733)	b=4.81	count=17500
Total loss:	84.496 (rec:1.195, round:83.301)	b=4.25	count=18000
Total loss:	22.282 (rec:1.096, round:21.185)	b=3.69	count=18500
Total loss:	3.752 (rec:1.091, round:2.662)	b=3.12	count=19000
Total loss:	1.481 (rec:1.333, round:0.148)	b=2.56	count=19500
Total loss:	1.163 (rec:1.161, round:0.002)	b=2.00	count=20000
finished reconstructing layers.2.blocks.17.
reconstructing layers.3.downsample ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.downsample ...
wraping quantizers in layers.3.downsample ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.906 (rec:1.906, round:0.000)	b=0.00	count=500
Total loss:	1.629 (rec:1.629, round:0.000)	b=0.00	count=1000
Total loss:	1.430 (rec:1.430, round:0.000)	b=0.00	count=1500
Total loss:	1.439 (rec:1.439, round:0.000)	b=0.00	count=2000
Total loss:	1.594 (rec:1.594, round:0.000)	b=0.00	count=2500
Total loss:	1.576 (rec:1.576, round:0.000)	b=0.00	count=3000
Total loss:	1.646 (rec:1.646, round:0.000)	b=0.00	count=3500
Total loss:	18845.848 (rec:1.617, round:18844.230)	b=20.00	count=4000
Total loss:	8869.927 (rec:1.547, round:8868.379)	b=19.44	count=4500
Total loss:	8147.069 (rec:1.472, round:8145.597)	b=18.88	count=5000
Total loss:	7659.282 (rec:1.657, round:7657.625)	b=18.31	count=5500
Total loss:	7233.012 (rec:1.748, round:7231.264)	b=17.75	count=6000
Total loss:	6828.732 (rec:1.457, round:6827.275)	b=17.19	count=6500
Total loss:	6446.807 (rec:1.789, round:6445.018)	b=16.62	count=7000
Total loss:	6073.741 (rec:1.642, round:6072.099)	b=16.06	count=7500
Total loss:	5706.261 (rec:1.437, round:5704.824)	b=15.50	count=8000
Total loss:	5349.238 (rec:1.493, round:5347.745)	b=14.94	count=8500
Total loss:	5002.268 (rec:1.359, round:5000.909)	b=14.38	count=9000
Total loss:	4656.930 (rec:1.609, round:4655.321)	b=13.81	count=9500
Total loss:	4315.672 (rec:1.692, round:4313.980)	b=13.25	count=10000
Total loss:	3985.812 (rec:1.545, round:3984.267)	b=12.69	count=10500
Total loss:	3662.222 (rec:1.414, round:3660.809)	b=12.12	count=11000
Total loss:	3347.528 (rec:1.727, round:3345.801)	b=11.56	count=11500
Total loss:	3038.608 (rec:1.710, round:3036.899)	b=11.00	count=12000
Total loss:	2732.633 (rec:1.445, round:2731.188)	b=10.44	count=12500
Total loss:	2433.450 (rec:1.581, round:2431.869)	b=9.88	count=13000
Total loss:	2143.194 (rec:1.726, round:2141.468)	b=9.31	count=13500
Total loss:	1856.933 (rec:1.282, round:1855.650)	b=8.75	count=14000
Total loss:	1575.706 (rec:1.354, round:1574.352)	b=8.19	count=14500
Total loss:	1303.571 (rec:1.531, round:1302.039)	b=7.62	count=15000
Total loss:	1037.578 (rec:1.711, round:1035.867)	b=7.06	count=15500
Total loss:	787.195 (rec:1.593, round:785.603)	b=6.50	count=16000
Total loss:	553.053 (rec:1.649, round:551.404)	b=5.94	count=16500
Total loss:	343.275 (rec:1.364, round:341.911)	b=5.38	count=17000
Total loss:	171.918 (rec:1.425, round:170.493)	b=4.81	count=17500
Total loss:	61.621 (rec:1.300, round:60.321)	b=4.25	count=18000
Total loss:	13.064 (rec:1.403, round:11.660)	b=3.69	count=18500
Total loss:	2.623 (rec:1.664, round:0.959)	b=3.12	count=19000
Total loss:	1.499 (rec:1.437, round:0.062)	b=2.56	count=19500
Total loss:	1.424 (rec:1.416, round:0.008)	b=2.00	count=20000
finished reconstructing layers.3.downsample.
reconstructing layers.3.blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.blocks.0 ...
wraping quantizers in layers.3.blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.486 (rec:1.486, round:0.000)	b=0.00	count=500
Total loss:	1.392 (rec:1.392, round:0.000)	b=0.00	count=1000
Total loss:	1.321 (rec:1.321, round:0.000)	b=0.00	count=1500
Total loss:	1.164 (rec:1.164, round:0.000)	b=0.00	count=2000
Total loss:	1.131 (rec:1.131, round:0.000)	b=0.00	count=2500
Total loss:	1.233 (rec:1.233, round:0.000)	b=0.00	count=3000
Total loss:	1.312 (rec:1.312, round:0.000)	b=0.00	count=3500
Total loss:	116881.258 (rec:1.208, round:116880.047)	b=20.00	count=4000
Total loss:	55226.734 (rec:1.242, round:55225.492)	b=19.44	count=4500
Total loss:	51034.410 (rec:1.161, round:51033.250)	b=18.88	count=5000
Total loss:	48254.344 (rec:0.909, round:48253.434)	b=18.31	count=5500
Total loss:	45818.105 (rec:1.118, round:45816.988)	b=17.75	count=6000
Total loss:	43508.504 (rec:1.054, round:43507.449)	b=17.19	count=6500
Total loss:	41265.414 (rec:0.973, round:41264.441)	b=16.62	count=7000
Total loss:	39049.266 (rec:0.946, round:39048.320)	b=16.06	count=7500
Total loss:	36842.848 (rec:1.036, round:36841.812)	b=15.50	count=8000
Total loss:	34647.320 (rec:1.138, round:34646.184)	b=14.94	count=8500
Total loss:	32452.324 (rec:1.027, round:32451.297)	b=14.38	count=9000
Total loss:	30270.043 (rec:1.065, round:30268.979)	b=13.81	count=9500
Total loss:	28100.227 (rec:1.038, round:28099.188)	b=13.25	count=10000
Total loss:	25949.215 (rec:1.034, round:25948.180)	b=12.69	count=10500
Total loss:	23806.055 (rec:0.997, round:23805.057)	b=12.12	count=11000
Total loss:	21699.748 (rec:1.089, round:21698.660)	b=11.56	count=11500
Total loss:	19628.471 (rec:1.027, round:19627.443)	b=11.00	count=12000
Total loss:	17596.131 (rec:0.942, round:17595.188)	b=10.44	count=12500
Total loss:	15614.701 (rec:0.951, round:15613.750)	b=9.88	count=13000
Total loss:	13679.849 (rec:0.924, round:13678.925)	b=9.31	count=13500
Total loss:	11805.145 (rec:1.004, round:11804.141)	b=8.75	count=14000
Total loss:	9997.763 (rec:0.999, round:9996.764)	b=8.19	count=14500
Total loss:	8274.461 (rec:0.952, round:8273.509)	b=7.62	count=15000
Total loss:	6638.891 (rec:1.081, round:6637.810)	b=7.06	count=15500
Total loss:	5110.182 (rec:1.027, round:5109.154)	b=6.50	count=16000
Total loss:	3708.758 (rec:0.991, round:3707.767)	b=5.94	count=16500
Total loss:	2463.965 (rec:0.984, round:2462.982)	b=5.38	count=17000
Total loss:	1425.654 (rec:0.929, round:1424.725)	b=4.81	count=17500
Total loss:	631.779 (rec:1.043, round:630.735)	b=4.25	count=18000
Total loss:	155.981 (rec:1.053, round:154.928)	b=3.69	count=18500
Total loss:	15.151 (rec:0.931, round:14.220)	b=3.12	count=19000
Total loss:	1.314 (rec:0.875, round:0.440)	b=2.56	count=19500
Total loss:	0.968 (rec:0.962, round:0.005)	b=2.00	count=20000
finished reconstructing layers.3.blocks.0.
reconstructing layers.3.blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for layers.3.blocks.1 ...
wraping quantizers in layers.3.blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.309 (rec:1.309, round:0.000)	b=0.00	count=500
Total loss:	1.292 (rec:1.292, round:0.000)	b=0.00	count=1000
Total loss:	1.169 (rec:1.169, round:0.000)	b=0.00	count=1500
Total loss:	1.263 (rec:1.263, round:0.000)	b=0.00	count=2000
Total loss:	1.308 (rec:1.308, round:0.000)	b=0.00	count=2500
Total loss:	1.210 (rec:1.210, round:0.000)	b=0.00	count=3000
Total loss:	1.177 (rec:1.177, round:0.000)	b=0.00	count=3500
Total loss:	116109.445 (rec:1.178, round:116108.266)	b=20.00	count=4000
Total loss:	54030.758 (rec:1.079, round:54029.680)	b=19.44	count=4500
Total loss:	49792.207 (rec:1.241, round:49790.965)	b=18.88	count=5000
Total loss:	46897.832 (rec:1.106, round:46896.727)	b=18.31	count=5500
Total loss:	44341.496 (rec:1.039, round:44340.457)	b=17.75	count=6000
Total loss:	41923.102 (rec:1.061, round:41922.039)	b=17.19	count=6500
Total loss:	39563.555 (rec:1.069, round:39562.484)	b=16.62	count=7000
Total loss:	37243.227 (rec:1.087, round:37242.141)	b=16.06	count=7500
Total loss:	34958.461 (rec:1.053, round:34957.406)	b=15.50	count=8000
Total loss:	32693.803 (rec:0.972, round:32692.832)	b=14.94	count=8500
Total loss:	30446.582 (rec:1.118, round:30445.465)	b=14.38	count=9000
Total loss:	28220.473 (rec:1.012, round:28219.461)	b=13.81	count=9500
Total loss:	26032.480 (rec:1.020, round:26031.461)	b=13.25	count=10000
Total loss:	23872.613 (rec:1.082, round:23871.531)	b=12.69	count=10500
Total loss:	21761.430 (rec:0.877, round:21760.553)	b=12.12	count=11000
Total loss:	19694.404 (rec:1.072, round:19693.332)	b=11.56	count=11500
Total loss:	17690.109 (rec:0.978, round:17689.131)	b=11.00	count=12000
Total loss:	15745.808 (rec:0.957, round:15744.851)	b=10.44	count=12500
Total loss:	13864.633 (rec:0.913, round:13863.720)	b=9.88	count=13000
Total loss:	12059.472 (rec:0.924, round:12058.548)	b=9.31	count=13500
Total loss:	10332.663 (rec:1.095, round:10331.568)	b=8.75	count=14000
Total loss:	8679.734 (rec:0.978, round:8678.756)	b=8.19	count=14500
Total loss:	7104.507 (rec:1.041, round:7103.466)	b=7.62	count=15000
Total loss:	5626.325 (rec:1.044, round:5625.281)	b=7.06	count=15500
Total loss:	4258.258 (rec:1.117, round:4257.141)	b=6.50	count=16000
Total loss:	3005.997 (rec:0.986, round:3005.011)	b=5.94	count=16500
Total loss:	1909.375 (rec:1.061, round:1908.314)	b=5.38	count=17000
Total loss:	1029.762 (rec:0.917, round:1028.846)	b=4.81	count=17500
Total loss:	411.882 (rec:0.903, round:410.979)	b=4.25	count=18000
Total loss:	77.203 (rec:1.038, round:76.165)	b=3.69	count=18500
Total loss:	5.932 (rec:1.009, round:4.924)	b=3.12	count=19000
Total loss:	1.200 (rec:1.031, round:0.169)	b=2.56	count=19500
Total loss:	1.091 (rec:1.089, round:0.001)	b=2.00	count=20000
finished reconstructing layers.3.blocks.1.
reconstructing head ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for head ...
wraping quantizers in head ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.975 (rec:0.975, round:0.000)	b=0.00	count=500
Total loss:	0.838 (rec:0.838, round:0.000)	b=0.00	count=1000
Total loss:	0.741 (rec:0.741, round:0.000)	b=0.00	count=1500
Total loss:	0.580 (rec:0.580, round:0.000)	b=0.00	count=2000
Total loss:	0.305 (rec:0.305, round:0.000)	b=0.00	count=2500
Total loss:	0.269 (rec:0.269, round:0.000)	b=0.00	count=3000
Total loss:	0.325 (rec:0.325, round:0.000)	b=0.00	count=3500
Total loss:	9329.981 (rec:0.271, round:9329.710)	b=20.00	count=4000
Total loss:	5356.066 (rec:0.270, round:5355.796)	b=19.44	count=4500
Total loss:	4979.089 (rec:0.220, round:4978.869)	b=18.88	count=5000
Total loss:	4736.615 (rec:0.204, round:4736.411)	b=18.31	count=5500
Total loss:	4532.476 (rec:0.161, round:4532.314)	b=17.75	count=6000
Total loss:	4343.594 (rec:0.162, round:4343.433)	b=17.19	count=6500
Total loss:	4166.496 (rec:0.210, round:4166.286)	b=16.62	count=7000
Total loss:	3994.149 (rec:0.138, round:3994.011)	b=16.06	count=7500
Total loss:	3824.701 (rec:0.120, round:3824.581)	b=15.50	count=8000
Total loss:	3657.439 (rec:0.120, round:3657.320)	b=14.94	count=8500
Total loss:	3495.825 (rec:0.157, round:3495.669)	b=14.38	count=9000
Total loss:	3333.794 (rec:0.161, round:3333.633)	b=13.81	count=9500
Total loss:	3172.370 (rec:0.169, round:3172.201)	b=13.25	count=10000
Total loss:	3010.579 (rec:0.131, round:3010.448)	b=12.69	count=10500
Total loss:	2847.735 (rec:0.108, round:2847.627)	b=12.12	count=11000
Total loss:	2684.474 (rec:0.143, round:2684.331)	b=11.56	count=11500
Total loss:	2520.391 (rec:0.147, round:2520.245)	b=11.00	count=12000
Total loss:	2352.935 (rec:0.141, round:2352.795)	b=10.44	count=12500
Total loss:	2181.563 (rec:0.125, round:2181.438)	b=9.88	count=13000
Total loss:	2009.350 (rec:0.135, round:2009.214)	b=9.31	count=13500
Total loss:	1832.155 (rec:0.171, round:1831.984)	b=8.75	count=14000
Total loss:	1650.455 (rec:0.120, round:1650.335)	b=8.19	count=14500
Total loss:	1464.149 (rec:0.135, round:1464.013)	b=7.62	count=15000
Total loss:	1276.303 (rec:0.118, round:1276.185)	b=7.06	count=15500
Total loss:	1083.391 (rec:0.102, round:1083.289)	b=6.50	count=16000
Total loss:	889.438 (rec:0.135, round:889.303)	b=5.94	count=16500
Total loss:	695.361 (rec:0.155, round:695.205)	b=5.38	count=17000
Total loss:	505.297 (rec:0.123, round:505.175)	b=4.81	count=17500
Total loss:	328.036 (rec:0.113, round:327.924)	b=4.25	count=18000
Total loss:	171.375 (rec:0.142, round:171.234)	b=3.69	count=18500
Total loss:	58.018 (rec:0.146, round:57.873)	b=3.12	count=19000
Total loss:	8.667 (rec:0.129, round:8.538)	b=2.56	count=19500
Total loss:	0.556 (rec:0.127, round:0.429)	b=2.00	count=20000
finished reconstructing head.
2025-09-11 15:44:11 - mse guided block reconstruction finished.
Saving checkpoint to ./checkpoint/quant_result/20250911_1054/swin_base_w4_a4_optimsize_1024_mse_qdrop.pth
Validating on calibration set after block reconstruction ...
Test: [0/32]	Time 0.516 (0.516)	Loss 0.5601 (0.5601)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Test: [10/32]	Time 0.097 (0.135)	Loss 1.0501 (0.8833)	Prec@1 87.500 (86.080)	Prec@5 90.625 (95.455)
Test: [20/32]	Time 0.097 (0.117)	Loss 0.5856 (0.7846)	Prec@1 93.750 (87.351)	Prec@5 100.000 (97.470)
Test: [30/32]	Time 0.097 (0.111)	Loss 1.0213 (0.8403)	Prec@1 84.375 (86.391)	Prec@5 93.750 (96.774)
 * Prec@1 86.621 Prec@5 96.777 Loss 0.835 Time 3.650
Validating on test set after block reconstruction ...
Test: [0/100]	Time 5.521 (5.521)	Loss 0.6918 (0.6918)	Prec@1 90.800 (90.800)	Prec@5 98.200 (98.200)
Test: [10/100]	Time 2.366 (2.650)	Loss 0.7052 (0.8244)	Prec@1 89.400 (86.455)	Prec@5 98.000 (97.709)
Test: [20/100]	Time 2.365 (2.515)	Loss 0.9000 (0.8465)	Prec@1 81.000 (85.505)	Prec@5 97.800 (97.457)
Test: [30/100]	Time 2.370 (2.468)	Loss 0.7372 (0.8597)	Prec@1 85.600 (84.697)	Prec@5 99.400 (97.426)
Test: [40/100]	Time 2.371 (2.444)	Loss 1.0374 (0.8681)	Prec@1 78.400 (84.873)	Prec@5 95.200 (97.454)
Test: [50/100]	Time 2.372 (2.430)	Loss 1.1849 (0.9068)	Prec@1 76.200 (83.596)	Prec@5 93.000 (97.047)
Test: [60/100]	Time 2.369 (2.421)	Loss 0.9726 (0.9127)	Prec@1 84.600 (83.475)	Prec@5 95.400 (96.944)
Test: [70/100]	Time 2.371 (2.414)	Loss 0.9979 (0.9295)	Prec@1 81.600 (82.896)	Prec@5 96.600 (96.763)
Test: [80/100]	Time 2.373 (2.409)	Loss 0.8598 (0.9356)	Prec@1 86.400 (82.822)	Prec@5 97.000 (96.657)
Test: [90/100]	Time 2.369 (2.405)	Loss 1.2724 (0.9542)	Prec@1 72.800 (82.198)	Prec@5 93.000 (96.543)
 * Prec@1 82.252 Prec@5 96.590 Loss 0.947 Time 240.445
2025-09-11 15:48:15 - finished the process.
Extracting logits from quantized and full-precision models...
Testing combinations:
  Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
  Cluster numbers: [8, 16, 32, 64, 128, 256]
  PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.63%
Result: Top-1: 82.24%, Top-5: 96.63%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.22%
[Alpha=0.10] Top-5 Accuracy: 96.64%
Result: Top-1: 82.22%, Top-5: 96.64%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.64%
Result: Top-1: 82.24%, Top-5: 96.64%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.63%
Result: Top-1: 82.24%, Top-5: 96.63%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.22%
[Alpha=0.10] Top-5 Accuracy: 96.63%
Result: Top-1: 82.22%, Top-5: 96.63%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.25%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.25%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.23%
[Alpha=0.10] Top-5 Accuracy: 96.62%
Result: Top-1: 82.23%, Top-5: 96.62%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.62%
Result: Top-1: 82.24%, Top-5: 96.62%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.63%
Result: Top-1: 82.24%, Top-5: 96.63%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.28%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.28%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.27%
[Alpha=0.10] Top-5 Accuracy: 96.63%
Result: Top-1: 82.27%, Top-5: 96.63%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.25%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.25%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.24%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.26%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.26%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.25%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.25%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.60%
Result: Top-1: 82.24%, Top-5: 96.60%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.25%
[Alpha=0.10] Top-5 Accuracy: 96.62%
Result: Top-1: 82.25%, Top-5: 96.62%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.27%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.27%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.26%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.26%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.25%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.25%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.24%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.26%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.26%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.27%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.27%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.25%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.25%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.22%
[Alpha=0.10] Top-5 Accuracy: 96.60%
Result: Top-1: 82.22%, Top-5: 96.60%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.28%
[Alpha=0.10] Top-5 Accuracy: 96.60%
Result: Top-1: 82.28%, Top-5: 96.60%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.26%
[Alpha=0.10] Top-5 Accuracy: 96.59%
Result: Top-1: 82.26%, Top-5: 96.59%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.30%
[Alpha=0.10] Top-5 Accuracy: 96.60%
Result: Top-1: 82.30%, Top-5: 96.60%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.24%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.26%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.26%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.17%
[Alpha=0.10] Top-5 Accuracy: 96.60%
Result: Top-1: 82.17%, Top-5: 96.60%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.26%
[Alpha=0.10] Top-5 Accuracy: 96.59%
Result: Top-1: 82.26%, Top-5: 96.59%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.23%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.23%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.18%
[Alpha=0.10] Top-5 Accuracy: 96.60%
Result: Top-1: 82.18%, Top-5: 96.60%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.20%
[Alpha=0.10] Top-5 Accuracy: 96.62%
Result: Top-1: 82.20%, Top-5: 96.62%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.28%
[Alpha=0.10] Top-5 Accuracy: 96.61%
Result: Top-1: 82.28%, Top-5: 96.61%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.23%
[Alpha=0.10] Top-5 Accuracy: 96.60%
Result: Top-1: 82.23%, Top-5: 96.60%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.29%
[Alpha=0.10] Top-5 Accuracy: 96.60%
Result: Top-1: 82.29%, Top-5: 96.60%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.60%
Result: Top-1: 82.24%, Top-5: 96.60%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.20%
[Alpha=0.10] Top-5 Accuracy: 96.62%
Result: Top-1: 82.20%, Top-5: 96.62%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.11%
[Alpha=0.10] Top-5 Accuracy: 96.52%
Result: Top-1: 82.11%, Top-5: 96.52%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.03%
[Alpha=0.10] Top-5 Accuracy: 96.50%
Result: Top-1: 82.03%, Top-5: 96.50%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.23%
[Alpha=0.10] Top-5 Accuracy: 96.59%
Result: Top-1: 82.23%, Top-5: 96.59%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.18%
[Alpha=0.10] Top-5 Accuracy: 96.59%
Result: Top-1: 82.18%, Top-5: 96.59%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.23%
[Alpha=0.10] Top-5 Accuracy: 96.59%
Result: Top-1: 82.23%, Top-5: 96.59%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.14%
[Alpha=0.10] Top-5 Accuracy: 96.62%
Result: Top-1: 82.14%, Top-5: 96.62%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.22%
[Alpha=0.10] Top-5 Accuracy: 96.56%
Result: Top-1: 82.22%, Top-5: 96.56%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.24%
[Alpha=0.10] Top-5 Accuracy: 96.59%
Result: Top-1: 82.24%, Top-5: 96.59%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.22%
[Alpha=0.10] Top-5 Accuracy: 96.57%
Result: Top-1: 82.22%, Top-5: 96.57%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.18%
[Alpha=0.10] Top-5 Accuracy: 96.56%
Result: Top-1: 82.18%, Top-5: 96.56%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 79.26%
[Alpha=0.10] Top-5 Accuracy: 95.98%
Result: Top-1: 79.26%, Top-5: 95.98%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 81.87%
[Alpha=0.10] Top-5 Accuracy: 96.45%
Result: Top-1: 81.87%, Top-5: 96.45%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 81.85%
[Alpha=0.10] Top-5 Accuracy: 96.41%
Result: Top-1: 81.85%, Top-5: 96.41%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.05%
[Alpha=0.10] Top-5 Accuracy: 96.54%
Result: Top-1: 82.05%, Top-5: 96.54%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 81.93%
[Alpha=0.10] Top-5 Accuracy: 96.43%
Result: Top-1: 81.93%, Top-5: 96.43%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 82.16%
[Alpha=0.10] Top-5 Accuracy: 96.57%
Result: Top-1: 82.16%, Top-5: 96.57%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 81.99%
[Alpha=0.10] Top-5 Accuracy: 96.53%
Result: Top-1: 81.99%, Top-5: 96.53%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 81.62%
[Alpha=0.10] Top-5 Accuracy: 96.38%
Result: Top-1: 81.62%, Top-5: 96.38%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 81.94%
[Alpha=0.10] Top-5 Accuracy: 96.47%
Result: Top-1: 81.94%, Top-5: 96.47%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 81.98%
[Alpha=0.10] Top-5 Accuracy: 96.49%
Result: Top-1: 81.98%, Top-5: 96.49%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.18%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.18%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.26%
[Alpha=0.20] Top-5 Accuracy: 96.63%
Result: Top-1: 82.26%, Top-5: 96.63%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.23%
[Alpha=0.20] Top-5 Accuracy: 96.62%
Result: Top-1: 82.23%, Top-5: 96.62%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.20%
[Alpha=0.20] Top-5 Accuracy: 96.63%
Result: Top-1: 82.20%, Top-5: 96.63%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.23%
[Alpha=0.20] Top-5 Accuracy: 96.62%
Result: Top-1: 82.23%, Top-5: 96.62%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.22%
[Alpha=0.20] Top-5 Accuracy: 96.60%
Result: Top-1: 82.22%, Top-5: 96.60%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.24%
[Alpha=0.20] Top-5 Accuracy: 96.63%
Result: Top-1: 82.24%, Top-5: 96.63%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.20%
[Alpha=0.20] Top-5 Accuracy: 96.63%
Result: Top-1: 82.20%, Top-5: 96.63%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.23%
[Alpha=0.20] Top-5 Accuracy: 96.62%
Result: Top-1: 82.23%, Top-5: 96.62%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.23%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.23%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.16%
[Alpha=0.20] Top-5 Accuracy: 96.64%
Result: Top-1: 82.16%, Top-5: 96.64%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.21%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.21%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.20%
[Alpha=0.20] Top-5 Accuracy: 96.60%
Result: Top-1: 82.20%, Top-5: 96.60%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.21%
[Alpha=0.20] Top-5 Accuracy: 96.59%
Result: Top-1: 82.21%, Top-5: 96.59%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.23%
[Alpha=0.20] Top-5 Accuracy: 96.62%
Result: Top-1: 82.23%, Top-5: 96.62%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.24%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.24%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.24%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.24%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.25%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.25%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.28%
[Alpha=0.20] Top-5 Accuracy: 96.62%
Result: Top-1: 82.28%, Top-5: 96.62%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.25%
[Alpha=0.20] Top-5 Accuracy: 96.59%
Result: Top-1: 82.25%, Top-5: 96.59%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.12%
[Alpha=0.20] Top-5 Accuracy: 96.62%
Result: Top-1: 82.12%, Top-5: 96.62%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.27%
[Alpha=0.20] Top-5 Accuracy: 96.60%
Result: Top-1: 82.27%, Top-5: 96.60%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.28%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.28%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.25%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.25%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.18%
[Alpha=0.20] Top-5 Accuracy: 96.59%
Result: Top-1: 82.18%, Top-5: 96.59%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.24%
[Alpha=0.20] Top-5 Accuracy: 96.61%
Result: Top-1: 82.24%, Top-5: 96.61%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.22%
[Alpha=0.20] Top-5 Accuracy: 96.60%
Result: Top-1: 82.22%, Top-5: 96.60%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.26%
[Alpha=0.20] Top-5 Accuracy: 96.57%
Result: Top-1: 82.26%, Top-5: 96.57%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.22%
[Alpha=0.20] Top-5 Accuracy: 96.62%
Result: Top-1: 82.22%, Top-5: 96.62%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.29%
[Alpha=0.20] Top-5 Accuracy: 96.60%
Result: Top-1: 82.29%, Top-5: 96.60%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.87%
[Alpha=0.20] Top-5 Accuracy: 96.55%
Result: Top-1: 81.87%, Top-5: 96.55%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.14%
[Alpha=0.20] Top-5 Accuracy: 96.58%
Result: Top-1: 82.14%, Top-5: 96.58%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.15%
[Alpha=0.20] Top-5 Accuracy: 96.60%
Result: Top-1: 82.15%, Top-5: 96.60%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.12%
[Alpha=0.20] Top-5 Accuracy: 96.57%
Result: Top-1: 82.12%, Top-5: 96.57%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.18%
[Alpha=0.20] Top-5 Accuracy: 96.57%
Result: Top-1: 82.18%, Top-5: 96.57%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.23%
[Alpha=0.20] Top-5 Accuracy: 96.57%
Result: Top-1: 82.23%, Top-5: 96.57%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.21%
[Alpha=0.20] Top-5 Accuracy: 96.56%
Result: Top-1: 82.21%, Top-5: 96.56%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.20%
[Alpha=0.20] Top-5 Accuracy: 96.57%
Result: Top-1: 82.20%, Top-5: 96.57%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.16%
[Alpha=0.20] Top-5 Accuracy: 96.59%
Result: Top-1: 82.16%, Top-5: 96.59%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.13%
[Alpha=0.20] Top-5 Accuracy: 96.58%
Result: Top-1: 82.13%, Top-5: 96.58%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.44%
[Alpha=0.20] Top-5 Accuracy: 96.35%
Result: Top-1: 81.44%, Top-5: 96.35%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.89%
[Alpha=0.20] Top-5 Accuracy: 96.36%
Result: Top-1: 81.89%, Top-5: 96.36%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.05%
[Alpha=0.20] Top-5 Accuracy: 96.56%
Result: Top-1: 82.05%, Top-5: 96.56%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.02%
[Alpha=0.20] Top-5 Accuracy: 96.56%
Result: Top-1: 82.02%, Top-5: 96.56%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.02%
[Alpha=0.20] Top-5 Accuracy: 96.57%
Result: Top-1: 82.02%, Top-5: 96.57%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.00%
[Alpha=0.20] Top-5 Accuracy: 96.51%
Result: Top-1: 82.00%, Top-5: 96.51%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.05%
[Alpha=0.20] Top-5 Accuracy: 96.53%
Result: Top-1: 82.05%, Top-5: 96.53%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.15%
[Alpha=0.20] Top-5 Accuracy: 96.57%
Result: Top-1: 82.15%, Top-5: 96.57%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.99%
[Alpha=0.20] Top-5 Accuracy: 96.50%
Result: Top-1: 81.99%, Top-5: 96.50%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 82.09%
[Alpha=0.20] Top-5 Accuracy: 96.52%
Result: Top-1: 82.09%, Top-5: 96.52%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 75.79%
[Alpha=0.20] Top-5 Accuracy: 94.69%
Result: Top-1: 75.79%, Top-5: 94.69%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.29%
[Alpha=0.20] Top-5 Accuracy: 96.13%
Result: Top-1: 81.29%, Top-5: 96.13%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.44%
[Alpha=0.20] Top-5 Accuracy: 96.10%
Result: Top-1: 81.44%, Top-5: 96.10%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.64%
[Alpha=0.20] Top-5 Accuracy: 96.30%
Result: Top-1: 81.64%, Top-5: 96.30%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.64%
[Alpha=0.20] Top-5 Accuracy: 96.24%
Result: Top-1: 81.64%, Top-5: 96.24%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.95%
[Alpha=0.20] Top-5 Accuracy: 96.47%
Result: Top-1: 81.95%, Top-5: 96.47%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.66%
[Alpha=0.20] Top-5 Accuracy: 96.32%
Result: Top-1: 81.66%, Top-5: 96.32%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.13%
[Alpha=0.20] Top-5 Accuracy: 95.94%
Result: Top-1: 81.13%, Top-5: 95.94%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.37%
[Alpha=0.20] Top-5 Accuracy: 96.20%
Result: Top-1: 81.37%, Top-5: 96.20%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 81.78%
[Alpha=0.20] Top-5 Accuracy: 96.29%
Result: Top-1: 81.78%, Top-5: 96.29%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.15%
[Alpha=0.30] Top-5 Accuracy: 96.57%
Result: Top-1: 82.15%, Top-5: 96.57%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.20%
[Alpha=0.30] Top-5 Accuracy: 96.62%
Result: Top-1: 82.20%, Top-5: 96.62%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.21%
[Alpha=0.30] Top-5 Accuracy: 96.63%
Result: Top-1: 82.21%, Top-5: 96.63%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.20%
[Alpha=0.30] Top-5 Accuracy: 96.63%
Result: Top-1: 82.20%, Top-5: 96.63%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.20%
[Alpha=0.30] Top-5 Accuracy: 96.63%
Result: Top-1: 82.20%, Top-5: 96.63%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.15%
[Alpha=0.30] Top-5 Accuracy: 96.61%
Result: Top-1: 82.15%, Top-5: 96.61%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.17%
[Alpha=0.30] Top-5 Accuracy: 96.62%
Result: Top-1: 82.17%, Top-5: 96.62%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.16%
[Alpha=0.30] Top-5 Accuracy: 96.62%
Result: Top-1: 82.16%, Top-5: 96.62%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.22%
[Alpha=0.30] Top-5 Accuracy: 96.63%
Result: Top-1: 82.22%, Top-5: 96.63%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.19%
[Alpha=0.30] Top-5 Accuracy: 96.62%
Result: Top-1: 82.19%, Top-5: 96.62%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.03%
[Alpha=0.30] Top-5 Accuracy: 96.61%
Result: Top-1: 82.03%, Top-5: 96.61%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.11%
[Alpha=0.30] Top-5 Accuracy: 96.59%
Result: Top-1: 82.11%, Top-5: 96.59%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.16%
[Alpha=0.30] Top-5 Accuracy: 96.58%
Result: Top-1: 82.16%, Top-5: 96.58%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.13%
[Alpha=0.30] Top-5 Accuracy: 96.57%
Result: Top-1: 82.13%, Top-5: 96.57%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.13%
[Alpha=0.30] Top-5 Accuracy: 96.61%
Result: Top-1: 82.13%, Top-5: 96.61%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.20%
[Alpha=0.30] Top-5 Accuracy: 96.60%
Result: Top-1: 82.20%, Top-5: 96.60%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.17%
[Alpha=0.30] Top-5 Accuracy: 96.61%
Result: Top-1: 82.17%, Top-5: 96.61%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.14%
[Alpha=0.30] Top-5 Accuracy: 96.60%
Result: Top-1: 82.14%, Top-5: 96.60%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.19%
[Alpha=0.30] Top-5 Accuracy: 96.60%
Result: Top-1: 82.19%, Top-5: 96.60%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.17%
[Alpha=0.30] Top-5 Accuracy: 96.58%
Result: Top-1: 82.17%, Top-5: 96.58%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.80%
[Alpha=0.30] Top-5 Accuracy: 96.58%
Result: Top-1: 81.80%, Top-5: 96.58%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.14%
[Alpha=0.30] Top-5 Accuracy: 96.58%
Result: Top-1: 82.14%, Top-5: 96.58%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.12%
[Alpha=0.30] Top-5 Accuracy: 96.55%
Result: Top-1: 82.12%, Top-5: 96.55%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.17%
[Alpha=0.30] Top-5 Accuracy: 96.59%
Result: Top-1: 82.17%, Top-5: 96.59%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.08%
[Alpha=0.30] Top-5 Accuracy: 96.55%
Result: Top-1: 82.08%, Top-5: 96.55%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.14%
[Alpha=0.30] Top-5 Accuracy: 96.57%
Result: Top-1: 82.14%, Top-5: 96.57%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.09%
[Alpha=0.30] Top-5 Accuracy: 96.57%
Result: Top-1: 82.09%, Top-5: 96.57%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.17%
[Alpha=0.30] Top-5 Accuracy: 96.54%
Result: Top-1: 82.17%, Top-5: 96.54%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.12%
[Alpha=0.30] Top-5 Accuracy: 96.57%
Result: Top-1: 82.12%, Top-5: 96.57%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.21%
[Alpha=0.30] Top-5 Accuracy: 96.58%
Result: Top-1: 82.21%, Top-5: 96.58%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.40%
[Alpha=0.30] Top-5 Accuracy: 96.48%
Result: Top-1: 81.40%, Top-5: 96.48%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.08%
[Alpha=0.30] Top-5 Accuracy: 96.55%
Result: Top-1: 82.08%, Top-5: 96.55%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.99%
[Alpha=0.30] Top-5 Accuracy: 96.53%
Result: Top-1: 81.99%, Top-5: 96.53%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.96%
[Alpha=0.30] Top-5 Accuracy: 96.50%
Result: Top-1: 81.96%, Top-5: 96.50%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.09%
[Alpha=0.30] Top-5 Accuracy: 96.55%
Result: Top-1: 82.09%, Top-5: 96.55%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.17%
[Alpha=0.30] Top-5 Accuracy: 96.56%
Result: Top-1: 82.17%, Top-5: 96.56%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.05%
[Alpha=0.30] Top-5 Accuracy: 96.53%
Result: Top-1: 82.05%, Top-5: 96.53%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.12%
[Alpha=0.30] Top-5 Accuracy: 96.53%
Result: Top-1: 82.12%, Top-5: 96.53%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 82.05%
[Alpha=0.30] Top-5 Accuracy: 96.55%
Result: Top-1: 82.05%, Top-5: 96.55%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.96%
[Alpha=0.30] Top-5 Accuracy: 96.57%
Result: Top-1: 81.96%, Top-5: 96.57%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.29%
[Alpha=0.30] Top-5 Accuracy: 96.12%
Result: Top-1: 80.29%, Top-5: 96.12%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.61%
[Alpha=0.30] Top-5 Accuracy: 96.26%
Result: Top-1: 81.61%, Top-5: 96.26%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.71%
[Alpha=0.30] Top-5 Accuracy: 96.46%
Result: Top-1: 81.71%, Top-5: 96.46%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.75%
[Alpha=0.30] Top-5 Accuracy: 96.44%
Result: Top-1: 81.75%, Top-5: 96.44%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.78%
[Alpha=0.30] Top-5 Accuracy: 96.49%
Result: Top-1: 81.78%, Top-5: 96.49%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.66%
[Alpha=0.30] Top-5 Accuracy: 96.42%
Result: Top-1: 81.66%, Top-5: 96.42%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.72%
[Alpha=0.30] Top-5 Accuracy: 96.44%
Result: Top-1: 81.72%, Top-5: 96.44%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.91%
[Alpha=0.30] Top-5 Accuracy: 96.50%
Result: Top-1: 81.91%, Top-5: 96.50%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.70%
[Alpha=0.30] Top-5 Accuracy: 96.45%
Result: Top-1: 81.70%, Top-5: 96.45%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.84%
[Alpha=0.30] Top-5 Accuracy: 96.44%
Result: Top-1: 81.84%, Top-5: 96.44%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 72.38%
[Alpha=0.30] Top-5 Accuracy: 92.91%
Result: Top-1: 72.38%, Top-5: 92.91%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.49%
[Alpha=0.30] Top-5 Accuracy: 95.76%
Result: Top-1: 80.49%, Top-5: 95.76%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.79%
[Alpha=0.30] Top-5 Accuracy: 95.79%
Result: Top-1: 80.79%, Top-5: 95.79%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.99%
[Alpha=0.30] Top-5 Accuracy: 95.98%
Result: Top-1: 80.99%, Top-5: 95.98%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.14%
[Alpha=0.30] Top-5 Accuracy: 96.08%
Result: Top-1: 81.14%, Top-5: 96.08%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.56%
[Alpha=0.30] Top-5 Accuracy: 96.36%
Result: Top-1: 81.56%, Top-5: 96.36%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.09%
[Alpha=0.30] Top-5 Accuracy: 96.13%
Result: Top-1: 81.09%, Top-5: 96.13%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.64%
[Alpha=0.30] Top-5 Accuracy: 95.61%
Result: Top-1: 80.64%, Top-5: 95.61%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 80.57%
[Alpha=0.30] Top-5 Accuracy: 95.99%
Result: Top-1: 80.57%, Top-5: 95.99%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 81.36%
[Alpha=0.30] Top-5 Accuracy: 96.10%
Result: Top-1: 81.36%, Top-5: 96.10%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.11%
[Alpha=0.40] Top-5 Accuracy: 96.55%
Result: Top-1: 82.11%, Top-5: 96.55%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.07%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.07%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.11%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.11%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.11%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.11%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.09%
[Alpha=0.40] Top-5 Accuracy: 96.57%
Result: Top-1: 82.09%, Top-5: 96.57%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.06%
[Alpha=0.40] Top-5 Accuracy: 96.57%
Result: Top-1: 82.06%, Top-5: 96.57%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.04%
[Alpha=0.40] Top-5 Accuracy: 96.57%
Result: Top-1: 82.04%, Top-5: 96.57%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.09%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.09%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.11%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.11%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.13%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.13%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.82%
[Alpha=0.40] Top-5 Accuracy: 96.59%
Result: Top-1: 81.82%, Top-5: 96.59%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.01%
[Alpha=0.40] Top-5 Accuracy: 96.54%
Result: Top-1: 82.01%, Top-5: 96.54%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.02%
[Alpha=0.40] Top-5 Accuracy: 96.56%
Result: Top-1: 82.02%, Top-5: 96.56%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.03%
[Alpha=0.40] Top-5 Accuracy: 96.53%
Result: Top-1: 82.03%, Top-5: 96.53%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.05%
[Alpha=0.40] Top-5 Accuracy: 96.59%
Result: Top-1: 82.05%, Top-5: 96.59%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.12%
[Alpha=0.40] Top-5 Accuracy: 96.58%
Result: Top-1: 82.12%, Top-5: 96.58%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.09%
[Alpha=0.40] Top-5 Accuracy: 96.57%
Result: Top-1: 82.09%, Top-5: 96.57%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.06%
[Alpha=0.40] Top-5 Accuracy: 96.54%
Result: Top-1: 82.06%, Top-5: 96.54%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.09%
[Alpha=0.40] Top-5 Accuracy: 96.54%
Result: Top-1: 82.09%, Top-5: 96.54%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.05%
[Alpha=0.40] Top-5 Accuracy: 96.55%
Result: Top-1: 82.05%, Top-5: 96.55%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.31%
[Alpha=0.40] Top-5 Accuracy: 96.53%
Result: Top-1: 81.31%, Top-5: 96.53%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.92%
[Alpha=0.40] Top-5 Accuracy: 96.54%
Result: Top-1: 81.92%, Top-5: 96.54%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.98%
[Alpha=0.40] Top-5 Accuracy: 96.53%
Result: Top-1: 81.98%, Top-5: 96.53%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.02%
[Alpha=0.40] Top-5 Accuracy: 96.59%
Result: Top-1: 82.02%, Top-5: 96.59%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.94%
[Alpha=0.40] Top-5 Accuracy: 96.54%
Result: Top-1: 81.94%, Top-5: 96.54%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.96%
[Alpha=0.40] Top-5 Accuracy: 96.55%
Result: Top-1: 81.96%, Top-5: 96.55%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.00%
[Alpha=0.40] Top-5 Accuracy: 96.52%
Result: Top-1: 82.00%, Top-5: 96.52%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.00%
[Alpha=0.40] Top-5 Accuracy: 96.50%
Result: Top-1: 82.00%, Top-5: 96.50%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.95%
[Alpha=0.40] Top-5 Accuracy: 96.53%
Result: Top-1: 81.95%, Top-5: 96.53%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 82.00%
[Alpha=0.40] Top-5 Accuracy: 96.52%
Result: Top-1: 82.00%, Top-5: 96.52%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 80.71%
[Alpha=0.40] Top-5 Accuracy: 96.35%
Result: Top-1: 80.71%, Top-5: 96.35%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.83%
[Alpha=0.40] Top-5 Accuracy: 96.49%
Result: Top-1: 81.83%, Top-5: 96.49%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.78%
[Alpha=0.40] Top-5 Accuracy: 96.47%
Result: Top-1: 81.78%, Top-5: 96.47%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.66%
[Alpha=0.40] Top-5 Accuracy: 96.46%
Result: Top-1: 81.66%, Top-5: 96.46%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.98%
[Alpha=0.40] Top-5 Accuracy: 96.49%
Result: Top-1: 81.98%, Top-5: 96.49%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.97%
[Alpha=0.40] Top-5 Accuracy: 96.53%
Result: Top-1: 81.97%, Top-5: 96.53%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.78%
[Alpha=0.40] Top-5 Accuracy: 96.45%
Result: Top-1: 81.78%, Top-5: 96.45%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.82%
[Alpha=0.40] Top-5 Accuracy: 96.47%
Result: Top-1: 81.82%, Top-5: 96.47%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.87%
[Alpha=0.40] Top-5 Accuracy: 96.48%
Result: Top-1: 81.87%, Top-5: 96.48%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.64%
[Alpha=0.40] Top-5 Accuracy: 96.48%
Result: Top-1: 81.64%, Top-5: 96.48%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 78.70%
[Alpha=0.40] Top-5 Accuracy: 95.71%
Result: Top-1: 78.70%, Top-5: 95.71%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.17%
[Alpha=0.40] Top-5 Accuracy: 96.12%
Result: Top-1: 81.17%, Top-5: 96.12%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.22%
[Alpha=0.40] Top-5 Accuracy: 96.36%
Result: Top-1: 81.22%, Top-5: 96.36%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.31%
[Alpha=0.40] Top-5 Accuracy: 96.31%
Result: Top-1: 81.31%, Top-5: 96.31%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.32%
[Alpha=0.40] Top-5 Accuracy: 96.40%
Result: Top-1: 81.32%, Top-5: 96.40%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 81.20%
[Alpha=0.40] Top-5 Accuracy: 96.33%
Result: Top-1: 81.20%, Top-5: 96.33%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=150
============================================================
slurmstepd-jnfat06: error: *** JOB 1659763 ON jnfat06 CANCELLED AT 2025-09-12T13:43:19 ***
