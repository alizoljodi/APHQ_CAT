Starting ViT-Small W2A6 QDROP experiment at Sun Sep 14 02:27:47 PM CEST 2025
2025-09-14 14:27:50,923 - INFO - Starting multi-seed experiment
2025-09-14 14:27:50,923 - INFO - Architecture: vit_small
2025-09-14 14:27:50,923 - INFO - Weight bits: 2
2025-09-14 14:27:50,923 - INFO - Activation bits: 6
2025-09-14 14:27:50,923 - INFO - Seeds: [1001, 1002, 1003]
2025-09-14 14:27:50,923 - INFO - Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
2025-09-14 14:27:50,923 - INFO - Cluster numbers: [8, 16, 32, 64, 128, 256]
2025-09-14 14:27:50,923 - INFO - PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]
2025-09-14 14:27:50,923 - INFO - Output directory: ./experiment_results/vit_small_w2_a6_20250914_142750
2025-09-14 14:27:50,923 - INFO - Checking basic requirements...
2025-09-14 14:27:50,923 - INFO - Basic checks passed
2025-09-14 14:27:50,923 - INFO - 
Starting experiments for 3 seeds...
2025-09-14 14:27:50,923 - INFO - Total parameter combinations: 600
2025-09-14 14:27:50,924 - INFO - Total experiments: 1800
2025-09-14 14:27:50,924 - INFO - 
============================================================
2025-09-14 14:27:50,924 - INFO - Running experiment 1/3 for seed 1001
2025-09-14 14:27:50,924 - INFO - ============================================================
2025-09-14 14:27:50,924 - INFO - Running experiment for seed 1001
2025-09-14 14:27:50,924 - INFO - Command: /home/alz07xz/project/PD-Quant/pd_quant/bin/python ../test_quant.py --model vit_small --w_bit 2 --a_bit 6 --seed 1001 --config ../configs/4bit/qdrop_baseline.py --dataset /home/alz07xz/imagenet --calib-size 1000 --calib-batch-size 32 --val-batch-size 500 --num-workers 8 --device cuda --alpha-list 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 --num-clusters-list 8 16 32 64 128 256 --pca-dim-list 1 25 50 75 100 125 150 175 200 220 --calibrate --optimize
2025-09-14 14:27:50,924 - INFO - Working directory: /home/alz07xz/project/APHQ_CAT/qdrop
2025-09-14 14:35:07 - start the process.
Namespace(model='vit_small', config='../configs/4bit/qdrop_baseline.py', dataset='/home/alz07xz/imagenet', val_batch_size=500, num_workers=8, device='cuda', reconstruct_mlp=False, load_reconstruct_checkpoint=False, test_reconstruct_checkpoint=False, calibrate=True, load_calibrate_checkpoint=None, test_calibrate_checkpoint=False, optimize=True, load_optimize_checkpoint='', test_optimize_checkpoint=False, print_freq=10, seed=1001, alpha_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], num_clusters_list=[8, 16, 32, 64, 128, 256], pca_dim_list=[1, 25, 50, 75, 100, 125, 150, 175, 200, 220], w_bit=2, a_bit=6, calib_size=1000, calib_batch_size=32)
deefe /home/alz07xz/project/APHQ_CAT/configs/4bit
Successfully imported Config class!
optim_size: 1024
calib_size: 1000
optim_batch_size: 32
calib_batch_size: 32
w_bit: 2
a_bit: 6
qconv_a_bit: 8
qhead_a_bit: 4
calib_metric: mse
matmul_head_channel_wise: True
token_channel_wise: False
eq_n: 128
search_round: 3
keep_gpu: True
optim_metric: mse
use_mean_hessian: False
temp: 20
recon_metric: hessian_perturb
pct: 0.99
optim_mode: qdrop
drop_prob: 0.5
reconstruct_mlp: False
Building model ...
No checkpoint found at './checkpoint/vit_raw/vit_small_patch16_224.bin'
Loading pretrained weights from Hugging Face hub (timm/vit_small_patch16_224.augreg_in21k_ft_in1k)
[timm/vit_small_patch16_224.augreg_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Building validation dataloader ...
Validating on test set on fp ...
Test: [0/100]	Time 11.975 (11.975)	Loss 0.3666 (0.3666)	Prec@1 89.400 (89.400)	Prec@5 98.800 (98.800)
Test: [10/100]	Time 0.290 (2.339)	Loss 0.4641 (0.4857)	Prec@1 87.400 (86.218)	Prec@5 97.200 (97.618)
Test: [20/100]	Time 0.291 (1.718)	Loss 0.6792 (0.5288)	Prec@1 79.200 (85.114)	Prec@5 96.800 (97.457)
Test: [30/100]	Time 0.295 (1.344)	Loss 0.4284 (0.5582)	Prec@1 87.000 (84.335)	Prec@5 99.200 (97.419)
Test: [40/100]	Time 0.297 (1.169)	Loss 0.7823 (0.5542)	Prec@1 79.000 (84.546)	Prec@5 94.800 (97.312)
Test: [50/100]	Time 0.383 (1.057)	Loss 1.0516 (0.6089)	Prec@1 72.400 (82.988)	Prec@5 92.600 (96.859)
Test: [60/100]	Time 0.297 (0.936)	Loss 0.6787 (0.6213)	Prec@1 84.600 (82.859)	Prec@5 94.600 (96.659)
Test: [70/100]	Time 0.299 (0.912)	Loss 0.7787 (0.6469)	Prec@1 80.800 (82.141)	Prec@5 96.200 (96.434)
Test: [80/100]	Time 0.297 (0.944)	Loss 0.6024 (0.6574)	Prec@1 83.000 (81.943)	Prec@5 96.600 (96.244)
Test: [90/100]	Time 0.294 (0.891)	Loss 1.0445 (0.6776)	Prec@1 70.600 (81.284)	Prec@5 92.800 (96.066)
 * Prec@1 81.386 Prec@5 96.132 Loss 0.671 Time 84.331
Wraping quantiztion modules (reparam: False, recon: False) ...
2025-09-14 14:37:15 - start mse guided calibration
  0%|          | 0/74 [00:00<?, ?it/s]calibrating patch_embed.proj:   0%|          | 0/74 [00:00<?, ?it/s]calibrating patch_embed.proj:   1%|▏         | 1/74 [00:06<07:20,  6.03s/it]calibrating blocks.0.attn.qkv:   1%|▏         | 1/74 [00:06<07:20,  6.03s/it]calibrating blocks.0.attn.qkv:   3%|▎         | 2/74 [00:27<18:20, 15.29s/it]calibrating blocks.0.attn.proj:   3%|▎         | 2/74 [00:27<18:20, 15.29s/it]calibrating blocks.0.attn.proj:   4%|▍         | 3/74 [00:37<14:49, 12.53s/it]calibrating blocks.0.attn.matmul1:   4%|▍         | 3/74 [00:37<14:49, 12.53s/it]calibrating blocks.0.attn.matmul1:   5%|▌         | 4/74 [01:12<25:19, 21.70s/it]calibrating blocks.0.attn.matmul2:   5%|▌         | 4/74 [01:12<25:19, 21.70s/it]calibrating blocks.0.attn.matmul2:   7%|▋         | 5/74 [01:41<27:45, 24.13s/it]calibrating blocks.0.mlp.fc1:   7%|▋         | 5/74 [01:41<27:45, 24.13s/it]     calibrating blocks.0.mlp.fc1:   8%|▊         | 6/74 [02:11<29:44, 26.25s/it]calibrating blocks.0.mlp.fc2:   8%|▊         | 6/74 [02:11<29:44, 26.25s/it]calibrating blocks.0.mlp.fc2:   9%|▉         | 7/74 [02:42<31:01, 27.79s/it]calibrating blocks.1.attn.qkv:   9%|▉         | 7/74 [02:42<31:01, 27.79s/it]calibrating blocks.1.attn.qkv:  11%|█         | 8/74 [03:04<28:35, 26.00s/it]calibrating blocks.1.attn.proj:  11%|█         | 8/74 [03:04<28:35, 26.00s/it]calibrating blocks.1.attn.proj:  12%|█▏        | 9/74 [03:14<22:33, 20.82s/it]calibrating blocks.1.attn.matmul1:  12%|█▏        | 9/74 [03:14<22:33, 20.82s/it]calibrating blocks.1.attn.matmul1:  14%|█▎        | 10/74 [03:49<27:06, 25.41s/it]calibrating blocks.1.attn.matmul2:  14%|█▎        | 10/74 [03:49<27:06, 25.41s/it]calibrating blocks.1.attn.matmul2:  15%|█▍        | 11/74 [04:18<27:41, 26.38s/it]calibrating blocks.1.mlp.fc1:  15%|█▍        | 11/74 [04:18<27:41, 26.38s/it]     calibrating blocks.1.mlp.fc1:  16%|█▌        | 12/74 [04:48<28:26, 27.53s/it]calibrating blocks.1.mlp.fc2:  16%|█▌        | 12/74 [04:48<28:26, 27.53s/it]calibrating blocks.1.mlp.fc2:  18%|█▊        | 13/74 [05:18<28:52, 28.40s/it]calibrating blocks.2.attn.qkv:  18%|█▊        | 13/74 [05:18<28:52, 28.40s/it]calibrating blocks.2.attn.qkv:  19%|█▉        | 14/74 [05:41<26:30, 26.51s/it]calibrating blocks.2.attn.proj:  19%|█▉        | 14/74 [05:41<26:30, 26.51s/it]calibrating blocks.2.attn.proj:  20%|██        | 15/74 [05:50<21:01, 21.38s/it]calibrating blocks.2.attn.matmul1:  20%|██        | 15/74 [05:50<21:01, 21.38s/it]calibrating blocks.2.attn.matmul1:  22%|██▏       | 16/74 [06:26<24:56, 25.80s/it]calibrating blocks.2.attn.matmul2:  22%|██▏       | 16/74 [06:26<24:56, 25.80s/it]calibrating blocks.2.attn.matmul2:  23%|██▎       | 17/74 [06:55<25:24, 26.75s/it]calibrating blocks.2.mlp.fc1:  23%|██▎       | 17/74 [06:55<25:24, 26.75s/it]     calibrating blocks.2.mlp.fc1:  24%|██▍       | 18/74 [07:26<26:04, 27.94s/it]calibrating blocks.2.mlp.fc2:  24%|██▍       | 18/74 [07:26<26:04, 27.94s/it]calibrating blocks.2.mlp.fc2:  26%|██▌       | 19/74 [07:56<26:14, 28.62s/it]calibrating blocks.3.attn.qkv:  26%|██▌       | 19/74 [07:56<26:14, 28.62s/it]calibrating blocks.3.attn.qkv:  27%|██▋       | 20/74 [08:18<24:00, 26.68s/it]calibrating blocks.3.attn.proj:  27%|██▋       | 20/74 [08:18<24:00, 26.68s/it]calibrating blocks.3.attn.proj:  28%|██▊       | 21/74 [08:28<18:58, 21.48s/it]calibrating blocks.3.attn.matmul1:  28%|██▊       | 21/74 [08:28<18:58, 21.48s/it]calibrating blocks.3.attn.matmul1:  30%|██▉       | 22/74 [09:03<22:18, 25.75s/it]calibrating blocks.3.attn.matmul2:  30%|██▉       | 22/74 [09:03<22:18, 25.75s/it]calibrating blocks.3.attn.matmul2:  31%|███       | 23/74 [09:32<22:33, 26.54s/it]calibrating blocks.3.mlp.fc1:  31%|███       | 23/74 [09:32<22:33, 26.54s/it]     calibrating blocks.3.mlp.fc1:  32%|███▏      | 24/74 [10:02<23:03, 27.68s/it]calibrating blocks.3.mlp.fc2:  32%|███▏      | 24/74 [10:02<23:03, 27.68s/it]calibrating blocks.3.mlp.fc2:  34%|███▍      | 25/74 [10:32<23:16, 28.50s/it]calibrating blocks.4.attn.qkv:  34%|███▍      | 25/74 [10:32<23:16, 28.50s/it]calibrating blocks.4.attn.qkv:  35%|███▌      | 26/74 [10:55<21:16, 26.60s/it]calibrating blocks.4.attn.proj:  35%|███▌      | 26/74 [10:55<21:16, 26.60s/it]calibrating blocks.4.attn.proj:  36%|███▋      | 27/74 [11:04<16:47, 21.43s/it]calibrating blocks.4.attn.matmul1:  36%|███▋      | 27/74 [11:04<16:47, 21.43s/it]calibrating blocks.4.attn.matmul1:  38%|███▊      | 28/74 [11:40<19:42, 25.71s/it]calibrating blocks.4.attn.matmul2:  38%|███▊      | 28/74 [11:40<19:42, 25.71s/it]calibrating blocks.4.attn.matmul2:  39%|███▉      | 29/74 [12:09<20:00, 26.68s/it]calibrating blocks.4.mlp.fc1:  39%|███▉      | 29/74 [12:09<20:00, 26.68s/it]     calibrating blocks.4.mlp.fc1:  41%|████      | 30/74 [12:39<20:23, 27.80s/it]calibrating blocks.4.mlp.fc2:  41%|████      | 30/74 [12:39<20:23, 27.80s/it]calibrating blocks.4.mlp.fc2:  42%|████▏     | 31/74 [13:09<20:27, 28.55s/it]calibrating blocks.5.attn.qkv:  42%|████▏     | 31/74 [13:09<20:27, 28.55s/it]calibrating blocks.5.attn.qkv:  43%|████▎     | 32/74 [13:32<18:39, 26.65s/it]calibrating blocks.5.attn.proj:  43%|████▎     | 32/74 [13:32<18:39, 26.65s/it]calibrating blocks.5.attn.proj:  45%|████▍     | 33/74 [13:41<14:40, 21.48s/it]calibrating blocks.5.attn.matmul1:  45%|████▍     | 33/74 [13:41<14:40, 21.48s/it]calibrating blocks.5.attn.matmul1:  46%|████▌     | 34/74 [14:17<17:09, 25.74s/it]calibrating blocks.5.attn.matmul2:  46%|████▌     | 34/74 [14:17<17:09, 25.74s/it]calibrating blocks.5.attn.matmul2:  47%|████▋     | 35/74 [14:46<17:22, 26.74s/it]calibrating blocks.5.mlp.fc1:  47%|████▋     | 35/74 [14:46<17:22, 26.74s/it]     calibrating blocks.5.mlp.fc1:  49%|████▊     | 36/74 [15:16<17:35, 27.78s/it]calibrating blocks.5.mlp.fc2:  49%|████▊     | 36/74 [15:16<17:35, 27.78s/it]calibrating blocks.5.mlp.fc2:  50%|█████     | 37/74 [15:46<17:35, 28.54s/it]calibrating blocks.6.attn.qkv:  50%|█████     | 37/74 [15:46<17:35, 28.54s/it]calibrating blocks.6.attn.qkv:  51%|█████▏    | 38/74 [16:09<15:59, 26.66s/it]calibrating blocks.6.attn.proj:  51%|█████▏    | 38/74 [16:09<15:59, 26.66s/it]calibrating blocks.6.attn.proj:  53%|█████▎    | 39/74 [16:18<12:31, 21.48s/it]calibrating blocks.6.attn.matmul1:  53%|█████▎    | 39/74 [16:18<12:31, 21.48s/it]calibrating blocks.6.attn.matmul1:  54%|█████▍    | 40/74 [16:53<14:33, 25.70s/it]calibrating blocks.6.attn.matmul2:  54%|█████▍    | 40/74 [16:53<14:33, 25.70s/it]calibrating blocks.6.attn.matmul2:  55%|█████▌    | 41/74 [17:22<14:40, 26.68s/it]calibrating blocks.6.mlp.fc1:  55%|█████▌    | 41/74 [17:22<14:40, 26.68s/it]     calibrating blocks.6.mlp.fc1:  57%|█████▋    | 42/74 [17:53<14:52, 27.89s/it]calibrating blocks.6.mlp.fc2:  57%|█████▋    | 42/74 [17:53<14:52, 27.89s/it]calibrating blocks.6.mlp.fc2:  58%|█████▊    | 43/74 [18:24<14:48, 28.65s/it]calibrating blocks.7.attn.qkv:  58%|█████▊    | 43/74 [18:24<14:48, 28.65s/it]calibrating blocks.7.attn.qkv:  59%|█████▉    | 44/74 [18:46<13:21, 26.71s/it]calibrating blocks.7.attn.proj:  59%|█████▉    | 44/74 [18:46<13:21, 26.71s/it]calibrating blocks.7.attn.proj:  61%|██████    | 45/74 [18:55<10:24, 21.52s/it]calibrating blocks.7.attn.matmul1:  61%|██████    | 45/74 [18:55<10:24, 21.52s/it]calibrating blocks.7.attn.matmul1:  62%|██████▏   | 46/74 [19:31<11:59, 25.70s/it]calibrating blocks.7.attn.matmul2:  62%|██████▏   | 46/74 [19:31<11:59, 25.70s/it]calibrating blocks.7.attn.matmul2:  64%|██████▎   | 47/74 [19:59<11:56, 26.55s/it]calibrating blocks.7.mlp.fc1:  64%|██████▎   | 47/74 [19:59<11:56, 26.55s/it]     calibrating blocks.7.mlp.fc1:  65%|██████▍   | 48/74 [20:29<11:58, 27.63s/it]calibrating blocks.7.mlp.fc2:  65%|██████▍   | 48/74 [20:29<11:58, 27.63s/it]calibrating blocks.7.mlp.fc2:  66%|██████▌   | 49/74 [20:59<11:49, 28.40s/it]calibrating blocks.8.attn.qkv:  66%|██████▌   | 49/74 [20:59<11:49, 28.40s/it]calibrating blocks.8.attn.qkv:  68%|██████▊   | 50/74 [21:22<10:36, 26.52s/it]calibrating blocks.8.attn.proj:  68%|██████▊   | 50/74 [21:22<10:36, 26.52s/it]calibrating blocks.8.attn.proj:  69%|██████▉   | 51/74 [21:31<08:11, 21.38s/it]calibrating blocks.8.attn.matmul1:  69%|██████▉   | 51/74 [21:31<08:11, 21.38s/it]calibrating blocks.8.attn.matmul1:  70%|███████   | 52/74 [22:07<09:23, 25.62s/it]calibrating blocks.8.attn.matmul2:  70%|███████   | 52/74 [22:07<09:23, 25.62s/it]calibrating blocks.8.attn.matmul2:  72%|███████▏  | 53/74 [22:35<09:15, 26.45s/it]calibrating blocks.8.mlp.fc1:  72%|███████▏  | 53/74 [22:35<09:15, 26.45s/it]     calibrating blocks.8.mlp.fc1:  73%|███████▎  | 54/74 [23:05<09:11, 27.57s/it]calibrating blocks.8.mlp.fc2:  73%|███████▎  | 54/74 [23:05<09:11, 27.57s/it]calibrating blocks.8.mlp.fc2:  74%|███████▍  | 55/74 [23:35<08:59, 28.40s/it]calibrating blocks.9.attn.qkv:  74%|███████▍  | 55/74 [23:35<08:59, 28.40s/it]calibrating blocks.9.attn.qkv:  76%|███████▌  | 56/74 [23:58<07:57, 26.52s/it]calibrating blocks.9.attn.proj:  76%|███████▌  | 56/74 [23:58<07:57, 26.52s/it]calibrating blocks.9.attn.proj:  77%|███████▋  | 57/74 [24:07<06:03, 21.38s/it]calibrating blocks.9.attn.matmul1:  77%|███████▋  | 57/74 [24:07<06:03, 21.38s/it]calibrating blocks.9.attn.matmul1:  78%|███████▊  | 58/74 [24:43<06:51, 25.69s/it]calibrating blocks.9.attn.matmul2:  78%|███████▊  | 58/74 [24:43<06:51, 25.69s/it]calibrating blocks.9.attn.matmul2:  80%|███████▉  | 59/74 [25:11<06:38, 26.55s/it]calibrating blocks.9.mlp.fc1:  80%|███████▉  | 59/74 [25:11<06:38, 26.55s/it]     calibrating blocks.9.mlp.fc1:  81%|████████  | 60/74 [25:42<06:28, 27.72s/it]calibrating blocks.9.mlp.fc2:  81%|████████  | 60/74 [25:42<06:28, 27.72s/it]calibrating blocks.9.mlp.fc2:  82%|████████▏ | 61/74 [26:12<06:11, 28.57s/it]calibrating blocks.10.attn.qkv:  82%|████████▏ | 61/74 [26:12<06:11, 28.57s/it]calibrating blocks.10.attn.qkv:  84%|████████▍ | 62/74 [26:35<05:20, 26.71s/it]calibrating blocks.10.attn.proj:  84%|████████▍ | 62/74 [26:35<05:20, 26.71s/it]calibrating blocks.10.attn.proj:  85%|████████▌ | 63/74 [26:44<03:57, 21.55s/it]calibrating blocks.10.attn.matmul1:  85%|████████▌ | 63/74 [26:44<03:57, 21.55s/it]calibrating blocks.10.attn.matmul1:  86%|████████▋ | 64/74 [27:20<04:18, 25.80s/it]calibrating blocks.10.attn.matmul2:  86%|████████▋ | 64/74 [27:20<04:18, 25.80s/it]calibrating blocks.10.attn.matmul2:  88%|████████▊ | 65/74 [27:49<04:00, 26.68s/it]calibrating blocks.10.mlp.fc1:  88%|████████▊ | 65/74 [27:49<04:00, 26.68s/it]     calibrating blocks.10.mlp.fc1:  89%|████████▉ | 66/74 [28:19<03:42, 27.77s/it]calibrating blocks.10.mlp.fc2:  89%|████████▉ | 66/74 [28:19<03:42, 27.77s/it]calibrating blocks.10.mlp.fc2:  91%|█████████ | 67/74 [28:50<03:21, 28.75s/it]calibrating blocks.11.attn.qkv:  91%|█████████ | 67/74 [28:50<03:21, 28.75s/it]calibrating blocks.11.attn.qkv:  92%|█████████▏| 68/74 [29:12<02:40, 26.82s/it]calibrating blocks.11.attn.proj:  92%|█████████▏| 68/74 [29:12<02:40, 26.82s/it]calibrating blocks.11.attn.proj:  93%|█████████▎| 69/74 [29:22<01:48, 21.60s/it]calibrating blocks.11.attn.matmul1:  93%|█████████▎| 69/74 [29:22<01:48, 21.60s/it]calibrating blocks.11.attn.matmul1:  95%|█████████▍| 70/74 [29:57<01:43, 25.84s/it]calibrating blocks.11.attn.matmul2:  95%|█████████▍| 70/74 [29:57<01:43, 25.84s/it]calibrating blocks.11.attn.matmul2:  96%|█████████▌| 71/74 [30:26<01:20, 26.77s/it]calibrating blocks.11.mlp.fc1:  96%|█████████▌| 71/74 [30:26<01:20, 26.77s/it]     calibrating blocks.11.mlp.fc1:  97%|█████████▋| 72/74 [30:57<00:55, 27.91s/it]calibrating blocks.11.mlp.fc2:  97%|█████████▋| 72/74 [30:57<00:55, 27.91s/it]calibrating blocks.11.mlp.fc2:  99%|█████████▊| 73/74 [31:28<00:28, 28.84s/it]calibrating head:  99%|█████████▊| 73/74 [31:28<00:28, 28.84s/it]             calibrating head: 100%|██████████| 74/74 [31:30<00:00, 20.75s/it]calibrating head: 100%|██████████| 74/74 [31:30<00:00, 25.54s/it]
2025-09-14 15:08:50 - mse guided calibration finished.
Saving checkpoint to ./checkpoint/quant_result/20250914_1435/vit_small_w2_a6_calibsize_1000_mse.pth
Validating after calibration ...
Test: [0/100]	Time 3.886 (3.886)	Loss 5.9796 (5.9796)	Prec@1 2.000 (2.000)	Prec@5 10.000 (10.000)
Test: [10/100]	Time 0.787 (1.069)	Loss 6.7330 (6.3588)	Prec@1 1.400 (1.164)	Prec@5 7.600 (4.491)
Test: [20/100]	Time 0.788 (0.934)	Loss 8.6171 (6.9669)	Prec@1 0.000 (0.819)	Prec@5 0.400 (3.400)
Test: [30/100]	Time 0.786 (0.887)	Loss 6.3574 (7.2511)	Prec@1 0.000 (0.581)	Prec@5 3.800 (2.632)
Test: [40/100]	Time 0.786 (0.863)	Loss 6.6762 (7.1209)	Prec@1 0.400 (0.663)	Prec@5 4.200 (2.951)
Test: [50/100]	Time 0.790 (0.848)	Loss 7.1630 (7.1059)	Prec@1 0.000 (0.663)	Prec@5 1.400 (3.059)
Test: [60/100]	Time 0.783 (0.838)	Loss 7.8954 (7.1179)	Prec@1 0.000 (0.774)	Prec@5 0.600 (3.269)
Test: [70/100]	Time 0.789 (0.831)	Loss 7.3975 (7.1356)	Prec@1 3.000 (0.746)	Prec@5 5.600 (3.276)
Test: [80/100]	Time 0.794 (0.826)	Loss 6.8656 (7.1078)	Prec@1 1.400 (0.820)	Prec@5 3.600 (3.516)
Test: [90/100]	Time 0.787 (0.822)	Loss 6.0990 (7.0908)	Prec@1 3.600 (0.844)	Prec@5 9.000 (3.596)
 * Prec@1 1.144 Prec@5 4.522 Loss 6.971 Time 82.115
Building calibrator ...
2025-09-14 15:10:17 - start mse guided block reconstruction
reconstructing patch_embed ...
initializing raw input and raw output ...
adaround training for patch_embed ...
wraping quantizers in patch_embed ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.024 (rec:0.024, round:0.000)	b=0.00	count=500
Total loss:	0.018 (rec:0.018, round:0.000)	b=0.00	count=1000
Total loss:	0.009 (rec:0.009, round:0.000)	b=0.00	count=1500
Total loss:	0.013 (rec:0.013, round:0.000)	b=0.00	count=2000
Total loss:	0.010 (rec:0.010, round:0.000)	b=0.00	count=2500
Total loss:	0.006 (rec:0.006, round:0.000)	b=0.00	count=3000
Total loss:	0.009 (rec:0.009, round:0.000)	b=0.00	count=3500
Total loss:	2777.647 (rec:0.005, round:2777.642)	b=20.00	count=4000
Total loss:	1431.068 (rec:0.023, round:1431.046)	b=19.44	count=4500
Total loss:	1323.702 (rec:0.016, round:1323.687)	b=18.88	count=5000
Total loss:	1254.439 (rec:0.018, round:1254.422)	b=18.31	count=5500
Total loss:	1192.686 (rec:0.012, round:1192.674)	b=17.75	count=6000
Total loss:	1136.318 (rec:0.013, round:1136.305)	b=17.19	count=6500
Total loss:	1080.651 (rec:0.015, round:1080.636)	b=16.62	count=7000
Total loss:	1022.995 (rec:0.015, round:1022.980)	b=16.06	count=7500
Total loss:	963.364 (rec:0.012, round:963.351)	b=15.50	count=8000
Total loss:	902.894 (rec:0.013, round:902.882)	b=14.94	count=8500
Total loss:	840.605 (rec:0.015, round:840.591)	b=14.38	count=9000
Total loss:	776.945 (rec:0.012, round:776.934)	b=13.81	count=9500
Total loss:	710.742 (rec:0.018, round:710.724)	b=13.25	count=10000
Total loss:	643.093 (rec:0.017, round:643.076)	b=12.69	count=10500
Total loss:	574.806 (rec:0.026, round:574.779)	b=12.12	count=11000
Total loss:	505.276 (rec:0.017, round:505.259)	b=11.56	count=11500
Total loss:	434.992 (rec:0.026, round:434.967)	b=11.00	count=12000
Total loss:	365.051 (rec:0.035, round:365.016)	b=10.44	count=12500
Total loss:	296.099 (rec:0.025, round:296.075)	b=9.88	count=13000
Total loss:	229.085 (rec:0.029, round:229.056)	b=9.31	count=13500
Total loss:	167.601 (rec:0.055, round:167.546)	b=8.75	count=14000
Total loss:	114.416 (rec:0.029, round:114.386)	b=8.19	count=14500
Total loss:	69.321 (rec:0.041, round:69.280)	b=7.62	count=15000
Total loss:	36.050 (rec:0.051, round:35.999)	b=7.06	count=15500
Total loss:	16.020 (rec:0.052, round:15.968)	b=6.50	count=16000
Total loss:	6.367 (rec:0.062, round:6.305)	b=5.94	count=16500
Total loss:	2.320 (rec:0.069, round:2.251)	b=5.38	count=17000
Total loss:	1.076 (rec:0.069, round:1.007)	b=4.81	count=17500
Total loss:	0.678 (rec:0.088, round:0.589)	b=4.25	count=18000
Total loss:	0.322 (rec:0.071, round:0.251)	b=3.69	count=18500
Total loss:	0.168 (rec:0.083, round:0.085)	b=3.12	count=19000
Total loss:	0.085 (rec:0.073, round:0.012)	b=2.56	count=19500
Total loss:	0.057 (rec:0.057, round:0.000)	b=2.00	count=20000
finished reconstructing patch_embed.
reconstructing blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.0 ...
wraping quantizers in blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.485 (rec:0.485, round:0.000)	b=0.00	count=500
Total loss:	0.423 (rec:0.423, round:0.000)	b=0.00	count=1000
Total loss:	0.402 (rec:0.402, round:0.000)	b=0.00	count=1500
Total loss:	0.325 (rec:0.325, round:0.000)	b=0.00	count=2000
Total loss:	0.345 (rec:0.345, round:0.000)	b=0.00	count=2500
Total loss:	0.304 (rec:0.304, round:0.000)	b=0.00	count=3000
Total loss:	0.350 (rec:0.350, round:0.000)	b=0.00	count=3500
Total loss:	15779.184 (rec:0.278, round:15778.906)	b=20.00	count=4000
Total loss:	7655.315 (rec:0.331, round:7654.984)	b=19.44	count=4500
Total loss:	6983.853 (rec:0.296, round:6983.558)	b=18.88	count=5000
Total loss:	6542.634 (rec:0.354, round:6542.280)	b=18.31	count=5500
Total loss:	6173.432 (rec:0.309, round:6173.122)	b=17.75	count=6000
Total loss:	5837.365 (rec:0.337, round:5837.028)	b=17.19	count=6500
Total loss:	5518.693 (rec:0.368, round:5518.325)	b=16.62	count=7000
Total loss:	5211.527 (rec:0.362, round:5211.165)	b=16.06	count=7500
Total loss:	4916.882 (rec:0.322, round:4916.561)	b=15.50	count=8000
Total loss:	4625.079 (rec:0.324, round:4624.754)	b=14.94	count=8500
Total loss:	4337.561 (rec:0.380, round:4337.181)	b=14.38	count=9000
Total loss:	4053.356 (rec:0.373, round:4052.982)	b=13.81	count=9500
Total loss:	3767.034 (rec:0.392, round:3766.642)	b=13.25	count=10000
Total loss:	3483.762 (rec:0.422, round:3483.341)	b=12.69	count=10500
Total loss:	3196.067 (rec:0.384, round:3195.683)	b=12.12	count=11000
Total loss:	2907.826 (rec:0.341, round:2907.485)	b=11.56	count=11500
Total loss:	2617.114 (rec:0.394, round:2616.719)	b=11.00	count=12000
Total loss:	2324.128 (rec:0.460, round:2323.668)	b=10.44	count=12500
Total loss:	2028.797 (rec:0.534, round:2028.263)	b=9.88	count=13000
Total loss:	1735.320 (rec:0.453, round:1734.867)	b=9.31	count=13500
Total loss:	1441.388 (rec:0.554, round:1440.833)	b=8.75	count=14000
Total loss:	1158.588 (rec:0.543, round:1158.046)	b=8.19	count=14500
Total loss:	889.633 (rec:0.580, round:889.054)	b=7.62	count=15000
Total loss:	646.299 (rec:0.587, round:645.713)	b=7.06	count=15500
Total loss:	438.106 (rec:0.608, round:437.497)	b=6.50	count=16000
Total loss:	271.279 (rec:0.644, round:270.635)	b=5.94	count=16500
Total loss:	148.312 (rec:0.672, round:147.639)	b=5.38	count=17000
Total loss:	68.976 (rec:0.765, round:68.211)	b=4.81	count=17500
Total loss:	26.107 (rec:0.674, round:25.434)	b=4.25	count=18000
Total loss:	8.379 (rec:0.721, round:7.658)	b=3.69	count=18500
Total loss:	2.103 (rec:0.724, round:1.379)	b=3.12	count=19000
Total loss:	0.836 (rec:0.715, round:0.121)	b=2.56	count=19500
Total loss:	0.724 (rec:0.715, round:0.009)	b=2.00	count=20000
finished reconstructing blocks.0.
reconstructing blocks.1 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.1 ...
wraping quantizers in blocks.1 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.746 (rec:0.746, round:0.000)	b=0.00	count=500
Total loss:	0.643 (rec:0.643, round:0.000)	b=0.00	count=1000
Total loss:	0.640 (rec:0.640, round:0.000)	b=0.00	count=1500
Total loss:	0.570 (rec:0.570, round:0.000)	b=0.00	count=2000
Total loss:	0.569 (rec:0.569, round:0.000)	b=0.00	count=2500
Total loss:	0.573 (rec:0.573, round:0.000)	b=0.00	count=3000
Total loss:	0.525 (rec:0.525, round:0.000)	b=0.00	count=3500
Total loss:	16016.993 (rec:0.527, round:16016.467)	b=20.00	count=4000
Total loss:	8000.711 (rec:0.587, round:8000.125)	b=19.44	count=4500
Total loss:	7348.241 (rec:0.593, round:7347.648)	b=18.88	count=5000
Total loss:	6925.625 (rec:0.578, round:6925.047)	b=18.31	count=5500
Total loss:	6569.554 (rec:0.605, round:6568.949)	b=17.75	count=6000
Total loss:	6239.534 (rec:0.546, round:6238.988)	b=17.19	count=6500
Total loss:	5931.227 (rec:0.574, round:5930.652)	b=16.62	count=7000
Total loss:	5630.005 (rec:0.594, round:5629.412)	b=16.06	count=7500
Total loss:	5334.723 (rec:0.593, round:5334.130)	b=15.50	count=8000
Total loss:	5045.718 (rec:0.551, round:5045.167)	b=14.94	count=8500
Total loss:	4756.688 (rec:0.595, round:4756.093)	b=14.38	count=9000
Total loss:	4465.098 (rec:0.576, round:4464.521)	b=13.81	count=9500
Total loss:	4175.802 (rec:0.610, round:4175.192)	b=13.25	count=10000
Total loss:	3883.235 (rec:0.572, round:3882.663)	b=12.69	count=10500
Total loss:	3587.438 (rec:0.606, round:3586.832)	b=12.12	count=11000
Total loss:	3287.376 (rec:0.603, round:3286.773)	b=11.56	count=11500
Total loss:	2983.809 (rec:0.651, round:2983.157)	b=11.00	count=12000
Total loss:	2674.537 (rec:0.603, round:2673.934)	b=10.44	count=12500
Total loss:	2358.396 (rec:0.641, round:2357.754)	b=9.88	count=13000
Total loss:	2040.111 (rec:0.704, round:2039.408)	b=9.31	count=13500
Total loss:	1716.021 (rec:0.656, round:1715.365)	b=8.75	count=14000
Total loss:	1392.664 (rec:0.672, round:1391.992)	b=8.19	count=14500
Total loss:	1082.293 (rec:0.750, round:1081.542)	b=7.62	count=15000
Total loss:	789.787 (rec:0.777, round:789.010)	b=7.06	count=15500
Total loss:	528.610 (rec:0.781, round:527.829)	b=6.50	count=16000
Total loss:	310.157 (rec:0.831, round:309.327)	b=5.94	count=16500
Total loss:	151.875 (rec:0.801, round:151.074)	b=5.38	count=17000
Total loss:	60.531 (rec:0.796, round:59.735)	b=4.81	count=17500
Total loss:	20.949 (rec:0.806, round:20.143)	b=4.25	count=18000
Total loss:	6.236 (rec:0.882, round:5.354)	b=3.69	count=18500
Total loss:	1.706 (rec:0.842, round:0.864)	b=3.12	count=19000
Total loss:	0.887 (rec:0.807, round:0.079)	b=2.56	count=19500
Total loss:	0.867 (rec:0.852, round:0.016)	b=2.00	count=20000
finished reconstructing blocks.1.
reconstructing blocks.2 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.2 ...
wraping quantizers in blocks.2 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.964 (rec:0.964, round:0.000)	b=0.00	count=500
Total loss:	0.853 (rec:0.853, round:0.000)	b=0.00	count=1000
Total loss:	0.844 (rec:0.844, round:0.000)	b=0.00	count=1500
Total loss:	0.802 (rec:0.802, round:0.000)	b=0.00	count=2000
Total loss:	0.758 (rec:0.758, round:0.000)	b=0.00	count=2500
Total loss:	0.731 (rec:0.731, round:0.000)	b=0.00	count=3000
Total loss:	0.707 (rec:0.707, round:0.000)	b=0.00	count=3500
Total loss:	16005.195 (rec:0.721, round:16004.475)	b=20.00	count=4000
Total loss:	8206.511 (rec:0.780, round:8205.730)	b=19.44	count=4500
Total loss:	7570.283 (rec:0.789, round:7569.494)	b=18.88	count=5000
Total loss:	7156.746 (rec:0.752, round:7155.994)	b=18.31	count=5500
Total loss:	6802.296 (rec:0.735, round:6801.561)	b=17.75	count=6000
Total loss:	6481.859 (rec:0.721, round:6481.138)	b=17.19	count=6500
Total loss:	6178.387 (rec:0.758, round:6177.629)	b=16.62	count=7000
Total loss:	5884.107 (rec:0.712, round:5883.396)	b=16.06	count=7500
Total loss:	5594.512 (rec:0.746, round:5593.766)	b=15.50	count=8000
Total loss:	5306.663 (rec:0.762, round:5305.900)	b=14.94	count=8500
Total loss:	5021.416 (rec:0.741, round:5020.674)	b=14.38	count=9000
Total loss:	4734.941 (rec:0.809, round:4734.132)	b=13.81	count=9500
Total loss:	4443.334 (rec:0.798, round:4442.536)	b=13.25	count=10000
Total loss:	4149.509 (rec:0.787, round:4148.722)	b=12.69	count=10500
Total loss:	3852.915 (rec:0.815, round:3852.100)	b=12.12	count=11000
Total loss:	3551.064 (rec:0.789, round:3550.276)	b=11.56	count=11500
Total loss:	3244.558 (rec:0.830, round:3243.728)	b=11.00	count=12000
Total loss:	2931.819 (rec:0.826, round:2930.993)	b=10.44	count=12500
Total loss:	2610.670 (rec:0.848, round:2609.823)	b=9.88	count=13000
Total loss:	2285.305 (rec:0.795, round:2284.511)	b=9.31	count=13500
Total loss:	1954.713 (rec:0.850, round:1953.863)	b=8.75	count=14000
Total loss:	1625.445 (rec:0.867, round:1624.578)	b=8.19	count=14500
Total loss:	1296.721 (rec:0.869, round:1295.852)	b=7.62	count=15000
Total loss:	977.928 (rec:0.877, round:977.051)	b=7.06	count=15500
Total loss:	685.277 (rec:0.901, round:684.376)	b=6.50	count=16000
Total loss:	430.025 (rec:0.927, round:429.098)	b=5.94	count=16500
Total loss:	227.653 (rec:1.014, round:226.639)	b=5.38	count=17000
Total loss:	94.694 (rec:0.966, round:93.729)	b=4.81	count=17500
Total loss:	30.866 (rec:0.997, round:29.869)	b=4.25	count=18000
Total loss:	8.458 (rec:0.982, round:7.476)	b=3.69	count=18500
Total loss:	2.309 (rec:0.999, round:1.310)	b=3.12	count=19000
Total loss:	1.124 (rec:1.022, round:0.103)	b=2.56	count=19500
Total loss:	1.026 (rec:1.024, round:0.002)	b=2.00	count=20000
finished reconstructing blocks.2.
reconstructing blocks.3 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.3 ...
wraping quantizers in blocks.3 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.903 (rec:0.903, round:0.000)	b=0.00	count=500
Total loss:	0.829 (rec:0.829, round:0.000)	b=0.00	count=1000
Total loss:	0.813 (rec:0.813, round:0.000)	b=0.00	count=1500
Total loss:	0.755 (rec:0.755, round:0.000)	b=0.00	count=2000
Total loss:	0.742 (rec:0.742, round:0.000)	b=0.00	count=2500
Total loss:	0.758 (rec:0.758, round:0.000)	b=0.00	count=3000
Total loss:	0.715 (rec:0.715, round:0.000)	b=0.00	count=3500
Total loss:	16108.509 (rec:0.696, round:16107.812)	b=20.00	count=4000
Total loss:	8276.133 (rec:0.744, round:8275.389)	b=19.44	count=4500
Total loss:	7648.565 (rec:0.724, round:7647.841)	b=18.88	count=5000
Total loss:	7244.588 (rec:0.738, round:7243.850)	b=18.31	count=5500
Total loss:	6901.672 (rec:0.737, round:6900.935)	b=17.75	count=6000
Total loss:	6587.634 (rec:0.740, round:6586.894)	b=17.19	count=6500
Total loss:	6285.722 (rec:0.708, round:6285.014)	b=16.62	count=7000
Total loss:	5994.809 (rec:0.717, round:5994.092)	b=16.06	count=7500
Total loss:	5706.846 (rec:0.712, round:5706.133)	b=15.50	count=8000
Total loss:	5422.785 (rec:0.723, round:5422.062)	b=14.94	count=8500
Total loss:	5140.273 (rec:0.721, round:5139.552)	b=14.38	count=9000
Total loss:	4856.391 (rec:0.736, round:4855.654)	b=13.81	count=9500
Total loss:	4569.943 (rec:0.732, round:4569.211)	b=13.25	count=10000
Total loss:	4279.964 (rec:0.756, round:4279.208)	b=12.69	count=10500
Total loss:	3983.954 (rec:0.719, round:3983.235)	b=12.12	count=11000
Total loss:	3686.499 (rec:0.761, round:3685.738)	b=11.56	count=11500
Total loss:	3378.538 (rec:0.747, round:3377.791)	b=11.00	count=12000
Total loss:	3066.721 (rec:0.752, round:3065.969)	b=10.44	count=12500
Total loss:	2750.741 (rec:0.788, round:2749.953)	b=9.88	count=13000
Total loss:	2431.764 (rec:0.758, round:2431.006)	b=9.31	count=13500
Total loss:	2105.191 (rec:0.801, round:2104.391)	b=8.75	count=14000
Total loss:	1777.481 (rec:0.771, round:1776.710)	b=8.19	count=14500
Total loss:	1449.827 (rec:0.801, round:1449.026)	b=7.62	count=15000
Total loss:	1131.408 (rec:0.825, round:1130.584)	b=7.06	count=15500
Total loss:	829.452 (rec:0.832, round:828.620)	b=6.50	count=16000
Total loss:	555.615 (rec:0.890, round:554.725)	b=5.94	count=16500
Total loss:	323.546 (rec:0.912, round:322.634)	b=5.38	count=17000
Total loss:	155.362 (rec:0.899, round:154.463)	b=4.81	count=17500
Total loss:	59.517 (rec:0.927, round:58.590)	b=4.25	count=18000
Total loss:	16.579 (rec:0.921, round:15.658)	b=3.69	count=18500
Total loss:	2.879 (rec:0.919, round:1.959)	b=3.12	count=19000
Total loss:	1.051 (rec:0.953, round:0.098)	b=2.56	count=19500
Total loss:	0.911 (rec:0.910, round:0.001)	b=2.00	count=20000
finished reconstructing blocks.3.
reconstructing blocks.4 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.4 ...
wraping quantizers in blocks.4 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.954 (rec:0.954, round:0.000)	b=0.00	count=500
Total loss:	0.873 (rec:0.873, round:0.000)	b=0.00	count=1000
Total loss:	0.838 (rec:0.838, round:0.000)	b=0.00	count=1500
Total loss:	0.796 (rec:0.796, round:0.000)	b=0.00	count=2000
Total loss:	0.788 (rec:0.788, round:0.000)	b=0.00	count=2500
Total loss:	0.756 (rec:0.756, round:0.000)	b=0.00	count=3000
Total loss:	0.739 (rec:0.739, round:0.000)	b=0.00	count=3500
Total loss:	16124.440 (rec:0.732, round:16123.708)	b=20.00	count=4000
Total loss:	8304.016 (rec:0.769, round:8303.247)	b=19.44	count=4500
Total loss:	7675.776 (rec:0.763, round:7675.013)	b=18.88	count=5000
Total loss:	7267.713 (rec:0.750, round:7266.963)	b=18.31	count=5500
Total loss:	6931.130 (rec:0.771, round:6930.359)	b=17.75	count=6000
Total loss:	6622.663 (rec:0.754, round:6621.909)	b=17.19	count=6500
Total loss:	6328.441 (rec:0.756, round:6327.685)	b=16.62	count=7000
Total loss:	6042.617 (rec:0.781, round:6041.836)	b=16.06	count=7500
Total loss:	5761.350 (rec:0.748, round:5760.603)	b=15.50	count=8000
Total loss:	5481.598 (rec:0.781, round:5480.816)	b=14.94	count=8500
Total loss:	5201.246 (rec:0.792, round:5200.453)	b=14.38	count=9000
Total loss:	4921.804 (rec:0.751, round:4921.054)	b=13.81	count=9500
Total loss:	4640.920 (rec:0.766, round:4640.155)	b=13.25	count=10000
Total loss:	4354.132 (rec:0.789, round:4353.344)	b=12.69	count=10500
Total loss:	4063.512 (rec:0.780, round:4062.733)	b=12.12	count=11000
Total loss:	3765.276 (rec:0.809, round:3764.467)	b=11.56	count=11500
Total loss:	3462.272 (rec:0.794, round:3461.478)	b=11.00	count=12000
Total loss:	3156.904 (rec:0.804, round:3156.100)	b=10.44	count=12500
Total loss:	2842.155 (rec:0.786, round:2841.370)	b=9.88	count=13000
Total loss:	2524.178 (rec:0.819, round:2523.360)	b=9.31	count=13500
Total loss:	2200.785 (rec:0.850, round:2199.935)	b=8.75	count=14000
Total loss:	1872.406 (rec:0.794, round:1871.612)	b=8.19	count=14500
Total loss:	1543.812 (rec:0.843, round:1542.969)	b=7.62	count=15000
Total loss:	1215.960 (rec:0.886, round:1215.073)	b=7.06	count=15500
Total loss:	897.949 (rec:0.899, round:897.050)	b=6.50	count=16000
Total loss:	609.945 (rec:0.916, round:609.029)	b=5.94	count=16500
Total loss:	364.163 (rec:0.935, round:363.229)	b=5.38	count=17000
Total loss:	181.904 (rec:0.933, round:180.971)	b=4.81	count=17500
Total loss:	75.147 (rec:0.957, round:74.190)	b=4.25	count=18000
Total loss:	23.504 (rec:0.971, round:22.533)	b=3.69	count=18500
Total loss:	4.385 (rec:0.934, round:3.451)	b=3.12	count=19000
Total loss:	1.083 (rec:0.960, round:0.123)	b=2.56	count=19500
Total loss:	0.954 (rec:0.951, round:0.003)	b=2.00	count=20000
finished reconstructing blocks.4.
reconstructing blocks.5 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.5 ...
wraping quantizers in blocks.5 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.076 (rec:1.076, round:0.000)	b=0.00	count=500
Total loss:	0.963 (rec:0.963, round:0.000)	b=0.00	count=1000
Total loss:	0.908 (rec:0.908, round:0.000)	b=0.00	count=1500
Total loss:	0.844 (rec:0.844, round:0.000)	b=0.00	count=2000
Total loss:	0.840 (rec:0.840, round:0.000)	b=0.00	count=2500
Total loss:	0.855 (rec:0.855, round:0.000)	b=0.00	count=3000
Total loss:	0.829 (rec:0.829, round:0.000)	b=0.00	count=3500
Total loss:	16083.120 (rec:0.802, round:16082.318)	b=20.00	count=4000
Total loss:	8315.061 (rec:0.859, round:8314.201)	b=19.44	count=4500
Total loss:	7702.214 (rec:0.808, round:7701.406)	b=18.88	count=5000
Total loss:	7310.370 (rec:0.823, round:7309.546)	b=18.31	count=5500
Total loss:	6982.807 (rec:0.816, round:6981.990)	b=17.75	count=6000
Total loss:	6685.214 (rec:0.765, round:6684.449)	b=17.19	count=6500
Total loss:	6403.053 (rec:0.800, round:6402.253)	b=16.62	count=7000
Total loss:	6129.229 (rec:0.795, round:6128.434)	b=16.06	count=7500
Total loss:	5859.409 (rec:0.882, round:5858.527)	b=15.50	count=8000
Total loss:	5587.706 (rec:0.787, round:5586.919)	b=14.94	count=8500
Total loss:	5319.887 (rec:0.775, round:5319.112)	b=14.38	count=9000
Total loss:	5050.169 (rec:0.820, round:5049.349)	b=13.81	count=9500
Total loss:	4775.246 (rec:0.823, round:4774.422)	b=13.25	count=10000
Total loss:	4497.705 (rec:0.799, round:4496.906)	b=12.69	count=10500
Total loss:	4213.438 (rec:0.813, round:4212.625)	b=12.12	count=11000
Total loss:	3924.921 (rec:0.799, round:3924.122)	b=11.56	count=11500
Total loss:	3630.383 (rec:0.830, round:3629.553)	b=11.00	count=12000
Total loss:	3326.945 (rec:0.802, round:3326.144)	b=10.44	count=12500
Total loss:	3015.361 (rec:0.840, round:3014.521)	b=9.88	count=13000
Total loss:	2697.014 (rec:0.861, round:2696.154)	b=9.31	count=13500
Total loss:	2373.622 (rec:0.844, round:2372.778)	b=8.75	count=14000
Total loss:	2045.239 (rec:0.865, round:2044.374)	b=8.19	count=14500
Total loss:	1710.453 (rec:0.852, round:1709.602)	b=7.62	count=15000
Total loss:	1378.774 (rec:0.876, round:1377.898)	b=7.06	count=15500
Total loss:	1055.461 (rec:0.863, round:1054.598)	b=6.50	count=16000
Total loss:	754.606 (rec:0.881, round:753.725)	b=5.94	count=16500
Total loss:	491.059 (rec:0.921, round:490.138)	b=5.38	count=17000
Total loss:	285.143 (rec:0.954, round:284.190)	b=4.81	count=17500
Total loss:	150.055 (rec:0.994, round:149.061)	b=4.25	count=18000
Total loss:	68.634 (rec:0.942, round:67.692)	b=3.69	count=18500
Total loss:	20.871 (rec:0.936, round:19.935)	b=3.12	count=19000
Total loss:	3.221 (rec:0.968, round:2.253)	b=2.56	count=19500
Total loss:	1.004 (rec:0.937, round:0.067)	b=2.00	count=20000
finished reconstructing blocks.5.
reconstructing blocks.6 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.6 ...
wraping quantizers in blocks.6 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.326 (rec:1.326, round:0.000)	b=0.00	count=500
Total loss:	1.500 (rec:1.500, round:0.000)	b=0.00	count=1000
Total loss:	1.514 (rec:1.514, round:0.000)	b=0.00	count=1500
Total loss:	1.224 (rec:1.224, round:0.000)	b=0.00	count=2000
Total loss:	1.270 (rec:1.270, round:0.000)	b=0.00	count=2500
Total loss:	1.319 (rec:1.319, round:0.000)	b=0.00	count=3000
Total loss:	1.158 (rec:1.158, round:0.000)	b=0.00	count=3500
Total loss:	15973.119 (rec:1.282, round:15971.838)	b=20.00	count=4000
Total loss:	8314.818 (rec:1.159, round:8313.659)	b=19.44	count=4500
Total loss:	7710.814 (rec:1.045, round:7709.769)	b=18.88	count=5000
Total loss:	7322.718 (rec:1.281, round:7321.438)	b=18.31	count=5500
Total loss:	6995.538 (rec:1.331, round:6994.207)	b=17.75	count=6000
Total loss:	6696.023 (rec:1.340, round:6694.683)	b=17.19	count=6500
Total loss:	6412.353 (rec:1.342, round:6411.011)	b=16.62	count=7000
Total loss:	6138.123 (rec:1.024, round:6137.100)	b=16.06	count=7500
Total loss:	5871.309 (rec:0.984, round:5870.324)	b=15.50	count=8000
Total loss:	5606.320 (rec:1.374, round:5604.946)	b=14.94	count=8500
Total loss:	5341.232 (rec:0.990, round:5340.242)	b=14.38	count=9000
Total loss:	5073.106 (rec:1.139, round:5071.967)	b=13.81	count=9500
Total loss:	4785.105 (rec:1.018, round:4784.086)	b=13.25	count=10000
Total loss:	4480.949 (rec:1.118, round:4479.831)	b=12.69	count=10500
Total loss:	4178.007 (rec:1.084, round:4176.923)	b=12.12	count=11000
Total loss:	3876.942 (rec:1.030, round:3875.912)	b=11.56	count=11500
Total loss:	3582.699 (rec:1.051, round:3581.648)	b=11.00	count=12000
Total loss:	3285.202 (rec:0.992, round:3284.210)	b=10.44	count=12500
Total loss:	2986.657 (rec:0.941, round:2985.716)	b=9.88	count=13000
Total loss:	2688.803 (rec:1.151, round:2687.652)	b=9.31	count=13500
Total loss:	2390.897 (rec:1.171, round:2389.726)	b=8.75	count=14000
Total loss:	2094.256 (rec:1.224, round:2093.032)	b=8.19	count=14500
Total loss:	1797.660 (rec:1.079, round:1796.581)	b=7.62	count=15000
Total loss:	1499.562 (rec:1.106, round:1498.457)	b=7.06	count=15500
Total loss:	1208.512 (rec:1.110, round:1207.402)	b=6.50	count=16000
Total loss:	937.134 (rec:1.105, round:936.030)	b=5.94	count=16500
Total loss:	693.908 (rec:1.052, round:692.856)	b=5.38	count=17000
Total loss:	491.327 (rec:1.131, round:490.196)	b=4.81	count=17500
Total loss:	328.724 (rec:0.921, round:327.803)	b=4.25	count=18000
Total loss:	199.063 (rec:1.006, round:198.057)	b=3.69	count=18500
Total loss:	97.149 (rec:0.863, round:96.287)	b=3.12	count=19000
Total loss:	30.837 (rec:1.146, round:29.690)	b=2.56	count=19500
Total loss:	5.390 (rec:0.911, round:4.479)	b=2.00	count=20000
finished reconstructing blocks.6.
reconstructing blocks.7 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.7 ...
wraping quantizers in blocks.7 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.549 (rec:1.549, round:0.000)	b=0.00	count=500
Total loss:	1.615 (rec:1.615, round:0.000)	b=0.00	count=1000
Total loss:	1.546 (rec:1.546, round:0.000)	b=0.00	count=1500
Total loss:	1.551 (rec:1.551, round:0.000)	b=0.00	count=2000
Total loss:	0.992 (rec:0.992, round:0.000)	b=0.00	count=2500
Total loss:	1.178 (rec:1.178, round:0.000)	b=0.00	count=3000
Total loss:	1.134 (rec:1.134, round:0.000)	b=0.00	count=3500
Total loss:	15832.251 (rec:1.143, round:15831.108)	b=20.00	count=4000
Total loss:	8038.486 (rec:0.995, round:8037.491)	b=19.44	count=4500
Total loss:	7459.659 (rec:1.208, round:7458.451)	b=18.88	count=5000
Total loss:	7084.272 (rec:1.372, round:7082.900)	b=18.31	count=5500
Total loss:	6766.885 (rec:0.987, round:6765.898)	b=17.75	count=6000
Total loss:	6475.882 (rec:1.114, round:6474.769)	b=17.19	count=6500
Total loss:	6197.451 (rec:1.106, round:6196.346)	b=16.62	count=7000
Total loss:	5926.302 (rec:1.161, round:5925.140)	b=16.06	count=7500
Total loss:	5660.930 (rec:0.991, round:5659.938)	b=15.50	count=8000
Total loss:	5397.588 (rec:1.252, round:5396.336)	b=14.94	count=8500
Total loss:	5136.874 (rec:0.979, round:5135.895)	b=14.38	count=9000
Total loss:	4873.456 (rec:1.035, round:4872.421)	b=13.81	count=9500
Total loss:	4610.855 (rec:0.985, round:4609.870)	b=13.25	count=10000
Total loss:	4346.891 (rec:1.028, round:4345.863)	b=12.69	count=10500
Total loss:	4079.350 (rec:1.534, round:4077.816)	b=12.12	count=11000
Total loss:	3805.718 (rec:1.267, round:3804.451)	b=11.56	count=11500
Total loss:	3530.954 (rec:1.292, round:3529.661)	b=11.00	count=12000
Total loss:	3250.989 (rec:1.058, round:3249.931)	b=10.44	count=12500
Total loss:	2966.584 (rec:1.033, round:2965.551)	b=9.88	count=13000
Total loss:	2674.090 (rec:1.031, round:2673.059)	b=9.31	count=13500
Total loss:	2379.958 (rec:1.030, round:2378.928)	b=8.75	count=14000
Total loss:	2080.429 (rec:1.029, round:2079.400)	b=8.19	count=14500
Total loss:	1778.759 (rec:1.094, round:1777.665)	b=7.62	count=15000
Total loss:	1478.362 (rec:1.071, round:1477.291)	b=7.06	count=15500
Total loss:	1182.890 (rec:1.265, round:1181.626)	b=6.50	count=16000
Total loss:	903.150 (rec:1.167, round:901.983)	b=5.94	count=16500
Total loss:	645.835 (rec:1.168, round:644.667)	b=5.38	count=17000
Total loss:	427.086 (rec:1.180, round:425.907)	b=4.81	count=17500
Total loss:	258.418 (rec:1.077, round:257.340)	b=4.25	count=18000
Total loss:	134.959 (rec:1.498, round:133.461)	b=3.69	count=18500
Total loss:	53.353 (rec:0.933, round:52.419)	b=3.12	count=19000
Total loss:	12.762 (rec:1.093, round:11.669)	b=2.56	count=19500
Total loss:	2.190 (rec:1.226, round:0.963)	b=2.00	count=20000
finished reconstructing blocks.7.
reconstructing blocks.8 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.8 ...
wraping quantizers in blocks.8 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.850 (rec:0.850, round:0.000)	b=0.00	count=500
Total loss:	0.796 (rec:0.796, round:0.000)	b=0.00	count=1000
Total loss:	0.670 (rec:0.670, round:0.000)	b=0.00	count=1500
Total loss:	0.657 (rec:0.657, round:0.000)	b=0.00	count=2000
Total loss:	0.686 (rec:0.686, round:0.000)	b=0.00	count=2500
Total loss:	0.579 (rec:0.579, round:0.000)	b=0.00	count=3000
Total loss:	0.623 (rec:0.623, round:0.000)	b=0.00	count=3500
Total loss:	16167.543 (rec:0.631, round:16166.912)	b=20.00	count=4000
Total loss:	8034.276 (rec:0.523, round:8033.753)	b=19.44	count=4500
Total loss:	7421.871 (rec:0.586, round:7421.284)	b=18.88	count=5000
Total loss:	7022.629 (rec:0.626, round:7022.004)	b=18.31	count=5500
Total loss:	6677.544 (rec:0.593, round:6676.951)	b=17.75	count=6000
Total loss:	6359.410 (rec:0.573, round:6358.837)	b=17.19	count=6500
Total loss:	6052.659 (rec:0.610, round:6052.049)	b=16.62	count=7000
Total loss:	5753.818 (rec:0.522, round:5753.296)	b=16.06	count=7500
Total loss:	5458.863 (rec:0.568, round:5458.295)	b=15.50	count=8000
Total loss:	5168.866 (rec:0.570, round:5168.296)	b=14.94	count=8500
Total loss:	4882.727 (rec:0.537, round:4882.190)	b=14.38	count=9000
Total loss:	4598.834 (rec:0.622, round:4598.212)	b=13.81	count=9500
Total loss:	4312.538 (rec:0.641, round:4311.897)	b=13.25	count=10000
Total loss:	4028.025 (rec:0.562, round:4027.463)	b=12.69	count=10500
Total loss:	3741.443 (rec:0.573, round:3740.871)	b=12.12	count=11000
Total loss:	3456.057 (rec:0.557, round:3455.500)	b=11.56	count=11500
Total loss:	3169.593 (rec:0.584, round:3169.009)	b=11.00	count=12000
Total loss:	2878.478 (rec:0.517, round:2877.961)	b=10.44	count=12500
Total loss:	2584.123 (rec:0.559, round:2583.563)	b=9.88	count=13000
Total loss:	2288.921 (rec:0.543, round:2288.378)	b=9.31	count=13500
Total loss:	1993.225 (rec:0.561, round:1992.664)	b=8.75	count=14000
Total loss:	1700.523 (rec:0.524, round:1699.999)	b=8.19	count=14500
Total loss:	1413.091 (rec:0.608, round:1412.483)	b=7.62	count=15000
Total loss:	1131.118 (rec:0.574, round:1130.544)	b=7.06	count=15500
Total loss:	859.127 (rec:0.510, round:858.617)	b=6.50	count=16000
Total loss:	601.856 (rec:0.552, round:601.304)	b=5.94	count=16500
Total loss:	377.080 (rec:0.533, round:376.547)	b=5.38	count=17000
Total loss:	202.360 (rec:0.635, round:201.725)	b=4.81	count=17500
Total loss:	88.268 (rec:0.525, round:87.743)	b=4.25	count=18000
Total loss:	29.435 (rec:0.625, round:28.810)	b=3.69	count=18500
Total loss:	6.949 (rec:0.572, round:6.376)	b=3.12	count=19000
Total loss:	1.162 (rec:0.571, round:0.591)	b=2.56	count=19500
Total loss:	0.574 (rec:0.562, round:0.012)	b=2.00	count=20000
finished reconstructing blocks.8.
reconstructing blocks.9 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.9 ...
wraping quantizers in blocks.9 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.754 (rec:0.754, round:0.000)	b=0.00	count=500
Total loss:	0.691 (rec:0.691, round:0.000)	b=0.00	count=1000
Total loss:	0.651 (rec:0.651, round:0.000)	b=0.00	count=1500
Total loss:	0.696 (rec:0.696, round:0.000)	b=0.00	count=2000
Total loss:	0.537 (rec:0.537, round:0.000)	b=0.00	count=2500
Total loss:	0.579 (rec:0.579, round:0.000)	b=0.00	count=3000
Total loss:	0.546 (rec:0.546, round:0.000)	b=0.00	count=3500
Total loss:	16293.105 (rec:0.599, round:16292.506)	b=20.00	count=4000
Total loss:	8239.292 (rec:0.585, round:8238.707)	b=19.44	count=4500
Total loss:	7636.245 (rec:0.592, round:7635.652)	b=18.88	count=5000
Total loss:	7238.748 (rec:0.498, round:7238.250)	b=18.31	count=5500
Total loss:	6899.737 (rec:0.572, round:6899.165)	b=17.75	count=6000
Total loss:	6585.954 (rec:0.597, round:6585.356)	b=17.19	count=6500
Total loss:	6285.848 (rec:0.524, round:6285.324)	b=16.62	count=7000
Total loss:	5994.332 (rec:0.520, round:5993.812)	b=16.06	count=7500
Total loss:	5707.440 (rec:0.584, round:5706.857)	b=15.50	count=8000
Total loss:	5423.139 (rec:0.580, round:5422.559)	b=14.94	count=8500
Total loss:	5139.645 (rec:0.494, round:5139.151)	b=14.38	count=9000
Total loss:	4855.864 (rec:0.522, round:4855.342)	b=13.81	count=9500
Total loss:	4573.306 (rec:0.503, round:4572.803)	b=13.25	count=10000
Total loss:	4285.463 (rec:0.494, round:4284.969)	b=12.69	count=10500
Total loss:	3993.743 (rec:0.563, round:3993.180)	b=12.12	count=11000
Total loss:	3702.913 (rec:0.542, round:3702.371)	b=11.56	count=11500
Total loss:	3412.311 (rec:0.502, round:3411.808)	b=11.00	count=12000
Total loss:	3119.120 (rec:0.530, round:3118.590)	b=10.44	count=12500
Total loss:	2826.535 (rec:0.548, round:2825.987)	b=9.88	count=13000
Total loss:	2529.169 (rec:0.549, round:2528.621)	b=9.31	count=13500
Total loss:	2231.652 (rec:0.513, round:2231.139)	b=8.75	count=14000
Total loss:	1932.297 (rec:0.563, round:1931.734)	b=8.19	count=14500
Total loss:	1634.593 (rec:0.515, round:1634.077)	b=7.62	count=15000
Total loss:	1333.969 (rec:0.509, round:1333.460)	b=7.06	count=15500
Total loss:	1039.628 (rec:0.562, round:1039.066)	b=6.50	count=16000
Total loss:	758.805 (rec:0.527, round:758.279)	b=5.94	count=16500
Total loss:	504.191 (rec:0.554, round:503.636)	b=5.38	count=17000
Total loss:	294.049 (rec:0.489, round:293.560)	b=4.81	count=17500
Total loss:	143.642 (rec:0.524, round:143.118)	b=4.25	count=18000
Total loss:	54.080 (rec:0.543, round:53.537)	b=3.69	count=18500
Total loss:	13.386 (rec:0.534, round:12.853)	b=3.12	count=19000
Total loss:	1.760 (rec:0.570, round:1.190)	b=2.56	count=19500
Total loss:	0.647 (rec:0.606, round:0.041)	b=2.00	count=20000
finished reconstructing blocks.9.
reconstructing blocks.10 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.10 ...
wraping quantizers in blocks.10 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.290 (rec:0.290, round:0.000)	b=0.00	count=500
Total loss:	0.237 (rec:0.237, round:0.000)	b=0.00	count=1000
Total loss:	0.228 (rec:0.228, round:0.000)	b=0.00	count=1500
Total loss:	0.213 (rec:0.213, round:0.000)	b=0.00	count=2000
Total loss:	0.200 (rec:0.200, round:0.000)	b=0.00	count=2500
Total loss:	0.205 (rec:0.205, round:0.000)	b=0.00	count=3000
Total loss:	0.192 (rec:0.192, round:0.000)	b=0.00	count=3500
Total loss:	16566.891 (rec:0.193, round:16566.697)	b=20.00	count=4000
Total loss:	8332.124 (rec:0.191, round:8331.934)	b=19.44	count=4500
Total loss:	7721.590 (rec:0.183, round:7721.407)	b=18.88	count=5000
Total loss:	7321.675 (rec:0.187, round:7321.487)	b=18.31	count=5500
Total loss:	6974.068 (rec:0.174, round:6973.895)	b=17.75	count=6000
Total loss:	6645.733 (rec:0.191, round:6645.542)	b=17.19	count=6500
Total loss:	6327.118 (rec:0.198, round:6326.920)	b=16.62	count=7000
Total loss:	6010.961 (rec:0.192, round:6010.769)	b=16.06	count=7500
Total loss:	5697.747 (rec:0.180, round:5697.567)	b=15.50	count=8000
Total loss:	5382.018 (rec:0.179, round:5381.838)	b=14.94	count=8500
Total loss:	5070.252 (rec:0.171, round:5070.081)	b=14.38	count=9000
Total loss:	4757.123 (rec:0.180, round:4756.943)	b=13.81	count=9500
Total loss:	4444.678 (rec:0.182, round:4444.496)	b=13.25	count=10000
Total loss:	4132.206 (rec:0.181, round:4132.025)	b=12.69	count=10500
Total loss:	3820.261 (rec:0.184, round:3820.077)	b=12.12	count=11000
Total loss:	3507.947 (rec:0.174, round:3507.773)	b=11.56	count=11500
Total loss:	3194.255 (rec:0.187, round:3194.069)	b=11.00	count=12000
Total loss:	2885.419 (rec:0.188, round:2885.231)	b=10.44	count=12500
Total loss:	2575.866 (rec:0.193, round:2575.673)	b=9.88	count=13000
Total loss:	2268.125 (rec:0.187, round:2267.938)	b=9.31	count=13500
Total loss:	1965.806 (rec:0.178, round:1965.627)	b=8.75	count=14000
Total loss:	1667.244 (rec:0.178, round:1667.066)	b=8.19	count=14500
Total loss:	1377.018 (rec:0.189, round:1376.829)	b=7.62	count=15000
Total loss:	1093.017 (rec:0.184, round:1092.833)	b=7.06	count=15500
Total loss:	827.500 (rec:0.181, round:827.319)	b=6.50	count=16000
Total loss:	580.637 (rec:0.195, round:580.442)	b=5.94	count=16500
Total loss:	365.010 (rec:0.189, round:364.821)	b=5.38	count=17000
Total loss:	191.501 (rec:0.187, round:191.313)	b=4.81	count=17500
Total loss:	74.928 (rec:0.192, round:74.736)	b=4.25	count=18000
Total loss:	18.848 (rec:0.195, round:18.653)	b=3.69	count=18500
Total loss:	2.374 (rec:0.194, round:2.179)	b=3.12	count=19000
Total loss:	0.295 (rec:0.202, round:0.093)	b=2.56	count=19500
Total loss:	0.197 (rec:0.196, round:0.001)	b=2.00	count=20000
finished reconstructing blocks.10.
reconstructing blocks.11 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.11 ...
wraping quantizers in blocks.11 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.727 (rec:1.727, round:0.000)	b=0.00	count=500
Total loss:	1.548 (rec:1.548, round:0.000)	b=0.00	count=1000
Total loss:	1.457 (rec:1.457, round:0.000)	b=0.00	count=1500
Total loss:	1.529 (rec:1.529, round:0.000)	b=0.00	count=2000
Total loss:	1.528 (rec:1.528, round:0.000)	b=0.00	count=2500
Total loss:	1.411 (rec:1.411, round:0.000)	b=0.00	count=3000
Total loss:	1.468 (rec:1.468, round:0.000)	b=0.00	count=3500
Total loss:	16112.671 (rec:1.469, round:16111.202)	b=20.00	count=4000
Total loss:	8204.493 (rec:1.396, round:8203.097)	b=19.44	count=4500
Total loss:	7604.888 (rec:1.405, round:7603.482)	b=18.88	count=5000
Total loss:	7207.385 (rec:1.347, round:7206.038)	b=18.31	count=5500
Total loss:	6869.290 (rec:1.344, round:6867.946)	b=17.75	count=6000
Total loss:	6549.606 (rec:1.306, round:6548.300)	b=17.19	count=6500
Total loss:	6243.794 (rec:1.453, round:6242.342)	b=16.62	count=7000
Total loss:	5945.814 (rec:1.375, round:5944.439)	b=16.06	count=7500
Total loss:	5650.216 (rec:1.318, round:5648.898)	b=15.50	count=8000
Total loss:	5358.516 (rec:1.394, round:5357.122)	b=14.94	count=8500
Total loss:	5066.313 (rec:1.409, round:5064.904)	b=14.38	count=9000
Total loss:	4780.402 (rec:1.310, round:4779.092)	b=13.81	count=9500
Total loss:	4493.755 (rec:1.400, round:4492.355)	b=13.25	count=10000
Total loss:	4204.782 (rec:1.286, round:4203.496)	b=12.69	count=10500
Total loss:	3921.307 (rec:1.345, round:3919.961)	b=12.12	count=11000
Total loss:	3637.384 (rec:1.331, round:3636.053)	b=11.56	count=11500
Total loss:	3351.648 (rec:1.342, round:3350.307)	b=11.00	count=12000
Total loss:	3065.483 (rec:1.331, round:3064.151)	b=10.44	count=12500
Total loss:	2777.699 (rec:1.307, round:2776.392)	b=9.88	count=13000
Total loss:	2489.685 (rec:1.296, round:2488.389)	b=9.31	count=13500
Total loss:	2202.345 (rec:1.390, round:2200.955)	b=8.75	count=14000
Total loss:	1916.957 (rec:1.340, round:1915.617)	b=8.19	count=14500
Total loss:	1629.239 (rec:1.422, round:1627.818)	b=7.62	count=15000
Total loss:	1348.766 (rec:1.371, round:1347.396)	b=7.06	count=15500
Total loss:	1075.287 (rec:1.357, round:1073.930)	b=6.50	count=16000
Total loss:	814.399 (rec:1.467, round:812.932)	b=5.94	count=16500
Total loss:	567.052 (rec:1.293, round:565.759)	b=5.38	count=17000
Total loss:	349.567 (rec:1.463, round:348.104)	b=4.81	count=17500
Total loss:	177.005 (rec:1.368, round:175.637)	b=4.25	count=18000
Total loss:	65.776 (rec:1.373, round:64.404)	b=3.69	count=18500
Total loss:	14.955 (rec:1.359, round:13.596)	b=3.12	count=19000
Total loss:	2.349 (rec:1.368, round:0.980)	b=2.56	count=19500
Total loss:	1.460 (rec:1.436, round:0.023)	b=2.00	count=20000
finished reconstructing blocks.11.
reconstructing head ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for head ...
wraping quantizers in head ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	1.620 (rec:1.620, round:0.000)	b=0.00	count=500
Total loss:	1.010 (rec:1.010, round:0.000)	b=0.00	count=1000
Total loss:	1.005 (rec:1.005, round:0.000)	b=0.00	count=1500
Total loss:	0.522 (rec:0.522, round:0.000)	b=0.00	count=2000
Total loss:	0.746 (rec:0.746, round:0.000)	b=0.00	count=2500
Total loss:	0.368 (rec:0.368, round:0.000)	b=0.00	count=3000
Total loss:	0.661 (rec:0.661, round:0.000)	b=0.00	count=3500
Total loss:	3604.016 (rec:0.597, round:3603.418)	b=20.00	count=4000
Total loss:	2310.025 (rec:0.643, round:2309.382)	b=19.44	count=4500
Total loss:	2172.595 (rec:0.630, round:2171.966)	b=18.88	count=5000
Total loss:	2088.837 (rec:0.245, round:2088.591)	b=18.31	count=5500
Total loss:	2023.542 (rec:0.543, round:2022.999)	b=17.75	count=6000
Total loss:	1966.114 (rec:0.402, round:1965.712)	b=17.19	count=6500
Total loss:	1912.445 (rec:0.435, round:1912.011)	b=16.62	count=7000
Total loss:	1860.800 (rec:0.233, round:1860.567)	b=16.06	count=7500
Total loss:	1811.129 (rec:0.794, round:1810.335)	b=15.50	count=8000
Total loss:	1760.456 (rec:0.386, round:1760.070)	b=14.94	count=8500
Total loss:	1710.618 (rec:0.393, round:1710.226)	b=14.38	count=9000
Total loss:	1659.671 (rec:0.568, round:1659.103)	b=13.81	count=9500
Total loss:	1608.905 (rec:0.426, round:1608.479)	b=13.25	count=10000
Total loss:	1556.795 (rec:0.426, round:1556.369)	b=12.69	count=10500
Total loss:	1502.158 (rec:0.485, round:1501.673)	b=12.12	count=11000
Total loss:	1445.030 (rec:0.501, round:1444.530)	b=11.56	count=11500
Total loss:	1386.212 (rec:0.373, round:1385.840)	b=11.00	count=12000
Total loss:	1324.502 (rec:0.441, round:1324.062)	b=10.44	count=12500
Total loss:	1260.885 (rec:0.618, round:1260.266)	b=9.88	count=13000
Total loss:	1193.001 (rec:0.332, round:1192.669)	b=9.31	count=13500
Total loss:	1121.604 (rec:0.529, round:1121.075)	b=8.75	count=14000
Total loss:	1046.041 (rec:0.501, round:1045.540)	b=8.19	count=14500
Total loss:	965.778 (rec:0.477, round:965.301)	b=7.62	count=15000
Total loss:	881.648 (rec:0.348, round:881.300)	b=7.06	count=15500
Total loss:	793.259 (rec:0.332, round:792.927)	b=6.50	count=16000
Total loss:	700.233 (rec:0.414, round:699.819)	b=5.94	count=16500
Total loss:	601.795 (rec:0.435, round:601.361)	b=5.38	count=17000
Total loss:	499.518 (rec:0.431, round:499.087)	b=4.81	count=17500
Total loss:	392.652 (rec:0.664, round:391.987)	b=4.25	count=18000
Total loss:	283.460 (rec:0.459, round:283.000)	b=3.69	count=18500
Total loss:	176.114 (rec:0.422, round:175.692)	b=3.12	count=19000
Total loss:	82.222 (rec:0.577, round:81.645)	b=2.56	count=19500
Total loss:	22.354 (rec:0.592, round:21.763)	b=2.00	count=20000
finished reconstructing head.
2025-09-14 16:09:27 - mse guided block reconstruction finished.
Saving checkpoint to ./checkpoint/quant_result/20250914_1435/vit_small_w2_a6_optimsize_1024_mse_qdrop.pth
Validating on calibration set after block reconstruction ...
Test: [0/32]	Time 0.557 (0.557)	Loss 0.5621 (0.5621)	Prec@1 90.625 (90.625)	Prec@5 96.875 (96.875)
Test: [10/32]	Time 0.025 (0.074)	Loss 1.4844 (0.9800)	Prec@1 78.125 (77.841)	Prec@5 87.500 (91.761)
Test: [20/32]	Time 0.025 (0.051)	Loss 0.6872 (0.8510)	Prec@1 71.875 (79.018)	Prec@5 100.000 (93.899)
Test: [30/32]	Time 0.025 (0.042)	Loss 1.4983 (0.8677)	Prec@1 71.875 (79.738)	Prec@5 78.125 (93.750)
 * Prec@1 79.688 Prec@5 93.750 Loss 0.864 Time 1.469
Validating on test set after block reconstruction ...
Test: [0/100]	Time 9.606 (9.606)	Loss 1.4191 (1.4191)	Prec@1 66.200 (66.200)	Prec@5 89.800 (89.800)
Test: [10/100]	Time 2.774 (2.003)	Loss 1.3542 (1.5406)	Prec@1 71.400 (64.455)	Prec@5 89.200 (88.527)
Test: [20/100]	Time 0.782 (1.423)	Loss 1.6227 (1.4995)	Prec@1 53.600 (64.362)	Prec@5 90.200 (89.333)
Test: [30/100]	Time 0.786 (1.227)	Loss 1.2345 (1.4547)	Prec@1 71.800 (64.490)	Prec@5 94.000 (90.258)
Test: [40/100]	Time 0.789 (1.169)	Loss 1.7168 (1.4810)	Prec@1 57.600 (64.249)	Prec@5 86.000 (89.844)
Test: [50/100]	Time 0.790 (1.144)	Loss 2.2951 (1.6070)	Prec@1 46.400 (61.741)	Prec@5 77.000 (87.937)
Test: [60/100]	Time 0.781 (1.085)	Loss 1.9310 (1.6659)	Prec@1 58.000 (60.856)	Prec@5 84.400 (86.911)
Test: [70/100]	Time 0.789 (1.043)	Loss 1.9000 (1.7243)	Prec@1 57.200 (59.842)	Prec@5 84.200 (86.037)
Test: [80/100]	Time 0.791 (1.011)	Loss 1.8016 (1.7633)	Prec@1 61.200 (59.235)	Prec@5 86.200 (85.430)
Test: [90/100]	Time 0.783 (0.986)	Loss 2.4840 (1.8024)	Prec@1 45.800 (58.547)	Prec@5 72.800 (84.866)
 * Prec@1 58.710 Prec@5 84.802 Loss 1.808 Time 97.035
2025-09-14 16:11:06 - finished the process.
Extracting logits from quantized and full-precision models...
Testing combinations:
  Alpha values: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
  Cluster numbers: [8, 16, 32, 64, 128, 256]
  PCA dimensions: [1, 25, 50, 75, 100, 125, 150, 175, 200, 220]

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.63%
[Alpha=0.10] Top-5 Accuracy: 84.69%
Result: Top-1: 58.63%, Top-5: 84.69%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.72%
[Alpha=0.10] Top-5 Accuracy: 84.71%
Result: Top-1: 58.72%, Top-5: 84.71%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.69%
[Alpha=0.10] Top-5 Accuracy: 84.72%
Result: Top-1: 58.69%, Top-5: 84.72%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.69%
[Alpha=0.10] Top-5 Accuracy: 84.74%
Result: Top-1: 58.69%, Top-5: 84.74%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.71%
[Alpha=0.10] Top-5 Accuracy: 84.74%
Result: Top-1: 58.71%, Top-5: 84.74%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.69%
[Alpha=0.10] Top-5 Accuracy: 84.73%
Result: Top-1: 58.69%, Top-5: 84.73%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.67%
[Alpha=0.10] Top-5 Accuracy: 84.72%
Result: Top-1: 58.67%, Top-5: 84.72%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.75%
[Alpha=0.10] Top-5 Accuracy: 84.73%
Result: Top-1: 58.75%, Top-5: 84.73%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.73%
[Alpha=0.10] Top-5 Accuracy: 84.74%
Result: Top-1: 58.73%, Top-5: 84.74%

============================================================
Testing: alpha=0.1, clusters=8, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.76%
[Alpha=0.10] Top-5 Accuracy: 84.75%
Result: Top-1: 58.76%, Top-5: 84.75%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.62%
[Alpha=0.10] Top-5 Accuracy: 84.68%
Result: Top-1: 58.62%, Top-5: 84.68%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.72%
[Alpha=0.10] Top-5 Accuracy: 84.74%
Result: Top-1: 58.72%, Top-5: 84.74%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.77%
[Alpha=0.10] Top-5 Accuracy: 84.77%
Result: Top-1: 58.77%, Top-5: 84.77%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.72%
[Alpha=0.10] Top-5 Accuracy: 84.80%
Result: Top-1: 58.72%, Top-5: 84.80%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.64%
[Alpha=0.10] Top-5 Accuracy: 84.80%
Result: Top-1: 58.64%, Top-5: 84.80%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.71%
[Alpha=0.10] Top-5 Accuracy: 84.77%
Result: Top-1: 58.71%, Top-5: 84.77%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.68%
[Alpha=0.10] Top-5 Accuracy: 84.77%
Result: Top-1: 58.68%, Top-5: 84.77%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.70%
[Alpha=0.10] Top-5 Accuracy: 84.77%
Result: Top-1: 58.70%, Top-5: 84.77%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.73%
[Alpha=0.10] Top-5 Accuracy: 84.77%
Result: Top-1: 58.73%, Top-5: 84.77%

============================================================
Testing: alpha=0.1, clusters=16, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.66%
[Alpha=0.10] Top-5 Accuracy: 84.79%
Result: Top-1: 58.66%, Top-5: 84.79%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.65%
[Alpha=0.10] Top-5 Accuracy: 84.64%
Result: Top-1: 58.65%, Top-5: 84.64%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.69%
[Alpha=0.10] Top-5 Accuracy: 84.79%
Result: Top-1: 58.69%, Top-5: 84.79%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.70%
[Alpha=0.10] Top-5 Accuracy: 84.76%
Result: Top-1: 58.70%, Top-5: 84.76%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.73%
[Alpha=0.10] Top-5 Accuracy: 84.74%
Result: Top-1: 58.73%, Top-5: 84.74%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.77%
[Alpha=0.10] Top-5 Accuracy: 84.79%
Result: Top-1: 58.77%, Top-5: 84.79%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.74%
[Alpha=0.10] Top-5 Accuracy: 84.78%
Result: Top-1: 58.74%, Top-5: 84.78%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.77%
[Alpha=0.10] Top-5 Accuracy: 84.77%
Result: Top-1: 58.77%, Top-5: 84.77%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.67%
[Alpha=0.10] Top-5 Accuracy: 84.81%
Result: Top-1: 58.67%, Top-5: 84.81%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.78%
[Alpha=0.10] Top-5 Accuracy: 84.78%
Result: Top-1: 58.78%, Top-5: 84.78%

============================================================
Testing: alpha=0.1, clusters=32, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.72%
[Alpha=0.10] Top-5 Accuracy: 84.78%
Result: Top-1: 58.72%, Top-5: 84.78%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.55%
[Alpha=0.10] Top-5 Accuracy: 84.58%
Result: Top-1: 58.55%, Top-5: 84.58%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.69%
[Alpha=0.10] Top-5 Accuracy: 84.76%
Result: Top-1: 58.69%, Top-5: 84.76%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.67%
[Alpha=0.10] Top-5 Accuracy: 84.80%
Result: Top-1: 58.67%, Top-5: 84.80%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.70%
[Alpha=0.10] Top-5 Accuracy: 84.77%
Result: Top-1: 58.70%, Top-5: 84.77%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.61%
[Alpha=0.10] Top-5 Accuracy: 84.80%
Result: Top-1: 58.61%, Top-5: 84.80%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.73%
[Alpha=0.10] Top-5 Accuracy: 84.82%
Result: Top-1: 58.73%, Top-5: 84.82%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.75%
[Alpha=0.10] Top-5 Accuracy: 84.74%
Result: Top-1: 58.75%, Top-5: 84.74%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.70%
[Alpha=0.10] Top-5 Accuracy: 84.81%
Result: Top-1: 58.70%, Top-5: 84.81%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.69%
[Alpha=0.10] Top-5 Accuracy: 84.75%
Result: Top-1: 58.69%, Top-5: 84.75%

============================================================
Testing: alpha=0.1, clusters=64, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.66%
[Alpha=0.10] Top-5 Accuracy: 84.78%
Result: Top-1: 58.66%, Top-5: 84.78%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.37%
[Alpha=0.10] Top-5 Accuracy: 84.55%
Result: Top-1: 58.37%, Top-5: 84.55%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.58%
[Alpha=0.10] Top-5 Accuracy: 84.74%
Result: Top-1: 58.58%, Top-5: 84.74%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.61%
[Alpha=0.10] Top-5 Accuracy: 84.69%
Result: Top-1: 58.61%, Top-5: 84.69%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.57%
[Alpha=0.10] Top-5 Accuracy: 84.67%
Result: Top-1: 58.57%, Top-5: 84.67%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.52%
[Alpha=0.10] Top-5 Accuracy: 84.75%
Result: Top-1: 58.52%, Top-5: 84.75%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.63%
[Alpha=0.10] Top-5 Accuracy: 84.74%
Result: Top-1: 58.63%, Top-5: 84.74%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.47%
[Alpha=0.10] Top-5 Accuracy: 84.79%
Result: Top-1: 58.47%, Top-5: 84.79%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.72%
[Alpha=0.10] Top-5 Accuracy: 84.72%
Result: Top-1: 58.72%, Top-5: 84.72%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.57%
[Alpha=0.10] Top-5 Accuracy: 84.74%
Result: Top-1: 58.57%, Top-5: 84.74%

============================================================
Testing: alpha=0.1, clusters=128, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.69%
[Alpha=0.10] Top-5 Accuracy: 84.76%
Result: Top-1: 58.69%, Top-5: 84.76%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=1
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.04%
[Alpha=0.10] Top-5 Accuracy: 84.40%
Result: Top-1: 58.04%, Top-5: 84.40%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=25
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.41%
[Alpha=0.10] Top-5 Accuracy: 84.61%
Result: Top-1: 58.41%, Top-5: 84.61%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=50
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.17%
[Alpha=0.10] Top-5 Accuracy: 84.54%
Result: Top-1: 58.17%, Top-5: 84.54%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=75
============================================================
[Alpha=0.10] Top-1 Accuracy: 57.46%
[Alpha=0.10] Top-5 Accuracy: 84.46%
Result: Top-1: 57.46%, Top-5: 84.46%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=100
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.24%
[Alpha=0.10] Top-5 Accuracy: 84.56%
Result: Top-1: 58.24%, Top-5: 84.56%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=125
============================================================
[Alpha=0.10] Top-1 Accuracy: 56.10%
[Alpha=0.10] Top-5 Accuracy: 83.77%
Result: Top-1: 56.10%, Top-5: 83.77%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=150
============================================================
[Alpha=0.10] Top-1 Accuracy: 57.80%
[Alpha=0.10] Top-5 Accuracy: 84.49%
Result: Top-1: 57.80%, Top-5: 84.49%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=175
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.43%
[Alpha=0.10] Top-5 Accuracy: 84.70%
Result: Top-1: 58.43%, Top-5: 84.70%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=200
============================================================
[Alpha=0.10] Top-1 Accuracy: 58.08%
[Alpha=0.10] Top-5 Accuracy: 84.34%
Result: Top-1: 58.08%, Top-5: 84.34%

============================================================
Testing: alpha=0.1, clusters=256, pca_dim=220
============================================================
[Alpha=0.10] Top-1 Accuracy: 57.96%
[Alpha=0.10] Top-5 Accuracy: 84.45%
Result: Top-1: 57.96%, Top-5: 84.45%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.18%
[Alpha=0.20] Top-5 Accuracy: 84.45%
Result: Top-1: 58.18%, Top-5: 84.45%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.36%
[Alpha=0.20] Top-5 Accuracy: 84.58%
Result: Top-1: 58.36%, Top-5: 84.58%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.42%
[Alpha=0.20] Top-5 Accuracy: 84.60%
Result: Top-1: 58.42%, Top-5: 84.60%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.35%
[Alpha=0.20] Top-5 Accuracy: 84.53%
Result: Top-1: 58.35%, Top-5: 84.53%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.36%
[Alpha=0.20] Top-5 Accuracy: 84.54%
Result: Top-1: 58.36%, Top-5: 84.54%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.34%
[Alpha=0.20] Top-5 Accuracy: 84.54%
Result: Top-1: 58.34%, Top-5: 84.54%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.39%
[Alpha=0.20] Top-5 Accuracy: 84.57%
Result: Top-1: 58.39%, Top-5: 84.57%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.50%
[Alpha=0.20] Top-5 Accuracy: 84.55%
Result: Top-1: 58.50%, Top-5: 84.55%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.51%
[Alpha=0.20] Top-5 Accuracy: 84.60%
Result: Top-1: 58.51%, Top-5: 84.60%

============================================================
Testing: alpha=0.2, clusters=8, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.49%
[Alpha=0.20] Top-5 Accuracy: 84.58%
Result: Top-1: 58.49%, Top-5: 84.58%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.14%
[Alpha=0.20] Top-5 Accuracy: 84.45%
Result: Top-1: 58.14%, Top-5: 84.45%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.48%
[Alpha=0.20] Top-5 Accuracy: 84.60%
Result: Top-1: 58.48%, Top-5: 84.60%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.51%
[Alpha=0.20] Top-5 Accuracy: 84.57%
Result: Top-1: 58.51%, Top-5: 84.57%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.53%
[Alpha=0.20] Top-5 Accuracy: 84.57%
Result: Top-1: 58.53%, Top-5: 84.57%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.48%
[Alpha=0.20] Top-5 Accuracy: 84.63%
Result: Top-1: 58.48%, Top-5: 84.63%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.53%
[Alpha=0.20] Top-5 Accuracy: 84.63%
Result: Top-1: 58.53%, Top-5: 84.63%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.47%
[Alpha=0.20] Top-5 Accuracy: 84.55%
Result: Top-1: 58.47%, Top-5: 84.55%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.47%
[Alpha=0.20] Top-5 Accuracy: 84.63%
Result: Top-1: 58.47%, Top-5: 84.63%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.53%
[Alpha=0.20] Top-5 Accuracy: 84.61%
Result: Top-1: 58.53%, Top-5: 84.61%

============================================================
Testing: alpha=0.2, clusters=16, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.53%
[Alpha=0.20] Top-5 Accuracy: 84.63%
Result: Top-1: 58.53%, Top-5: 84.63%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.16%
[Alpha=0.20] Top-5 Accuracy: 84.42%
Result: Top-1: 58.16%, Top-5: 84.42%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.31%
[Alpha=0.20] Top-5 Accuracy: 84.50%
Result: Top-1: 58.31%, Top-5: 84.50%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.30%
[Alpha=0.20] Top-5 Accuracy: 84.50%
Result: Top-1: 58.30%, Top-5: 84.50%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.39%
[Alpha=0.20] Top-5 Accuracy: 84.54%
Result: Top-1: 58.39%, Top-5: 84.54%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.45%
[Alpha=0.20] Top-5 Accuracy: 84.54%
Result: Top-1: 58.45%, Top-5: 84.54%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.39%
[Alpha=0.20] Top-5 Accuracy: 84.58%
Result: Top-1: 58.39%, Top-5: 84.58%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.52%
[Alpha=0.20] Top-5 Accuracy: 84.60%
Result: Top-1: 58.52%, Top-5: 84.60%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.42%
[Alpha=0.20] Top-5 Accuracy: 84.57%
Result: Top-1: 58.42%, Top-5: 84.57%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.41%
[Alpha=0.20] Top-5 Accuracy: 84.60%
Result: Top-1: 58.41%, Top-5: 84.60%

============================================================
Testing: alpha=0.2, clusters=32, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.36%
[Alpha=0.20] Top-5 Accuracy: 84.56%
Result: Top-1: 58.36%, Top-5: 84.56%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 57.93%
[Alpha=0.20] Top-5 Accuracy: 84.22%
Result: Top-1: 57.93%, Top-5: 84.22%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.26%
[Alpha=0.20] Top-5 Accuracy: 84.53%
Result: Top-1: 58.26%, Top-5: 84.53%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.34%
[Alpha=0.20] Top-5 Accuracy: 84.54%
Result: Top-1: 58.34%, Top-5: 84.54%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.19%
[Alpha=0.20] Top-5 Accuracy: 84.54%
Result: Top-1: 58.19%, Top-5: 84.54%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.07%
[Alpha=0.20] Top-5 Accuracy: 84.45%
Result: Top-1: 58.07%, Top-5: 84.45%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.32%
[Alpha=0.20] Top-5 Accuracy: 84.60%
Result: Top-1: 58.32%, Top-5: 84.60%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.40%
[Alpha=0.20] Top-5 Accuracy: 84.46%
Result: Top-1: 58.40%, Top-5: 84.46%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.43%
[Alpha=0.20] Top-5 Accuracy: 84.56%
Result: Top-1: 58.43%, Top-5: 84.56%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.19%
[Alpha=0.20] Top-5 Accuracy: 84.53%
Result: Top-1: 58.19%, Top-5: 84.53%

============================================================
Testing: alpha=0.2, clusters=64, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.19%
[Alpha=0.20] Top-5 Accuracy: 84.46%
Result: Top-1: 58.19%, Top-5: 84.46%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 57.40%
[Alpha=0.20] Top-5 Accuracy: 84.07%
Result: Top-1: 57.40%, Top-5: 84.07%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 57.77%
[Alpha=0.20] Top-5 Accuracy: 84.38%
Result: Top-1: 57.77%, Top-5: 84.38%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 57.82%
[Alpha=0.20] Top-5 Accuracy: 84.31%
Result: Top-1: 57.82%, Top-5: 84.31%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 57.75%
[Alpha=0.20] Top-5 Accuracy: 84.28%
Result: Top-1: 57.75%, Top-5: 84.28%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 57.81%
[Alpha=0.20] Top-5 Accuracy: 84.38%
Result: Top-1: 57.81%, Top-5: 84.38%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 57.87%
[Alpha=0.20] Top-5 Accuracy: 84.37%
Result: Top-1: 57.87%, Top-5: 84.37%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 57.59%
[Alpha=0.20] Top-5 Accuracy: 84.38%
Result: Top-1: 57.59%, Top-5: 84.38%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.10%
[Alpha=0.20] Top-5 Accuracy: 84.39%
Result: Top-1: 58.10%, Top-5: 84.39%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 57.69%
[Alpha=0.20] Top-5 Accuracy: 84.43%
Result: Top-1: 57.69%, Top-5: 84.43%

============================================================
Testing: alpha=0.2, clusters=128, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 58.09%
[Alpha=0.20] Top-5 Accuracy: 84.46%
Result: Top-1: 58.09%, Top-5: 84.46%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=1
============================================================
[Alpha=0.20] Top-1 Accuracy: 56.43%
[Alpha=0.20] Top-5 Accuracy: 83.43%
Result: Top-1: 56.43%, Top-5: 83.43%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=25
============================================================
[Alpha=0.20] Top-1 Accuracy: 56.97%
[Alpha=0.20] Top-5 Accuracy: 83.77%
Result: Top-1: 56.97%, Top-5: 83.77%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=50
============================================================
[Alpha=0.20] Top-1 Accuracy: 56.08%
[Alpha=0.20] Top-5 Accuracy: 83.66%
Result: Top-1: 56.08%, Top-5: 83.66%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=75
============================================================
[Alpha=0.20] Top-1 Accuracy: 55.35%
[Alpha=0.20] Top-5 Accuracy: 83.43%
Result: Top-1: 55.35%, Top-5: 83.43%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=100
============================================================
[Alpha=0.20] Top-1 Accuracy: 56.06%
[Alpha=0.20] Top-5 Accuracy: 83.53%
Result: Top-1: 56.06%, Top-5: 83.53%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=125
============================================================
[Alpha=0.20] Top-1 Accuracy: 53.83%
[Alpha=0.20] Top-5 Accuracy: 80.88%
Result: Top-1: 53.83%, Top-5: 80.88%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=150
============================================================
[Alpha=0.20] Top-1 Accuracy: 55.32%
[Alpha=0.20] Top-5 Accuracy: 83.10%
Result: Top-1: 55.32%, Top-5: 83.10%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=175
============================================================
[Alpha=0.20] Top-1 Accuracy: 56.97%
[Alpha=0.20] Top-5 Accuracy: 83.98%
Result: Top-1: 56.97%, Top-5: 83.98%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=200
============================================================
[Alpha=0.20] Top-1 Accuracy: 56.38%
[Alpha=0.20] Top-5 Accuracy: 83.43%
Result: Top-1: 56.38%, Top-5: 83.43%

============================================================
Testing: alpha=0.2, clusters=256, pca_dim=220
============================================================
[Alpha=0.20] Top-1 Accuracy: 55.82%
[Alpha=0.20] Top-5 Accuracy: 83.31%
Result: Top-1: 55.82%, Top-5: 83.31%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.07%
[Alpha=0.30] Top-5 Accuracy: 83.94%
Result: Top-1: 57.07%, Top-5: 83.94%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.61%
[Alpha=0.30] Top-5 Accuracy: 84.14%
Result: Top-1: 57.61%, Top-5: 84.14%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.61%
[Alpha=0.30] Top-5 Accuracy: 84.17%
Result: Top-1: 57.61%, Top-5: 84.17%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.56%
[Alpha=0.30] Top-5 Accuracy: 84.11%
Result: Top-1: 57.56%, Top-5: 84.11%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.58%
[Alpha=0.30] Top-5 Accuracy: 84.12%
Result: Top-1: 57.58%, Top-5: 84.12%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.54%
[Alpha=0.30] Top-5 Accuracy: 84.11%
Result: Top-1: 57.54%, Top-5: 84.11%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.62%
[Alpha=0.30] Top-5 Accuracy: 84.18%
Result: Top-1: 57.62%, Top-5: 84.18%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.63%
[Alpha=0.30] Top-5 Accuracy: 84.08%
Result: Top-1: 57.63%, Top-5: 84.08%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.67%
[Alpha=0.30] Top-5 Accuracy: 84.08%
Result: Top-1: 57.67%, Top-5: 84.08%

============================================================
Testing: alpha=0.3, clusters=8, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.66%
[Alpha=0.30] Top-5 Accuracy: 84.08%
Result: Top-1: 57.66%, Top-5: 84.08%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.07%
[Alpha=0.30] Top-5 Accuracy: 83.91%
Result: Top-1: 57.07%, Top-5: 83.91%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.76%
[Alpha=0.30] Top-5 Accuracy: 84.17%
Result: Top-1: 57.76%, Top-5: 84.17%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.81%
[Alpha=0.30] Top-5 Accuracy: 84.19%
Result: Top-1: 57.81%, Top-5: 84.19%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.80%
[Alpha=0.30] Top-5 Accuracy: 84.15%
Result: Top-1: 57.80%, Top-5: 84.15%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.86%
[Alpha=0.30] Top-5 Accuracy: 84.21%
Result: Top-1: 57.86%, Top-5: 84.21%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.79%
[Alpha=0.30] Top-5 Accuracy: 84.18%
Result: Top-1: 57.79%, Top-5: 84.18%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.68%
[Alpha=0.30] Top-5 Accuracy: 84.16%
Result: Top-1: 57.68%, Top-5: 84.16%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.77%
[Alpha=0.30] Top-5 Accuracy: 84.17%
Result: Top-1: 57.77%, Top-5: 84.17%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.88%
[Alpha=0.30] Top-5 Accuracy: 84.21%
Result: Top-1: 57.88%, Top-5: 84.21%

============================================================
Testing: alpha=0.3, clusters=16, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.80%
[Alpha=0.30] Top-5 Accuracy: 84.17%
Result: Top-1: 57.80%, Top-5: 84.17%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 56.89%
[Alpha=0.30] Top-5 Accuracy: 83.86%
Result: Top-1: 56.89%, Top-5: 83.86%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.46%
[Alpha=0.30] Top-5 Accuracy: 84.07%
Result: Top-1: 57.46%, Top-5: 84.07%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.45%
[Alpha=0.30] Top-5 Accuracy: 84.09%
Result: Top-1: 57.45%, Top-5: 84.09%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.56%
[Alpha=0.30] Top-5 Accuracy: 84.06%
Result: Top-1: 57.56%, Top-5: 84.06%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.64%
[Alpha=0.30] Top-5 Accuracy: 84.10%
Result: Top-1: 57.64%, Top-5: 84.10%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.57%
[Alpha=0.30] Top-5 Accuracy: 84.26%
Result: Top-1: 57.57%, Top-5: 84.26%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.69%
[Alpha=0.30] Top-5 Accuracy: 84.28%
Result: Top-1: 57.69%, Top-5: 84.28%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.57%
[Alpha=0.30] Top-5 Accuracy: 84.19%
Result: Top-1: 57.57%, Top-5: 84.19%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.62%
[Alpha=0.30] Top-5 Accuracy: 84.21%
Result: Top-1: 57.62%, Top-5: 84.21%

============================================================
Testing: alpha=0.3, clusters=32, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.42%
[Alpha=0.30] Top-5 Accuracy: 84.08%
Result: Top-1: 57.42%, Top-5: 84.08%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 56.45%
[Alpha=0.30] Top-5 Accuracy: 83.54%
Result: Top-1: 56.45%, Top-5: 83.54%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.05%
[Alpha=0.30] Top-5 Accuracy: 83.97%
Result: Top-1: 57.05%, Top-5: 83.97%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.41%
[Alpha=0.30] Top-5 Accuracy: 84.04%
Result: Top-1: 57.41%, Top-5: 84.04%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 56.99%
[Alpha=0.30] Top-5 Accuracy: 83.97%
Result: Top-1: 56.99%, Top-5: 83.97%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 56.95%
[Alpha=0.30] Top-5 Accuracy: 83.89%
Result: Top-1: 56.95%, Top-5: 83.89%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.38%
[Alpha=0.30] Top-5 Accuracy: 84.07%
Result: Top-1: 57.38%, Top-5: 84.07%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.25%
[Alpha=0.30] Top-5 Accuracy: 84.01%
Result: Top-1: 57.25%, Top-5: 84.01%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.40%
[Alpha=0.30] Top-5 Accuracy: 84.13%
Result: Top-1: 57.40%, Top-5: 84.13%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.03%
[Alpha=0.30] Top-5 Accuracy: 84.00%
Result: Top-1: 57.03%, Top-5: 84.00%

============================================================
Testing: alpha=0.3, clusters=64, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 57.11%
[Alpha=0.30] Top-5 Accuracy: 84.01%
Result: Top-1: 57.11%, Top-5: 84.01%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 55.53%
[Alpha=0.30] Top-5 Accuracy: 83.09%
Result: Top-1: 55.53%, Top-5: 83.09%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 56.07%
[Alpha=0.30] Top-5 Accuracy: 83.55%
Result: Top-1: 56.07%, Top-5: 83.55%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 56.34%
[Alpha=0.30] Top-5 Accuracy: 83.56%
Result: Top-1: 56.34%, Top-5: 83.56%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 55.75%
[Alpha=0.30] Top-5 Accuracy: 83.33%
Result: Top-1: 55.75%, Top-5: 83.33%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 56.18%
[Alpha=0.30] Top-5 Accuracy: 83.63%
Result: Top-1: 56.18%, Top-5: 83.63%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 56.28%
[Alpha=0.30] Top-5 Accuracy: 83.66%
Result: Top-1: 56.28%, Top-5: 83.66%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 55.55%
[Alpha=0.30] Top-5 Accuracy: 83.43%
Result: Top-1: 55.55%, Top-5: 83.43%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 56.59%
[Alpha=0.30] Top-5 Accuracy: 83.70%
Result: Top-1: 56.59%, Top-5: 83.70%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 55.84%
[Alpha=0.30] Top-5 Accuracy: 83.63%
Result: Top-1: 55.84%, Top-5: 83.63%

============================================================
Testing: alpha=0.3, clusters=128, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 56.64%
[Alpha=0.30] Top-5 Accuracy: 83.68%
Result: Top-1: 56.64%, Top-5: 83.68%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=1
============================================================
[Alpha=0.30] Top-1 Accuracy: 53.71%
[Alpha=0.30] Top-5 Accuracy: 81.80%
Result: Top-1: 53.71%, Top-5: 81.80%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=25
============================================================
[Alpha=0.30] Top-1 Accuracy: 54.08%
[Alpha=0.30] Top-5 Accuracy: 82.40%
Result: Top-1: 54.08%, Top-5: 82.40%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=50
============================================================
[Alpha=0.30] Top-1 Accuracy: 52.69%
[Alpha=0.30] Top-5 Accuracy: 81.83%
Result: Top-1: 52.69%, Top-5: 81.83%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=75
============================================================
[Alpha=0.30] Top-1 Accuracy: 51.96%
[Alpha=0.30] Top-5 Accuracy: 81.47%
Result: Top-1: 51.96%, Top-5: 81.47%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=100
============================================================
[Alpha=0.30] Top-1 Accuracy: 52.60%
[Alpha=0.30] Top-5 Accuracy: 81.58%
Result: Top-1: 52.60%, Top-5: 81.58%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=125
============================================================
[Alpha=0.30] Top-1 Accuracy: 50.63%
[Alpha=0.30] Top-5 Accuracy: 78.44%
Result: Top-1: 50.63%, Top-5: 78.44%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=150
============================================================
[Alpha=0.30] Top-1 Accuracy: 51.40%
[Alpha=0.30] Top-5 Accuracy: 80.65%
Result: Top-1: 51.40%, Top-5: 80.65%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=175
============================================================
[Alpha=0.30] Top-1 Accuracy: 54.37%
[Alpha=0.30] Top-5 Accuracy: 82.73%
Result: Top-1: 54.37%, Top-5: 82.73%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=200
============================================================
[Alpha=0.30] Top-1 Accuracy: 53.58%
[Alpha=0.30] Top-5 Accuracy: 81.75%
Result: Top-1: 53.58%, Top-5: 81.75%

============================================================
Testing: alpha=0.3, clusters=256, pca_dim=220
============================================================
[Alpha=0.30] Top-1 Accuracy: 52.50%
[Alpha=0.30] Top-5 Accuracy: 81.29%
Result: Top-1: 52.50%, Top-5: 81.29%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.40%
[Alpha=0.40] Top-5 Accuracy: 82.97%
Result: Top-1: 55.40%, Top-5: 82.97%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.40%
[Alpha=0.40] Top-5 Accuracy: 83.42%
Result: Top-1: 56.40%, Top-5: 83.42%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.41%
[Alpha=0.40] Top-5 Accuracy: 83.41%
Result: Top-1: 56.41%, Top-5: 83.41%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.31%
[Alpha=0.40] Top-5 Accuracy: 83.32%
Result: Top-1: 56.31%, Top-5: 83.32%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.32%
[Alpha=0.40] Top-5 Accuracy: 83.34%
Result: Top-1: 56.32%, Top-5: 83.34%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.29%
[Alpha=0.40] Top-5 Accuracy: 83.32%
Result: Top-1: 56.29%, Top-5: 83.32%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.24%
[Alpha=0.40] Top-5 Accuracy: 83.39%
Result: Top-1: 56.24%, Top-5: 83.39%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.07%
[Alpha=0.40] Top-5 Accuracy: 83.13%
Result: Top-1: 56.07%, Top-5: 83.13%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.06%
[Alpha=0.40] Top-5 Accuracy: 83.18%
Result: Top-1: 56.06%, Top-5: 83.18%

============================================================
Testing: alpha=0.4, clusters=8, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.09%
[Alpha=0.40] Top-5 Accuracy: 83.20%
Result: Top-1: 56.09%, Top-5: 83.20%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.26%
[Alpha=0.40] Top-5 Accuracy: 82.87%
Result: Top-1: 55.26%, Top-5: 82.87%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.29%
[Alpha=0.40] Top-5 Accuracy: 83.40%
Result: Top-1: 56.29%, Top-5: 83.40%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.37%
[Alpha=0.40] Top-5 Accuracy: 83.48%
Result: Top-1: 56.37%, Top-5: 83.48%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.48%
[Alpha=0.40] Top-5 Accuracy: 83.39%
Result: Top-1: 56.48%, Top-5: 83.39%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.47%
[Alpha=0.40] Top-5 Accuracy: 83.40%
Result: Top-1: 56.47%, Top-5: 83.40%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.42%
[Alpha=0.40] Top-5 Accuracy: 83.44%
Result: Top-1: 56.42%, Top-5: 83.44%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.21%
[Alpha=0.40] Top-5 Accuracy: 83.26%
Result: Top-1: 56.21%, Top-5: 83.26%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.35%
[Alpha=0.40] Top-5 Accuracy: 83.40%
Result: Top-1: 56.35%, Top-5: 83.40%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.55%
[Alpha=0.40] Top-5 Accuracy: 83.45%
Result: Top-1: 56.55%, Top-5: 83.45%

============================================================
Testing: alpha=0.4, clusters=16, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.51%
[Alpha=0.40] Top-5 Accuracy: 83.47%
Result: Top-1: 56.51%, Top-5: 83.47%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 54.87%
[Alpha=0.40] Top-5 Accuracy: 82.73%
Result: Top-1: 54.87%, Top-5: 82.73%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.86%
[Alpha=0.40] Top-5 Accuracy: 83.26%
Result: Top-1: 55.86%, Top-5: 83.26%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.86%
[Alpha=0.40] Top-5 Accuracy: 83.20%
Result: Top-1: 55.86%, Top-5: 83.20%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.94%
[Alpha=0.40] Top-5 Accuracy: 83.23%
Result: Top-1: 55.94%, Top-5: 83.23%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.95%
[Alpha=0.40] Top-5 Accuracy: 83.23%
Result: Top-1: 55.95%, Top-5: 83.23%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.12%
[Alpha=0.40] Top-5 Accuracy: 83.41%
Result: Top-1: 56.12%, Top-5: 83.41%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.21%
[Alpha=0.40] Top-5 Accuracy: 83.39%
Result: Top-1: 56.21%, Top-5: 83.39%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.19%
[Alpha=0.40] Top-5 Accuracy: 83.33%
Result: Top-1: 56.19%, Top-5: 83.33%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 56.17%
[Alpha=0.40] Top-5 Accuracy: 83.31%
Result: Top-1: 56.17%, Top-5: 83.31%

============================================================
Testing: alpha=0.4, clusters=32, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.71%
[Alpha=0.40] Top-5 Accuracy: 83.31%
Result: Top-1: 55.71%, Top-5: 83.31%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 54.04%
[Alpha=0.40] Top-5 Accuracy: 82.30%
Result: Top-1: 54.04%, Top-5: 82.30%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.02%
[Alpha=0.40] Top-5 Accuracy: 82.82%
Result: Top-1: 55.02%, Top-5: 82.82%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.57%
[Alpha=0.40] Top-5 Accuracy: 83.01%
Result: Top-1: 55.57%, Top-5: 83.01%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 54.90%
[Alpha=0.40] Top-5 Accuracy: 82.79%
Result: Top-1: 54.90%, Top-5: 82.79%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.04%
[Alpha=0.40] Top-5 Accuracy: 82.95%
Result: Top-1: 55.04%, Top-5: 82.95%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.18%
[Alpha=0.40] Top-5 Accuracy: 83.11%
Result: Top-1: 55.18%, Top-5: 83.11%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.03%
[Alpha=0.40] Top-5 Accuracy: 83.01%
Result: Top-1: 55.03%, Top-5: 83.01%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.46%
[Alpha=0.40] Top-5 Accuracy: 83.17%
Result: Top-1: 55.46%, Top-5: 83.17%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 54.95%
[Alpha=0.40] Top-5 Accuracy: 82.92%
Result: Top-1: 54.95%, Top-5: 82.92%

============================================================
Testing: alpha=0.4, clusters=64, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 55.20%
[Alpha=0.40] Top-5 Accuracy: 83.08%
Result: Top-1: 55.20%, Top-5: 83.08%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 52.74%
[Alpha=0.40] Top-5 Accuracy: 81.32%
Result: Top-1: 52.74%, Top-5: 81.32%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 53.23%
[Alpha=0.40] Top-5 Accuracy: 82.01%
Result: Top-1: 53.23%, Top-5: 82.01%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 53.62%
[Alpha=0.40] Top-5 Accuracy: 82.21%
Result: Top-1: 53.62%, Top-5: 82.21%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 52.90%
[Alpha=0.40] Top-5 Accuracy: 81.70%
Result: Top-1: 52.90%, Top-5: 81.70%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 53.50%
[Alpha=0.40] Top-5 Accuracy: 82.14%
Result: Top-1: 53.50%, Top-5: 82.14%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 53.47%
[Alpha=0.40] Top-5 Accuracy: 82.19%
Result: Top-1: 53.47%, Top-5: 82.19%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 52.37%
[Alpha=0.40] Top-5 Accuracy: 81.62%
Result: Top-1: 52.37%, Top-5: 81.62%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 53.89%
[Alpha=0.40] Top-5 Accuracy: 82.34%
Result: Top-1: 53.89%, Top-5: 82.34%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 52.84%
[Alpha=0.40] Top-5 Accuracy: 82.10%
Result: Top-1: 52.84%, Top-5: 82.10%

============================================================
Testing: alpha=0.4, clusters=128, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 53.96%
[Alpha=0.40] Top-5 Accuracy: 82.13%
Result: Top-1: 53.96%, Top-5: 82.13%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=1
============================================================
[Alpha=0.40] Top-1 Accuracy: 49.95%
[Alpha=0.40] Top-5 Accuracy: 79.25%
Result: Top-1: 49.95%, Top-5: 79.25%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=25
============================================================
[Alpha=0.40] Top-1 Accuracy: 50.01%
[Alpha=0.40] Top-5 Accuracy: 80.06%
Result: Top-1: 50.01%, Top-5: 80.06%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=50
============================================================
[Alpha=0.40] Top-1 Accuracy: 48.15%
[Alpha=0.40] Top-5 Accuracy: 78.84%
Result: Top-1: 48.15%, Top-5: 78.84%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=75
============================================================
[Alpha=0.40] Top-1 Accuracy: 47.51%
[Alpha=0.40] Top-5 Accuracy: 78.16%
Result: Top-1: 47.51%, Top-5: 78.16%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=100
============================================================
[Alpha=0.40] Top-1 Accuracy: 48.05%
[Alpha=0.40] Top-5 Accuracy: 78.30%
Result: Top-1: 48.05%, Top-5: 78.30%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=125
============================================================
[Alpha=0.40] Top-1 Accuracy: 46.30%
[Alpha=0.40] Top-5 Accuracy: 75.34%
Result: Top-1: 46.30%, Top-5: 75.34%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=150
============================================================
[Alpha=0.40] Top-1 Accuracy: 46.96%
[Alpha=0.40] Top-5 Accuracy: 77.35%
Result: Top-1: 46.96%, Top-5: 77.35%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=175
============================================================
[Alpha=0.40] Top-1 Accuracy: 50.65%
[Alpha=0.40] Top-5 Accuracy: 80.38%
Result: Top-1: 50.65%, Top-5: 80.38%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=200
============================================================
[Alpha=0.40] Top-1 Accuracy: 49.06%
[Alpha=0.40] Top-5 Accuracy: 79.19%
Result: Top-1: 49.06%, Top-5: 79.19%

============================================================
Testing: alpha=0.4, clusters=256, pca_dim=220
============================================================
[Alpha=0.40] Top-1 Accuracy: 47.94%
[Alpha=0.40] Top-5 Accuracy: 78.21%
Result: Top-1: 47.94%, Top-5: 78.21%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.65%
[Alpha=0.50] Top-5 Accuracy: 81.40%
Result: Top-1: 52.65%, Top-5: 81.40%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 54.19%
[Alpha=0.50] Top-5 Accuracy: 82.17%
Result: Top-1: 54.19%, Top-5: 82.17%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 54.25%
[Alpha=0.50] Top-5 Accuracy: 82.22%
Result: Top-1: 54.25%, Top-5: 82.22%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 54.11%
[Alpha=0.50] Top-5 Accuracy: 81.97%
Result: Top-1: 54.11%, Top-5: 81.97%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 54.13%
[Alpha=0.50] Top-5 Accuracy: 81.98%
Result: Top-1: 54.13%, Top-5: 81.98%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 54.08%
[Alpha=0.50] Top-5 Accuracy: 81.96%
Result: Top-1: 54.08%, Top-5: 81.96%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.77%
[Alpha=0.50] Top-5 Accuracy: 82.14%
Result: Top-1: 53.77%, Top-5: 82.14%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.68%
[Alpha=0.50] Top-5 Accuracy: 81.52%
Result: Top-1: 52.68%, Top-5: 81.52%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.75%
[Alpha=0.50] Top-5 Accuracy: 81.60%
Result: Top-1: 52.75%, Top-5: 81.60%

============================================================
Testing: alpha=0.5, clusters=8, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.67%
[Alpha=0.50] Top-5 Accuracy: 81.58%
Result: Top-1: 52.67%, Top-5: 81.58%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.46%
[Alpha=0.50] Top-5 Accuracy: 81.10%
Result: Top-1: 52.46%, Top-5: 81.10%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.46%
[Alpha=0.50] Top-5 Accuracy: 81.97%
Result: Top-1: 53.46%, Top-5: 81.97%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.52%
[Alpha=0.50] Top-5 Accuracy: 82.15%
Result: Top-1: 53.52%, Top-5: 82.15%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.65%
[Alpha=0.50] Top-5 Accuracy: 82.06%
Result: Top-1: 53.65%, Top-5: 82.06%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.59%
[Alpha=0.50] Top-5 Accuracy: 81.89%
Result: Top-1: 53.59%, Top-5: 81.89%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.63%
[Alpha=0.50] Top-5 Accuracy: 82.19%
Result: Top-1: 53.63%, Top-5: 82.19%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.22%
[Alpha=0.50] Top-5 Accuracy: 81.78%
Result: Top-1: 53.22%, Top-5: 81.78%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.49%
[Alpha=0.50] Top-5 Accuracy: 82.01%
Result: Top-1: 53.49%, Top-5: 82.01%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.67%
[Alpha=0.50] Top-5 Accuracy: 82.11%
Result: Top-1: 53.67%, Top-5: 82.11%

============================================================
Testing: alpha=0.5, clusters=16, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.77%
[Alpha=0.50] Top-5 Accuracy: 82.10%
Result: Top-1: 53.77%, Top-5: 82.10%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.03%
[Alpha=0.50] Top-5 Accuracy: 80.93%
Result: Top-1: 52.03%, Top-5: 80.93%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.97%
[Alpha=0.50] Top-5 Accuracy: 81.93%
Result: Top-1: 52.97%, Top-5: 81.93%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.56%
[Alpha=0.50] Top-5 Accuracy: 81.55%
Result: Top-1: 52.56%, Top-5: 81.55%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.36%
[Alpha=0.50] Top-5 Accuracy: 81.61%
Result: Top-1: 52.36%, Top-5: 81.61%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.55%
[Alpha=0.50] Top-5 Accuracy: 81.75%
Result: Top-1: 52.55%, Top-5: 81.75%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.14%
[Alpha=0.50] Top-5 Accuracy: 81.89%
Result: Top-1: 53.14%, Top-5: 81.89%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.21%
[Alpha=0.50] Top-5 Accuracy: 81.95%
Result: Top-1: 53.21%, Top-5: 81.95%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.12%
[Alpha=0.50] Top-5 Accuracy: 81.89%
Result: Top-1: 53.12%, Top-5: 81.89%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 53.02%
[Alpha=0.50] Top-5 Accuracy: 81.78%
Result: Top-1: 53.02%, Top-5: 81.78%

============================================================
Testing: alpha=0.5, clusters=32, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.60%
[Alpha=0.50] Top-5 Accuracy: 81.86%
Result: Top-1: 52.60%, Top-5: 81.86%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 50.82%
[Alpha=0.50] Top-5 Accuracy: 80.22%
Result: Top-1: 50.82%, Top-5: 80.22%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 51.31%
[Alpha=0.50] Top-5 Accuracy: 80.94%
Result: Top-1: 51.31%, Top-5: 80.94%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.04%
[Alpha=0.50] Top-5 Accuracy: 81.20%
Result: Top-1: 52.04%, Top-5: 81.20%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 51.15%
[Alpha=0.50] Top-5 Accuracy: 81.02%
Result: Top-1: 51.15%, Top-5: 81.02%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 51.50%
[Alpha=0.50] Top-5 Accuracy: 81.06%
Result: Top-1: 51.50%, Top-5: 81.06%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 51.02%
[Alpha=0.50] Top-5 Accuracy: 81.26%
Result: Top-1: 51.02%, Top-5: 81.26%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 51.33%
[Alpha=0.50] Top-5 Accuracy: 81.30%
Result: Top-1: 51.33%, Top-5: 81.30%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 52.21%
[Alpha=0.50] Top-5 Accuracy: 81.58%
Result: Top-1: 52.21%, Top-5: 81.58%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 51.05%
[Alpha=0.50] Top-5 Accuracy: 81.14%
Result: Top-1: 51.05%, Top-5: 81.14%

============================================================
Testing: alpha=0.5, clusters=64, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 51.68%
[Alpha=0.50] Top-5 Accuracy: 81.24%
Result: Top-1: 51.68%, Top-5: 81.24%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 48.94%
[Alpha=0.50] Top-5 Accuracy: 78.70%
Result: Top-1: 48.94%, Top-5: 78.70%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 48.61%
[Alpha=0.50] Top-5 Accuracy: 79.45%
Result: Top-1: 48.61%, Top-5: 79.45%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 49.29%
[Alpha=0.50] Top-5 Accuracy: 79.83%
Result: Top-1: 49.29%, Top-5: 79.83%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 47.83%
[Alpha=0.50] Top-5 Accuracy: 78.93%
Result: Top-1: 47.83%, Top-5: 78.93%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 48.81%
[Alpha=0.50] Top-5 Accuracy: 79.50%
Result: Top-1: 48.81%, Top-5: 79.50%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 48.74%
[Alpha=0.50] Top-5 Accuracy: 79.69%
Result: Top-1: 48.74%, Top-5: 79.69%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 47.40%
[Alpha=0.50] Top-5 Accuracy: 78.54%
Result: Top-1: 47.40%, Top-5: 78.54%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 49.64%
[Alpha=0.50] Top-5 Accuracy: 80.15%
Result: Top-1: 49.64%, Top-5: 80.15%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 48.24%
[Alpha=0.50] Top-5 Accuracy: 79.45%
Result: Top-1: 48.24%, Top-5: 79.45%

============================================================
Testing: alpha=0.5, clusters=128, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 49.02%
[Alpha=0.50] Top-5 Accuracy: 79.75%
Result: Top-1: 49.02%, Top-5: 79.75%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=1
============================================================
[Alpha=0.50] Top-1 Accuracy: 45.26%
[Alpha=0.50] Top-5 Accuracy: 75.56%
Result: Top-1: 45.26%, Top-5: 75.56%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=25
============================================================
[Alpha=0.50] Top-1 Accuracy: 44.62%
[Alpha=0.50] Top-5 Accuracy: 76.29%
Result: Top-1: 44.62%, Top-5: 76.29%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=50
============================================================
[Alpha=0.50] Top-1 Accuracy: 42.34%
[Alpha=0.50] Top-5 Accuracy: 74.87%
Result: Top-1: 42.34%, Top-5: 74.87%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=75
============================================================
[Alpha=0.50] Top-1 Accuracy: 41.82%
[Alpha=0.50] Top-5 Accuracy: 73.45%
Result: Top-1: 41.82%, Top-5: 73.45%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=100
============================================================
[Alpha=0.50] Top-1 Accuracy: 41.68%
[Alpha=0.50] Top-5 Accuracy: 73.86%
Result: Top-1: 41.68%, Top-5: 73.86%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=125
============================================================
[Alpha=0.50] Top-1 Accuracy: 40.06%
[Alpha=0.50] Top-5 Accuracy: 71.10%
Result: Top-1: 40.06%, Top-5: 71.10%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=150
============================================================
[Alpha=0.50] Top-1 Accuracy: 41.41%
[Alpha=0.50] Top-5 Accuracy: 73.01%
Result: Top-1: 41.41%, Top-5: 73.01%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=175
============================================================
[Alpha=0.50] Top-1 Accuracy: 44.97%
[Alpha=0.50] Top-5 Accuracy: 76.58%
Result: Top-1: 44.97%, Top-5: 76.58%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=200
============================================================
[Alpha=0.50] Top-1 Accuracy: 42.88%
[Alpha=0.50] Top-5 Accuracy: 75.31%
Result: Top-1: 42.88%, Top-5: 75.31%

============================================================
Testing: alpha=0.5, clusters=256, pca_dim=220
============================================================
[Alpha=0.50] Top-1 Accuracy: 41.94%
[Alpha=0.50] Top-5 Accuracy: 73.92%
Result: Top-1: 41.94%, Top-5: 73.92%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.82%
[Alpha=0.60] Top-5 Accuracy: 78.79%
Result: Top-1: 48.82%, Top-5: 78.79%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 50.54%
[Alpha=0.60] Top-5 Accuracy: 80.00%
Result: Top-1: 50.54%, Top-5: 80.00%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 50.59%
[Alpha=0.60] Top-5 Accuracy: 80.05%
Result: Top-1: 50.59%, Top-5: 80.05%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 50.45%
[Alpha=0.60] Top-5 Accuracy: 79.73%
Result: Top-1: 50.45%, Top-5: 79.73%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 50.49%
[Alpha=0.60] Top-5 Accuracy: 79.75%
Result: Top-1: 50.49%, Top-5: 79.75%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 50.41%
[Alpha=0.60] Top-5 Accuracy: 79.68%
Result: Top-1: 50.41%, Top-5: 79.68%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 49.68%
[Alpha=0.60] Top-5 Accuracy: 79.92%
Result: Top-1: 49.68%, Top-5: 79.92%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 46.27%
[Alpha=0.60] Top-5 Accuracy: 78.88%
Result: Top-1: 46.27%, Top-5: 78.88%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 46.38%
[Alpha=0.60] Top-5 Accuracy: 78.98%
Result: Top-1: 46.38%, Top-5: 78.98%

============================================================
Testing: alpha=0.6, clusters=8, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 46.30%
[Alpha=0.60] Top-5 Accuracy: 79.06%
Result: Top-1: 46.30%, Top-5: 79.06%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.52%
[Alpha=0.60] Top-5 Accuracy: 78.42%
Result: Top-1: 48.52%, Top-5: 78.42%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.29%
[Alpha=0.60] Top-5 Accuracy: 79.56%
Result: Top-1: 48.29%, Top-5: 79.56%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.55%
[Alpha=0.60] Top-5 Accuracy: 79.65%
Result: Top-1: 48.55%, Top-5: 79.65%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.96%
[Alpha=0.60] Top-5 Accuracy: 79.72%
Result: Top-1: 48.96%, Top-5: 79.72%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.53%
[Alpha=0.60] Top-5 Accuracy: 79.41%
Result: Top-1: 48.53%, Top-5: 79.41%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.79%
[Alpha=0.60] Top-5 Accuracy: 79.81%
Result: Top-1: 48.79%, Top-5: 79.81%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 47.79%
[Alpha=0.60] Top-5 Accuracy: 79.28%
Result: Top-1: 47.79%, Top-5: 79.28%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.26%
[Alpha=0.60] Top-5 Accuracy: 79.41%
Result: Top-1: 48.26%, Top-5: 79.41%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.80%
[Alpha=0.60] Top-5 Accuracy: 79.87%
Result: Top-1: 48.80%, Top-5: 79.87%

============================================================
Testing: alpha=0.6, clusters=16, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.81%
[Alpha=0.60] Top-5 Accuracy: 79.74%
Result: Top-1: 48.81%, Top-5: 79.74%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 47.85%
[Alpha=0.60] Top-5 Accuracy: 77.99%
Result: Top-1: 47.85%, Top-5: 77.99%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.37%
[Alpha=0.60] Top-5 Accuracy: 79.55%
Result: Top-1: 48.37%, Top-5: 79.55%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 46.71%
[Alpha=0.60] Top-5 Accuracy: 78.69%
Result: Top-1: 46.71%, Top-5: 78.69%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 46.14%
[Alpha=0.60] Top-5 Accuracy: 79.04%
Result: Top-1: 46.14%, Top-5: 79.04%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 46.84%
[Alpha=0.60] Top-5 Accuracy: 79.06%
Result: Top-1: 46.84%, Top-5: 79.06%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 48.25%
[Alpha=0.60] Top-5 Accuracy: 79.61%
Result: Top-1: 48.25%, Top-5: 79.61%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 47.66%
[Alpha=0.60] Top-5 Accuracy: 79.27%
Result: Top-1: 47.66%, Top-5: 79.27%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 47.96%
[Alpha=0.60] Top-5 Accuracy: 79.53%
Result: Top-1: 47.96%, Top-5: 79.53%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 47.04%
[Alpha=0.60] Top-5 Accuracy: 79.09%
Result: Top-1: 47.04%, Top-5: 79.09%

============================================================
Testing: alpha=0.6, clusters=32, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 47.91%
[Alpha=0.60] Top-5 Accuracy: 79.44%
Result: Top-1: 47.91%, Top-5: 79.44%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 46.60%
[Alpha=0.60] Top-5 Accuracy: 76.82%
Result: Top-1: 46.60%, Top-5: 76.82%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 45.61%
[Alpha=0.60] Top-5 Accuracy: 77.77%
Result: Top-1: 45.61%, Top-5: 77.77%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 46.46%
[Alpha=0.60] Top-5 Accuracy: 78.20%
Result: Top-1: 46.46%, Top-5: 78.20%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 45.42%
[Alpha=0.60] Top-5 Accuracy: 78.14%
Result: Top-1: 45.42%, Top-5: 78.14%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 45.97%
[Alpha=0.60] Top-5 Accuracy: 77.96%
Result: Top-1: 45.97%, Top-5: 77.96%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 44.59%
[Alpha=0.60] Top-5 Accuracy: 78.29%
Result: Top-1: 44.59%, Top-5: 78.29%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 45.75%
[Alpha=0.60] Top-5 Accuracy: 78.31%
Result: Top-1: 45.75%, Top-5: 78.31%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 47.05%
[Alpha=0.60] Top-5 Accuracy: 78.90%
Result: Top-1: 47.05%, Top-5: 78.90%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 45.15%
[Alpha=0.60] Top-5 Accuracy: 78.10%
Result: Top-1: 45.15%, Top-5: 78.10%

============================================================
Testing: alpha=0.6, clusters=64, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 45.94%
[Alpha=0.60] Top-5 Accuracy: 78.11%
Result: Top-1: 45.94%, Top-5: 78.11%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 44.00%
[Alpha=0.60] Top-5 Accuracy: 74.47%
Result: Top-1: 44.00%, Top-5: 74.47%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 42.51%
[Alpha=0.60] Top-5 Accuracy: 75.39%
Result: Top-1: 42.51%, Top-5: 75.39%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 43.18%
[Alpha=0.60] Top-5 Accuracy: 76.11%
Result: Top-1: 43.18%, Top-5: 76.11%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 41.09%
[Alpha=0.60] Top-5 Accuracy: 74.94%
Result: Top-1: 41.09%, Top-5: 74.94%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 41.80%
[Alpha=0.60] Top-5 Accuracy: 75.45%
Result: Top-1: 41.80%, Top-5: 75.45%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 42.08%
[Alpha=0.60] Top-5 Accuracy: 75.79%
Result: Top-1: 42.08%, Top-5: 75.79%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 40.81%
[Alpha=0.60] Top-5 Accuracy: 74.54%
Result: Top-1: 40.81%, Top-5: 74.54%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 44.25%
[Alpha=0.60] Top-5 Accuracy: 76.72%
Result: Top-1: 44.25%, Top-5: 76.72%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 42.04%
[Alpha=0.60] Top-5 Accuracy: 75.50%
Result: Top-1: 42.04%, Top-5: 75.50%

============================================================
Testing: alpha=0.6, clusters=128, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 41.62%
[Alpha=0.60] Top-5 Accuracy: 75.95%
Result: Top-1: 41.62%, Top-5: 75.95%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=1
============================================================
[Alpha=0.60] Top-1 Accuracy: 39.94%
[Alpha=0.60] Top-5 Accuracy: 70.36%
Result: Top-1: 39.94%, Top-5: 70.36%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=25
============================================================
[Alpha=0.60] Top-1 Accuracy: 38.24%
[Alpha=0.60] Top-5 Accuracy: 71.05%
Result: Top-1: 38.24%, Top-5: 71.05%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=50
============================================================
[Alpha=0.60] Top-1 Accuracy: 35.69%
[Alpha=0.60] Top-5 Accuracy: 69.76%
Result: Top-1: 35.69%, Top-5: 69.76%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=75
============================================================
[Alpha=0.60] Top-1 Accuracy: 34.73%
[Alpha=0.60] Top-5 Accuracy: 67.63%
Result: Top-1: 34.73%, Top-5: 67.63%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=100
============================================================
[Alpha=0.60] Top-1 Accuracy: 34.74%
[Alpha=0.60] Top-5 Accuracy: 68.07%
Result: Top-1: 34.74%, Top-5: 68.07%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=125
============================================================
[Alpha=0.60] Top-1 Accuracy: 33.08%
[Alpha=0.60] Top-5 Accuracy: 65.61%
Result: Top-1: 33.08%, Top-5: 65.61%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=150
============================================================
[Alpha=0.60] Top-1 Accuracy: 35.28%
[Alpha=0.60] Top-5 Accuracy: 67.96%
Result: Top-1: 35.28%, Top-5: 67.96%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=175
============================================================
[Alpha=0.60] Top-1 Accuracy: 38.28%
[Alpha=0.60] Top-5 Accuracy: 71.43%
Result: Top-1: 38.28%, Top-5: 71.43%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=200
============================================================
[Alpha=0.60] Top-1 Accuracy: 36.23%
[Alpha=0.60] Top-5 Accuracy: 69.98%
Result: Top-1: 36.23%, Top-5: 69.98%

============================================================
Testing: alpha=0.6, clusters=256, pca_dim=220
============================================================
[Alpha=0.60] Top-1 Accuracy: 35.69%
[Alpha=0.60] Top-5 Accuracy: 68.65%
Result: Top-1: 35.69%, Top-5: 68.65%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 43.74%
[Alpha=0.70] Top-5 Accuracy: 74.55%
Result: Top-1: 43.74%, Top-5: 74.55%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 45.16%
[Alpha=0.70] Top-5 Accuracy: 76.27%
Result: Top-1: 45.16%, Top-5: 76.27%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 45.28%
[Alpha=0.70] Top-5 Accuracy: 76.34%
Result: Top-1: 45.28%, Top-5: 76.34%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 44.85%
[Alpha=0.70] Top-5 Accuracy: 75.97%
Result: Top-1: 44.85%, Top-5: 75.97%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 44.98%
[Alpha=0.70] Top-5 Accuracy: 76.00%
Result: Top-1: 44.98%, Top-5: 76.00%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 44.82%
[Alpha=0.70] Top-5 Accuracy: 75.93%
Result: Top-1: 44.82%, Top-5: 75.93%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 44.02%
[Alpha=0.70] Top-5 Accuracy: 76.17%
Result: Top-1: 44.02%, Top-5: 76.17%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.18%
[Alpha=0.70] Top-5 Accuracy: 74.44%
Result: Top-1: 38.18%, Top-5: 74.44%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.31%
[Alpha=0.70] Top-5 Accuracy: 74.69%
Result: Top-1: 38.31%, Top-5: 74.69%

============================================================
Testing: alpha=0.7, clusters=8, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.37%
[Alpha=0.70] Top-5 Accuracy: 74.83%
Result: Top-1: 38.37%, Top-5: 74.83%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 43.10%
[Alpha=0.70] Top-5 Accuracy: 74.10%
Result: Top-1: 43.10%, Top-5: 74.10%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 40.70%
[Alpha=0.70] Top-5 Accuracy: 75.55%
Result: Top-1: 40.70%, Top-5: 75.55%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 41.01%
[Alpha=0.70] Top-5 Accuracy: 75.62%
Result: Top-1: 41.01%, Top-5: 75.62%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 41.95%
[Alpha=0.70] Top-5 Accuracy: 75.85%
Result: Top-1: 41.95%, Top-5: 75.85%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 39.93%
[Alpha=0.70] Top-5 Accuracy: 75.46%
Result: Top-1: 39.93%, Top-5: 75.46%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 41.16%
[Alpha=0.70] Top-5 Accuracy: 75.82%
Result: Top-1: 41.16%, Top-5: 75.82%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 40.31%
[Alpha=0.70] Top-5 Accuracy: 75.26%
Result: Top-1: 40.31%, Top-5: 75.26%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 40.21%
[Alpha=0.70] Top-5 Accuracy: 75.59%
Result: Top-1: 40.21%, Top-5: 75.59%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 41.77%
[Alpha=0.70] Top-5 Accuracy: 76.02%
Result: Top-1: 41.77%, Top-5: 76.02%

============================================================
Testing: alpha=0.7, clusters=16, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 41.53%
[Alpha=0.70] Top-5 Accuracy: 76.07%
Result: Top-1: 41.53%, Top-5: 76.07%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 42.44%
[Alpha=0.70] Top-5 Accuracy: 73.49%
Result: Top-1: 42.44%, Top-5: 73.49%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 41.77%
[Alpha=0.70] Top-5 Accuracy: 75.62%
Result: Top-1: 41.77%, Top-5: 75.62%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.68%
[Alpha=0.70] Top-5 Accuracy: 74.17%
Result: Top-1: 38.68%, Top-5: 74.17%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 37.89%
[Alpha=0.70] Top-5 Accuracy: 74.64%
Result: Top-1: 37.89%, Top-5: 74.64%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.96%
[Alpha=0.70] Top-5 Accuracy: 74.74%
Result: Top-1: 38.96%, Top-5: 74.74%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 40.95%
[Alpha=0.70] Top-5 Accuracy: 75.86%
Result: Top-1: 40.95%, Top-5: 75.86%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 39.67%
[Alpha=0.70] Top-5 Accuracy: 75.21%
Result: Top-1: 39.67%, Top-5: 75.21%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 41.18%
[Alpha=0.70] Top-5 Accuracy: 75.68%
Result: Top-1: 41.18%, Top-5: 75.68%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.66%
[Alpha=0.70] Top-5 Accuracy: 74.98%
Result: Top-1: 38.66%, Top-5: 74.98%

============================================================
Testing: alpha=0.7, clusters=32, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 41.30%
[Alpha=0.70] Top-5 Accuracy: 75.70%
Result: Top-1: 41.30%, Top-5: 75.70%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 40.98%
[Alpha=0.70] Top-5 Accuracy: 71.76%
Result: Top-1: 40.98%, Top-5: 71.76%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.17%
[Alpha=0.70] Top-5 Accuracy: 73.00%
Result: Top-1: 38.17%, Top-5: 73.00%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.94%
[Alpha=0.70] Top-5 Accuracy: 73.49%
Result: Top-1: 38.94%, Top-5: 73.49%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.38%
[Alpha=0.70] Top-5 Accuracy: 73.82%
Result: Top-1: 38.38%, Top-5: 73.82%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.66%
[Alpha=0.70] Top-5 Accuracy: 73.06%
Result: Top-1: 38.66%, Top-5: 73.06%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 37.02%
[Alpha=0.70] Top-5 Accuracy: 73.80%
Result: Top-1: 37.02%, Top-5: 73.80%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.84%
[Alpha=0.70] Top-5 Accuracy: 73.74%
Result: Top-1: 38.84%, Top-5: 73.74%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 40.12%
[Alpha=0.70] Top-5 Accuracy: 74.52%
Result: Top-1: 40.12%, Top-5: 74.52%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.39%
[Alpha=0.70] Top-5 Accuracy: 73.58%
Result: Top-1: 38.39%, Top-5: 73.58%

============================================================
Testing: alpha=0.7, clusters=64, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.26%
[Alpha=0.70] Top-5 Accuracy: 73.24%
Result: Top-1: 38.26%, Top-5: 73.24%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.43%
[Alpha=0.70] Top-5 Accuracy: 68.68%
Result: Top-1: 38.43%, Top-5: 68.68%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 35.12%
[Alpha=0.70] Top-5 Accuracy: 69.83%
Result: Top-1: 35.12%, Top-5: 69.83%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 36.50%
[Alpha=0.70] Top-5 Accuracy: 71.11%
Result: Top-1: 36.50%, Top-5: 71.11%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 34.18%
[Alpha=0.70] Top-5 Accuracy: 69.26%
Result: Top-1: 34.18%, Top-5: 69.26%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 33.85%
[Alpha=0.70] Top-5 Accuracy: 69.89%
Result: Top-1: 33.85%, Top-5: 69.89%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 34.85%
[Alpha=0.70] Top-5 Accuracy: 70.40%
Result: Top-1: 34.85%, Top-5: 70.40%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 33.79%
[Alpha=0.70] Top-5 Accuracy: 68.83%
Result: Top-1: 33.79%, Top-5: 68.83%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 38.14%
[Alpha=0.70] Top-5 Accuracy: 72.00%
Result: Top-1: 38.14%, Top-5: 72.00%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 35.86%
[Alpha=0.70] Top-5 Accuracy: 70.23%
Result: Top-1: 35.86%, Top-5: 70.23%

============================================================
Testing: alpha=0.7, clusters=128, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 34.08%
[Alpha=0.70] Top-5 Accuracy: 70.42%
Result: Top-1: 34.08%, Top-5: 70.42%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=1
============================================================
[Alpha=0.70] Top-1 Accuracy: 34.25%
[Alpha=0.70] Top-5 Accuracy: 63.59%
Result: Top-1: 34.25%, Top-5: 63.59%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=25
============================================================
[Alpha=0.70] Top-1 Accuracy: 31.83%
[Alpha=0.70] Top-5 Accuracy: 64.83%
Result: Top-1: 31.83%, Top-5: 64.83%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=50
============================================================
[Alpha=0.70] Top-1 Accuracy: 29.58%
[Alpha=0.70] Top-5 Accuracy: 63.07%
Result: Top-1: 29.58%, Top-5: 63.07%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=75
============================================================
[Alpha=0.70] Top-1 Accuracy: 28.81%
[Alpha=0.70] Top-5 Accuracy: 61.36%
Result: Top-1: 28.81%, Top-5: 61.36%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=100
============================================================
[Alpha=0.70] Top-1 Accuracy: 28.84%
[Alpha=0.70] Top-5 Accuracy: 61.34%
Result: Top-1: 28.84%, Top-5: 61.34%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=125
============================================================
[Alpha=0.70] Top-1 Accuracy: 27.06%
[Alpha=0.70] Top-5 Accuracy: 59.22%
Result: Top-1: 27.06%, Top-5: 59.22%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=150
============================================================
[Alpha=0.70] Top-1 Accuracy: 29.21%
[Alpha=0.70] Top-5 Accuracy: 61.93%
Result: Top-1: 29.21%, Top-5: 61.93%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=175
============================================================
[Alpha=0.70] Top-1 Accuracy: 31.69%
[Alpha=0.70] Top-5 Accuracy: 64.93%
Result: Top-1: 31.69%, Top-5: 64.93%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=200
============================================================
[Alpha=0.70] Top-1 Accuracy: 30.07%
[Alpha=0.70] Top-5 Accuracy: 63.66%
Result: Top-1: 30.07%, Top-5: 63.66%

============================================================
Testing: alpha=0.7, clusters=256, pca_dim=220
============================================================
[Alpha=0.70] Top-1 Accuracy: 29.75%
[Alpha=0.70] Top-5 Accuracy: 62.05%
Result: Top-1: 29.75%, Top-5: 62.05%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 37.24%
[Alpha=0.80] Top-5 Accuracy: 68.27%
Result: Top-1: 37.24%, Top-5: 68.27%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 37.62%
[Alpha=0.80] Top-5 Accuracy: 70.44%
Result: Top-1: 37.62%, Top-5: 70.44%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 37.43%
[Alpha=0.80] Top-5 Accuracy: 70.52%
Result: Top-1: 37.43%, Top-5: 70.52%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 36.66%
[Alpha=0.80] Top-5 Accuracy: 69.76%
Result: Top-1: 36.66%, Top-5: 69.76%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 36.85%
[Alpha=0.80] Top-5 Accuracy: 69.87%
Result: Top-1: 36.85%, Top-5: 69.87%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 36.59%
[Alpha=0.80] Top-5 Accuracy: 69.75%
Result: Top-1: 36.59%, Top-5: 69.75%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 36.63%
[Alpha=0.80] Top-5 Accuracy: 70.20%
Result: Top-1: 36.63%, Top-5: 70.20%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 28.93%
[Alpha=0.80] Top-5 Accuracy: 67.37%
Result: Top-1: 28.93%, Top-5: 67.37%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 29.85%
[Alpha=0.80] Top-5 Accuracy: 68.19%
Result: Top-1: 29.85%, Top-5: 68.19%

============================================================
Testing: alpha=0.8, clusters=8, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.14%
[Alpha=0.80] Top-5 Accuracy: 68.38%
Result: Top-1: 30.14%, Top-5: 68.38%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 36.29%
[Alpha=0.80] Top-5 Accuracy: 67.65%
Result: Top-1: 36.29%, Top-5: 67.65%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 31.21%
[Alpha=0.80] Top-5 Accuracy: 69.33%
Result: Top-1: 31.21%, Top-5: 69.33%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 31.75%
[Alpha=0.80] Top-5 Accuracy: 69.18%
Result: Top-1: 31.75%, Top-5: 69.18%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 33.28%
[Alpha=0.80] Top-5 Accuracy: 69.83%
Result: Top-1: 33.28%, Top-5: 69.83%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 29.85%
[Alpha=0.80] Top-5 Accuracy: 69.24%
Result: Top-1: 29.85%, Top-5: 69.24%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 32.26%
[Alpha=0.80] Top-5 Accuracy: 69.90%
Result: Top-1: 32.26%, Top-5: 69.90%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 31.46%
[Alpha=0.80] Top-5 Accuracy: 68.82%
Result: Top-1: 31.46%, Top-5: 68.82%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.69%
[Alpha=0.80] Top-5 Accuracy: 69.34%
Result: Top-1: 30.69%, Top-5: 69.34%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 33.39%
[Alpha=0.80] Top-5 Accuracy: 70.06%
Result: Top-1: 33.39%, Top-5: 70.06%

============================================================
Testing: alpha=0.8, clusters=16, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 32.69%
[Alpha=0.80] Top-5 Accuracy: 70.38%
Result: Top-1: 32.69%, Top-5: 70.38%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 35.51%
[Alpha=0.80] Top-5 Accuracy: 66.67%
Result: Top-1: 35.51%, Top-5: 66.67%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 33.65%
[Alpha=0.80] Top-5 Accuracy: 69.64%
Result: Top-1: 33.65%, Top-5: 69.64%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.74%
[Alpha=0.80] Top-5 Accuracy: 67.57%
Result: Top-1: 30.74%, Top-5: 67.57%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 29.74%
[Alpha=0.80] Top-5 Accuracy: 68.15%
Result: Top-1: 29.74%, Top-5: 68.15%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.00%
[Alpha=0.80] Top-5 Accuracy: 67.82%
Result: Top-1: 30.00%, Top-5: 67.82%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 32.44%
[Alpha=0.80] Top-5 Accuracy: 69.86%
Result: Top-1: 32.44%, Top-5: 69.86%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.76%
[Alpha=0.80] Top-5 Accuracy: 68.76%
Result: Top-1: 30.76%, Top-5: 68.76%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 32.90%
[Alpha=0.80] Top-5 Accuracy: 70.05%
Result: Top-1: 32.90%, Top-5: 70.05%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 29.46%
[Alpha=0.80] Top-5 Accuracy: 68.52%
Result: Top-1: 29.46%, Top-5: 68.52%

============================================================
Testing: alpha=0.8, clusters=32, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 33.68%
[Alpha=0.80] Top-5 Accuracy: 69.93%
Result: Top-1: 33.68%, Top-5: 69.93%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 34.28%
[Alpha=0.80] Top-5 Accuracy: 64.61%
Result: Top-1: 34.28%, Top-5: 64.61%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.14%
[Alpha=0.80] Top-5 Accuracy: 66.39%
Result: Top-1: 30.14%, Top-5: 66.39%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.96%
[Alpha=0.80] Top-5 Accuracy: 66.98%
Result: Top-1: 30.96%, Top-5: 66.98%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 31.32%
[Alpha=0.80] Top-5 Accuracy: 67.84%
Result: Top-1: 31.32%, Top-5: 67.84%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 31.06%
[Alpha=0.80] Top-5 Accuracy: 66.16%
Result: Top-1: 31.06%, Top-5: 66.16%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 29.61%
[Alpha=0.80] Top-5 Accuracy: 67.60%
Result: Top-1: 29.61%, Top-5: 67.60%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 31.48%
[Alpha=0.80] Top-5 Accuracy: 67.29%
Result: Top-1: 31.48%, Top-5: 67.29%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 32.44%
[Alpha=0.80] Top-5 Accuracy: 68.43%
Result: Top-1: 32.44%, Top-5: 68.43%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.79%
[Alpha=0.80] Top-5 Accuracy: 67.31%
Result: Top-1: 30.79%, Top-5: 67.31%

============================================================
Testing: alpha=0.8, clusters=64, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.32%
[Alpha=0.80] Top-5 Accuracy: 66.13%
Result: Top-1: 30.32%, Top-5: 66.13%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 31.87%
[Alpha=0.80] Top-5 Accuracy: 61.19%
Result: Top-1: 31.87%, Top-5: 61.19%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 28.18%
[Alpha=0.80] Top-5 Accuracy: 63.00%
Result: Top-1: 28.18%, Top-5: 63.00%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.29%
[Alpha=0.80] Top-5 Accuracy: 64.47%
Result: Top-1: 30.29%, Top-5: 64.47%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 28.03%
[Alpha=0.80] Top-5 Accuracy: 62.28%
Result: Top-1: 28.03%, Top-5: 62.28%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 26.59%
[Alpha=0.80] Top-5 Accuracy: 62.83%
Result: Top-1: 26.59%, Top-5: 62.83%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 28.04%
[Alpha=0.80] Top-5 Accuracy: 63.78%
Result: Top-1: 28.04%, Top-5: 63.78%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 27.77%
[Alpha=0.80] Top-5 Accuracy: 61.88%
Result: Top-1: 27.77%, Top-5: 61.88%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 31.93%
[Alpha=0.80] Top-5 Accuracy: 65.73%
Result: Top-1: 31.93%, Top-5: 65.73%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 30.01%
[Alpha=0.80] Top-5 Accuracy: 63.62%
Result: Top-1: 30.01%, Top-5: 63.62%

============================================================
Testing: alpha=0.8, clusters=128, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 27.37%
[Alpha=0.80] Top-5 Accuracy: 63.21%
Result: Top-1: 27.37%, Top-5: 63.21%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=1
============================================================
[Alpha=0.80] Top-1 Accuracy: 28.55%
[Alpha=0.80] Top-5 Accuracy: 55.84%
Result: Top-1: 28.55%, Top-5: 55.84%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=25
============================================================
[Alpha=0.80] Top-1 Accuracy: 25.95%
[Alpha=0.80] Top-5 Accuracy: 58.02%
Result: Top-1: 25.95%, Top-5: 58.02%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=50
============================================================
[Alpha=0.80] Top-1 Accuracy: 24.43%
[Alpha=0.80] Top-5 Accuracy: 55.69%
Result: Top-1: 24.43%, Top-5: 55.69%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=75
============================================================
[Alpha=0.80] Top-1 Accuracy: 23.73%
[Alpha=0.80] Top-5 Accuracy: 54.36%
Result: Top-1: 23.73%, Top-5: 54.36%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=100
============================================================
[Alpha=0.80] Top-1 Accuracy: 23.59%
[Alpha=0.80] Top-5 Accuracy: 54.44%
Result: Top-1: 23.59%, Top-5: 54.44%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=125
============================================================
[Alpha=0.80] Top-1 Accuracy: 22.51%
[Alpha=0.80] Top-5 Accuracy: 52.52%
Result: Top-1: 22.51%, Top-5: 52.52%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=150
============================================================
[Alpha=0.80] Top-1 Accuracy: 23.67%
[Alpha=0.80] Top-5 Accuracy: 55.21%
Result: Top-1: 23.67%, Top-5: 55.21%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=175
============================================================
[Alpha=0.80] Top-1 Accuracy: 26.52%
[Alpha=0.80] Top-5 Accuracy: 57.97%
Result: Top-1: 26.52%, Top-5: 57.97%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=200
============================================================
[Alpha=0.80] Top-1 Accuracy: 24.55%
[Alpha=0.80] Top-5 Accuracy: 56.85%
Result: Top-1: 24.55%, Top-5: 56.85%

============================================================
Testing: alpha=0.8, clusters=256, pca_dim=220
============================================================
[Alpha=0.80] Top-1 Accuracy: 24.42%
[Alpha=0.80] Top-5 Accuracy: 55.25%
Result: Top-1: 24.42%, Top-5: 55.25%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 29.34%
[Alpha=0.90] Top-5 Accuracy: 59.52%
Result: Top-1: 29.34%, Top-5: 59.52%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 28.32%
[Alpha=0.90] Top-5 Accuracy: 61.79%
Result: Top-1: 28.32%, Top-5: 61.79%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 28.19%
[Alpha=0.90] Top-5 Accuracy: 61.76%
Result: Top-1: 28.19%, Top-5: 61.76%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 26.89%
[Alpha=0.90] Top-5 Accuracy: 60.82%
Result: Top-1: 26.89%, Top-5: 60.82%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 27.21%
[Alpha=0.90] Top-5 Accuracy: 60.90%
Result: Top-1: 27.21%, Top-5: 60.90%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 26.84%
[Alpha=0.90] Top-5 Accuracy: 60.81%
Result: Top-1: 26.84%, Top-5: 60.81%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 27.69%
[Alpha=0.90] Top-5 Accuracy: 61.63%
Result: Top-1: 27.69%, Top-5: 61.63%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 19.08%
[Alpha=0.90] Top-5 Accuracy: 57.38%
Result: Top-1: 19.08%, Top-5: 57.38%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 21.06%
[Alpha=0.90] Top-5 Accuracy: 59.00%
Result: Top-1: 21.06%, Top-5: 59.00%

============================================================
Testing: alpha=0.9, clusters=8, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 21.42%
[Alpha=0.90] Top-5 Accuracy: 59.23%
Result: Top-1: 21.42%, Top-5: 59.23%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 28.14%
[Alpha=0.90] Top-5 Accuracy: 58.74%
Result: Top-1: 28.14%, Top-5: 58.74%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 22.30%
[Alpha=0.90] Top-5 Accuracy: 60.55%
Result: Top-1: 22.30%, Top-5: 60.55%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 22.67%
[Alpha=0.90] Top-5 Accuracy: 60.22%
Result: Top-1: 22.67%, Top-5: 60.22%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 24.47%
[Alpha=0.90] Top-5 Accuracy: 61.71%
Result: Top-1: 24.47%, Top-5: 61.71%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 21.42%
[Alpha=0.90] Top-5 Accuracy: 60.20%
Result: Top-1: 21.42%, Top-5: 60.20%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 23.73%
[Alpha=0.90] Top-5 Accuracy: 61.36%
Result: Top-1: 23.73%, Top-5: 61.36%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 23.01%
[Alpha=0.90] Top-5 Accuracy: 60.23%
Result: Top-1: 23.01%, Top-5: 60.23%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 22.02%
[Alpha=0.90] Top-5 Accuracy: 60.84%
Result: Top-1: 22.02%, Top-5: 60.84%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 24.92%
[Alpha=0.90] Top-5 Accuracy: 61.72%
Result: Top-1: 24.92%, Top-5: 61.72%

============================================================
Testing: alpha=0.9, clusters=16, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 23.82%
[Alpha=0.90] Top-5 Accuracy: 62.12%
Result: Top-1: 23.82%, Top-5: 62.12%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 27.77%
[Alpha=0.90] Top-5 Accuracy: 57.51%
Result: Top-1: 27.77%, Top-5: 57.51%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 24.89%
[Alpha=0.90] Top-5 Accuracy: 61.39%
Result: Top-1: 24.89%, Top-5: 61.39%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 23.12%
[Alpha=0.90] Top-5 Accuracy: 58.63%
Result: Top-1: 23.12%, Top-5: 58.63%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 22.67%
[Alpha=0.90] Top-5 Accuracy: 59.67%
Result: Top-1: 22.67%, Top-5: 59.67%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 21.89%
[Alpha=0.90] Top-5 Accuracy: 58.61%
Result: Top-1: 21.89%, Top-5: 58.61%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 24.25%
[Alpha=0.90] Top-5 Accuracy: 61.46%
Result: Top-1: 24.25%, Top-5: 61.46%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 22.84%
[Alpha=0.90] Top-5 Accuracy: 60.33%
Result: Top-1: 22.84%, Top-5: 60.33%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 25.19%
[Alpha=0.90] Top-5 Accuracy: 62.35%
Result: Top-1: 25.19%, Top-5: 62.35%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 21.48%
[Alpha=0.90] Top-5 Accuracy: 59.57%
Result: Top-1: 21.48%, Top-5: 59.57%

============================================================
Testing: alpha=0.9, clusters=32, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 26.00%
[Alpha=0.90] Top-5 Accuracy: 61.98%
Result: Top-1: 26.00%, Top-5: 61.98%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 27.02%
[Alpha=0.90] Top-5 Accuracy: 55.54%
Result: Top-1: 27.02%, Top-5: 55.54%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 22.94%
[Alpha=0.90] Top-5 Accuracy: 57.64%
Result: Top-1: 22.94%, Top-5: 57.64%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 23.70%
[Alpha=0.90] Top-5 Accuracy: 58.42%
Result: Top-1: 23.70%, Top-5: 58.42%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 25.12%
[Alpha=0.90] Top-5 Accuracy: 60.31%
Result: Top-1: 25.12%, Top-5: 60.31%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 23.81%
[Alpha=0.90] Top-5 Accuracy: 57.61%
Result: Top-1: 23.81%, Top-5: 57.61%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 23.22%
[Alpha=0.90] Top-5 Accuracy: 59.35%
Result: Top-1: 23.22%, Top-5: 59.35%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 24.63%
[Alpha=0.90] Top-5 Accuracy: 59.43%
Result: Top-1: 24.63%, Top-5: 59.43%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 25.35%
[Alpha=0.90] Top-5 Accuracy: 60.64%
Result: Top-1: 25.35%, Top-5: 60.64%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 24.26%
[Alpha=0.90] Top-5 Accuracy: 59.22%
Result: Top-1: 24.26%, Top-5: 59.22%

============================================================
Testing: alpha=0.9, clusters=64, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 23.04%
[Alpha=0.90] Top-5 Accuracy: 57.23%
Result: Top-1: 23.04%, Top-5: 57.23%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 25.38%
[Alpha=0.90] Top-5 Accuracy: 52.57%
Result: Top-1: 25.38%, Top-5: 52.57%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 22.53%
[Alpha=0.90] Top-5 Accuracy: 55.40%
Result: Top-1: 22.53%, Top-5: 55.40%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 24.99%
[Alpha=0.90] Top-5 Accuracy: 57.09%
Result: Top-1: 24.99%, Top-5: 57.09%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 22.86%
[Alpha=0.90] Top-5 Accuracy: 54.52%
Result: Top-1: 22.86%, Top-5: 54.52%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 20.67%
[Alpha=0.90] Top-5 Accuracy: 54.82%
Result: Top-1: 20.67%, Top-5: 54.82%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 22.34%
[Alpha=0.90] Top-5 Accuracy: 55.88%
Result: Top-1: 22.34%, Top-5: 55.88%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 22.51%
[Alpha=0.90] Top-5 Accuracy: 54.28%
Result: Top-1: 22.51%, Top-5: 54.28%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 26.46%
[Alpha=0.90] Top-5 Accuracy: 58.58%
Result: Top-1: 26.46%, Top-5: 58.58%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 24.48%
[Alpha=0.90] Top-5 Accuracy: 56.28%
Result: Top-1: 24.48%, Top-5: 56.28%

============================================================
Testing: alpha=0.9, clusters=128, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 21.73%
[Alpha=0.90] Top-5 Accuracy: 55.09%
Result: Top-1: 21.73%, Top-5: 55.09%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=1
============================================================
[Alpha=0.90] Top-1 Accuracy: 23.07%
[Alpha=0.90] Top-5 Accuracy: 48.02%
Result: Top-1: 23.07%, Top-5: 48.02%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=25
============================================================
[Alpha=0.90] Top-1 Accuracy: 21.10%
[Alpha=0.90] Top-5 Accuracy: 51.13%
Result: Top-1: 21.10%, Top-5: 51.13%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=50
============================================================
[Alpha=0.90] Top-1 Accuracy: 20.32%
[Alpha=0.90] Top-5 Accuracy: 48.65%
Result: Top-1: 20.32%, Top-5: 48.65%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=75
============================================================
[Alpha=0.90] Top-1 Accuracy: 19.79%
[Alpha=0.90] Top-5 Accuracy: 47.71%
Result: Top-1: 19.79%, Top-5: 47.71%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=100
============================================================
[Alpha=0.90] Top-1 Accuracy: 19.45%
[Alpha=0.90] Top-5 Accuracy: 47.64%
Result: Top-1: 19.45%, Top-5: 47.64%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=125
============================================================
[Alpha=0.90] Top-1 Accuracy: 18.80%
[Alpha=0.90] Top-5 Accuracy: 45.95%
Result: Top-1: 18.80%, Top-5: 45.95%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=150
============================================================
[Alpha=0.90] Top-1 Accuracy: 18.99%
[Alpha=0.90] Top-5 Accuracy: 48.34%
Result: Top-1: 18.99%, Top-5: 48.34%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=175
============================================================
[Alpha=0.90] Top-1 Accuracy: 21.93%
[Alpha=0.90] Top-5 Accuracy: 50.76%
Result: Top-1: 21.93%, Top-5: 50.76%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=200
============================================================
[Alpha=0.90] Top-1 Accuracy: 20.11%
[Alpha=0.90] Top-5 Accuracy: 49.75%
Result: Top-1: 20.11%, Top-5: 49.75%

============================================================
Testing: alpha=0.9, clusters=256, pca_dim=220
============================================================
[Alpha=0.90] Top-1 Accuracy: 19.93%
[Alpha=0.90] Top-5 Accuracy: 47.96%
Result: Top-1: 19.93%, Top-5: 47.96%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 21.24%
[Alpha=1.00] Top-5 Accuracy: 48.78%
Result: Top-1: 21.24%, Top-5: 48.78%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 19.53%
[Alpha=1.00] Top-5 Accuracy: 50.69%
Result: Top-1: 19.53%, Top-5: 50.69%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 19.48%
[Alpha=1.00] Top-5 Accuracy: 50.80%
Result: Top-1: 19.48%, Top-5: 50.80%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.78%
[Alpha=1.00] Top-5 Accuracy: 49.23%
Result: Top-1: 17.78%, Top-5: 49.23%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 18.01%
[Alpha=1.00] Top-5 Accuracy: 49.33%
Result: Top-1: 18.01%, Top-5: 49.33%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.74%
[Alpha=1.00] Top-5 Accuracy: 49.15%
Result: Top-1: 17.74%, Top-5: 49.15%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 19.31%
[Alpha=1.00] Top-5 Accuracy: 50.39%
Result: Top-1: 19.31%, Top-5: 50.39%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 10.87%
[Alpha=1.00] Top-5 Accuracy: 45.50%
Result: Top-1: 10.87%, Top-5: 45.50%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 13.12%
[Alpha=1.00] Top-5 Accuracy: 47.84%
Result: Top-1: 13.12%, Top-5: 47.84%

============================================================
Testing: alpha=1.0, clusters=8, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 13.48%
[Alpha=1.00] Top-5 Accuracy: 48.01%
Result: Top-1: 13.48%, Top-5: 48.01%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 20.24%
[Alpha=1.00] Top-5 Accuracy: 48.02%
Result: Top-1: 20.24%, Top-5: 48.02%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 15.70%
[Alpha=1.00] Top-5 Accuracy: 49.53%
Result: Top-1: 15.70%, Top-5: 49.53%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 15.64%
[Alpha=1.00] Top-5 Accuracy: 49.20%
Result: Top-1: 15.64%, Top-5: 49.20%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.33%
[Alpha=1.00] Top-5 Accuracy: 51.66%
Result: Top-1: 17.33%, Top-5: 51.66%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 15.10%
[Alpha=1.00] Top-5 Accuracy: 49.34%
Result: Top-1: 15.10%, Top-5: 49.34%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.75%
[Alpha=1.00] Top-5 Accuracy: 50.70%
Result: Top-1: 16.75%, Top-5: 50.70%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.01%
[Alpha=1.00] Top-5 Accuracy: 49.53%
Result: Top-1: 16.01%, Top-5: 49.53%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 15.60%
[Alpha=1.00] Top-5 Accuracy: 50.14%
Result: Top-1: 15.60%, Top-5: 50.14%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.86%
[Alpha=1.00] Top-5 Accuracy: 51.14%
Result: Top-1: 17.86%, Top-5: 51.14%

============================================================
Testing: alpha=1.0, clusters=16, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.69%
[Alpha=1.00] Top-5 Accuracy: 51.69%
Result: Top-1: 16.69%, Top-5: 51.69%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 20.30%
[Alpha=1.00] Top-5 Accuracy: 47.41%
Result: Top-1: 20.30%, Top-5: 47.41%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.98%
[Alpha=1.00] Top-5 Accuracy: 51.59%
Result: Top-1: 17.98%, Top-5: 51.59%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.79%
[Alpha=1.00] Top-5 Accuracy: 48.75%
Result: Top-1: 16.79%, Top-5: 48.75%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.19%
[Alpha=1.00] Top-5 Accuracy: 50.12%
Result: Top-1: 17.19%, Top-5: 50.12%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 15.40%
[Alpha=1.00] Top-5 Accuracy: 48.06%
Result: Top-1: 15.40%, Top-5: 48.06%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.66%
[Alpha=1.00] Top-5 Accuracy: 51.47%
Result: Top-1: 17.66%, Top-5: 51.47%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.40%
[Alpha=1.00] Top-5 Accuracy: 50.12%
Result: Top-1: 16.40%, Top-5: 50.12%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 18.79%
[Alpha=1.00] Top-5 Accuracy: 52.85%
Result: Top-1: 18.79%, Top-5: 52.85%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 15.15%
[Alpha=1.00] Top-5 Accuracy: 49.37%
Result: Top-1: 15.15%, Top-5: 49.37%

============================================================
Testing: alpha=1.0, clusters=32, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 19.50%
[Alpha=1.00] Top-5 Accuracy: 52.62%
Result: Top-1: 19.50%, Top-5: 52.62%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 20.25%
[Alpha=1.00] Top-5 Accuracy: 46.06%
Result: Top-1: 20.25%, Top-5: 46.06%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.04%
[Alpha=1.00] Top-5 Accuracy: 48.16%
Result: Top-1: 17.04%, Top-5: 48.16%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.64%
[Alpha=1.00] Top-5 Accuracy: 49.06%
Result: Top-1: 17.64%, Top-5: 49.06%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 20.06%
[Alpha=1.00] Top-5 Accuracy: 51.65%
Result: Top-1: 20.06%, Top-5: 51.65%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 18.05%
[Alpha=1.00] Top-5 Accuracy: 48.26%
Result: Top-1: 18.05%, Top-5: 48.26%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.91%
[Alpha=1.00] Top-5 Accuracy: 50.47%
Result: Top-1: 17.91%, Top-5: 50.47%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 19.20%
[Alpha=1.00] Top-5 Accuracy: 51.03%
Result: Top-1: 19.20%, Top-5: 51.03%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 19.39%
[Alpha=1.00] Top-5 Accuracy: 52.34%
Result: Top-1: 19.39%, Top-5: 52.34%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 18.62%
[Alpha=1.00] Top-5 Accuracy: 50.27%
Result: Top-1: 18.62%, Top-5: 50.27%

============================================================
Testing: alpha=1.0, clusters=64, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.94%
[Alpha=1.00] Top-5 Accuracy: 47.61%
Result: Top-1: 16.94%, Top-5: 47.61%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 19.63%
[Alpha=1.00] Top-5 Accuracy: 43.44%
Result: Top-1: 19.63%, Top-5: 43.44%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 18.07%
[Alpha=1.00] Top-5 Accuracy: 47.54%
Result: Top-1: 18.07%, Top-5: 47.54%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 20.49%
[Alpha=1.00] Top-5 Accuracy: 49.25%
Result: Top-1: 20.49%, Top-5: 49.25%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 18.51%
[Alpha=1.00] Top-5 Accuracy: 46.84%
Result: Top-1: 18.51%, Top-5: 46.84%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.47%
[Alpha=1.00] Top-5 Accuracy: 46.69%
Result: Top-1: 16.47%, Top-5: 46.69%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.75%
[Alpha=1.00] Top-5 Accuracy: 48.14%
Result: Top-1: 17.75%, Top-5: 48.14%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 18.34%
[Alpha=1.00] Top-5 Accuracy: 46.64%
Result: Top-1: 18.34%, Top-5: 46.64%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 21.81%
[Alpha=1.00] Top-5 Accuracy: 51.42%
Result: Top-1: 21.81%, Top-5: 51.42%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 20.02%
[Alpha=1.00] Top-5 Accuracy: 48.90%
Result: Top-1: 20.02%, Top-5: 48.90%

============================================================
Testing: alpha=1.0, clusters=128, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.17%
[Alpha=1.00] Top-5 Accuracy: 47.10%
Result: Top-1: 17.17%, Top-5: 47.10%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=1
============================================================
[Alpha=1.00] Top-1 Accuracy: 18.44%
[Alpha=1.00] Top-5 Accuracy: 40.40%
Result: Top-1: 18.44%, Top-5: 40.40%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=25
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.18%
[Alpha=1.00] Top-5 Accuracy: 44.78%
Result: Top-1: 17.18%, Top-5: 44.78%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=50
============================================================
[Alpha=1.00] Top-1 Accuracy: 17.06%
[Alpha=1.00] Top-5 Accuracy: 42.55%
Result: Top-1: 17.06%, Top-5: 42.55%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=75
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.62%
[Alpha=1.00] Top-5 Accuracy: 41.62%
Result: Top-1: 16.62%, Top-5: 41.62%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=100
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.05%
[Alpha=1.00] Top-5 Accuracy: 41.41%
Result: Top-1: 16.05%, Top-5: 41.41%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=125
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.03%
[Alpha=1.00] Top-5 Accuracy: 40.20%
Result: Top-1: 16.03%, Top-5: 40.20%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=150
============================================================
[Alpha=1.00] Top-1 Accuracy: 15.36%
[Alpha=1.00] Top-5 Accuracy: 41.70%
Result: Top-1: 15.36%, Top-5: 41.70%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=175
============================================================
[Alpha=1.00] Top-1 Accuracy: 18.20%
[Alpha=1.00] Top-5 Accuracy: 44.35%
Result: Top-1: 18.20%, Top-5: 44.35%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=200
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.40%
[Alpha=1.00] Top-5 Accuracy: 43.33%
Result: Top-1: 16.40%, Top-5: 43.33%

============================================================
Testing: alpha=1.0, clusters=256, pca_dim=220
============================================================
[Alpha=1.00] Top-1 Accuracy: 16.49%
[Alpha=1.00] Top-5 Accuracy: 41.32%
Result: Top-1: 16.49%, Top-5: 41.32%

================================================================================
SUMMARY OF ALL RESULTS
================================================================================
Alpha    Clusters   PCA_dim    Top-1      Top-5     
--------------------------------------------------
0.10     8          1          58.63      84.69     
0.10     8          25         58.72      84.71     
0.10     8          50         58.69      84.72     
0.10     8          75         58.69      84.74     
0.10     8          100        58.71      84.74     
0.10     8          125        58.69      84.73     
0.10     8          150        58.67      84.72     
0.10     8          175        58.75      84.73     
0.10     8          200        58.73      84.74     
0.10     8          220        58.76      84.75     
0.10     16         1          58.62      84.68     
0.10     16         25         58.72      84.74     
0.10     16         50         58.77      84.77     
0.10     16         75         58.72      84.80     
0.10     16         100        58.64      84.80     
0.10     16         125        58.71      84.77     
0.10     16         150        58.68      84.77     
0.10     16         175        58.70      84.77     
0.10     16         200        58.73      84.77     
0.10     16         220        58.66      84.79     
0.10     32         1          58.65      84.64     
0.10     32         25         58.69      84.79     
0.10     32         50         58.70      84.76     
0.10     32         75         58.73      84.74     
0.10     32         100        58.77      84.79     
0.10     32         125        58.74      84.78     
0.10     32         150        58.77      84.77     
0.10     32         175        58.67      84.81     
0.10     32         200        58.78      84.78     
0.10     32         220        58.72      84.78     
0.10     64         1          58.55      84.58     
0.10     64         25         58.69      84.76     
0.10     64         50         58.67      84.80     
0.10     64         75         58.70      84.77     
0.10     64         100        58.61      84.80     
0.10     64         125        58.73      84.82     
0.10     64         150        58.75      84.74     
0.10     64         175        58.70      84.81     
0.10     64         200        58.69      84.75     
0.10     64         220        58.66      84.78     
0.10     128        1          58.37      84.55     
0.10     128        25         58.58      84.74     
0.10     128        50         58.61      84.69     
0.10     128        75         58.57      84.67     
0.10     128        100        58.52      84.75     
0.10     128        125        58.63      84.74     
0.10     128        150        58.47      84.79     
0.10     128        175        58.72      84.72     
0.10     128        200        58.57      84.74     
0.10     128        220        58.69      84.76     
0.10     256        1          58.04      84.40     
0.10     256        25         58.41      84.61     
0.10     256        50         58.17      84.54     
0.10     256        75         57.46      84.46     
0.10     256        100        58.24      84.56     
0.10     256        125        56.10      83.77     
0.10     256        150        57.80      84.49     
0.10     256        175        58.43      84.70     
0.10     256        200        58.08      84.34     
0.10     256        220        57.96      84.45     
0.20     8          1          58.18      84.45     
0.20     8          25         58.36      84.58     
0.20     8          50         58.42      84.60     
0.20     8          75         58.35      84.53     
0.20     8          100        58.36      84.54     
0.20     8          125        58.34      84.54     
0.20     8          150        58.39      84.57     
0.20     8          175        58.50      84.55     
0.20     8          200        58.51      84.60     
0.20     8          220        58.49      84.58     
0.20     16         1          58.14      84.45     
0.20     16         25         58.48      84.60     
0.20     16         50         58.51      84.57     
0.20     16         75         58.53      84.57     
0.20     16         100        58.48      84.63     
0.20     16         125        58.53      84.63     
0.20     16         150        58.47      84.55     
0.20     16         175        58.47      84.63     
0.20     16         200        58.53      84.61     
0.20     16         220        58.53      84.63     
0.20     32         1          58.16      84.42     
0.20     32         25         58.31      84.50     
0.20     32         50         58.30      84.50     
0.20     32         75         58.39      84.54     
0.20     32         100        58.45      84.54     
0.20     32         125        58.39      84.58     
0.20     32         150        58.52      84.60     
0.20     32         175        58.42      84.57     
0.20     32         200        58.41      84.60     
0.20     32         220        58.36      84.56     
0.20     64         1          57.93      84.22     
0.20     64         25         58.26      84.53     
0.20     64         50         58.34      84.54     
0.20     64         75         58.19      84.54     
0.20     64         100        58.07      84.45     
0.20     64         125        58.32      84.60     
0.20     64         150        58.40      84.46     
0.20     64         175        58.43      84.56     
0.20     64         200        58.19      84.53     
0.20     64         220        58.19      84.46     
0.20     128        1          57.40      84.07     
0.20     128        25         57.77      84.38     
0.20     128        50         57.82      84.31     
0.20     128        75         57.75      84.28     
0.20     128        100        57.81      84.38     
0.20     128        125        57.87      84.37     
0.20     128        150        57.59      84.38     
0.20     128        175        58.10      84.39     
0.20     128        200        57.69      84.43     
0.20     128        220        58.09      84.46     
0.20     256        1          56.43      83.43     
0.20     256        25         56.97      83.77     
0.20     256        50         56.08      83.66     
0.20     256        75         55.35      83.43     
0.20     256        100        56.06      83.53     
0.20     256        125        53.83      80.88     
0.20     256        150        55.32      83.10     
0.20     256        175        56.97      83.98     
0.20     256        200        56.38      83.43     
0.20     256        220        55.82      83.31     
0.30     8          1          57.07      83.94     
0.30     8          25         57.61      84.14     
0.30     8          50         57.61      84.17     
0.30     8          75         57.56      84.11     
0.30     8          100        57.58      84.12     
0.30     8          125        57.54      84.11     
0.30     8          150        57.62      84.18     
0.30     8          175        57.63      84.08     
0.30     8          200        57.67      84.08     
0.30     8          220        57.66      84.08     
0.30     16         1          57.07      83.91     
0.30     16         25         57.76      84.17     
0.30     16         50         57.81      84.19     
0.30     16         75         57.80      84.15     
0.30     16         100        57.86      84.21     
0.30     16         125        57.79      84.18     
0.30     16         150        57.68      84.16     
0.30     16         175        57.77      84.17     
0.30     16         200        57.88      84.21     
0.30     16         220        57.80      84.17     
0.30     32         1          56.89      83.86     
0.30     32         25         57.46      84.07     
0.30     32         50         57.45      84.09     
0.30     32         75         57.56      84.06     
0.30     32         100        57.64      84.10     
0.30     32         125        57.57      84.26     
0.30     32         150        57.69      84.28     
0.30     32         175        57.57      84.19     
0.30     32         200        57.62      84.21     
0.30     32         220        57.42      84.08     
0.30     64         1          56.45      83.54     
0.30     64         25         57.05      83.97     
0.30     64         50         57.41      84.04     
0.30     64         75         56.99      83.97     
0.30     64         100        56.95      83.89     
0.30     64         125        57.38      84.07     
0.30     64         150        57.25      84.01     
0.30     64         175        57.40      84.13     
0.30     64         200        57.03      84.00     
0.30     64         220        57.11      84.01     
0.30     128        1          55.53      83.09     
0.30     128        25         56.07      83.55     
0.30     128        50         56.34      83.56     
0.30     128        75         55.75      83.33     
0.30     128        100        56.18      83.63     
0.30     128        125        56.28      83.66     
0.30     128        150        55.55      83.43     
0.30     128        175        56.59      83.70     
0.30     128        200        55.84      83.63     
0.30     128        220        56.64      83.68     
0.30     256        1          53.71      81.80     
0.30     256        25         54.08      82.40     
0.30     256        50         52.69      81.83     
0.30     256        75         51.96      81.47     
0.30     256        100        52.60      81.58     
0.30     256        125        50.63      78.44     
0.30     256        150        51.40      80.65     
0.30     256        175        54.37      82.73     
0.30     256        200        53.58      81.75     
0.30     256        220        52.50      81.29     
0.40     8          1          55.40      82.97     
0.40     8          25         56.40      83.42     
0.40     8          50         56.41      83.41     
0.40     8          75         56.31      83.32     
0.40     8          100        56.32      83.34     
0.40     8          125        56.29      83.32     
0.40     8          150        56.24      83.39     
0.40     8          175        56.07      83.13     
0.40     8          200        56.06      83.18     
0.40     8          220        56.09      83.20     
0.40     16         1          55.26      82.87     
0.40     16         25         56.29      83.40     
0.40     16         50         56.37      83.48     
0.40     16         75         56.48      83.39     
0.40     16         100        56.47      83.40     
0.40     16         125        56.42      83.44     
0.40     16         150        56.21      83.26     
0.40     16         175        56.35      83.40     
0.40     16         200        56.55      83.45     
0.40     16         220        56.51      83.47     
0.40     32         1          54.87      82.73     
0.40     32         25         55.86      83.26     
0.40     32         50         55.86      83.20     
0.40     32         75         55.94      83.23     
0.40     32         100        55.95      83.23     
0.40     32         125        56.12      83.41     
0.40     32         150        56.21      83.39     
0.40     32         175        56.19      83.33     
0.40     32         200        56.17      83.31     
0.40     32         220        55.71      83.31     
0.40     64         1          54.04      82.30     
0.40     64         25         55.02      82.82     
0.40     64         50         55.57      83.01     
0.40     64         75         54.90      82.79     
0.40     64         100        55.04      82.95     
0.40     64         125        55.18      83.11     
0.40     64         150        55.03      83.01     
0.40     64         175        55.46      83.17     
0.40     64         200        54.95      82.92     
0.40     64         220        55.20      83.08     
0.40     128        1          52.74      81.32     
0.40     128        25         53.23      82.01     
0.40     128        50         53.62      82.21     
0.40     128        75         52.90      81.70     
0.40     128        100        53.50      82.14     
0.40     128        125        53.47      82.19     
0.40     128        150        52.37      81.62     
0.40     128        175        53.89      82.34     
0.40     128        200        52.84      82.10     
0.40     128        220        53.96      82.13     
0.40     256        1          49.95      79.25     
0.40     256        25         50.01      80.06     
0.40     256        50         48.15      78.84     
0.40     256        75         47.51      78.16     
0.40     256        100        48.05      78.30     
0.40     256        125        46.30      75.34     
0.40     256        150        46.96      77.35     
0.40     256        175        50.65      80.38     
0.40     256        200        49.06      79.19     
0.40     256        220        47.94      78.21     
0.50     8          1          52.65      81.40     
0.50     8          25         54.19      82.17     
0.50     8          50         54.25      82.22     
0.50     8          75         54.11      81.97     
0.50     8          100        54.13      81.98     
0.50     8          125        54.08      81.96     
0.50     8          150        53.77      82.14     
0.50     8          175        52.68      81.52     
0.50     8          200        52.75      81.60     
0.50     8          220        52.67      81.58     
0.50     16         1          52.46      81.10     
0.50     16         25         53.46      81.97     
0.50     16         50         53.52      82.15     
0.50     16         75         53.65      82.06     
0.50     16         100        53.59      81.89     
0.50     16         125        53.63      82.19     
0.50     16         150        53.22      81.78     
0.50     16         175        53.49      82.01     
0.50     16         200        53.67      82.11     
0.50     16         220        53.77      82.10     
0.50     32         1          52.03      80.93     
0.50     32         25         52.97      81.93     
0.50     32         50         52.56      81.55     
0.50     32         75         52.36      81.61     
0.50     32         100        52.55      81.75     
0.50     32         125        53.14      81.89     
0.50     32         150        53.21      81.95     
0.50     32         175        53.12      81.89     
0.50     32         200        53.02      81.78     
0.50     32         220        52.60      81.86     
0.50     64         1          50.82      80.22     
0.50     64         25         51.31      80.94     
0.50     64         50         52.04      81.20     
0.50     64         75         51.15      81.02     
0.50     64         100        51.50      81.06     
0.50     64         125        51.02      81.26     
0.50     64         150        51.33      81.30     
0.50     64         175        52.21      81.58     
0.50     64         200        51.05      81.14     
0.50     64         220        51.68      81.24     
0.50     128        1          48.94      78.70     
0.50     128        25         48.61      79.45     
0.50     128        50         49.29      79.83     
0.50     128        75         47.83      78.93     
0.50     128        100        48.81      79.50     
0.50     128        125        48.74      79.69     
0.50     128        150        47.40      78.54     
0.50     128        175        49.64      80.15     
0.50     128        200        48.24      79.45     
0.50     128        220        49.02      79.75     
0.50     256        1          45.26      75.56     
0.50     256        25         44.62      76.29     
0.50     256        50         42.34      74.87     
0.50     256        75         41.82      73.45     
0.50     256        100        41.68      73.86     
0.50     256        125        40.06      71.10     
0.50     256        150        41.41      73.01     
0.50     256        175        44.97      76.58     
0.50     256        200        42.88      75.31     
0.50     256        220        41.94      73.92     
0.60     8          1          48.82      78.79     
0.60     8          25         50.54      80.00     
0.60     8          50         50.59      80.05     
0.60     8          75         50.45      79.73     
0.60     8          100        50.49      79.75     
0.60     8          125        50.41      79.68     
0.60     8          150        49.68      79.92     
0.60     8          175        46.27      78.88     
0.60     8          200        46.38      78.98     
0.60     8          220        46.30      79.06     
0.60     16         1          48.52      78.42     
0.60     16         25         48.29      79.56     
0.60     16         50         48.55      79.65     
0.60     16         75         48.96      79.72     
0.60     16         100        48.53      79.41     
0.60     16         125        48.79      79.81     
0.60     16         150        47.79      79.28     
0.60     16         175        48.26      79.41     
0.60     16         200        48.80      79.87     
0.60     16         220        48.81      79.74     
0.60     32         1          47.85      77.99     
0.60     32         25         48.37      79.55     
0.60     32         50         46.71      78.69     
0.60     32         75         46.14      79.04     
0.60     32         100        46.84      79.06     
0.60     32         125        48.25      79.61     
0.60     32         150        47.66      79.27     
0.60     32         175        47.96      79.53     
0.60     32         200        47.04      79.09     
0.60     32         220        47.91      79.44     
0.60     64         1          46.60      76.82     
0.60     64         25         45.61      77.77     
0.60     64         50         46.46      78.20     
0.60     64         75         45.42      78.14     
0.60     64         100        45.97      77.96     
0.60     64         125        44.59      78.29     
0.60     64         150        45.75      78.31     
0.60     64         175        47.05      78.90     
0.60     64         200        45.15      78.10     
0.60     64         220        45.94      78.11     
0.60     128        1          44.00      74.47     
0.60     128        25         42.51      75.39     
0.60     128        50         43.18      76.11     
0.60     128        75         41.09      74.94     
0.60     128        100        41.80      75.45     
0.60     128        125        42.08      75.79     
0.60     128        150        40.81      74.54     
0.60     128        175        44.25      76.72     
0.60     128        200        42.04      75.50     
0.60     128        220        41.62      75.95     
0.60     256        1          39.94      70.36     
0.60     256        25         38.24      71.05     
0.60     256        50         35.69      69.76     
0.60     256        75         34.73      67.63     
0.60     256        100        34.74      68.07     
0.60     256        125        33.08      65.61     
0.60     256        150        35.28      67.96     
0.60     256        175        38.28      71.43     
0.60     256        200        36.23      69.98     
0.60     256        220        35.69      68.65     
0.70     8          1          43.74      74.55     
0.70     8          25         45.16      76.27     
0.70     8          50         45.28      76.34     
0.70     8          75         44.85      75.97     
0.70     8          100        44.98      76.00     
0.70     8          125        44.82      75.93     
0.70     8          150        44.02      76.17     
0.70     8          175        38.18      74.44     
0.70     8          200        38.31      74.69     
0.70     8          220        38.37      74.83     
0.70     16         1          43.10      74.10     
0.70     16         25         40.70      75.55     
0.70     16         50         41.01      75.62     
0.70     16         75         41.95      75.85     
0.70     16         100        39.93      75.46     
0.70     16         125        41.16      75.82     
0.70     16         150        40.31      75.26     
0.70     16         175        40.21      75.59     
0.70     16         200        41.77      76.02     
0.70     16         220        41.53      76.07     
0.70     32         1          42.44      73.49     
0.70     32         25         41.77      75.62     
0.70     32         50         38.68      74.17     
0.70     32         75         37.89      74.64     
0.70     32         100        38.96      74.74     
0.70     32         125        40.95      75.86     
0.70     32         150        39.67      75.21     
0.70     32         175        41.18      75.68     
0.70     32         200        38.66      74.98     
0.70     32         220        41.30      75.70     
0.70     64         1          40.98      71.76     
0.70     64         25         38.17      73.00     
0.70     64         50         38.94      73.49     
0.70     64         75         38.38      73.82     
0.70     64         100        38.66      73.06     
0.70     64         125        37.02      73.80     
0.70     64         150        38.84      73.74     
0.70     64         175        40.12      74.52     
0.70     64         200        38.39      73.58     
0.70     64         220        38.26      73.24     
0.70     128        1          38.43      68.68     
0.70     128        25         35.12      69.83     
0.70     128        50         36.50      71.11     
0.70     128        75         34.18      69.26     
0.70     128        100        33.85      69.89     
0.70     128        125        34.85      70.40     
0.70     128        150        33.79      68.83     
0.70     128        175        38.14      72.00     
0.70     128        200        35.86      70.23     
0.70     128        220        34.08      70.42     
0.70     256        1          34.25      63.59     
0.70     256        25         31.83      64.83     
0.70     256        50         29.58      63.07     
0.70     256        75         28.81      61.36     
0.70     256        100        28.84      61.34     
0.70     256        125        27.06      59.22     
0.70     256        150        29.21      61.93     
0.70     256        175        31.69      64.93     
0.70     256        200        30.07      63.66     
0.70     256        220        29.75      62.05     
0.80     8          1          37.24      68.27     
0.80     8          25         37.62      70.44     
0.80     8          50         37.43      70.52     
0.80     8          75         36.66      69.76     
0.80     8          100        36.85      69.87     
0.80     8          125        36.59      69.75     
0.80     8          150        36.63      70.20     
0.80     8          175        28.93      67.37     
0.80     8          200        29.85      68.19     
0.80     8          220        30.14      68.38     
0.80     16         1          36.29      67.65     
0.80     16         25         31.21      69.33     
0.80     16         50         31.75      69.18     
0.80     16         75         33.28      69.83     
0.80     16         100        29.85      69.24     
0.80     16         125        32.26      69.90     
0.80     16         150        31.46      68.82     
0.80     16         175        30.69      69.34     
0.80     16         200        33.39      70.06     
0.80     16         220        32.69      70.38     
0.80     32         1          35.51      66.67     
0.80     32         25         33.65      69.64     
0.80     32         50         30.74      67.57     
0.80     32         75         29.74      68.15     
0.80     32         100        30.00      67.82     
0.80     32         125        32.44      69.86     
0.80     32         150        30.76      68.76     
0.80     32         175        32.90      70.05     
0.80     32         200        29.46      68.52     
0.80     32         220        33.68      69.93     
0.80     64         1          34.28      64.61     
0.80     64         25         30.14      66.39     
0.80     64         50         30.96      66.98     
0.80     64         75         31.32      67.84     
0.80     64         100        31.06      66.16     
0.80     64         125        29.61      67.60     
0.80     64         150        31.48      67.29     
0.80     64         175        32.44      68.43     
0.80     64         200        30.79      67.31     
0.80     64         220        30.32      66.13     
0.80     128        1          31.87      61.19     
0.80     128        25         28.18      63.00     
0.80     128        50         30.29      64.47     
0.80     128        75         28.03      62.28     
0.80     128        100        26.59      62.83     
0.80     128        125        28.04      63.78     
0.80     128        150        27.77      61.88     
0.80     128        175        31.93      65.73     
0.80     128        200        30.01      63.62     
0.80     128        220        27.37      63.21     
0.80     256        1          28.55      55.84     
0.80     256        25         25.95      58.02     
0.80     256        50         24.43      55.69     
0.80     256        75         23.73      54.36     
0.80     256        100        23.59      54.44     
0.80     256        125        22.51      52.52     
0.80     256        150        23.67      55.21     
0.80     256        175        26.52      57.97     
0.80     256        200        24.55      56.85     
0.80     256        220        24.42      55.25     
0.90     8          1          29.34      59.52     
0.90     8          25         28.32      61.79     
0.90     8          50         28.19      61.76     
0.90     8          75         26.89      60.82     
0.90     8          100        27.21      60.90     
0.90     8          125        26.84      60.81     
0.90     8          150        27.69      61.63     
0.90     8          175        19.08      57.38     
0.90     8          200        21.06      59.00     
0.90     8          220        21.42      59.23     
0.90     16         1          28.14      58.74     
0.90     16         25         22.30      60.55     
0.90     16         50         22.67      60.22     
0.90     16         75         24.47      61.71     
0.90     16         100        21.42      60.20     
0.90     16         125        23.73      61.36     
0.90     16         150        23.01      60.23     
0.90     16         175        22.02      60.84     
0.90     16         200        24.92      61.72     
0.90     16         220        23.82      62.12     
0.90     32         1          27.77      57.51     
0.90     32         25         24.89      61.39     
0.90     32         50         23.12      58.63     
0.90     32         75         22.67      59.67     
0.90     32         100        21.89      58.61     
0.90     32         125        24.25      61.46     
0.90     32         150        22.84      60.33     
0.90     32         175        25.19      62.35     
0.90     32         200        21.48      59.57     
0.90     32         220        26.00      61.98     
0.90     64         1          27.02      55.54     
0.90     64         25         22.94      57.64     
0.90     64         50         23.70      58.42     
0.90     64         75         25.12      60.31     
0.90     64         100        23.81      57.61     
0.90     64         125        23.22      59.35     
0.90     64         150        24.63      59.43     
0.90     64         175        25.35      60.64     
0.90     64         200        24.26      59.22     
0.90     64         220        23.04      57.23     
0.90     128        1          25.38      52.57     
0.90     128        25         22.53      55.40     
0.90     128        50         24.99      57.09     
0.90     128        75         22.86      54.52     
0.90     128        100        20.67      54.82     
0.90     128        125        22.34      55.88     
0.90     128        150        22.51      54.28     
0.90     128        175        26.46      58.58     
0.90     128        200        24.48      56.28     
0.90     128        220        21.73      55.09     
0.90     256        1          23.07      48.02     
0.90     256        25         21.10      51.13     
0.90     256        50         20.32      48.65     
0.90     256        75         19.79      47.71     
0.90     256        100        19.45      47.64     
0.90     256        125        18.80      45.95     
0.90     256        150        18.99      48.34     
0.90     256        175        21.93      50.76     
0.90     256        200        20.11      49.75     
0.90     256        220        19.93      47.96     
1.00     8          1          21.24      48.78     
1.00     8          25         19.53      50.69     
1.00     8          50         19.48      50.80     
1.00     8          75         17.78      49.23     
1.00     8          100        18.01      49.33     
1.00     8          125        17.74      49.15     
1.00     8          150        19.31      50.39     
1.00     8          175        10.87      45.50     
1.00     8          200        13.12      47.84     
1.00     8          220        13.48      48.01     
1.00     16         1          20.24      48.02     
1.00     16         25         15.70      49.53     
1.00     16         50         15.64      49.20     
1.00     16         75         17.33      51.66     
1.00     16         100        15.10      49.34     
1.00     16         125        16.75      50.70     
1.00     16         150        16.01      49.53     
1.00     16         175        15.60      50.14     
1.00     16         200        17.86      51.14     
1.00     16         220        16.69      51.69     
1.00     32         1          20.30      47.41     
1.00     32         25         17.98      51.59     
1.00     32         50         16.79      48.75     
1.00     32         75         17.19      50.12     
1.00     32         100        15.40      48.06     
1.00     32         125        17.66      51.47     
1.00     32         150        16.40      50.12     
1.00     32         175        18.79      52.85     
1.00     32         200        15.15      49.37     
1.00     32         220        19.50      52.62     
1.00     64         1          20.25      46.06     
1.00     64         25         17.04      48.16     
1.00     64         50         17.64      49.06     
1.00     64         75         20.06      51.65     
1.00     64         100        18.05      48.26     
1.00     64         125        17.91      50.47     
1.00     64         150        19.20      51.03     
1.00     64         175        19.39      52.34     
1.00     64         200        18.62      50.27     
1.00     64         220        16.94      47.61     
1.00     128        1          19.63      43.44     
1.00     128        25         18.07      47.54     
1.00     128        50         20.49      49.25     
1.00     128        75         18.51      46.84     
1.00     128        100        16.47      46.69     
1.00     128        125        17.75      48.14     
1.00     128        150        18.34      46.64     
1.00     128        175        21.81      51.42     
1.00     128        200        20.02      48.90     
1.00     128        220        17.17      47.10     
1.00     256        1          18.44      40.40     
1.00     256        25         17.18      44.78     
1.00     256        50         17.06      42.55     
1.00     256        75         16.62      41.62     
1.00     256        100        16.05      41.41     
1.00     256        125        16.03      40.20     
1.00     256        150        15.36      41.70     
1.00     256        175        18.20      44.35     
1.00     256        200        16.40      43.33     
1.00     256        220        16.49      41.32     

BEST RESULT:
  Alpha: 0.1
  Clusters: 32
  PCA_dim: 200
  Top-1 Accuracy: 58.78%
  Top-5 Accuracy: 84.78%
2025-09-15 11:28:30,408 - INFO - Experiment for seed 1001 completed in 75639.48 seconds
2025-09-15 11:28:30,412 - INFO - SUCCESS: Experiment for seed 1001 completed successfully
2025-09-15 11:28:30,414 - INFO - Looking for results in: ./checkpoint/quant_result/20250915_1125
2025-09-15 11:28:30,415 - INFO - Parsed 0 reconstructed results from log file for seed 1001
2025-09-15 11:28:30,415 - INFO - Parsed 0 baseline results from log file for seed 1001
2025-09-15 11:28:30,415 - INFO - Seed 1001 completed successfully
2025-09-15 11:28:30,415 - INFO - Sleeping for 0.5 seconds before next seed...
2025-09-15 11:28:30,915 - INFO - 
============================================================
2025-09-15 11:28:30,915 - INFO - Running experiment 2/3 for seed 1002
2025-09-15 11:28:30,915 - INFO - ============================================================
2025-09-15 11:28:30,916 - INFO - Running experiment for seed 1002
2025-09-15 11:28:30,916 - INFO - Command: /home/alz07xz/project/PD-Quant/pd_quant/bin/python ../test_quant.py --model vit_small --w_bit 2 --a_bit 6 --seed 1002 --config ../configs/4bit/qdrop_baseline.py --dataset /home/alz07xz/imagenet --calib-size 1000 --calib-batch-size 32 --val-batch-size 500 --num-workers 8 --device cuda --alpha-list 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 --num-clusters-list 8 16 32 64 128 256 --pca-dim-list 1 25 50 75 100 125 150 175 200 220 --calibrate --optimize
2025-09-15 11:28:30,916 - INFO - Working directory: /home/alz07xz/project/APHQ_CAT/qdrop
2025-09-15 11:28:48 - start the process.
Namespace(model='vit_small', config='../configs/4bit/qdrop_baseline.py', dataset='/home/alz07xz/imagenet', val_batch_size=500, num_workers=8, device='cuda', reconstruct_mlp=False, load_reconstruct_checkpoint=False, test_reconstruct_checkpoint=False, calibrate=True, load_calibrate_checkpoint=None, test_calibrate_checkpoint=False, optimize=True, load_optimize_checkpoint='', test_optimize_checkpoint=False, print_freq=10, seed=1002, alpha_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], num_clusters_list=[8, 16, 32, 64, 128, 256], pca_dim_list=[1, 25, 50, 75, 100, 125, 150, 175, 200, 220], w_bit=2, a_bit=6, calib_size=1000, calib_batch_size=32)
deefe /home/alz07xz/project/APHQ_CAT/configs/4bit
Successfully imported Config class!
optim_size: 1024
calib_size: 1000
optim_batch_size: 32
calib_batch_size: 32
w_bit: 2
a_bit: 6
qconv_a_bit: 8
qhead_a_bit: 4
calib_metric: mse
matmul_head_channel_wise: True
token_channel_wise: False
eq_n: 128
search_round: 3
keep_gpu: True
optim_metric: mse
use_mean_hessian: False
temp: 20
recon_metric: hessian_perturb
pct: 0.99
optim_mode: qdrop
drop_prob: 0.5
reconstruct_mlp: False
Building model ...
No checkpoint found at './checkpoint/vit_raw/vit_small_patch16_224.bin'
Loading pretrained weights from Hugging Face hub (timm/vit_small_patch16_224.augreg_in21k_ft_in1k)
[timm/vit_small_patch16_224.augreg_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Building validation dataloader ...
Validating on test set on fp ...
Test: [0/100]	Time 17.033 (17.033)	Loss 0.3666 (0.3666)	Prec@1 89.400 (89.400)	Prec@5 98.800 (98.800)
Test: [10/100]	Time 0.304 (3.063)	Loss 0.4641 (0.4857)	Prec@1 87.400 (86.218)	Prec@5 97.200 (97.618)
Test: [20/100]	Time 0.301 (2.146)	Loss 0.6792 (0.5288)	Prec@1 79.200 (85.114)	Prec@5 96.800 (97.457)
Test: [30/100]	Time 0.306 (1.563)	Loss 0.4284 (0.5582)	Prec@1 87.000 (84.335)	Prec@5 99.200 (97.419)
Test: [40/100]	Time 0.311 (1.271)	Loss 0.7823 (0.5542)	Prec@1 79.000 (84.546)	Prec@5 94.800 (97.312)
Test: [50/100]	Time 0.314 (1.148)	Loss 1.0516 (0.6089)	Prec@1 72.400 (82.988)	Prec@5 92.600 (96.859)
Test: [60/100]	Time 0.313 (1.021)	Loss 0.6787 (0.6213)	Prec@1 84.600 (82.859)	Prec@5 94.600 (96.659)
Test: [70/100]	Time 0.313 (0.924)	Loss 0.7787 (0.6469)	Prec@1 80.800 (82.141)	Prec@5 96.200 (96.434)
Test: [80/100]	Time 0.431 (0.855)	Loss 0.6024 (0.6574)	Prec@1 83.000 (81.943)	Prec@5 96.600 (96.244)
Test: [90/100]	Time 0.303 (0.797)	Loss 1.0445 (0.6776)	Prec@1 70.600 (81.284)	Prec@5 92.800 (96.066)
 * Prec@1 81.386 Prec@5 96.132 Loss 0.671 Time 75.376
Wraping quantiztion modules (reparam: False, recon: False) ...
2025-09-15 11:31:04 - start mse guided calibration
  0%|          | 0/74 [00:00<?, ?it/s]calibrating patch_embed.proj:   0%|          | 0/74 [00:00<?, ?it/s]calibrating patch_embed.proj:   1%|▏         | 1/74 [00:06<07:22,  6.06s/it]calibrating blocks.0.attn.qkv:   1%|▏         | 1/74 [00:06<07:22,  6.06s/it]calibrating blocks.0.attn.qkv:   3%|▎         | 2/74 [00:27<18:23, 15.32s/it]calibrating blocks.0.attn.proj:   3%|▎         | 2/74 [00:27<18:23, 15.32s/it]calibrating blocks.0.attn.proj:   4%|▍         | 3/74 [00:37<14:54, 12.59s/it]calibrating blocks.0.attn.matmul1:   4%|▍         | 3/74 [00:37<14:54, 12.59s/it]calibrating blocks.0.attn.matmul1:   5%|▌         | 4/74 [01:12<25:18, 21.69s/it]calibrating blocks.0.attn.matmul2:   5%|▌         | 4/74 [01:12<25:18, 21.69s/it]calibrating blocks.0.attn.matmul2:   7%|▋         | 5/74 [01:41<27:47, 24.16s/it]calibrating blocks.0.mlp.fc1:   7%|▋         | 5/74 [01:41<27:47, 24.16s/it]     calibrating blocks.0.mlp.fc1:   8%|▊         | 6/74 [02:11<29:40, 26.18s/it]calibrating blocks.0.mlp.fc2:   8%|▊         | 6/74 [02:11<29:40, 26.18s/it]calibrating blocks.0.mlp.fc2:   9%|▉         | 7/74 [02:42<30:52, 27.65s/it]calibrating blocks.1.attn.qkv:   9%|▉         | 7/74 [02:42<30:52, 27.65s/it]calibrating blocks.1.attn.qkv:  11%|█         | 8/74 [03:04<28:32, 25.94s/it]calibrating blocks.1.attn.proj:  11%|█         | 8/74 [03:04<28:32, 25.94s/it]calibrating blocks.1.attn.proj:  12%|█▏        | 9/74 [03:13<22:30, 20.78s/it]calibrating blocks.1.attn.matmul1:  12%|█▏        | 9/74 [03:13<22:30, 20.78s/it]calibrating blocks.1.attn.matmul1:  14%|█▎        | 10/74 [03:49<27:06, 25.42s/it]calibrating blocks.1.attn.matmul2:  14%|█▎        | 10/74 [03:49<27:06, 25.42s/it]calibrating blocks.1.attn.matmul2:  15%|█▍        | 11/74 [04:18<27:44, 26.42s/it]calibrating blocks.1.mlp.fc1:  15%|█▍        | 11/74 [04:18<27:44, 26.42s/it]     calibrating blocks.1.mlp.fc1:  16%|█▌        | 12/74 [04:48<28:31, 27.60s/it]calibrating blocks.1.mlp.fc2:  16%|█▌        | 12/74 [04:48<28:31, 27.60s/it]calibrating blocks.1.mlp.fc2:  18%|█▊        | 13/74 [05:19<28:58, 28.50s/it]calibrating blocks.2.attn.qkv:  18%|█▊        | 13/74 [05:19<28:58, 28.50s/it]calibrating blocks.2.attn.qkv:  19%|█▉        | 14/74 [05:41<26:40, 26.68s/it]calibrating blocks.2.attn.proj:  19%|█▉        | 14/74 [05:41<26:40, 26.68s/it]calibrating blocks.2.attn.proj:  20%|██        | 15/74 [05:51<21:11, 21.55s/it]calibrating blocks.2.attn.matmul1:  20%|██        | 15/74 [05:51<21:11, 21.55s/it]calibrating blocks.2.attn.matmul1:  22%|██▏       | 16/74 [06:27<24:59, 25.85s/it]calibrating blocks.2.attn.matmul2:  22%|██▏       | 16/74 [06:27<24:59, 25.85s/it]calibrating blocks.2.attn.matmul2:  23%|██▎       | 17/74 [06:55<25:22, 26.71s/it]calibrating blocks.2.mlp.fc1:  23%|██▎       | 17/74 [06:55<25:22, 26.71s/it]     calibrating blocks.2.mlp.fc1:  24%|██▍       | 18/74 [07:26<25:54, 27.77s/it]calibrating blocks.2.mlp.fc2:  24%|██▍       | 18/74 [07:26<25:54, 27.77s/it]calibrating blocks.2.mlp.fc2:  26%|██▌       | 19/74 [07:56<26:15, 28.64s/it]calibrating blocks.3.attn.qkv:  26%|██▌       | 19/74 [07:56<26:15, 28.64s/it]calibrating blocks.3.attn.qkv:  27%|██▋       | 20/74 [08:19<24:07, 26.80s/it]calibrating blocks.3.attn.proj:  27%|██▋       | 20/74 [08:19<24:07, 26.80s/it]calibrating blocks.3.attn.proj:  28%|██▊       | 21/74 [08:29<19:08, 21.68s/it]calibrating blocks.3.attn.matmul1:  28%|██▊       | 21/74 [08:29<19:08, 21.68s/it]calibrating blocks.3.attn.matmul1:  30%|██▉       | 22/74 [09:04<22:29, 25.95s/it]calibrating blocks.3.attn.matmul2:  30%|██▉       | 22/74 [09:04<22:29, 25.95s/it]calibrating blocks.3.attn.matmul2:  31%|███       | 23/74 [09:33<22:46, 26.79s/it]calibrating blocks.3.mlp.fc1:  31%|███       | 23/74 [09:33<22:46, 26.79s/it]     calibrating blocks.3.mlp.fc1:  32%|███▏      | 24/74 [10:03<23:09, 27.79s/it]calibrating blocks.3.mlp.fc2:  32%|███▏      | 24/74 [10:03<23:09, 27.79s/it]calibrating blocks.3.mlp.fc2:  34%|███▍      | 25/74 [10:34<23:21, 28.61s/it]calibrating blocks.4.attn.qkv:  34%|███▍      | 25/74 [10:34<23:21, 28.61s/it]calibrating blocks.4.attn.qkv:  35%|███▌      | 26/74 [10:56<21:23, 26.75s/it]calibrating blocks.4.attn.proj:  35%|███▌      | 26/74 [10:56<21:23, 26.75s/it]calibrating blocks.4.attn.proj:  36%|███▋      | 27/74 [11:06<16:58, 21.67s/it]calibrating blocks.4.attn.matmul1:  36%|███▋      | 27/74 [11:06<16:58, 21.67s/it]calibrating blocks.4.attn.matmul1:  38%|███▊      | 28/74 [11:42<19:52, 25.92s/it]calibrating blocks.4.attn.matmul2:  38%|███▊      | 28/74 [11:42<19:52, 25.92s/it]calibrating blocks.4.attn.matmul2:  39%|███▉      | 29/74 [12:11<20:03, 26.74s/it]calibrating blocks.4.mlp.fc1:  39%|███▉      | 29/74 [12:11<20:03, 26.74s/it]     calibrating blocks.4.mlp.fc1:  41%|████      | 30/74 [12:41<20:26, 27.88s/it]calibrating blocks.4.mlp.fc2:  41%|████      | 30/74 [12:41<20:26, 27.88s/it]calibrating blocks.4.mlp.fc2:  42%|████▏     | 31/74 [13:12<20:40, 28.84s/it]calibrating blocks.5.attn.qkv:  42%|████▏     | 31/74 [13:12<20:40, 28.84s/it]calibrating blocks.5.attn.qkv:  43%|████▎     | 32/74 [13:35<18:58, 27.11s/it]calibrating blocks.5.attn.proj:  43%|████▎     | 32/74 [13:35<18:58, 27.11s/it]calibrating blocks.5.attn.proj:  45%|████▍     | 33/74 [13:45<14:58, 21.92s/it]calibrating blocks.5.attn.matmul1:  45%|████▍     | 33/74 [13:45<14:58, 21.92s/it]calibrating blocks.5.attn.matmul1:  46%|████▌     | 34/74 [14:22<17:34, 26.36s/it]calibrating blocks.5.attn.matmul2:  46%|████▌     | 34/74 [14:22<17:34, 26.36s/it]calibrating blocks.5.attn.matmul2:  47%|████▋     | 35/74 [14:51<17:40, 27.20s/it]calibrating blocks.5.mlp.fc1:  47%|████▋     | 35/74 [14:51<17:40, 27.20s/it]     calibrating blocks.5.mlp.fc1:  49%|████▊     | 36/74 [15:21<17:48, 28.11s/it]calibrating blocks.5.mlp.fc2:  49%|████▊     | 36/74 [15:21<17:48, 28.11s/it]calibrating blocks.5.mlp.fc2:  50%|█████     | 37/74 [15:52<17:45, 28.79s/it]calibrating blocks.6.attn.qkv:  50%|█████     | 37/74 [15:52<17:45, 28.79s/it]calibrating blocks.6.attn.qkv:  51%|█████▏    | 38/74 [16:14<16:07, 26.87s/it]calibrating blocks.6.attn.proj:  51%|█████▏    | 38/74 [16:14<16:07, 26.87s/it]calibrating blocks.6.attn.proj:  53%|█████▎    | 39/74 [16:24<12:40, 21.73s/it]calibrating blocks.6.attn.matmul1:  53%|█████▎    | 39/74 [16:24<12:40, 21.73s/it]calibrating blocks.6.attn.matmul1:  54%|█████▍    | 40/74 [17:00<14:43, 25.98s/it]calibrating blocks.6.attn.matmul2:  54%|█████▍    | 40/74 [17:00<14:43, 25.98s/it]calibrating blocks.6.attn.matmul2:  55%|█████▌    | 41/74 [17:28<14:44, 26.81s/it]calibrating blocks.6.mlp.fc1:  55%|█████▌    | 41/74 [17:28<14:44, 26.81s/it]     calibrating blocks.6.mlp.fc1:  57%|█████▋    | 42/74 [17:59<14:55, 27.98s/it]calibrating blocks.6.mlp.fc2:  57%|█████▋    | 42/74 [17:59<14:55, 27.98s/it]calibrating blocks.6.mlp.fc2:  58%|█████▊    | 43/74 [18:30<14:51, 28.77s/it]calibrating blocks.7.attn.qkv:  58%|█████▊    | 43/74 [18:30<14:51, 28.77s/it]calibrating blocks.7.attn.qkv:  59%|█████▉    | 44/74 [18:52<13:26, 26.88s/it]calibrating blocks.7.attn.proj:  59%|█████▉    | 44/74 [18:52<13:26, 26.88s/it]calibrating blocks.7.attn.proj:  61%|██████    | 45/74 [19:02<10:29, 21.71s/it]calibrating blocks.7.attn.matmul1:  61%|██████    | 45/74 [19:02<10:29, 21.71s/it]calibrating blocks.7.attn.matmul1:  62%|██████▏   | 46/74 [19:38<12:07, 25.98s/it]calibrating blocks.7.attn.matmul2:  62%|██████▏   | 46/74 [19:38<12:07, 25.98s/it]calibrating blocks.7.attn.matmul2:  64%|██████▎   | 47/74 [20:06<12:02, 26.77s/it]calibrating blocks.7.mlp.fc1:  64%|██████▎   | 47/74 [20:06<12:02, 26.77s/it]     calibrating blocks.7.mlp.fc1:  65%|██████▍   | 48/74 [20:37<12:04, 27.85s/it]calibrating blocks.7.mlp.fc2:  65%|██████▍   | 48/74 [20:37<12:04, 27.85s/it]calibrating blocks.7.mlp.fc2:  66%|██████▌   | 49/74 [21:07<11:56, 28.66s/it]calibrating blocks.8.attn.qkv:  66%|██████▌   | 49/74 [21:07<11:56, 28.66s/it]calibrating blocks.8.attn.qkv:  68%|██████▊   | 50/74 [21:30<10:43, 26.82s/it]calibrating blocks.8.attn.proj:  68%|██████▊   | 50/74 [21:30<10:43, 26.82s/it]calibrating blocks.8.attn.proj:  69%|██████▉   | 51/74 [21:39<08:17, 21.64s/it]calibrating blocks.8.attn.matmul1:  69%|██████▉   | 51/74 [21:39<08:17, 21.64s/it]calibrating blocks.8.attn.matmul1:  70%|███████   | 52/74 [22:15<09:30, 25.91s/it]calibrating blocks.8.attn.matmul2:  70%|███████   | 52/74 [22:15<09:30, 25.91s/it]calibrating blocks.8.attn.matmul2:  72%|███████▏  | 53/74 [22:44<09:23, 26.82s/it]calibrating blocks.8.mlp.fc1:  72%|███████▏  | 53/74 [22:44<09:23, 26.82s/it]     calibrating blocks.8.mlp.fc1:  73%|███████▎  | 54/74 [23:14<09:16, 27.81s/it]calibrating blocks.8.mlp.fc2:  73%|███████▎  | 54/74 [23:14<09:16, 27.81s/it]calibrating blocks.8.mlp.fc2:  74%|███████▍  | 55/74 [23:45<09:04, 28.67s/it]calibrating blocks.9.attn.qkv:  74%|███████▍  | 55/74 [23:45<09:04, 28.67s/it]calibrating blocks.9.attn.qkv:  76%|███████▌  | 56/74 [24:07<08:02, 26.80s/it]calibrating blocks.9.attn.proj:  76%|███████▌  | 56/74 [24:07<08:02, 26.80s/it]calibrating blocks.9.attn.proj:  77%|███████▋  | 57/74 [24:17<06:07, 21.64s/it]calibrating blocks.9.attn.matmul1:  77%|███████▋  | 57/74 [24:17<06:07, 21.64s/it]calibrating blocks.9.attn.matmul1:  78%|███████▊  | 58/74 [24:53<06:53, 25.86s/it]calibrating blocks.9.attn.matmul2:  78%|███████▊  | 58/74 [24:53<06:53, 25.86s/it]calibrating blocks.9.attn.matmul2:  80%|███████▉  | 59/74 [25:21<06:40, 26.69s/it]calibrating blocks.9.mlp.fc1:  80%|███████▉  | 59/74 [25:21<06:40, 26.69s/it]     calibrating blocks.9.mlp.fc1:  81%|████████  | 60/74 [25:52<06:29, 27.82s/it]calibrating blocks.9.mlp.fc2:  81%|████████  | 60/74 [25:52<06:29, 27.82s/it]calibrating blocks.9.mlp.fc2:  82%|████████▏ | 61/74 [26:23<06:13, 28.70s/it]calibrating blocks.10.attn.qkv:  82%|████████▏ | 61/74 [26:23<06:13, 28.70s/it]calibrating blocks.10.attn.qkv:  84%|████████▍ | 62/74 [26:45<05:22, 26.89s/it]calibrating blocks.10.attn.proj:  84%|████████▍ | 62/74 [26:45<05:22, 26.89s/it]calibrating blocks.10.attn.proj:  85%|████████▌ | 63/74 [26:55<03:58, 21.72s/it]calibrating blocks.10.attn.matmul1:  85%|████████▌ | 63/74 [26:55<03:58, 21.72s/it]calibrating blocks.10.attn.matmul1:  86%|████████▋ | 64/74 [27:31<04:19, 25.98s/it]calibrating blocks.10.attn.matmul2:  86%|████████▋ | 64/74 [27:31<04:19, 25.98s/it]calibrating blocks.10.attn.matmul2:  88%|████████▊ | 65/74 [28:00<04:02, 26.89s/it]calibrating blocks.10.mlp.fc1:  88%|████████▊ | 65/74 [28:00<04:02, 26.89s/it]     calibrating blocks.10.mlp.fc1:  89%|████████▉ | 66/74 [28:31<03:44, 28.07s/it]calibrating blocks.10.mlp.fc2:  89%|████████▉ | 66/74 [28:31<03:44, 28.07s/it]calibrating blocks.10.mlp.fc2:  91%|█████████ | 67/74 [29:01<03:22, 28.91s/it]calibrating blocks.11.attn.qkv:  91%|█████████ | 67/74 [29:01<03:22, 28.91s/it]calibrating blocks.11.attn.qkv:  92%|█████████▏| 68/74 [29:24<02:42, 27.09s/it]calibrating blocks.11.attn.proj:  92%|█████████▏| 68/74 [29:24<02:42, 27.09s/it]calibrating blocks.11.attn.proj:  93%|█████████▎| 69/74 [29:34<01:49, 21.89s/it]calibrating blocks.11.attn.matmul1:  93%|█████████▎| 69/74 [29:34<01:49, 21.89s/it]calibrating blocks.11.attn.matmul1:  95%|█████████▍| 70/74 [30:10<01:44, 26.24s/it]calibrating blocks.11.attn.matmul2:  95%|█████████▍| 70/74 [30:10<01:44, 26.24s/it]calibrating blocks.11.attn.matmul2:  96%|█████████▌| 71/74 [30:40<01:21, 27.12s/it]calibrating blocks.11.mlp.fc1:  96%|█████████▌| 71/74 [30:40<01:21, 27.12s/it]     calibrating blocks.11.mlp.fc1:  97%|█████████▋| 72/74 [31:11<00:56, 28.30s/it]calibrating blocks.11.mlp.fc2:  97%|█████████▋| 72/74 [31:11<00:56, 28.30s/it]calibrating blocks.11.mlp.fc2:  99%|█████████▊| 73/74 [31:42<00:29, 29.10s/it]calibrating head:  99%|█████████▊| 73/74 [31:42<00:29, 29.10s/it]             calibrating head: 100%|██████████| 74/74 [31:43<00:00, 20.90s/it]calibrating head: 100%|██████████| 74/74 [31:43<00:00, 25.73s/it]
2025-09-15 12:03:10 - mse guided calibration finished.
Saving checkpoint to ./checkpoint/quant_result/20250915_1128/vit_small_w2_a6_calibsize_1000_mse.pth
Validating after calibration ...
Test: [0/100]	Time 3.916 (3.916)	Loss 6.1759 (6.1759)	Prec@1 1.600 (1.600)	Prec@5 5.000 (5.000)
Test: [10/100]	Time 0.788 (1.077)	Loss 6.5331 (6.4902)	Prec@1 0.400 (0.782)	Prec@5 4.400 (3.309)
Test: [20/100]	Time 0.790 (0.940)	Loss 8.5195 (6.9670)	Prec@1 0.000 (0.590)	Prec@5 0.400 (2.562)
Test: [30/100]	Time 0.797 (0.893)	Loss 6.6857 (7.1966)	Prec@1 0.000 (0.426)	Prec@5 0.800 (1.987)
Test: [40/100]	Time 0.791 (0.869)	Loss 7.0971 (7.0910)	Prec@1 1.800 (0.590)	Prec@5 4.000 (2.620)
Test: [50/100]	Time 0.795 (0.855)	Loss 6.9264 (7.0722)	Prec@1 0.400 (0.573)	Prec@5 2.200 (2.702)
Test: [60/100]	Time 0.795 (0.845)	Loss 7.8692 (7.0855)	Prec@1 0.000 (0.666)	Prec@5 2.200 (2.856)
Test: [70/100]	Time 0.793 (0.838)	Loss 7.1184 (7.1110)	Prec@1 1.000 (0.735)	Prec@5 4.200 (3.028)
Test: [80/100]	Time 0.791 (0.833)	Loss 6.8739 (7.0749)	Prec@1 1.000 (0.859)	Prec@5 2.000 (3.400)
Test: [90/100]	Time 0.796 (0.828)	Loss 6.1578 (7.0605)	Prec@1 1.800 (0.873)	Prec@5 9.000 (3.442)
 * Prec@1 1.176 Prec@5 4.238 Loss 6.950 Time 82.686
Building calibrator ...
2025-09-15 12:04:37 - start mse guided block reconstruction
reconstructing patch_embed ...
initializing raw input and raw output ...
adaround training for patch_embed ...
wraping quantizers in patch_embed ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.050 (rec:0.050, round:0.000)	b=0.00	count=500
Total loss:	0.034 (rec:0.034, round:0.000)	b=0.00	count=1000
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=1500
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=2000
Total loss:	0.017 (rec:0.017, round:0.000)	b=0.00	count=2500
Total loss:	0.013 (rec:0.013, round:0.000)	b=0.00	count=3000
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=3500
Total loss:	2777.039 (rec:0.008, round:2777.030)	b=20.00	count=4000
Total loss:	1458.243 (rec:0.023, round:1458.220)	b=19.44	count=4500
Total loss:	1349.799 (rec:0.025, round:1349.774)	b=18.88	count=5000
Total loss:	1280.303 (rec:0.032, round:1280.271)	b=18.31	count=5500
Total loss:	1220.261 (rec:0.017, round:1220.244)	b=17.75	count=6000
Total loss:	1162.403 (rec:0.019, round:1162.384)	b=17.19	count=6500
Total loss:	1106.390 (rec:0.025, round:1106.364)	b=16.62	count=7000
Total loss:	1049.107 (rec:0.017, round:1049.090)	b=16.06	count=7500
Total loss:	990.796 (rec:0.021, round:990.775)	b=15.50	count=8000
Total loss:	931.520 (rec:0.026, round:931.494)	b=14.94	count=8500
Total loss:	872.117 (rec:0.019, round:872.098)	b=14.38	count=9000
Total loss:	809.591 (rec:0.030, round:809.561)	b=13.81	count=9500
Total loss:	745.813 (rec:0.027, round:745.787)	b=13.25	count=10000
Total loss:	679.924 (rec:0.022, round:679.902)	b=12.69	count=10500
Total loss:	614.612 (rec:0.043, round:614.569)	b=12.12	count=11000
Total loss:	547.420 (rec:0.028, round:547.392)	b=11.56	count=11500
Total loss:	478.911 (rec:0.030, round:478.881)	b=11.00	count=12000
Total loss:	409.499 (rec:0.054, round:409.445)	b=10.44	count=12500
Total loss:	340.582 (rec:0.032, round:340.549)	b=9.88	count=13000
Total loss:	271.899 (rec:0.036, round:271.863)	b=9.31	count=13500
Total loss:	206.650 (rec:0.046, round:206.604)	b=8.75	count=14000
Total loss:	147.309 (rec:0.076, round:147.233)	b=8.19	count=14500
Total loss:	95.661 (rec:0.072, round:95.590)	b=7.62	count=15000
Total loss:	54.795 (rec:0.099, round:54.696)	b=7.06	count=15500
Total loss:	26.918 (rec:0.078, round:26.840)	b=6.50	count=16000
Total loss:	10.880 (rec:0.091, round:10.788)	b=5.94	count=16500
Total loss:	3.864 (rec:0.097, round:3.768)	b=5.38	count=17000
Total loss:	1.478 (rec:0.151, round:1.327)	b=4.81	count=17500
Total loss:	0.711 (rec:0.122, round:0.588)	b=4.25	count=18000
Total loss:	0.356 (rec:0.103, round:0.253)	b=3.69	count=18500
Total loss:	0.211 (rec:0.137, round:0.073)	b=3.12	count=19000
Total loss:	0.145 (rec:0.133, round:0.012)	b=2.56	count=19500
Total loss:	0.089 (rec:0.088, round:0.001)	b=2.00	count=20000
finished reconstructing patch_embed.
reconstructing blocks.0 ...
initializing raw input and raw output ...
initializing quanted input ...
adaround training for blocks.0 ...
wraping quantizers in blocks.0 ...
Total loss:	2.000 (rec:2.000, round:0.000)	b=0.00	count=1
Total loss:	0.589 (rec:0.589, round:0.000)	b=0.00	count=500
Total loss:	0.443 (rec:0.443, round:0.000)	b=0.00	count=1000
Total loss:	0.346 (rec:0.346, round:0.000)	b=0.00	count=1500
Total loss:	0.328 (rec:0.328, round:0.000)	b=0.00	count=2000
Total loss:	0.314 (rec:0.314, round:0.000)	b=0.00	count=2500
Total loss:	0.283 (rec:0.283, round:0.000)	b=0.00	count=3000
Total loss:	0.295 (rec:0.295, round:0.000)	b=0.00	count=3500
Total loss:	15777.438 (rec:0.268, round:15777.170)	b=20.00	count=4000
Total loss:	7655.624 (rec:0.347, round:7655.277)	b=19.44	count=4500
Total loss:	6986.449 (rec:0.347, round:6986.102)	b=18.88	count=5000
Total loss:	6545.256 (rec:0.346, round:6544.910)	b=18.31	count=5500
Total loss:	6172.415 (rec:0.366, round:6172.049)	b=17.75	count=6000
Total loss:	5832.996 (rec:0.385, round:5832.611)	b=17.19	count=6500
Total loss:	5514.094 (rec:0.364, round:5513.729)	b=16.62	count=7000
Total loss:	5206.656 (rec:0.349, round:5206.307)	b=16.06	count=7500
Total loss:	4910.916 (rec:0.358, round:4910.557)	b=15.50	count=8000
Total loss:	4620.427 (rec:0.373, round:4620.054)	b=14.94	count=8500
Total loss:	4331.391 (rec:0.366, round:4331.025)	b=14.38	count=9000
Total loss:	4046.673 (rec:0.372, round:4046.301)	b=13.81	count=9500
Total loss:	3759.062 (rec:0.386, round:3758.676)	b=13.25	count=10000
Total loss:	3474.810 (rec:0.383, round:3474.427)	b=12.69	count=10500
Total loss:	3188.259 (rec:0.448, round:3187.810)	b=12.12	count=11000
Total loss:	2900.959 (rec:0.414, round:2900.544)	b=11.56	count=11500
Total loss:	2611.112 (rec:0.468, round:2610.644)	b=11.00	count=12000
Total loss:	2317.324 (rec:0.439, round:2316.885)	b=10.44	count=12500
Total loss:	2023.733 (rec:0.470, round:2023.263)	b=9.88	count=13000
Total loss:	1727.928 (rec:0.429, round:1727.498)	b=9.31	count=13500
Total loss:	1438.170 (rec:0.561, round:1437.610)	b=8.75	count=14000
slurmstepd-jnfat06: error: *** JOB 1675191 ON jnfat06 CANCELLED AT 2025-09-15T12:09:03 ***
